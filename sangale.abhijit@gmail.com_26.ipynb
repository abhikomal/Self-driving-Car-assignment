{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Driving Car\n",
    "Problem Definition\n",
    "We are here building a minimal version of self driving car. Here, we have a front camera view. This will transfer input to the computer. Then Deep Learning algorithm in computer predicts the steering angle to avoid all sorts of collisions. Predicting steering angle can be thought of as a regression problem. We will feed images to Convolutional Neural Network and the label will be the steering angle in that image. Model will learn the steering angle from the as per the turns in the image and will finally predicts steering angle for unknown images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "# Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "# NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "# Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "# Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "# Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "# If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "# Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "# Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "# Additional Installations:\n",
    "# pip3 install h5py\n",
    "\n",
    "\n",
    "# AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "# Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "# Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "# More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "6NaVc8aA8-oU",
    "outputId": "f418662e-b623-4c3e-ee63-3be521f509d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import scipy.misc\n",
    "\n",
    "import cv2\n",
    "import warnings\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "from tensorflow.python.client import device_lib\n",
    "from scipy import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from scipy import pi\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbvoL-QBGyLh"
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\driving_dataset\"\n",
    "DATA_FILE = os.path.join(DATA_FOLDER, r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\data.txt\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2s3lKuJAG8a9",
    "outputId": "774a8807-20e9-4b59-91a0-69637391067c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45406 45406\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_FOLDER, r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\driving_dataset\\data.txt\")) as f:\n",
    "    for line in f:\n",
    "        image_name, angle = line.split()\n",
    "        \n",
    "        image_path = os.path.join(DATA_FOLDER, image_name)\n",
    "        x.append(image_path)\n",
    "        \n",
    "        angle_radians = float(angle) * (pi / 180)  #converting angle into radians\n",
    "        y.append(angle_radians)\n",
    "y = np.array(y)\n",
    "print(str(len(x))+\" \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_Test split(80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XMIsz-2fHLAE",
    "outputId": "c519a801-53ab-48a2-a472-5593522edf58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36324, 36324, 9082, 9082)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = int(len(x) * 0.8)\n",
    "\n",
    "train_x = x[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = x[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "NTzHMqhuHPA7",
    "outputId": "41913784-dcb0-4c2d-f66a-cef5ad57e583"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.hist(train_y, bins = 50, histtype = \"step\")\n",
    "plt.hist(test_y, bins = 50, histtype = \"step\")\n",
    "plt.title(\"Steering Wheel angle in train and test\")\n",
    "plt.xlabel(\"Angle\")\n",
    "plt.ylabel(\"Bin count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoMIcY8KHXRc"
   },
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) #here % len(train_x) is used to make sure that\n",
    "        #\"train_batch_pointer + i\" should not cross the number of train images. As soon as the value of \"train_batch_pointer\" is\n",
    "        #equal to number of train images then it will again start reading the train images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)]) #appending corresponding labels\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeC3ko_WHezk"
   },
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) #here % len(test_x) is used to make sure that\n",
    "        #\"test_batch_pointer + i\" should not cross the number of test images. As soon as the value of \"test_batch_pointer\" is\n",
    "        #equal to number of test images then it will again start reading the test images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) #appending corresponding labels\n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07JX9t-jHi32"
   },
   "outputs": [],
   "source": [
    "def weightVariable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def convolution(previous_input, filter_input, strides):\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uz4Tu7RAHllH"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3], name = \"Plc_1\")\n",
    "y_true = tf.placeholder(tf.float32, name = \"Plc_2\")\n",
    "\n",
    "input_image = x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9PC6cCnHrzf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-16-ca8b8458dcc4>:48: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Convolution Layers\n",
    "#First convolution layer\n",
    "W_Conv1 = weightVariable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.relu(convolution(input_image, W_Conv1, 2) + B_Conv1)\n",
    "#strides = 2\n",
    "#Output size: 31*98*24\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weightVariable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.relu(convolution(Conv1, W_Conv2, 2) + B_Conv2)\n",
    "#strides = 2\n",
    "#Output size: 14*47*36\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weightVariable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.relu(convolution(Conv2, W_Conv3, 2) + B_Conv3)\n",
    "#strides = 2\n",
    "#Output size: 5*22*48\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weightVariable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.relu(convolution(Conv3, W_Conv4, 1) + B_Conv4)\n",
    "#strides = 1\n",
    "#Output size: 3*20*64\n",
    "\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weightVariable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.relu(convolution(Conv4, W_Conv5, 1) + B_Conv5)\n",
    "#strides = 1\n",
    "#Output size: 1*18*64\n",
    "\n",
    "#Fully-Connected Dense Layers\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#First FC-Dense\n",
    "#Input = 1*18*64 = 1152\n",
    "W_FC1 = weightVariable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152]) #here, -1 indicates 1. It means that the shape of FC1_Flatten will be 1*1152\n",
    "Output_FC1 = tf.nn.relu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) #so, here shape of FC1_Flatten is 1*1152 and shape of W_FC1 will\n",
    "#be 1152*1164. Therefore, there will be a matrix multiplication of matrices: (1*1152) * (1152*1164) = (1*1164).\n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "#Second FC-Dense\n",
    "#Input = 1*1164 = 1164\n",
    "W_FC2 = weightVariable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.relu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) #so, here shape of Output_FC1_drop is 1*1164 and shape of \n",
    "#W_FC2 will be 1164*100. Therefore, there will be a matrix multiplication of matrices: (1*1164) * (1164*100) = (1*100).\n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "#Third FC-Dense\n",
    "#Input = 1*100 = 100\n",
    "W_FC3 = weightVariable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.relu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) #so, here shape of Output_FC2_drop is 1*100 and shape of \n",
    "#W_FC3 will be 100*50. Therefore, there will be a matrix multiplication of matrices: (1*100) * (100*50) = (1*50).\n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "#Fourth FC-Dense\n",
    "#Input = 1*50 = 50\n",
    "W_FC4 = weightVariable([50, 10])\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.relu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) #so, here shape of Output_FC3_drop is 1*50 and shape of \n",
    "#W_FC4 will be 50*10. Therefore, there will be a matrix multiplication of matrices: (1*50) * (50*10) = (1*10).\n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "#Final Output to one neuron with linear/identity function\n",
    "#Input = 1*10 = 10\n",
    "W_FC5 = weightVariable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "y_predicted = tf.identity(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5) #so, here shape of Output_FC4_drop is 1*10 and shape of \n",
    "#W_FC5 will be 10*1. Therefore, there will be a matrix multiplication of matrices: (1*10) * (10*1) = (1*1). Since, this is a \n",
    "#regression problem so we have applied identity fuction in the end. We can also apply \"atan\" function here. If computational\n",
    "#power is available then the model should be tested with both identity and atan functions. In the end, that function should be\n",
    "#considered which gives better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nDZ0qjVhHyDf",
    "outputId": "f40b1af1-efff-4508-cec1-80815005f591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.174876689910889, Test_Loss: 6.241774082183838 *\n",
      "Epoch: 1, Train_Loss: 6.157304763793945, Test_Loss: 6.304219722747803\n",
      "Epoch: 1, Train_Loss: 6.1491570472717285, Test_Loss: 6.162303447723389 *\n",
      "Epoch: 1, Train_Loss: 6.142899990081787, Test_Loss: 6.161448955535889 *\n",
      "Epoch: 1, Train_Loss: 6.137180805206299, Test_Loss: 6.1463704109191895 *\n",
      "Epoch: 1, Train_Loss: 6.294179439544678, Test_Loss: 6.163755416870117\n",
      "Epoch: 1, Train_Loss: 10.779863357543945, Test_Loss: 6.185640811920166\n",
      "Epoch: 1, Train_Loss: 6.162837982177734, Test_Loss: 6.146432399749756 *\n",
      "Epoch: 1, Train_Loss: 6.111608505249023, Test_Loss: 6.10966157913208 *\n",
      "Epoch: 1, Train_Loss: 6.105567455291748, Test_Loss: 6.224362850189209\n",
      "Epoch: 1, Train_Loss: 6.089389324188232, Test_Loss: 6.52421760559082\n",
      "Epoch: 1, Train_Loss: 6.081638336181641, Test_Loss: 6.212978839874268 *\n",
      "Epoch: 1, Train_Loss: 6.075056076049805, Test_Loss: 6.225851535797119\n",
      "Epoch: 1, Train_Loss: 6.066782474517822, Test_Loss: 6.079155921936035 *\n",
      "Epoch: 1, Train_Loss: 6.0595598220825195, Test_Loss: 6.069850444793701 *\n",
      "Epoch: 1, Train_Loss: 6.052711486816406, Test_Loss: 6.060239315032959 *\n",
      "Epoch: 1, Train_Loss: 6.123520374298096, Test_Loss: 6.056812763214111 *\n",
      "Epoch: 1, Train_Loss: 6.130528926849365, Test_Loss: 6.093228816986084\n",
      "Epoch: 1, Train_Loss: 6.126798629760742, Test_Loss: 11.202508926391602\n",
      "Epoch: 1, Train_Loss: 6.081093788146973, Test_Loss: 6.212203025817871 *\n",
      "Epoch: 1, Train_Loss: 6.019360065460205, Test_Loss: 6.03865385055542 *\n",
      "Epoch: 1, Train_Loss: 6.1507463455200195, Test_Loss: 6.020252227783203 *\n",
      "Epoch: 1, Train_Loss: 6.219310760498047, Test_Loss: 6.020896911621094\n",
      "Epoch: 1, Train_Loss: 6.188976764678955, Test_Loss: 6.009989261627197 *\n",
      "Epoch: 1, Train_Loss: 6.155519485473633, Test_Loss: 5.994265079498291 *\n",
      "Epoch: 1, Train_Loss: 5.987919330596924, Test_Loss: 5.98420524597168 *\n",
      "Epoch: 1, Train_Loss: 5.976434230804443, Test_Loss: 5.9773945808410645 *\n",
      "Epoch: 1, Train_Loss: 5.969861030578613, Test_Loss: 5.972903728485107 *\n",
      "Epoch: 1, Train_Loss: 5.96009635925293, Test_Loss: 5.968347549438477 *\n",
      "Epoch: 1, Train_Loss: 5.955172061920166, Test_Loss: 5.963770866394043 *\n",
      "Epoch: 1, Train_Loss: 5.946035861968994, Test_Loss: 5.953967094421387 *\n",
      "Epoch: 1, Train_Loss: 5.941624164581299, Test_Loss: 5.958980560302734\n",
      "Epoch: 1, Train_Loss: 5.936477184295654, Test_Loss: 5.943212509155273 *\n",
      "Epoch: 1, Train_Loss: 5.92549991607666, Test_Loss: 5.931892395019531 *\n",
      "Epoch: 1, Train_Loss: 5.921187877655029, Test_Loss: 5.919751167297363 *\n",
      "Epoch: 1, Train_Loss: 6.047971248626709, Test_Loss: 5.917457580566406 *\n",
      "Epoch: 1, Train_Loss: 6.048266887664795, Test_Loss: 5.907320976257324 *\n",
      "Epoch: 1, Train_Loss: 6.056729316711426, Test_Loss: 5.898688316345215 *\n",
      "Epoch: 1, Train_Loss: 5.9474334716796875, Test_Loss: 5.897494316101074 *\n",
      "Epoch: 1, Train_Loss: 6.070281982421875, Test_Loss: 5.884561538696289 *\n",
      "Epoch: 1, Train_Loss: 6.018045425415039, Test_Loss: 5.880129814147949 *\n",
      "Epoch: 1, Train_Loss: 5.9718146324157715, Test_Loss: 5.875669479370117 *\n",
      "Epoch: 1, Train_Loss: 6.019113063812256, Test_Loss: 5.866446018218994 *\n",
      "Epoch: 1, Train_Loss: 6.197386264801025, Test_Loss: 5.858479022979736 *\n",
      "Epoch: 1, Train_Loss: 5.8837666511535645, Test_Loss: 5.853524684906006 *\n",
      "Epoch: 1, Train_Loss: 5.8670549392700195, Test_Loss: 5.8443603515625 *\n",
      "Epoch: 1, Train_Loss: 8.378379821777344, Test_Loss: 5.838409423828125 *\n",
      "Epoch: 1, Train_Loss: 6.736180305480957, Test_Loss: 5.839162826538086\n",
      "Epoch: 1, Train_Loss: 5.838597297668457, Test_Loss: 5.880086898803711\n",
      "Epoch: 1, Train_Loss: 5.844153881072998, Test_Loss: 7.253284931182861\n",
      "Epoch: 1, Train_Loss: 5.843105316162109, Test_Loss: 10.07510757446289\n",
      "Epoch: 1, Train_Loss: 5.820285320281982, Test_Loss: 5.811379432678223 *\n",
      "Epoch: 1, Train_Loss: 5.795328140258789, Test_Loss: 5.792595386505127 *\n",
      "Epoch: 1, Train_Loss: 5.837055683135986, Test_Loss: 5.815999507904053\n",
      "Epoch: 1, Train_Loss: 5.956063270568848, Test_Loss: 5.8288679122924805\n",
      "Epoch: 1, Train_Loss: 5.910877704620361, Test_Loss: 5.8208441734313965 *\n",
      "Epoch: 1, Train_Loss: 5.885688304901123, Test_Loss: 5.785218715667725 *\n",
      "Epoch: 1, Train_Loss: 5.896143913269043, Test_Loss: 5.892934322357178\n",
      "Epoch: 1, Train_Loss: 5.818325519561768, Test_Loss: 5.763065338134766 *\n",
      "Epoch: 1, Train_Loss: 5.79681396484375, Test_Loss: 5.7494964599609375 *\n",
      "Epoch: 1, Train_Loss: 5.74704122543335, Test_Loss: 5.7710862159729\n",
      "Epoch: 1, Train_Loss: 5.760373115539551, Test_Loss: 5.738952159881592 *\n",
      "Epoch: 1, Train_Loss: 5.740136623382568, Test_Loss: 5.7327728271484375 *\n",
      "Epoch: 1, Train_Loss: 5.7142438888549805, Test_Loss: 5.7815327644348145\n",
      "Epoch: 1, Train_Loss: 5.718087673187256, Test_Loss: 5.838071346282959\n",
      "Epoch: 1, Train_Loss: 5.76645040512085, Test_Loss: 5.758615016937256 *\n",
      "Epoch: 1, Train_Loss: 5.76297664642334, Test_Loss: 5.818530082702637\n",
      "Epoch: 1, Train_Loss: 5.693551540374756, Test_Loss: 5.711699962615967 *\n",
      "Epoch: 1, Train_Loss: 5.678175449371338, Test_Loss: 5.709913730621338 *\n",
      "Epoch: 1, Train_Loss: 5.670650482177734, Test_Loss: 5.691335201263428 *\n",
      "Epoch: 1, Train_Loss: 5.664123058319092, Test_Loss: 5.68147087097168 *\n",
      "Epoch: 1, Train_Loss: 5.657401084899902, Test_Loss: 5.676087856292725 *\n",
      "Epoch: 1, Train_Loss: 5.65117073059082, Test_Loss: 5.668119430541992 *\n",
      "Epoch: 1, Train_Loss: 5.642518520355225, Test_Loss: 5.659268856048584 *\n",
      "Epoch: 1, Train_Loss: 5.636463642120361, Test_Loss: 5.6515607833862305 *\n",
      "Epoch: 1, Train_Loss: 5.629366874694824, Test_Loss: 5.659037113189697\n",
      "Epoch: 1, Train_Loss: 5.622234344482422, Test_Loss: 5.638840198516846 *\n",
      "Epoch: 1, Train_Loss: 5.616549015045166, Test_Loss: 5.640529155731201\n",
      "Epoch: 1, Train_Loss: 5.613500595092773, Test_Loss: 5.61167573928833 *\n",
      "Epoch: 1, Train_Loss: 5.605092525482178, Test_Loss: 5.624698638916016\n",
      "Epoch: 1, Train_Loss: 5.5970258712768555, Test_Loss: 5.6733012199401855\n",
      "Epoch: 1, Train_Loss: 5.597903728485107, Test_Loss: 5.596597194671631 *\n",
      "Epoch: 1, Train_Loss: 5.581721782684326, Test_Loss: 6.017026901245117\n",
      "Epoch: 1, Train_Loss: 5.578065872192383, Test_Loss: 6.104201793670654\n",
      "Epoch: 1, Train_Loss: 5.569924831390381, Test_Loss: 5.717055320739746 *\n",
      "Epoch: 1, Train_Loss: 5.563078880310059, Test_Loss: 5.583736419677734 *\n",
      "Epoch: 1, Train_Loss: 5.558892250061035, Test_Loss: 5.5735764503479 *\n",
      "Epoch: 1, Train_Loss: 5.550216197967529, Test_Loss: 5.586527347564697\n",
      "Epoch: 1, Train_Loss: 5.5412983894348145, Test_Loss: 5.7710394859313965\n",
      "Epoch: 1, Train_Loss: 5.537619590759277, Test_Loss: 6.935922145843506\n",
      "Epoch: 1, Train_Loss: 5.584222316741943, Test_Loss: 6.7226080894470215 *\n",
      "Epoch: 1, Train_Loss: 5.60509729385376, Test_Loss: 5.565246105194092 *\n",
      "Epoch: 1, Train_Loss: 5.542079925537109, Test_Loss: 5.57666540145874\n",
      "Epoch: 1, Train_Loss: 5.518037796020508, Test_Loss: 5.5112385749816895 *\n",
      "Epoch: 1, Train_Loss: 5.503026962280273, Test_Loss: 5.51186990737915\n",
      "Epoch: 1, Train_Loss: 5.525459289550781, Test_Loss: 5.501727104187012 *\n",
      "Epoch: 1, Train_Loss: 5.4936933517456055, Test_Loss: 5.527379512786865\n",
      "Epoch: 1, Train_Loss: 5.49580717086792, Test_Loss: 5.569560527801514\n",
      "Epoch: 1, Train_Loss: 5.531245708465576, Test_Loss: 5.490742206573486 *\n",
      "Epoch: 1, Train_Loss: 5.4881181716918945, Test_Loss: 5.48503303527832 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 5.630350112915039, Test_Loss: 5.560134410858154\n",
      "Epoch: 1, Train_Loss: 5.58499002456665, Test_Loss: 5.818879127502441\n",
      "Epoch: 1, Train_Loss: 5.525052547454834, Test_Loss: 5.62314510345459 *\n",
      "Epoch: 1, Train_Loss: 5.466196537017822, Test_Loss: 5.445960521697998 *\n",
      "Epoch: 1, Train_Loss: 5.443180084228516, Test_Loss: 5.433500289916992 *\n",
      "Epoch: 1, Train_Loss: 5.45635986328125, Test_Loss: 5.427002906799316 *\n",
      "Epoch: 1, Train_Loss: 5.424826145172119, Test_Loss: 5.420309543609619 *\n",
      "Epoch: 1, Train_Loss: 5.4233269691467285, Test_Loss: 5.41316556930542 *\n",
      "Epoch: 1, Train_Loss: 5.414039134979248, Test_Loss: 5.812416076660156\n",
      "Epoch: 1, Train_Loss: 5.411431789398193, Test_Loss: 10.648582458496094\n",
      "Epoch: 1, Train_Loss: 5.465497970581055, Test_Loss: 5.4588751792907715 *\n",
      "Epoch: 1, Train_Loss: 5.408895969390869, Test_Loss: 5.390388011932373 *\n",
      "Epoch: 1, Train_Loss: 5.483093738555908, Test_Loss: 5.378350734710693 *\n",
      "Epoch: 1, Train_Loss: 5.385751247406006, Test_Loss: 5.373290061950684 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 5.379415035247803, Test_Loss: 5.368374347686768 *\n",
      "Epoch: 1, Train_Loss: 5.479959487915039, Test_Loss: 5.359352111816406 *\n",
      "Epoch: 1, Train_Loss: 5.537711143493652, Test_Loss: 5.3594441413879395\n",
      "Epoch: 1, Train_Loss: 5.348519802093506, Test_Loss: 5.350827693939209 *\n",
      "Epoch: 1, Train_Loss: 5.361721992492676, Test_Loss: 5.344637393951416 *\n",
      "Epoch: 1, Train_Loss: 5.33598518371582, Test_Loss: 5.3378400802612305 *\n",
      "Epoch: 1, Train_Loss: 5.330655097961426, Test_Loss: 5.333582878112793 *\n",
      "Epoch: 1, Train_Loss: 5.323042869567871, Test_Loss: 5.332274913787842 *\n",
      "Epoch: 1, Train_Loss: 5.314636707305908, Test_Loss: 5.331053256988525 *\n",
      "Epoch: 1, Train_Loss: 5.335860729217529, Test_Loss: 5.316256523132324 *\n",
      "Epoch: 1, Train_Loss: 5.334444522857666, Test_Loss: 5.303079605102539 *\n",
      "Epoch: 1, Train_Loss: 5.3284173011779785, Test_Loss: 5.294083595275879 *\n",
      "Epoch: 1, Train_Loss: 5.322332382202148, Test_Loss: 5.289494037628174 *\n",
      "Epoch: 1, Train_Loss: 5.316775321960449, Test_Loss: 5.281951904296875 *\n",
      "Epoch: 1, Train_Loss: 5.2881903648376465, Test_Loss: 5.274776935577393 *\n",
      "Epoch: 1, Train_Loss: 5.272896766662598, Test_Loss: 5.269519805908203 *\n",
      "Epoch: 1, Train_Loss: 5.261841773986816, Test_Loss: 5.260241508483887 *\n",
      "Epoch: 1, Train_Loss: 5.267255783081055, Test_Loss: 5.256790637969971 *\n",
      "Epoch: 1, Train_Loss: 5.260813236236572, Test_Loss: 5.2490386962890625 *\n",
      "Epoch: 1, Train_Loss: 5.2568039894104, Test_Loss: 5.242767810821533 *\n",
      "Epoch: 1, Train_Loss: 5.237391471862793, Test_Loss: 5.235448360443115 *\n",
      "Epoch: 1, Train_Loss: 5.307236671447754, Test_Loss: 5.2297844886779785 *\n",
      "Epoch: 1, Train_Loss: 5.293359756469727, Test_Loss: 5.221254348754883 *\n",
      "Epoch: 1, Train_Loss: 5.261580467224121, Test_Loss: 5.216116905212402 *\n",
      "Epoch: 1, Train_Loss: 5.210075378417969, Test_Loss: 5.219940185546875\n",
      "Epoch: 1, Train_Loss: 5.219747066497803, Test_Loss: 5.2586493492126465\n",
      "Epoch: 1, Train_Loss: 5.198622703552246, Test_Loss: 7.781795024871826\n",
      "Epoch: 1, Train_Loss: 5.214119911193848, Test_Loss: 8.26811408996582\n",
      "Epoch: 1, Train_Loss: 5.18992280960083, Test_Loss: 5.187343597412109 *\n",
      "Epoch: 1, Train_Loss: 5.196373462677002, Test_Loss: 5.177361488342285 *\n",
      "Epoch: 1, Train_Loss: 6.544895172119141, Test_Loss: 5.204285144805908\n",
      "Epoch: 1, Train_Loss: 9.07967758178711, Test_Loss: 5.207913875579834\n",
      "Epoch: 1, Train_Loss: 5.336450099945068, Test_Loss: 5.192307472229004 *\n",
      "Epoch: 1, Train_Loss: 5.175907611846924, Test_Loss: 5.203085899353027\n",
      "Epoch: 1, Train_Loss: 5.160916805267334, Test_Loss: 5.281933784484863\n",
      "Epoch: 1, Train_Loss: 5.3489227294921875, Test_Loss: 5.137600421905518 *\n",
      "Epoch: 1, Train_Loss: 5.213079452514648, Test_Loss: 5.162933349609375\n",
      "Epoch: 1, Train_Loss: 5.144752502441406, Test_Loss: 5.150942325592041 *\n",
      "Epoch: 1, Train_Loss: 5.117741584777832, Test_Loss: 5.126987934112549 *\n",
      "Epoch: 1, Train_Loss: 5.198262691497803, Test_Loss: 5.123876094818115 *\n",
      "Epoch: 1, Train_Loss: 5.123636722564697, Test_Loss: 5.1772236824035645\n",
      "Epoch: 1, Train_Loss: 5.110136985778809, Test_Loss: 5.183735370635986\n",
      "Epoch: 1, Train_Loss: 5.52892541885376, Test_Loss: 5.178813934326172 *\n",
      "Epoch: 1, Train_Loss: 6.568276882171631, Test_Loss: 5.214783668518066\n",
      "Epoch: 1, Train_Loss: 6.104230880737305, Test_Loss: 5.09178352355957 *\n",
      "Epoch: 1, Train_Loss: 5.16526460647583, Test_Loss: 5.112454891204834\n",
      "Epoch: 1, Train_Loss: 5.330108642578125, Test_Loss: 5.115676403045654\n",
      "Epoch: 1, Train_Loss: 7.668827056884766, Test_Loss: 5.102258205413818 *\n",
      "Epoch: 1, Train_Loss: 6.05888032913208, Test_Loss: 5.085280895233154 *\n",
      "Epoch: 1, Train_Loss: 5.099711894989014, Test_Loss: 5.071891784667969 *\n",
      "Epoch: 1, Train_Loss: 5.0681233406066895, Test_Loss: 5.061640739440918 *\n",
      "Epoch: 1, Train_Loss: 5.9490580558776855, Test_Loss: 5.053096771240234 *\n",
      "Epoch: 1, Train_Loss: 6.772738456726074, Test_Loss: 5.057314395904541\n",
      "Epoch: 1, Train_Loss: 5.723319053649902, Test_Loss: 5.045339107513428 *\n",
      "Epoch: 1, Train_Loss: 5.041085720062256, Test_Loss: 5.043221950531006 *\n",
      "Epoch: 1, Train_Loss: 5.020973205566406, Test_Loss: 5.012634754180908 *\n",
      "Epoch: 1, Train_Loss: 5.404270648956299, Test_Loss: 5.046424865722656\n",
      "Epoch: 1, Train_Loss: 5.314959526062012, Test_Loss: 5.087364196777344\n",
      "Epoch: 1, Train_Loss: 5.000613689422607, Test_Loss: 5.023786544799805 *\n",
      "Epoch: 1, Train_Loss: 5.022337913513184, Test_Loss: 5.583188056945801\n",
      "Epoch: 1, Train_Loss: 5.0837907791137695, Test_Loss: 5.514135360717773 *\n",
      "Epoch: 1, Train_Loss: 5.1497907638549805, Test_Loss: 5.119036674499512 *\n",
      "Epoch: 1, Train_Loss: 5.044244766235352, Test_Loss: 5.001646995544434 *\n",
      "Epoch: 1, Train_Loss: 5.414311408996582, Test_Loss: 4.980093479156494 *\n",
      "Epoch: 1, Train_Loss: 5.0987982749938965, Test_Loss: 4.99426794052124\n",
      "Epoch: 1, Train_Loss: 5.018362998962402, Test_Loss: 5.279850959777832\n",
      "Epoch: 1, Train_Loss: 5.183332920074463, Test_Loss: 6.396810531616211\n",
      "Epoch: 1, Train_Loss: 5.276150226593018, Test_Loss: 5.732481479644775 *\n",
      "Epoch: 1, Train_Loss: 5.397356986999512, Test_Loss: 5.004844665527344 *\n",
      "Epoch: 1, Train_Loss: 5.114230632781982, Test_Loss: 4.963204383850098 *\n",
      "Epoch: 1, Train_Loss: 5.027820110321045, Test_Loss: 4.9229512214660645 *\n",
      "Epoch: 1, Train_Loss: 5.073455810546875, Test_Loss: 4.914490699768066 *\n",
      "Epoch: 1, Train_Loss: 4.991656303405762, Test_Loss: 4.91448450088501 *\n",
      "Epoch: 1, Train_Loss: 4.92733907699585, Test_Loss: 4.935766220092773\n",
      "Epoch: 1, Train_Loss: 4.896245002746582, Test_Loss: 4.965875148773193\n",
      "Epoch: 1, Train_Loss: 4.892078399658203, Test_Loss: 4.8908185958862305 *\n",
      "Epoch: 1, Train_Loss: 4.891366481781006, Test_Loss: 4.926835060119629\n",
      "Epoch: 1, Train_Loss: 4.881124019622803, Test_Loss: 5.001425743103027\n",
      "Epoch: 1, Train_Loss: 4.91176700592041, Test_Loss: 5.218014717102051\n",
      "Epoch: 1, Train_Loss: 4.962345123291016, Test_Loss: 5.062214374542236 *\n",
      "Epoch: 1, Train_Loss: 4.978714466094971, Test_Loss: 4.867212295532227 *\n",
      "Epoch: 1, Train_Loss: 4.95184326171875, Test_Loss: 4.857167720794678 *\n",
      "Epoch: 1, Train_Loss: 5.254201412200928, Test_Loss: 4.8513031005859375 *\n",
      "Epoch: 1, Train_Loss: 4.983509063720703, Test_Loss: 4.845325946807861 *\n",
      "Epoch: 1, Train_Loss: 4.845448017120361, Test_Loss: 4.84226655960083 *\n",
      "Epoch: 1, Train_Loss: 4.972075939178467, Test_Loss: 6.12553071975708\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 5.283812522888184, Test_Loss: 9.147680282592773\n",
      "Epoch: 1, Train_Loss: 5.38429069519043, Test_Loss: 4.832554340362549 *\n",
      "Epoch: 1, Train_Loss: 4.815229892730713, Test_Loss: 4.815419673919678 *\n",
      "Epoch: 1, Train_Loss: 4.80900764465332, Test_Loss: 4.808655738830566 *\n",
      "Epoch: 1, Train_Loss: 5.374914169311523, Test_Loss: 4.805825710296631 *\n",
      "Epoch: 1, Train_Loss: 5.517491340637207, Test_Loss: 4.798233985900879 *\n",
      "Epoch: 1, Train_Loss: 4.94848108291626, Test_Loss: 4.7925591468811035 *\n",
      "Epoch: 1, Train_Loss: 4.816923141479492, Test_Loss: 4.789989471435547 *\n",
      "Epoch: 1, Train_Loss: 4.789597988128662, Test_Loss: 4.782990455627441 *\n",
      "Epoch: 1, Train_Loss: 5.520744323730469, Test_Loss: 4.778914451599121 *\n",
      "Epoch: 1, Train_Loss: 6.4602155685424805, Test_Loss: 4.7696380615234375 *\n",
      "Epoch: 1, Train_Loss: 4.876022815704346, Test_Loss: 4.767922401428223 *\n",
      "Epoch: 1, Train_Loss: 4.786282062530518, Test_Loss: 4.779604911804199\n",
      "Epoch: 1, Train_Loss: 4.759032249450684, Test_Loss: 4.77614688873291 *\n",
      "Epoch: 1, Train_Loss: 4.747923374176025, Test_Loss: 4.75265645980835 *\n",
      "Epoch: 1, Train_Loss: 5.17258358001709, Test_Loss: 4.739459037780762 *\n",
      "Epoch: 1, Train_Loss: 4.770735740661621, Test_Loss: 4.734492778778076 *\n",
      "Epoch: 1, Train_Loss: 4.868442058563232, Test_Loss: 4.728603839874268 *\n",
      "Epoch: 1, Train_Loss: 4.9959235191345215, Test_Loss: 4.722605228424072 *\n",
      "Epoch: 1, Train_Loss: 4.725430011749268, Test_Loss: 4.717793941497803 *\n",
      "Epoch: 1, Train_Loss: 4.796352863311768, Test_Loss: 4.7108473777771 *\n",
      "Epoch: 1, Train_Loss: 4.803227424621582, Test_Loss: 4.705206394195557 *\n",
      "Epoch: 1, Train_Loss: 5.041438579559326, Test_Loss: 4.70263147354126 *\n",
      "Epoch: 1, Train_Loss: 4.721661567687988, Test_Loss: 4.694432258605957 *\n",
      "Epoch: 1, Train_Loss: 4.876720428466797, Test_Loss: 4.689804553985596 *\n",
      "Epoch: 1, Train_Loss: 5.032325744628906, Test_Loss: 4.683556079864502 *\n",
      "Epoch: 1, Train_Loss: 4.8069586753845215, Test_Loss: 4.678319454193115 *\n",
      "Epoch: 1, Train_Loss: 4.725418567657471, Test_Loss: 4.671429634094238 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 4.673588275909424, Test_Loss: 4.666406154632568 *\n",
      "Epoch: 1, Train_Loss: 4.672527313232422, Test_Loss: 4.691978454589844\n",
      "Epoch: 1, Train_Loss: 4.682843208312988, Test_Loss: 4.692328929901123\n",
      "Epoch: 1, Train_Loss: 5.1773576736450195, Test_Loss: 8.213348388671875\n",
      "Epoch: 1, Train_Loss: 5.136793613433838, Test_Loss: 6.690032958984375 *\n",
      "Epoch: 1, Train_Loss: 5.13552188873291, Test_Loss: 4.640528678894043 *\n",
      "Epoch: 1, Train_Loss: 5.518103122711182, Test_Loss: 4.636523246765137 *\n",
      "Epoch: 1, Train_Loss: 4.7916178703308105, Test_Loss: 4.67399263381958\n",
      "Epoch: 1, Train_Loss: 5.278739929199219, Test_Loss: 4.678576946258545\n",
      "Epoch: 1, Train_Loss: 4.852049827575684, Test_Loss: 4.637874603271484 *\n",
      "Epoch: 1, Train_Loss: 4.616918087005615, Test_Loss: 4.687594890594482\n",
      "Epoch: 1, Train_Loss: 4.619369983673096, Test_Loss: 4.7138590812683105\n",
      "Epoch: 1, Train_Loss: 4.67611837387085, Test_Loss: 4.599643230438232 *\n",
      "Epoch: 1, Train_Loss: 4.754197120666504, Test_Loss: 4.635135650634766\n",
      "Epoch: 1, Train_Loss: 5.616972923278809, Test_Loss: 4.605637073516846 *\n",
      "Epoch: 1, Train_Loss: 4.902091979980469, Test_Loss: 4.597806930541992 *\n",
      "Epoch: 1, Train_Loss: 6.66197395324707, Test_Loss: 4.58458137512207 *\n",
      "Epoch: 1, Train_Loss: 5.103814601898193, Test_Loss: 4.670779228210449\n",
      "Epoch: 1, Train_Loss: 5.5392045974731445, Test_Loss: 4.620823383331299 *\n",
      "Epoch: 1, Train_Loss: 4.590926170349121, Test_Loss: 4.68401575088501\n",
      "Epoch: 1, Train_Loss: 4.559091567993164, Test_Loss: 4.675224781036377 *\n",
      "Epoch: 1, Train_Loss: 4.926481246948242, Test_Loss: 4.566450595855713 *\n",
      "Epoch: 1, Train_Loss: 5.93292760848999, Test_Loss: 4.553689479827881 *\n",
      "Epoch: 1, Train_Loss: 4.828575134277344, Test_Loss: 4.547225475311279 *\n",
      "Epoch: 1, Train_Loss: 4.684909343719482, Test_Loss: 4.54105281829834 *\n",
      "Epoch: 1, Train_Loss: 4.534873962402344, Test_Loss: 4.535637855529785 *\n",
      "Epoch: 1, Train_Loss: 4.625612258911133, Test_Loss: 4.5300445556640625 *\n",
      "Epoch: 1, Train_Loss: 4.843588829040527, Test_Loss: 4.524315357208252 *\n",
      "Epoch: 1, Train_Loss: 4.862642765045166, Test_Loss: 4.517617225646973 *\n",
      "Epoch: 1, Train_Loss: 5.898110389709473, Test_Loss: 4.520923137664795\n",
      "Epoch: 1, Train_Loss: 5.050346374511719, Test_Loss: 4.513358116149902 *\n",
      "Epoch: 1, Train_Loss: 4.501260280609131, Test_Loss: 4.50740909576416 *\n",
      "Epoch: 1, Train_Loss: 4.506965160369873, Test_Loss: 4.498230457305908 *\n",
      "Epoch: 1, Train_Loss: 4.513246536254883, Test_Loss: 4.519458293914795\n",
      "Epoch: 1, Train_Loss: 4.516432285308838, Test_Loss: 4.530250072479248\n",
      "Epoch: 1, Train_Loss: 4.49347448348999, Test_Loss: 4.580185413360596\n",
      "Epoch: 1, Train_Loss: 4.4911322593688965, Test_Loss: 5.015650272369385\n",
      "Epoch: 1, Train_Loss: 7.189802169799805, Test_Loss: 4.866671562194824 *\n",
      "Epoch: 1, Train_Loss: 19.153282165527344, Test_Loss: 4.562957763671875 *\n",
      "Epoch: 1, Train_Loss: 4.861841201782227, Test_Loss: 4.480274200439453 *\n",
      "Epoch: 1, Train_Loss: 7.8171796798706055, Test_Loss: 4.463818550109863 *\n",
      "Epoch: 1, Train_Loss: 5.082652568817139, Test_Loss: 4.517826557159424\n",
      "Epoch: 1, Train_Loss: 4.497981071472168, Test_Loss: 4.974947929382324\n",
      "Epoch: 1, Train_Loss: 4.490115642547607, Test_Loss: 5.900873184204102\n",
      "Epoch: 1, Train_Loss: 14.12299919128418, Test_Loss: 5.001874923706055 *\n",
      "Epoch: 1, Train_Loss: 7.222130298614502, Test_Loss: 4.509680271148682 *\n",
      "Epoch: 1, Train_Loss: 4.439270496368408, Test_Loss: 4.427753925323486 *\n",
      "Epoch: 1, Train_Loss: 6.3156819343566895, Test_Loss: 4.42531681060791 *\n",
      "Epoch: 1, Train_Loss: 8.881933212280273, Test_Loss: 4.416041851043701 *\n",
      "Epoch: 1, Train_Loss: 4.431524276733398, Test_Loss: 4.412653923034668 *\n",
      "Epoch: 1, Train_Loss: 4.411997318267822, Test_Loss: 4.458271026611328\n",
      "Epoch: 1, Train_Loss: 4.400564670562744, Test_Loss: 4.474808216094971\n",
      "Epoch: 1, Train_Loss: 4.3935866355896, Test_Loss: 4.3975911140441895 *\n",
      "Epoch: 1, Train_Loss: 4.390903949737549, Test_Loss: 4.445562839508057\n",
      "Epoch: 1, Train_Loss: 4.385928153991699, Test_Loss: 4.573971271514893\n",
      "Epoch: 1, Train_Loss: 4.3815460205078125, Test_Loss: 4.609050750732422\n",
      "Epoch: 1, Train_Loss: 4.377245903015137, Test_Loss: 4.532537460327148 *\n",
      "Epoch: 1, Train_Loss: 4.375371932983398, Test_Loss: 4.363501071929932 *\n",
      "Epoch: 1, Train_Loss: 4.3645453453063965, Test_Loss: 4.3576273918151855 *\n",
      "Epoch: 1, Train_Loss: 4.362005710601807, Test_Loss: 4.3527631759643555 *\n",
      "Epoch: 1, Train_Loss: 4.360593318939209, Test_Loss: 4.347918510437012 *\n",
      "Epoch: 1, Train_Loss: 4.4060893058776855, Test_Loss: 4.347629547119141 *\n",
      "Epoch: 1, Train_Loss: 4.390947341918945, Test_Loss: 6.989259243011475\n",
      "Epoch: 1, Train_Loss: 4.344622611999512, Test_Loss: 7.452278137207031\n",
      "Epoch: 1, Train_Loss: 4.334500312805176, Test_Loss: 4.335486888885498 *\n",
      "Epoch: 1, Train_Loss: 4.325648784637451, Test_Loss: 4.325338840484619 *\n",
      "Epoch: 1, Train_Loss: 4.317059516906738, Test_Loss: 4.31815767288208 *\n",
      "Epoch: 1, Train_Loss: 4.313508987426758, Test_Loss: 4.315431118011475 *\n",
      "Epoch: 1, Train_Loss: 4.309749126434326, Test_Loss: 4.310108661651611 *\n",
      "Epoch: 1, Train_Loss: 4.307361602783203, Test_Loss: 4.309823036193848 *\n",
      "Epoch: 1, Train_Loss: 4.304012298583984, Test_Loss: 4.304710865020752 *\n",
      "Epoch: 1, Train_Loss: 4.293975353240967, Test_Loss: 4.3024797439575195 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 4.289999008178711, Test_Loss: 4.296886920928955 *\n",
      "Epoch: 1, Train_Loss: 4.285216808319092, Test_Loss: 4.28879976272583 *\n",
      "Epoch: 1, Train_Loss: 4.2784881591796875, Test_Loss: 4.287895679473877 *\n",
      "Epoch: 1, Train_Loss: 4.294270038604736, Test_Loss: 4.293925762176514\n",
      "Epoch: 1, Train_Loss: 4.28192663192749, Test_Loss: 4.286398410797119 *\n",
      "Epoch: 1, Train_Loss: 4.280460357666016, Test_Loss: 4.271404266357422 *\n",
      "Epoch: 1, Train_Loss: 4.262235641479492, Test_Loss: 4.261566638946533 *\n",
      "Epoch: 1, Train_Loss: 11.973539352416992, Test_Loss: 4.260225296020508 *\n",
      "Epoch: 1, Train_Loss: 5.9218831062316895, Test_Loss: 4.253506183624268 *\n",
      "Epoch: 1, Train_Loss: 4.263051986694336, Test_Loss: 4.249690532684326 *\n",
      "Epoch: 1, Train_Loss: 4.244078636169434, Test_Loss: 4.248387813568115 *\n",
      "Epoch: 1, Train_Loss: 4.239321708679199, Test_Loss: 4.236516952514648 *\n",
      "Epoch: 1, Train_Loss: 4.265717029571533, Test_Loss: 4.234037399291992 *\n",
      "Epoch: 1, Train_Loss: 4.272808074951172, Test_Loss: 4.234520435333252\n",
      "Epoch: 1, Train_Loss: 4.2254638671875, Test_Loss: 4.224995136260986 *\n",
      "Epoch: 1, Train_Loss: 4.300070762634277, Test_Loss: 4.223135948181152 *\n",
      "Epoch: 1, Train_Loss: 4.425258159637451, Test_Loss: 4.216046333312988 *\n",
      "Epoch: 1, Train_Loss: 4.397602081298828, Test_Loss: 4.211991786956787 *\n",
      "Epoch: 1, Train_Loss: 4.235438823699951, Test_Loss: 4.207589626312256 *\n",
      "Epoch: 1, Train_Loss: 4.273044586181641, Test_Loss: 4.203445911407471 *\n",
      "Epoch: 1, Train_Loss: 4.307595729827881, Test_Loss: 4.2454915046691895\n",
      "Epoch: 1, Train_Loss: 4.314426422119141, Test_Loss: 4.212845325469971 *\n",
      "Epoch: 1, Train_Loss: 4.334811687469482, Test_Loss: 9.132277488708496\n",
      "Epoch: 1, Train_Loss: 4.301986217498779, Test_Loss: 5.006318092346191 *\n",
      "Epoch: 1, Train_Loss: 4.245645523071289, Test_Loss: 4.17798376083374 *\n",
      "Epoch: 1, Train_Loss: 4.175498962402344, Test_Loss: 4.18109130859375\n",
      "Epoch: 1, Train_Loss: 4.239907264709473, Test_Loss: 4.199197769165039\n",
      "Epoch: 1, Train_Loss: 4.184314250946045, Test_Loss: 4.2062835693359375\n",
      "Epoch: 1, Train_Loss: 4.16323709487915, Test_Loss: 4.159002780914307 *\n",
      "Epoch: 1, Train_Loss: 4.151128768920898, Test_Loss: 4.271358966827393\n",
      "Epoch: 1, Train_Loss: 4.146526336669922, Test_Loss: 4.2522149085998535 *\n",
      "Epoch: 1, Train_Loss: 4.178632736206055, Test_Loss: 4.141031742095947 *\n",
      "Epoch: 1, Train_Loss: 9.720102310180664, Test_Loss: 4.20601749420166\n",
      "Epoch: 1, Train_Loss: 4.289675712585449, Test_Loss: 4.134116172790527 *\n",
      "Epoch: 1, Train_Loss: 4.141249179840088, Test_Loss: 4.151031494140625\n",
      "Epoch: 1, Train_Loss: 4.138618469238281, Test_Loss: 4.124976634979248 *\n",
      "Epoch: 1, Train_Loss: 4.137262344360352, Test_Loss: 4.218977451324463\n",
      "Epoch: 1, Train_Loss: 4.130253314971924, Test_Loss: 4.14600133895874 *\n",
      "Epoch: 1, Train_Loss: 4.113197326660156, Test_Loss: 4.285853862762451\n",
      "Epoch: 1, Train_Loss: 4.110215663909912, Test_Loss: 4.210841178894043 *\n",
      "Epoch: 1, Train_Loss: 4.122414588928223, Test_Loss: 4.109689712524414 *\n",
      "Epoch: 1, Train_Loss: 4.119701385498047, Test_Loss: 4.0949907302856445 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 4.094793319702148, Test_Loss: 4.091252326965332 *\n",
      "Epoch: 1, Train_Loss: 4.0908403396606445, Test_Loss: 4.085369110107422 *\n",
      "Epoch: 1, Train_Loss: 4.086386680603027, Test_Loss: 4.080645561218262 *\n",
      "Epoch: 1, Train_Loss: 4.101454257965088, Test_Loss: 4.07609224319458 *\n",
      "Epoch: 1, Train_Loss: 4.070986270904541, Test_Loss: 4.07143497467041 *\n",
      "Epoch: 1, Train_Loss: 4.068503379821777, Test_Loss: 4.065938472747803 *\n",
      "Epoch: 1, Train_Loss: 4.086204528808594, Test_Loss: 4.071558475494385\n",
      "Epoch: 1, Train_Loss: 4.125800132751465, Test_Loss: 4.064357757568359 *\n",
      "Epoch: 1, Train_Loss: 4.097689628601074, Test_Loss: 4.055026531219482 *\n",
      "Epoch: 1, Train_Loss: 4.0482635498046875, Test_Loss: 4.053590297698975 *\n",
      "Epoch: 1, Train_Loss: 4.04626989364624, Test_Loss: 4.081964492797852\n",
      "Epoch: 1, Train_Loss: 4.089653015136719, Test_Loss: 4.059822082519531 *\n",
      "Epoch: 1, Train_Loss: 4.135678768157959, Test_Loss: 4.235864639282227\n",
      "Epoch: 1, Train_Loss: 4.112123012542725, Test_Loss: 4.537012100219727\n",
      "Epoch: 1, Train_Loss: 4.101102352142334, Test_Loss: 4.282006740570068 *\n",
      "Epoch: 1, Train_Loss: 4.04058837890625, Test_Loss: 4.088999271392822 *\n",
      "Epoch: 1, Train_Loss: 4.106730937957764, Test_Loss: 4.035800457000732 *\n",
      "Epoch: 1, Train_Loss: 4.086513996124268, Test_Loss: 4.023954391479492 *\n",
      "Epoch: 1, Train_Loss: 4.038747310638428, Test_Loss: 4.131410598754883\n",
      "Epoch: 1, Train_Loss: 4.083289623260498, Test_Loss: 4.812821865081787\n",
      "Epoch: 1, Train_Loss: 4.028060436248779, Test_Loss: 5.596879482269287\n",
      "Epoch: 1, Train_Loss: 4.007071018218994, Test_Loss: 4.351119041442871 *\n",
      "Epoch: 1, Train_Loss: 3.992216110229492, Test_Loss: 4.0684356689453125 *\n",
      "Epoch: 1, Train_Loss: 3.9873108863830566, Test_Loss: 3.9822282791137695 *\n",
      "Epoch: 1, Train_Loss: 3.9826951026916504, Test_Loss: 3.987959623336792\n",
      "Epoch: 1, Train_Loss: 3.9782729148864746, Test_Loss: 3.9795081615448 *\n",
      "Epoch: 1, Train_Loss: 3.9701757431030273, Test_Loss: 3.9818036556243896\n",
      "Epoch: 1, Train_Loss: 8.470598220825195, Test_Loss: 4.0231032371521\n",
      "Epoch: 1, Train_Loss: 4.490816116333008, Test_Loss: 4.023595333099365\n",
      "Epoch: 1, Train_Loss: 3.9608428478240967, Test_Loss: 3.9649288654327393 *\n",
      "Epoch: 1, Train_Loss: 3.9823086261749268, Test_Loss: 4.033121585845947\n",
      "Epoch: 1, Train_Loss: 3.951270580291748, Test_Loss: 4.29124116897583\n",
      "Epoch: 1, Train_Loss: 3.9395501613616943, Test_Loss: 4.020063400268555 *\n",
      "Epoch: 1, Train_Loss: 3.9364187717437744, Test_Loss: 4.092846870422363\n",
      "Epoch: 1, Train_Loss: 3.930358409881592, Test_Loss: 3.9330103397369385 *\n",
      "Epoch: 1, Train_Loss: 3.925377607345581, Test_Loss: 3.928741693496704 *\n",
      "Epoch: 1, Train_Loss: 3.9201085567474365, Test_Loss: 3.924294948577881 *\n",
      "Epoch: 1, Train_Loss: 3.97997784614563, Test_Loss: 3.9197580814361572 *\n",
      "Epoch: 1, Train_Loss: 4.0271735191345215, Test_Loss: 3.921999216079712\n",
      "Epoch: 1, Train_Loss: 4.0294694900512695, Test_Loss: 7.9193925857543945\n",
      "Epoch: 1, Train_Loss: 4.016546726226807, Test_Loss: 5.477225303649902 *\n",
      "Epoch: 1, Train_Loss: 3.8992292881011963, Test_Loss: 3.9051735401153564 *\n",
      "Epoch: 1, Train_Loss: 3.9444594383239746, Test_Loss: 3.894432783126831 *\n",
      "Epoch: 1, Train_Loss: 4.091001987457275, Test_Loss: 3.889832019805908 *\n",
      "Epoch: 1, Train_Loss: 4.082803249359131, Test_Loss: 3.892364740371704\n",
      "Epoch: 1, Train_Loss: 4.0817036628723145, Test_Loss: 3.881711721420288 *\n",
      "Epoch: 1, Train_Loss: 3.8838953971862793, Test_Loss: 3.8798375129699707 *\n",
      "Epoch: 1, Train_Loss: 3.8692712783813477, Test_Loss: 3.8723654747009277 *\n",
      "Epoch: 1, Train_Loss: 3.867205858230591, Test_Loss: 3.870774984359741 *\n",
      "Epoch: 1, Train_Loss: 3.8627004623413086, Test_Loss: 3.867722988128662 *\n",
      "Epoch: 1, Train_Loss: 3.8604681491851807, Test_Loss: 3.86271595954895 *\n",
      "Epoch: 1, Train_Loss: 3.855151891708374, Test_Loss: 3.860642194747925 *\n",
      "Epoch: 1, Train_Loss: 3.850081443786621, Test_Loss: 3.8740053176879883\n",
      "Epoch: 1, Train_Loss: 3.846477746963501, Test_Loss: 3.8610713481903076 *\n",
      "Epoch: 1, Train_Loss: 3.840625047683716, Test_Loss: 3.8451507091522217 *\n",
      "Epoch: 1, Train_Loss: 3.838320732116699, Test_Loss: 3.835993766784668 *\n",
      "Epoch: 1, Train_Loss: 3.948875904083252, Test_Loss: 3.8358941078186035 *\n",
      "Epoch: 1, Train_Loss: 3.9882123470306396, Test_Loss: 3.8293445110321045 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 3.990849494934082, Test_Loss: 3.824291229248047 *\n",
      "Epoch: 1, Train_Loss: 3.871073007583618, Test_Loss: 3.8245646953582764\n",
      "Epoch: 1, Train_Loss: 3.992605686187744, Test_Loss: 3.8142080307006836 *\n",
      "Epoch: 1, Train_Loss: 3.9835286140441895, Test_Loss: 3.811950445175171 *\n",
      "Epoch: 1, Train_Loss: 3.8366527557373047, Test_Loss: 3.8133726119995117\n",
      "Epoch: 1, Train_Loss: 3.9890003204345703, Test_Loss: 3.8045287132263184 *\n",
      "Epoch: 1, Train_Loss: 3.9603822231292725, Test_Loss: 3.801500082015991 *\n",
      "Epoch: 1, Train_Loss: 4.033862113952637, Test_Loss: 3.794947385787964 *\n",
      "Epoch: 1, Train_Loss: 3.8120150566101074, Test_Loss: 3.791511058807373 *\n",
      "Epoch: 1, Train_Loss: 5.276512622833252, Test_Loss: 3.787127733230591 *\n",
      "Epoch: 1, Train_Loss: 5.649808406829834, Test_Loss: 3.7848961353302 *\n",
      "Epoch: 1, Train_Loss: 3.808258056640625, Test_Loss: 3.8285694122314453\n",
      "Epoch: 1, Train_Loss: 3.807238817214966, Test_Loss: 3.828059434890747 *\n",
      "Epoch: 1, Train_Loss: 3.80704665184021, Test_Loss: 9.337196350097656\n",
      "Epoch: 1, Train_Loss: 3.7958178520202637, Test_Loss: 3.844592809677124 *\n",
      "Epoch: 1, Train_Loss: 3.7582197189331055, Test_Loss: 3.758913993835449 *\n",
      "Epoch: 1, Train_Loss: 3.771148443222046, Test_Loss: 3.7702786922454834\n",
      "Epoch: 1, Train_Loss: 3.9064221382141113, Test_Loss: 3.793058395385742\n",
      "Epoch: 1, Train_Loss: 3.8621037006378174, Test_Loss: 3.7916576862335205 *\n",
      "Epoch: 1, Train_Loss: 3.8643503189086914, Test_Loss: 3.7421672344207764 *\n",
      "Epoch: 1, Train_Loss: 3.8569846153259277, Test_Loss: 3.86700701713562\n",
      "Epoch: 1, Train_Loss: 3.8186821937561035, Test_Loss: 3.798835277557373 *\n",
      "Epoch: 1, Train_Loss: 3.7678732872009277, Test_Loss: 3.725285530090332 *\n",
      "Epoch: 1, Train_Loss: 3.7503302097320557, Test_Loss: 3.780273199081421\n",
      "Epoch: 1, Train_Loss: 3.730999708175659, Test_Loss: 3.723876476287842 *\n",
      "Epoch: 1, Train_Loss: 3.7295379638671875, Test_Loss: 3.7291834354400635\n",
      "Epoch: 1, Train_Loss: 3.7132437229156494, Test_Loss: 3.7314884662628174\n",
      "Epoch: 1, Train_Loss: 3.7042431831359863, Test_Loss: 3.8335554599761963\n",
      "Epoch: 1, Train_Loss: 3.7574610710144043, Test_Loss: 3.7410576343536377 *\n",
      "Epoch: 1, Train_Loss: 3.7607386112213135, Test_Loss: 3.8337080478668213\n",
      "Epoch: 1, Train_Loss: 3.7163710594177246, Test_Loss: 3.754154682159424 *\n",
      "Epoch: 1, Train_Loss: 3.6855618953704834, Test_Loss: 3.716806650161743 *\n",
      "Epoch: 1, Train_Loss: 3.6815335750579834, Test_Loss: 3.699915885925293 *\n",
      "Epoch: 1, Train_Loss: 3.6767053604125977, Test_Loss: 3.6933746337890625 *\n",
      "Epoch: 1, Train_Loss: 3.6724514961242676, Test_Loss: 3.6912078857421875 *\n",
      "Epoch: 1, Train_Loss: 3.668877363204956, Test_Loss: 3.6872711181640625 *\n",
      "Epoch: 1, Train_Loss: 3.664022445678711, Test_Loss: 3.6835498809814453 *\n",
      "Epoch: 1, Train_Loss: 3.660053253173828, Test_Loss: 3.6786489486694336 *\n",
      "Epoch: 1, Train_Loss: 3.65605092048645, Test_Loss: 3.677009344100952 *\n",
      "Epoch: 1, Train_Loss: 3.651000499725342, Test_Loss: 3.680410623550415\n",
      "Epoch: 1, Train_Loss: 3.6471238136291504, Test_Loss: 3.6815285682678223\n",
      "Epoch: 1, Train_Loss: 3.649691343307495, Test_Loss: 3.650735378265381 *\n",
      "Epoch: 1, Train_Loss: 3.6485226154327393, Test_Loss: 3.6566836833953857\n",
      "Epoch: 1, Train_Loss: 3.64206862449646, Test_Loss: 3.7174394130706787\n",
      "Epoch: 1, Train_Loss: 3.647681713104248, Test_Loss: 3.6554365158081055 *\n",
      "Epoch: 1, Train_Loss: 3.6283135414123535, Test_Loss: 3.947550058364868\n",
      "Epoch: 1, Train_Loss: 3.623133897781372, Test_Loss: 4.239377975463867\n",
      "Epoch: 1, Train_Loss: 3.618213176727295, Test_Loss: 3.8608474731445312 *\n",
      "Epoch: 1, Train_Loss: 3.6136016845703125, Test_Loss: 3.6849374771118164 *\n",
      "Epoch: 1, Train_Loss: 3.6225199699401855, Test_Loss: 3.645763635635376 *\n",
      "Epoch: 1, Train_Loss: 3.609097957611084, Test_Loss: 3.6200170516967773 *\n",
      "Epoch: 1, Train_Loss: 3.601297378540039, Test_Loss: 3.725220203399658\n",
      "Epoch: 1, Train_Loss: 3.597597599029541, Test_Loss: 4.535311698913574\n",
      "Epoch: 1, Train_Loss: 3.6102871894836426, Test_Loss: 5.0544352531433105\n",
      "Epoch: 2, Train_Loss: 3.6816132068634033, Test_Loss: 3.7289533615112305 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.611839532852173, Test_Loss: 3.689706802368164 *\n",
      "Epoch: 2, Train_Loss: 3.6093521118164062, Test_Loss: 3.580679416656494 *\n",
      "Epoch: 2, Train_Loss: 3.5762808322906494, Test_Loss: 3.5839428901672363\n",
      "Epoch: 2, Train_Loss: 3.607759952545166, Test_Loss: 3.5783016681671143 *\n",
      "Epoch: 2, Train_Loss: 3.5863401889801025, Test_Loss: 3.5856075286865234\n",
      "Epoch: 2, Train_Loss: 3.5670723915100098, Test_Loss: 3.612398862838745\n",
      "Epoch: 2, Train_Loss: 3.5932834148406982, Test_Loss: 3.6061465740203857 *\n",
      "Epoch: 2, Train_Loss: 3.5898873805999756, Test_Loss: 3.5639007091522217 *\n",
      "Epoch: 2, Train_Loss: 3.672260284423828, Test_Loss: 3.654019594192505\n",
      "Epoch: 2, Train_Loss: 3.661773204803467, Test_Loss: 3.942138195037842\n",
      "Epoch: 2, Train_Loss: 3.625065326690674, Test_Loss: 3.6198129653930664 *\n",
      "Epoch: 2, Train_Loss: 3.5721681118011475, Test_Loss: 3.6798274517059326\n",
      "Epoch: 2, Train_Loss: 3.5374059677124023, Test_Loss: 3.5416438579559326 *\n",
      "Epoch: 2, Train_Loss: 3.56101393699646, Test_Loss: 3.537785768508911 *\n",
      "Epoch: 2, Train_Loss: 3.527315378189087, Test_Loss: 3.533813953399658 *\n",
      "Epoch: 2, Train_Loss: 3.5330252647399902, Test_Loss: 3.5289647579193115 *\n",
      "Epoch: 2, Train_Loss: 3.5318522453308105, Test_Loss: 3.535344123840332\n",
      "Epoch: 2, Train_Loss: 3.5299293994903564, Test_Loss: 8.60484504699707\n",
      "Epoch: 2, Train_Loss: 3.600907802581787, Test_Loss: 3.9782583713531494 *\n",
      "Epoch: 2, Train_Loss: 3.512561082839966, Test_Loss: 3.5148017406463623 *\n",
      "Epoch: 2, Train_Loss: 3.5943615436553955, Test_Loss: 3.5034284591674805 *\n",
      "Epoch: 2, Train_Loss: 3.506943941116333, Test_Loss: 3.501007080078125 *\n",
      "Epoch: 2, Train_Loss: 3.5240442752838135, Test_Loss: 3.5015971660614014\n",
      "Epoch: 2, Train_Loss: 3.523564100265503, Test_Loss: 3.4924683570861816 *\n",
      "Epoch: 2, Train_Loss: 3.764631986618042, Test_Loss: 3.4899895191192627 *\n",
      "Epoch: 2, Train_Loss: 3.494353771209717, Test_Loss: 3.4843664169311523 *\n",
      "Epoch: 2, Train_Loss: 3.5059101581573486, Test_Loss: 3.4801034927368164 *\n",
      "Epoch: 2, Train_Loss: 3.474675178527832, Test_Loss: 3.478029489517212 *\n",
      "Epoch: 2, Train_Loss: 3.4711906909942627, Test_Loss: 3.476238250732422 *\n",
      "Epoch: 2, Train_Loss: 3.468308687210083, Test_Loss: 3.4759581089019775 *\n",
      "Epoch: 2, Train_Loss: 3.461854934692383, Test_Loss: 3.494941234588623\n",
      "Epoch: 2, Train_Loss: 3.4729106426239014, Test_Loss: 3.4755210876464844 *\n",
      "Epoch: 2, Train_Loss: 3.476944923400879, Test_Loss: 3.456456422805786 *\n",
      "Epoch: 2, Train_Loss: 3.480499505996704, Test_Loss: 3.449643611907959 *\n",
      "Epoch: 2, Train_Loss: 3.471363067626953, Test_Loss: 3.4486236572265625 *\n",
      "Epoch: 2, Train_Loss: 3.466460943222046, Test_Loss: 3.442880153656006 *\n",
      "Epoch: 2, Train_Loss: 3.4575467109680176, Test_Loss: 3.4371795654296875 *\n",
      "Epoch: 2, Train_Loss: 3.4374282360076904, Test_Loss: 3.436713218688965 *\n",
      "Epoch: 2, Train_Loss: 3.429689645767212, Test_Loss: 3.4290425777435303 *\n",
      "Epoch: 2, Train_Loss: 3.439786911010742, Test_Loss: 3.4266629219055176 *\n",
      "Epoch: 2, Train_Loss: 3.4401183128356934, Test_Loss: 3.4248580932617188 *\n",
      "Epoch: 2, Train_Loss: 3.4487040042877197, Test_Loss: 3.41947340965271 *\n",
      "Epoch: 2, Train_Loss: 3.4128215312957764, Test_Loss: 3.4147913455963135 *\n",
      "Epoch: 2, Train_Loss: 3.4613945484161377, Test_Loss: 3.4108071327209473 *\n",
      "Epoch: 2, Train_Loss: 3.4674973487854004, Test_Loss: 3.406493663787842 *\n",
      "Epoch: 2, Train_Loss: 3.4535484313964844, Test_Loss: 3.402820587158203 *\n",
      "Epoch: 2, Train_Loss: 3.397386074066162, Test_Loss: 3.402730703353882 *\n",
      "Epoch: 2, Train_Loss: 3.4260573387145996, Test_Loss: 3.4540305137634277\n",
      "Epoch: 2, Train_Loss: 3.391082525253296, Test_Loss: 4.0894904136657715\n",
      "Epoch: 2, Train_Loss: 3.4074010848999023, Test_Loss: 8.242761611938477\n",
      "Epoch: 2, Train_Loss: 3.3835721015930176, Test_Loss: 3.391758680343628 *\n",
      "Epoch: 2, Train_Loss: 3.4022727012634277, Test_Loss: 3.3781445026397705 *\n",
      "Epoch: 2, Train_Loss: 3.5035600662231445, Test_Loss: 3.407914400100708\n",
      "Epoch: 2, Train_Loss: 6.812747955322266, Test_Loss: 3.425689220428467\n",
      "Epoch: 2, Train_Loss: 5.299211502075195, Test_Loss: 3.4229564666748047 *\n",
      "Epoch: 2, Train_Loss: 3.384657144546509, Test_Loss: 3.371777057647705 *\n",
      "Epoch: 2, Train_Loss: 3.3606271743774414, Test_Loss: 3.4982049465179443\n",
      "Epoch: 2, Train_Loss: 3.5109012126922607, Test_Loss: 3.387364387512207 *\n",
      "Epoch: 2, Train_Loss: 3.4731900691986084, Test_Loss: 3.352874517440796 *\n",
      "Epoch: 2, Train_Loss: 3.370772123336792, Test_Loss: 3.3958816528320312\n",
      "Epoch: 2, Train_Loss: 3.3436756134033203, Test_Loss: 3.3523600101470947 *\n",
      "Epoch: 2, Train_Loss: 3.406517505645752, Test_Loss: 3.352316379547119 *\n",
      "Epoch: 2, Train_Loss: 3.368438482284546, Test_Loss: 3.3858225345611572\n",
      "Epoch: 2, Train_Loss: 3.3459725379943848, Test_Loss: 3.4515442848205566\n",
      "Epoch: 2, Train_Loss: 3.4977540969848633, Test_Loss: 3.3882954120635986 *\n",
      "Epoch: 2, Train_Loss: 4.687543869018555, Test_Loss: 3.4440605640411377\n",
      "Epoch: 2, Train_Loss: 4.675757884979248, Test_Loss: 3.3523688316345215 *\n",
      "Epoch: 2, Train_Loss: 3.4327945709228516, Test_Loss: 3.3654494285583496\n",
      "Epoch: 2, Train_Loss: 3.368884801864624, Test_Loss: 3.3473799228668213 *\n",
      "Epoch: 2, Train_Loss: 5.433377742767334, Test_Loss: 3.3400819301605225 *\n",
      "Epoch: 2, Train_Loss: 4.813775062561035, Test_Loss: 3.3355042934417725 *\n",
      "Epoch: 2, Train_Loss: 3.3513057231903076, Test_Loss: 3.327739715576172 *\n",
      "Epoch: 2, Train_Loss: 3.335010051727295, Test_Loss: 3.3199541568756104 *\n",
      "Epoch: 2, Train_Loss: 3.7068185806274414, Test_Loss: 3.313610076904297 *\n",
      "Epoch: 2, Train_Loss: 5.073423862457275, Test_Loss: 3.3181872367858887\n",
      "Epoch: 2, Train_Loss: 4.372851848602295, Test_Loss: 3.310701847076416 *\n",
      "Epoch: 2, Train_Loss: 3.2972655296325684, Test_Loss: 3.3145017623901367\n",
      "Epoch: 2, Train_Loss: 3.3010828495025635, Test_Loss: 3.280911922454834 *\n",
      "Epoch: 2, Train_Loss: 3.4523353576660156, Test_Loss: 3.300551176071167\n",
      "Epoch: 2, Train_Loss: 3.825481653213501, Test_Loss: 3.3658125400543213\n",
      "Epoch: 2, Train_Loss: 3.2782297134399414, Test_Loss: 3.280510425567627 *\n",
      "Epoch: 2, Train_Loss: 3.3101518154144287, Test_Loss: 3.709425687789917\n",
      "Epoch: 2, Train_Loss: 3.326786518096924, Test_Loss: 3.879985809326172\n",
      "Epoch: 2, Train_Loss: 3.4100022315979004, Test_Loss: 3.4626684188842773 *\n",
      "Epoch: 2, Train_Loss: 3.3612098693847656, Test_Loss: 3.296057939529419 *\n",
      "Epoch: 2, Train_Loss: 3.629380226135254, Test_Loss: 3.278188467025757 *\n",
      "Epoch: 2, Train_Loss: 3.4557180404663086, Test_Loss: 3.258824110031128 *\n",
      "Epoch: 2, Train_Loss: 3.30552339553833, Test_Loss: 3.3739092350006104\n",
      "Epoch: 2, Train_Loss: 3.4445743560791016, Test_Loss: 4.353654861450195\n",
      "Epoch: 2, Train_Loss: 3.4994659423828125, Test_Loss: 4.4725141525268555\n",
      "Epoch: 2, Train_Loss: 3.731522560119629, Test_Loss: 3.2860941886901855 *\n",
      "Epoch: 2, Train_Loss: 3.5069618225097656, Test_Loss: 3.315526008605957\n",
      "Epoch: 2, Train_Loss: 3.283987283706665, Test_Loss: 3.221524477005005 *\n",
      "Epoch: 2, Train_Loss: 3.370136260986328, Test_Loss: 3.223942518234253\n",
      "Epoch: 2, Train_Loss: 3.335827112197876, Test_Loss: 3.2210254669189453 *\n",
      "Epoch: 2, Train_Loss: 3.224884033203125, Test_Loss: 3.233832597732544\n",
      "Epoch: 2, Train_Loss: 3.2169079780578613, Test_Loss: 3.2634055614471436\n",
      "Epoch: 2, Train_Loss: 3.2031357288360596, Test_Loss: 3.2271595001220703 *\n",
      "Epoch: 2, Train_Loss: 3.2066311836242676, Test_Loss: 3.2085797786712646 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 3.200713872909546, Test_Loss: 3.2988686561584473\n",
      "Epoch: 2, Train_Loss: 3.210904598236084, Test_Loss: 3.5878310203552246\n",
      "Epoch: 2, Train_Loss: 3.296980142593384, Test_Loss: 3.3582308292388916 *\n",
      "Epoch: 2, Train_Loss: 3.2821857929229736, Test_Loss: 3.231977701187134 *\n",
      "Epoch: 2, Train_Loss: 3.3042445182800293, Test_Loss: 3.1887636184692383 *\n",
      "Epoch: 2, Train_Loss: 3.352476119995117, Test_Loss: 3.1853654384613037 *\n",
      "Epoch: 2, Train_Loss: 3.554537296295166, Test_Loss: 3.181631565093994 *\n",
      "Epoch: 2, Train_Loss: 3.1806015968322754, Test_Loss: 3.1770315170288086 *\n",
      "Epoch: 2, Train_Loss: 3.2321834564208984, Test_Loss: 3.311807155609131\n",
      "Epoch: 2, Train_Loss: 3.492816925048828, Test_Loss: 8.457502365112305\n",
      "Epoch: 2, Train_Loss: 3.7690324783325195, Test_Loss: 3.2763895988464355 *\n",
      "Epoch: 2, Train_Loss: 3.2346973419189453, Test_Loss: 3.1625208854675293 *\n",
      "Epoch: 2, Train_Loss: 3.151545286178589, Test_Loss: 3.153578996658325 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.6168148517608643, Test_Loss: 3.1542794704437256\n",
      "Epoch: 2, Train_Loss: 3.876542568206787, Test_Loss: 3.1506612300872803 *\n",
      "Epoch: 2, Train_Loss: 3.4560658931732178, Test_Loss: 3.142327070236206 *\n",
      "Epoch: 2, Train_Loss: 3.1654489040374756, Test_Loss: 3.1433780193328857\n",
      "Epoch: 2, Train_Loss: 3.1481895446777344, Test_Loss: 3.1378836631774902 *\n",
      "Epoch: 2, Train_Loss: 3.4070498943328857, Test_Loss: 3.134681224822998 *\n",
      "Epoch: 2, Train_Loss: 4.850989818572998, Test_Loss: 3.132046937942505 *\n",
      "Epoch: 2, Train_Loss: 3.585484027862549, Test_Loss: 3.131030559539795 *\n",
      "Epoch: 2, Train_Loss: 3.137394905090332, Test_Loss: 3.1332602500915527\n",
      "Epoch: 2, Train_Loss: 3.129789113998413, Test_Loss: 3.141328811645508\n",
      "Epoch: 2, Train_Loss: 3.114821672439575, Test_Loss: 3.126523733139038 *\n",
      "Epoch: 2, Train_Loss: 3.4577889442443848, Test_Loss: 3.1106626987457275 *\n",
      "Epoch: 2, Train_Loss: 3.2377607822418213, Test_Loss: 3.1057257652282715 *\n",
      "Epoch: 2, Train_Loss: 3.162233352661133, Test_Loss: 3.1026272773742676 *\n",
      "Epoch: 2, Train_Loss: 3.4366543292999268, Test_Loss: 3.0989489555358887 *\n",
      "Epoch: 2, Train_Loss: 3.1041812896728516, Test_Loss: 3.0944557189941406 *\n",
      "Epoch: 2, Train_Loss: 3.1590185165405273, Test_Loss: 3.092930316925049 *\n",
      "Epoch: 2, Train_Loss: 3.1666858196258545, Test_Loss: 3.087193250656128 *\n",
      "Epoch: 2, Train_Loss: 3.4687705039978027, Test_Loss: 3.0861573219299316 *\n",
      "Epoch: 2, Train_Loss: 3.1364455223083496, Test_Loss: 3.0827438831329346 *\n",
      "Epoch: 2, Train_Loss: 3.1880202293395996, Test_Loss: 3.0786774158477783 *\n",
      "Epoch: 2, Train_Loss: 3.3292183876037598, Test_Loss: 3.0743353366851807 *\n",
      "Epoch: 2, Train_Loss: 3.2970468997955322, Test_Loss: 3.0707526206970215 *\n",
      "Epoch: 2, Train_Loss: 3.092879295349121, Test_Loss: 3.066622495651245 *\n",
      "Epoch: 2, Train_Loss: 3.09144926071167, Test_Loss: 3.0635581016540527 *\n",
      "Epoch: 2, Train_Loss: 3.0668044090270996, Test_Loss: 3.067133665084839\n",
      "Epoch: 2, Train_Loss: 3.0975985527038574, Test_Loss: 3.1158721446990967\n",
      "Epoch: 2, Train_Loss: 3.209177017211914, Test_Loss: 4.94112491607666\n",
      "Epoch: 2, Train_Loss: 3.82592511177063, Test_Loss: 6.620639801025391\n",
      "Epoch: 2, Train_Loss: 3.3090882301330566, Test_Loss: 3.0501246452331543 *\n",
      "Epoch: 2, Train_Loss: 3.9412522315979004, Test_Loss: 3.0414516925811768 *\n",
      "Epoch: 2, Train_Loss: 3.4321627616882324, Test_Loss: 3.086892604827881\n",
      "Epoch: 2, Train_Loss: 3.5073530673980713, Test_Loss: 3.0785930156707764 *\n",
      "Epoch: 2, Train_Loss: 3.3528831005096436, Test_Loss: 3.0824389457702637\n",
      "Epoch: 2, Train_Loss: 3.0415778160095215, Test_Loss: 3.062882900238037 *\n",
      "Epoch: 2, Train_Loss: 3.0402095317840576, Test_Loss: 3.1487910747528076\n",
      "Epoch: 2, Train_Loss: 3.0676474571228027, Test_Loss: 3.0279288291931152 *\n",
      "Epoch: 2, Train_Loss: 3.1748263835906982, Test_Loss: 3.032888650894165\n",
      "Epoch: 2, Train_Loss: 3.860598564147949, Test_Loss: 3.039456605911255\n",
      "Epoch: 2, Train_Loss: 3.211125373840332, Test_Loss: 3.029757261276245 *\n",
      "Epoch: 2, Train_Loss: 5.058390140533447, Test_Loss: 3.0156474113464355 *\n",
      "Epoch: 2, Train_Loss: 3.5914525985717773, Test_Loss: 3.0759191513061523\n",
      "Epoch: 2, Train_Loss: 4.086770057678223, Test_Loss: 3.0795459747314453\n",
      "Epoch: 2, Train_Loss: 3.1216139793395996, Test_Loss: 3.081521987915039\n",
      "Epoch: 2, Train_Loss: 2.997476816177368, Test_Loss: 3.0990824699401855\n",
      "Epoch: 2, Train_Loss: 3.154388666152954, Test_Loss: 3.011387825012207 *\n",
      "Epoch: 2, Train_Loss: 4.2461018562316895, Test_Loss: 3.0163307189941406\n",
      "Epoch: 2, Train_Loss: 3.7000701427459717, Test_Loss: 2.9929025173187256 *\n",
      "Epoch: 2, Train_Loss: 3.090099573135376, Test_Loss: 2.9867258071899414 *\n",
      "Epoch: 2, Train_Loss: 3.008411169052124, Test_Loss: 2.9836652278900146 *\n",
      "Epoch: 2, Train_Loss: 3.047481060028076, Test_Loss: 2.9817378520965576 *\n",
      "Epoch: 2, Train_Loss: 3.3415918350219727, Test_Loss: 2.976792335510254 *\n",
      "Epoch: 2, Train_Loss: 3.1873414516448975, Test_Loss: 2.9725728034973145 *\n",
      "Epoch: 2, Train_Loss: 4.120847702026367, Test_Loss: 2.977936029434204\n",
      "Epoch: 2, Train_Loss: 3.7473292350769043, Test_Loss: 2.969194173812866 *\n",
      "Epoch: 2, Train_Loss: 3.032899856567383, Test_Loss: 2.969527006149292\n",
      "Epoch: 2, Train_Loss: 2.966041326522827, Test_Loss: 2.959367513656616 *\n",
      "Epoch: 2, Train_Loss: 2.968942165374756, Test_Loss: 2.9672608375549316\n",
      "Epoch: 2, Train_Loss: 2.9725894927978516, Test_Loss: 3.0074987411499023\n",
      "Epoch: 2, Train_Loss: 2.973893880844116, Test_Loss: 2.955148220062256 *\n",
      "Epoch: 2, Train_Loss: 2.95329213142395, Test_Loss: 3.4023993015289307\n",
      "Epoch: 2, Train_Loss: 2.952758312225342, Test_Loss: 3.4403388500213623\n",
      "Epoch: 2, Train_Loss: 20.334186553955078, Test_Loss: 3.0773911476135254 *\n",
      "Epoch: 2, Train_Loss: 2.942487955093384, Test_Loss: 2.9549171924591064 *\n",
      "Epoch: 2, Train_Loss: 5.790290355682373, Test_Loss: 2.9473507404327393 *\n",
      "Epoch: 2, Train_Loss: 4.3585896492004395, Test_Loss: 2.957775831222534\n",
      "Epoch: 2, Train_Loss: 2.9296722412109375, Test_Loss: 3.1479146480560303\n",
      "Epoch: 2, Train_Loss: 2.9770803451538086, Test_Loss: 4.210822105407715\n",
      "Epoch: 2, Train_Loss: 9.502556800842285, Test_Loss: 3.821946859359741 *\n",
      "Epoch: 2, Train_Loss: 8.834056854248047, Test_Loss: 2.963279962539673 *\n",
      "Epoch: 2, Train_Loss: 2.933008909225464, Test_Loss: 2.9572219848632812 *\n",
      "Epoch: 2, Train_Loss: 3.0506958961486816, Test_Loss: 2.912309169769287 *\n",
      "Epoch: 2, Train_Loss: 9.121847152709961, Test_Loss: 2.9142255783081055\n",
      "Epoch: 2, Train_Loss: 2.9157567024230957, Test_Loss: 2.909770965576172 *\n",
      "Epoch: 2, Train_Loss: 2.921793222427368, Test_Loss: 2.9411611557006836\n",
      "Epoch: 2, Train_Loss: 2.8980202674865723, Test_Loss: 2.98983097076416\n",
      "Epoch: 2, Train_Loss: 2.8953938484191895, Test_Loss: 2.8989837169647217 *\n",
      "Epoch: 2, Train_Loss: 2.8952205181121826, Test_Loss: 2.914944648742676\n",
      "Epoch: 2, Train_Loss: 2.891977310180664, Test_Loss: 2.9984686374664307\n",
      "Epoch: 2, Train_Loss: 2.8884224891662598, Test_Loss: 3.2175803184509277\n",
      "Epoch: 2, Train_Loss: 2.8856639862060547, Test_Loss: 3.054715871810913 *\n",
      "Epoch: 2, Train_Loss: 2.883568286895752, Test_Loss: 2.88069748878479 *\n",
      "Epoch: 2, Train_Loss: 2.879396677017212, Test_Loss: 2.8719351291656494 *\n",
      "Epoch: 2, Train_Loss: 2.877133369445801, Test_Loss: 2.8692028522491455 *\n",
      "Epoch: 2, Train_Loss: 2.8716084957122803, Test_Loss: 2.8661081790924072 *\n",
      "Epoch: 2, Train_Loss: 2.91328501701355, Test_Loss: 2.8635432720184326 *\n",
      "Epoch: 2, Train_Loss: 2.919776439666748, Test_Loss: 3.594515800476074\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 2.8717775344848633, Test_Loss: 7.831667423248291\n",
      "Epoch: 2, Train_Loss: 2.8606841564178467, Test_Loss: 2.889808416366577 *\n",
      "Epoch: 2, Train_Loss: 2.853320360183716, Test_Loss: 2.8538224697113037 *\n",
      "Epoch: 2, Train_Loss: 2.8456356525421143, Test_Loss: 2.8469982147216797 *\n",
      "Epoch: 2, Train_Loss: 2.843005657196045, Test_Loss: 2.8448920249938965 *\n",
      "Epoch: 2, Train_Loss: 2.8416383266448975, Test_Loss: 2.8435211181640625 *\n",
      "Epoch: 2, Train_Loss: 2.8393867015838623, Test_Loss: 2.839282751083374 *\n",
      "Epoch: 2, Train_Loss: 2.8394923210144043, Test_Loss: 2.844284772872925\n",
      "Epoch: 2, Train_Loss: 2.831704616546631, Test_Loss: 2.838174343109131 *\n",
      "Epoch: 2, Train_Loss: 2.829150676727295, Test_Loss: 2.835798978805542 *\n",
      "Epoch: 2, Train_Loss: 2.8263978958129883, Test_Loss: 2.8307909965515137 *\n",
      "Epoch: 2, Train_Loss: 2.8214781284332275, Test_Loss: 2.8310906887054443\n",
      "Epoch: 2, Train_Loss: 2.833916187286377, Test_Loss: 2.8350472450256348\n",
      "Epoch: 2, Train_Loss: 2.8374624252319336, Test_Loss: 2.8349666595458984 *\n",
      "Epoch: 2, Train_Loss: 2.8242130279541016, Test_Loss: 2.823232889175415 *\n",
      "Epoch: 2, Train_Loss: 2.8156261444091797, Test_Loss: 2.814305305480957 *\n",
      "Epoch: 2, Train_Loss: 6.4416632652282715, Test_Loss: 2.809650182723999 *\n",
      "Epoch: 2, Train_Loss: 8.591146469116211, Test_Loss: 2.808238983154297 *\n",
      "Epoch: 2, Train_Loss: 2.8328959941864014, Test_Loss: 2.8060569763183594 *\n",
      "Epoch: 2, Train_Loss: 2.802978277206421, Test_Loss: 2.8024637699127197 *\n",
      "Epoch: 2, Train_Loss: 2.798731803894043, Test_Loss: 2.7999300956726074 *\n",
      "Epoch: 2, Train_Loss: 2.82249116897583, Test_Loss: 2.7956950664520264 *\n",
      "Epoch: 2, Train_Loss: 2.8390612602233887, Test_Loss: 2.79559063911438 *\n",
      "Epoch: 2, Train_Loss: 2.7936887741088867, Test_Loss: 2.791139841079712 *\n",
      "Epoch: 2, Train_Loss: 2.812509536743164, Test_Loss: 2.788957357406616 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 3.003072738647461, Test_Loss: 2.785813808441162 *\n",
      "Epoch: 2, Train_Loss: 2.9660255908966064, Test_Loss: 2.783277750015259 *\n",
      "Epoch: 2, Train_Loss: 2.8524768352508545, Test_Loss: 2.7783639430999756 *\n",
      "Epoch: 2, Train_Loss: 2.82161545753479, Test_Loss: 2.7775719165802 *\n",
      "Epoch: 2, Train_Loss: 2.8876559734344482, Test_Loss: 2.794259786605835\n",
      "Epoch: 2, Train_Loss: 2.874924898147583, Test_Loss: 2.8186824321746826\n",
      "Epoch: 2, Train_Loss: 2.9139509201049805, Test_Loss: 5.882138252258301\n",
      "Epoch: 2, Train_Loss: 2.890718936920166, Test_Loss: 5.407672882080078 *\n",
      "Epoch: 2, Train_Loss: 2.8557348251342773, Test_Loss: 2.764086961746216 *\n",
      "Epoch: 2, Train_Loss: 2.7538437843322754, Test_Loss: 2.7586777210235596 *\n",
      "Epoch: 2, Train_Loss: 2.8153574466705322, Test_Loss: 2.7830772399902344\n",
      "Epoch: 2, Train_Loss: 2.788264274597168, Test_Loss: 2.7868919372558594\n",
      "Epoch: 2, Train_Loss: 2.7564070224761963, Test_Loss: 2.7710976600646973 *\n",
      "Epoch: 2, Train_Loss: 2.7438864707946777, Test_Loss: 2.8206281661987305\n",
      "Epoch: 2, Train_Loss: 2.7407891750335693, Test_Loss: 2.8869078159332275\n",
      "Epoch: 2, Train_Loss: 2.742208957672119, Test_Loss: 2.737680673599243 *\n",
      "Epoch: 2, Train_Loss: 6.278834342956543, Test_Loss: 2.7774975299835205\n",
      "Epoch: 2, Train_Loss: 4.9685139656066895, Test_Loss: 2.7574918270111084 *\n",
      "Epoch: 2, Train_Loss: 2.7410402297973633, Test_Loss: 2.741617202758789 *\n",
      "Epoch: 2, Train_Loss: 2.739778757095337, Test_Loss: 2.739367961883545 *\n",
      "Epoch: 2, Train_Loss: 2.742654323577881, Test_Loss: 2.7913787364959717\n",
      "Epoch: 2, Train_Loss: 2.735959529876709, Test_Loss: 2.775071620941162 *\n",
      "Epoch: 2, Train_Loss: 2.7240490913391113, Test_Loss: 2.850682258605957\n",
      "Epoch: 2, Train_Loss: 2.7197189331054688, Test_Loss: 2.873899459838867\n",
      "Epoch: 2, Train_Loss: 2.7269327640533447, Test_Loss: 2.71842885017395 *\n",
      "Epoch: 2, Train_Loss: 2.7403194904327393, Test_Loss: 2.714015483856201 *\n",
      "Epoch: 2, Train_Loss: 2.713207483291626, Test_Loss: 2.7072596549987793 *\n",
      "Epoch: 2, Train_Loss: 2.706970691680908, Test_Loss: 2.7029385566711426 *\n",
      "Epoch: 2, Train_Loss: 2.704641580581665, Test_Loss: 2.6999640464782715 *\n",
      "Epoch: 2, Train_Loss: 2.717623472213745, Test_Loss: 2.6970956325531006 *\n",
      "Epoch: 2, Train_Loss: 2.698941946029663, Test_Loss: 2.694148063659668 *\n",
      "Epoch: 2, Train_Loss: 2.692678451538086, Test_Loss: 2.691016912460327 *\n",
      "Epoch: 2, Train_Loss: 2.6929099559783936, Test_Loss: 2.69736647605896\n",
      "Epoch: 2, Train_Loss: 2.756304979324341, Test_Loss: 2.6907169818878174 *\n",
      "Epoch: 2, Train_Loss: 2.7428159713745117, Test_Loss: 2.6875269412994385 *\n",
      "Epoch: 2, Train_Loss: 2.6793761253356934, Test_Loss: 2.6824560165405273 *\n",
      "Epoch: 2, Train_Loss: 2.680314540863037, Test_Loss: 2.694087505340576\n",
      "Epoch: 2, Train_Loss: 2.702139377593994, Test_Loss: 2.719247579574585\n",
      "Epoch: 2, Train_Loss: 2.7784340381622314, Test_Loss: 2.713711738586426 *\n",
      "Epoch: 2, Train_Loss: 2.7402689456939697, Test_Loss: 3.162600517272949\n",
      "Epoch: 2, Train_Loss: 2.7549314498901367, Test_Loss: 3.073328733444214 *\n",
      "Epoch: 2, Train_Loss: 2.696720838546753, Test_Loss: 2.7550792694091797 *\n",
      "Epoch: 2, Train_Loss: 2.73972487449646, Test_Loss: 2.6744325160980225 *\n",
      "Epoch: 2, Train_Loss: 2.724104642868042, Test_Loss: 2.668074131011963 *\n",
      "Epoch: 2, Train_Loss: 2.704669713973999, Test_Loss: 2.730548858642578\n",
      "Epoch: 2, Train_Loss: 2.7101290225982666, Test_Loss: 3.1165049076080322\n",
      "Epoch: 2, Train_Loss: 2.7000012397766113, Test_Loss: 4.2280449867248535\n",
      "Epoch: 2, Train_Loss: 2.660573720932007, Test_Loss: 3.4117541313171387 *\n",
      "Epoch: 2, Train_Loss: 2.644397497177124, Test_Loss: 2.7065799236297607 *\n",
      "Epoch: 2, Train_Loss: 2.642826795578003, Test_Loss: 2.6546237468719482 *\n",
      "Epoch: 2, Train_Loss: 2.6399600505828857, Test_Loss: 2.6444432735443115 *\n",
      "Epoch: 2, Train_Loss: 2.6370956897735596, Test_Loss: 2.6366467475891113 *\n",
      "Epoch: 2, Train_Loss: 2.632129430770874, Test_Loss: 2.635570764541626 *\n",
      "Epoch: 2, Train_Loss: 5.966131210327148, Test_Loss: 2.678797483444214\n",
      "Epoch: 2, Train_Loss: 4.327153205871582, Test_Loss: 2.7095773220062256\n",
      "Epoch: 2, Train_Loss: 2.622410297393799, Test_Loss: 2.62381649017334 *\n",
      "Epoch: 2, Train_Loss: 2.653722047805786, Test_Loss: 2.66579270362854\n",
      "Epoch: 2, Train_Loss: 2.6320042610168457, Test_Loss: 2.733790636062622\n",
      "Epoch: 2, Train_Loss: 2.6120944023132324, Test_Loss: 2.930206298828125\n",
      "Epoch: 2, Train_Loss: 2.6125283241271973, Test_Loss: 2.779546022415161 *\n",
      "Epoch: 2, Train_Loss: 2.6071481704711914, Test_Loss: 2.606959342956543 *\n",
      "Epoch: 2, Train_Loss: 2.6053051948547363, Test_Loss: 2.601827621459961 *\n",
      "Epoch: 2, Train_Loss: 2.6009762287139893, Test_Loss: 2.5990729331970215 *\n",
      "Epoch: 2, Train_Loss: 2.638955593109131, Test_Loss: 2.59624981880188 *\n",
      "Epoch: 2, Train_Loss: 2.73342227935791, Test_Loss: 2.5963287353515625\n",
      "Epoch: 2, Train_Loss: 2.721879243850708, Test_Loss: 4.468338489532471\n",
      "Epoch: 2, Train_Loss: 2.733790159225464, Test_Loss: 6.46196174621582\n",
      "Epoch: 2, Train_Loss: 2.623455286026001, Test_Loss: 2.594499111175537 *\n",
      "Epoch: 2, Train_Loss: 2.591935157775879, Test_Loss: 2.5834524631500244 *\n",
      "Epoch: 2, Train_Loss: 2.7372941970825195, Test_Loss: 2.578460454940796 *\n",
      "Epoch: 2, Train_Loss: 2.738771677017212, Test_Loss: 2.577118158340454 *\n",
      "Epoch: 2, Train_Loss: 2.7358078956604004, Test_Loss: 2.5755205154418945 *\n",
      "Epoch: 2, Train_Loss: 2.6225812435150146, Test_Loss: 2.5747439861297607 *\n",
      "Epoch: 2, Train_Loss: 2.568708658218384, Test_Loss: 2.575774669647217\n",
      "Epoch: 2, Train_Loss: 2.5700948238372803, Test_Loss: 2.5736098289489746 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 2.5615994930267334, Test_Loss: 2.5715627670288086 *\n",
      "Epoch: 2, Train_Loss: 2.559830904006958, Test_Loss: 2.563776731491089 *\n",
      "Epoch: 2, Train_Loss: 2.557812452316284, Test_Loss: 2.5654098987579346\n",
      "Epoch: 2, Train_Loss: 2.553529977798462, Test_Loss: 2.572282075881958\n",
      "Epoch: 2, Train_Loss: 2.5566585063934326, Test_Loss: 2.5681540966033936 *\n",
      "Epoch: 2, Train_Loss: 2.5504941940307617, Test_Loss: 2.5570902824401855 *\n",
      "Epoch: 2, Train_Loss: 2.5459413528442383, Test_Loss: 2.548412561416626 *\n",
      "Epoch: 2, Train_Loss: 2.6089165210723877, Test_Loss: 2.5472118854522705 *\n",
      "Epoch: 2, Train_Loss: 2.7032840251922607, Test_Loss: 2.543815851211548 *\n",
      "Epoch: 2, Train_Loss: 2.6855921745300293, Test_Loss: 2.5420777797698975 *\n",
      "Epoch: 2, Train_Loss: 2.62048602104187, Test_Loss: 2.5414860248565674 *\n",
      "Epoch: 2, Train_Loss: 2.686169385910034, Test_Loss: 2.534058094024658 *\n",
      "Epoch: 2, Train_Loss: 2.731217622756958, Test_Loss: 2.5328328609466553 *\n",
      "Epoch: 2, Train_Loss: 2.5449142456054688, Test_Loss: 2.533921480178833\n",
      "Epoch: 2, Train_Loss: 2.70847487449646, Test_Loss: 2.5272154808044434 *\n",
      "Epoch: 2, Train_Loss: 2.6650569438934326, Test_Loss: 2.527683734893799\n",
      "Epoch: 2, Train_Loss: 2.8159914016723633, Test_Loss: 2.523303270339966 *\n",
      "Epoch: 2, Train_Loss: 2.532191276550293, Test_Loss: 2.521064519882202 *\n",
      "Epoch: 2, Train_Loss: 3.0837960243225098, Test_Loss: 2.5177173614501953 *\n",
      "Epoch: 2, Train_Loss: 5.3230671882629395, Test_Loss: 2.5160367488861084 *\n",
      "Epoch: 2, Train_Loss: 2.560262680053711, Test_Loss: 2.559929847717285\n",
      "Epoch: 2, Train_Loss: 2.5385565757751465, Test_Loss: 2.5314433574676514 *\n",
      "Epoch: 2, Train_Loss: 2.540313959121704, Test_Loss: 6.707276344299316\n",
      "Epoch: 2, Train_Loss: 2.5396978855133057, Test_Loss: 4.038986682891846 *\n",
      "Epoch: 2, Train_Loss: 2.499554395675659, Test_Loss: 2.502234935760498 *\n",
      "Epoch: 2, Train_Loss: 2.502042770385742, Test_Loss: 2.503326177597046\n",
      "Epoch: 2, Train_Loss: 2.631324052810669, Test_Loss: 2.526963233947754\n",
      "Epoch: 2, Train_Loss: 2.63393235206604, Test_Loss: 2.5385825634002686\n",
      "Epoch: 2, Train_Loss: 2.627819776535034, Test_Loss: 2.4961206912994385 *\n",
      "Epoch: 2, Train_Loss: 2.5956199169158936, Test_Loss: 2.5859434604644775\n",
      "Epoch: 2, Train_Loss: 2.5974385738372803, Test_Loss: 2.6011435985565186\n",
      "Epoch: 2, Train_Loss: 2.5227389335632324, Test_Loss: 2.4791336059570312 *\n",
      "Epoch: 2, Train_Loss: 2.5213894844055176, Test_Loss: 2.5320870876312256\n",
      "Epoch: 2, Train_Loss: 2.4795281887054443, Test_Loss: 2.4852256774902344 *\n",
      "Epoch: 2, Train_Loss: 2.4847216606140137, Test_Loss: 2.487872362136841\n",
      "Epoch: 2, Train_Loss: 2.4726343154907227, Test_Loss: 2.4732370376586914 *\n",
      "Epoch: 2, Train_Loss: 2.463562250137329, Test_Loss: 2.5603103637695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 2.5141165256500244, Test_Loss: 2.50290846824646 *\n",
      "Epoch: 2, Train_Loss: 2.529386281967163, Test_Loss: 2.606602907180786\n",
      "Epoch: 2, Train_Loss: 2.5014808177948, Test_Loss: 2.5655391216278076 *\n",
      "Epoch: 2, Train_Loss: 2.4521825313568115, Test_Loss: 2.4713027477264404 *\n",
      "Epoch: 2, Train_Loss: 2.4502477645874023, Test_Loss: 2.45811128616333 *\n",
      "Epoch: 2, Train_Loss: 2.4471497535705566, Test_Loss: 2.4566502571105957 *\n",
      "Epoch: 2, Train_Loss: 2.444173574447632, Test_Loss: 2.453275442123413 *\n",
      "Epoch: 2, Train_Loss: 2.4428162574768066, Test_Loss: 2.4512617588043213 *\n",
      "Epoch: 2, Train_Loss: 2.4380223751068115, Test_Loss: 2.4494943618774414 *\n",
      "Epoch: 2, Train_Loss: 2.435499668121338, Test_Loss: 2.4465808868408203 *\n",
      "Epoch: 2, Train_Loss: 2.4335639476776123, Test_Loss: 2.4411401748657227 *\n",
      "Epoch: 2, Train_Loss: 2.430101156234741, Test_Loss: 2.4511749744415283\n",
      "Epoch: 2, Train_Loss: 2.4274919033050537, Test_Loss: 2.4476401805877686 *\n",
      "Epoch: 2, Train_Loss: 2.428826332092285, Test_Loss: 2.4401965141296387 *\n",
      "Epoch: 2, Train_Loss: 2.429577589035034, Test_Loss: 2.43217396736145 *\n",
      "Epoch: 2, Train_Loss: 2.4270212650299072, Test_Loss: 2.469572067260742\n",
      "Epoch: 2, Train_Loss: 2.4336605072021484, Test_Loss: 2.4641990661621094 *\n",
      "Epoch: 2, Train_Loss: 2.415973663330078, Test_Loss: 2.5826516151428223\n",
      "Epoch: 2, Train_Loss: 2.4136147499084473, Test_Loss: 2.9638214111328125\n",
      "Epoch: 2, Train_Loss: 2.4099316596984863, Test_Loss: 2.7678916454315186 *\n",
      "Epoch: 2, Train_Loss: 2.4065961837768555, Test_Loss: 2.5104403495788574 *\n",
      "Epoch: 2, Train_Loss: 2.413841962814331, Test_Loss: 2.4355132579803467 *\n",
      "Epoch: 2, Train_Loss: 2.4090235233306885, Test_Loss: 2.414665460586548 *\n",
      "Epoch: 2, Train_Loss: 2.399832248687744, Test_Loss: 2.4859673976898193\n",
      "Epoch: 2, Train_Loss: 2.397207021713257, Test_Loss: 3.0166754722595215\n",
      "Epoch: 2, Train_Loss: 2.402452230453491, Test_Loss: 3.892575740814209\n",
      "Epoch: 2, Train_Loss: 2.4912664890289307, Test_Loss: 2.8595759868621826 *\n",
      "Epoch: 2, Train_Loss: 2.4088134765625, Test_Loss: 2.484689712524414 *\n",
      "Epoch: 2, Train_Loss: 2.423156261444092, Test_Loss: 2.392432689666748 *\n",
      "Epoch: 2, Train_Loss: 2.3849401473999023, Test_Loss: 2.3945274353027344\n",
      "Epoch: 2, Train_Loss: 2.397569417953491, Test_Loss: 2.3883485794067383 *\n",
      "Epoch: 2, Train_Loss: 2.415844202041626, Test_Loss: 2.3894731998443604\n",
      "Epoch: 2, Train_Loss: 2.378309726715088, Test_Loss: 2.427741765975952\n",
      "Epoch: 2, Train_Loss: 2.4034645557403564, Test_Loss: 2.439929246902466\n",
      "Epoch: 2, Train_Loss: 2.4121134281158447, Test_Loss: 2.3793587684631348 *\n",
      "Epoch: 2, Train_Loss: 2.4544012546539307, Test_Loss: 2.4485280513763428\n",
      "Epoch: 2, Train_Loss: 2.495401382446289, Test_Loss: 2.646735429763794\n",
      "Epoch: 2, Train_Loss: 2.464016914367676, Test_Loss: 2.523491621017456 *\n",
      "Epoch: 2, Train_Loss: 2.409125804901123, Test_Loss: 2.536497116088867\n",
      "Epoch: 2, Train_Loss: 2.3608412742614746, Test_Loss: 2.3621578216552734 *\n",
      "Epoch: 2, Train_Loss: 2.389939546585083, Test_Loss: 2.359703302383423 *\n",
      "Epoch: 2, Train_Loss: 2.3554348945617676, Test_Loss: 2.357246160507202 *\n",
      "Epoch: 2, Train_Loss: 2.3624844551086426, Test_Loss: 2.354964256286621 *\n",
      "Epoch: 2, Train_Loss: 2.3559041023254395, Test_Loss: 2.35904860496521\n",
      "Epoch: 2, Train_Loss: 2.3568646907806396, Test_Loss: 5.553897380828857\n",
      "Epoch: 2, Train_Loss: 2.414813995361328, Test_Loss: 4.7342681884765625 *\n",
      "Epoch: 2, Train_Loss: 2.360866069793701, Test_Loss: 2.3480582237243652 *\n",
      "Epoch: 2, Train_Loss: 2.427650213241577, Test_Loss: 2.3399291038513184 *\n",
      "Epoch: 2, Train_Loss: 2.3502824306488037, Test_Loss: 2.336085557937622 *\n",
      "Epoch: 2, Train_Loss: 2.3579483032226562, Test_Loss: 2.338628053665161\n",
      "Epoch: 2, Train_Loss: 2.3354148864746094, Test_Loss: 2.3320388793945312 *\n",
      "Epoch: 2, Train_Loss: 2.571079730987549, Test_Loss: 2.333547830581665\n",
      "Epoch: 2, Train_Loss: 2.4012746810913086, Test_Loss: 2.3278839588165283 *\n",
      "Epoch: 2, Train_Loss: 2.3352904319763184, Test_Loss: 2.3285279273986816\n",
      "Epoch: 2, Train_Loss: 2.336660861968994, Test_Loss: 2.325934886932373 *\n",
      "Epoch: 2, Train_Loss: 2.3188388347625732, Test_Loss: 2.321366786956787 *\n",
      "Epoch: 2, Train_Loss: 2.3177223205566406, Test_Loss: 2.32423996925354\n",
      "Epoch: 2, Train_Loss: 2.314335584640503, Test_Loss: 2.3380396366119385\n",
      "Epoch: 2, Train_Loss: 2.3184165954589844, Test_Loss: 2.3305044174194336 *\n",
      "Epoch: 2, Train_Loss: 2.331629514694214, Test_Loss: 2.3123159408569336 *\n",
      "Epoch: 2, Train_Loss: 2.3410634994506836, Test_Loss: 2.3062314987182617 *\n",
      "Epoch: 2, Train_Loss: 2.3232905864715576, Test_Loss: 2.3067808151245117\n",
      "Epoch: 2, Train_Loss: 2.324000835418701, Test_Loss: 2.3021843433380127 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 2.326221227645874, Test_Loss: 2.2997217178344727 *\n",
      "Epoch: 2, Train_Loss: 2.298391819000244, Test_Loss: 2.2991232872009277 *\n",
      "Epoch: 2, Train_Loss: 2.2949838638305664, Test_Loss: 2.2920238971710205 *\n",
      "Epoch: 2, Train_Loss: 2.294146776199341, Test_Loss: 2.291599750518799 *\n",
      "Epoch: 2, Train_Loss: 2.311220645904541, Test_Loss: 2.2938807010650635\n",
      "Epoch: 2, Train_Loss: 2.3142662048339844, Test_Loss: 2.2879433631896973 *\n",
      "Epoch: 2, Train_Loss: 2.28629994392395, Test_Loss: 2.2864742279052734 *\n",
      "Epoch: 2, Train_Loss: 2.3163342475891113, Test_Loss: 2.282059669494629 *\n",
      "Epoch: 2, Train_Loss: 2.342526912689209, Test_Loss: 2.2798807621002197 *\n",
      "Epoch: 2, Train_Loss: 2.339887857437134, Test_Loss: 2.2786340713500977 *\n",
      "Epoch: 2, Train_Loss: 2.273092269897461, Test_Loss: 2.276752471923828 *\n",
      "Epoch: 2, Train_Loss: 2.293839454650879, Test_Loss: 2.315570831298828\n",
      "Epoch: 2, Train_Loss: 2.27518892288208, Test_Loss: 2.2967538833618164 *\n",
      "Epoch: 2, Train_Loss: 2.286050319671631, Test_Loss: 7.517962455749512\n",
      "Epoch: 2, Train_Loss: 2.2658729553222656, Test_Loss: 2.5908122062683105 *\n",
      "Epoch: 2, Train_Loss: 2.2846341133117676, Test_Loss: 2.2612671852111816 *\n",
      "Epoch: 2, Train_Loss: 2.3215441703796387, Test_Loss: 2.276254177093506\n",
      "Epoch: 2, Train_Loss: 4.615545272827148, Test_Loss: 2.305452346801758\n",
      "Epoch: 2, Train_Loss: 5.3453826904296875, Test_Loss: 2.3114356994628906\n",
      "Epoch: 2, Train_Loss: 2.270648717880249, Test_Loss: 2.256242275238037 *\n",
      "Epoch: 2, Train_Loss: 2.2484312057495117, Test_Loss: 2.3596243858337402\n",
      "Epoch: 2, Train_Loss: 2.347316026687622, Test_Loss: 2.322667121887207 *\n",
      "Epoch: 2, Train_Loss: 2.4022774696350098, Test_Loss: 2.2433769702911377 *\n",
      "Epoch: 2, Train_Loss: 2.267012119293213, Test_Loss: 2.2908833026885986\n",
      "Epoch: 2, Train_Loss: 2.2414777278900146, Test_Loss: 2.2479472160339355 *\n",
      "Epoch: 2, Train_Loss: 2.2697882652282715, Test_Loss: 2.24973464012146\n",
      "Epoch: 2, Train_Loss: 2.2903316020965576, Test_Loss: 2.2437667846679688 *\n",
      "Epoch: 2, Train_Loss: 2.243833541870117, Test_Loss: 2.3651418685913086\n",
      "Epoch: 2, Train_Loss: 2.2428195476531982, Test_Loss: 2.2630703449249268 *\n",
      "Epoch: 2, Train_Loss: 3.405646800994873, Test_Loss: 2.3458328247070312\n",
      "Epoch: 2, Train_Loss: 3.604797840118408, Test_Loss: 2.277759552001953 *\n",
      "Epoch: 2, Train_Loss: 2.4733574390411377, Test_Loss: 2.2762677669525146 *\n",
      "Epoch: 2, Train_Loss: 2.278106451034546, Test_Loss: 2.2635581493377686 *\n",
      "Epoch: 2, Train_Loss: 3.7766361236572266, Test_Loss: 2.260862112045288 *\n",
      "Epoch: 2, Train_Loss: 4.32908821105957, Test_Loss: 2.2516870498657227 *\n",
      "Epoch: 2, Train_Loss: 2.3012123107910156, Test_Loss: 2.240342855453491 *\n",
      "Epoch: 2, Train_Loss: 2.249955654144287, Test_Loss: 2.232168674468994 *\n",
      "Epoch: 2, Train_Loss: 2.338604211807251, Test_Loss: 2.223985195159912 *\n",
      "Epoch: 2, Train_Loss: 3.9318461418151855, Test_Loss: 2.215421438217163 *\n",
      "Epoch: 2, Train_Loss: 3.5616068840026855, Test_Loss: 2.226919412612915\n",
      "Epoch: 2, Train_Loss: 2.2197468280792236, Test_Loss: 2.2253997325897217 *\n",
      "Epoch: 2, Train_Loss: 2.2280898094177246, Test_Loss: 2.206601142883301 *\n",
      "Epoch: 2, Train_Loss: 2.2257444858551025, Test_Loss: 2.2079720497131348\n",
      "Epoch: 2, Train_Loss: 2.884523868560791, Test_Loss: 2.2627127170562744\n",
      "Epoch: 2, Train_Loss: 2.215015411376953, Test_Loss: 2.2167916297912598 *\n",
      "Epoch: 2, Train_Loss: 2.239727735519409, Test_Loss: 2.461409091949463\n",
      "Epoch: 2, Train_Loss: 2.203646421432495, Test_Loss: 2.7703640460968018\n",
      "Epoch: 2, Train_Loss: 2.3370354175567627, Test_Loss: 2.461052894592285 *\n",
      "Epoch: 2, Train_Loss: 2.3213155269622803, Test_Loss: 2.2689616680145264 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 2.459071159362793, Test_Loss: 2.2120261192321777 *\n",
      "Epoch: 2, Train_Loss: 2.483790397644043, Test_Loss: 2.1872060298919678 *\n",
      "Epoch: 2, Train_Loss: 2.2597625255584717, Test_Loss: 2.271826982498169\n",
      "Epoch: 2, Train_Loss: 2.3101212978363037, Test_Loss: 2.9193005561828613\n",
      "Epoch: 2, Train_Loss: 2.389533758163452, Test_Loss: 3.5502007007598877\n",
      "Epoch: 3, Train_Loss: 2.634864091873169, Test_Loss: 2.401892900466919 *\n",
      "Epoch: 3, Train_Loss: 2.519993305206299, Test_Loss: 2.2658767700195312 *\n",
      "Epoch: 3, Train_Loss: 2.190070867538452, Test_Loss: 2.163952589035034 *\n",
      "Epoch: 3, Train_Loss: 2.3025407791137695, Test_Loss: 2.1684083938598633\n",
      "Epoch: 3, Train_Loss: 2.30753755569458, Test_Loss: 2.1622724533081055 *\n",
      "Epoch: 3, Train_Loss: 2.166210651397705, Test_Loss: 2.172966718673706\n",
      "Epoch: 3, Train_Loss: 2.168769121170044, Test_Loss: 2.1796841621398926\n",
      "Epoch: 3, Train_Loss: 2.1517579555511475, Test_Loss: 2.1882400512695312\n",
      "Epoch: 3, Train_Loss: 2.1517090797424316, Test_Loss: 2.1543524265289307 *\n",
      "Epoch: 3, Train_Loss: 2.1550447940826416, Test_Loss: 2.261688232421875\n",
      "Epoch: 3, Train_Loss: 2.1512343883514404, Test_Loss: 2.5283203125\n",
      "Epoch: 3, Train_Loss: 2.226095676422119, Test_Loss: 2.24973464012146 *\n",
      "Epoch: 3, Train_Loss: 2.2027435302734375, Test_Loss: 2.3281469345092773\n",
      "Epoch: 3, Train_Loss: 2.2443625926971436, Test_Loss: 2.1510605812072754 *\n",
      "Epoch: 3, Train_Loss: 2.250408172607422, Test_Loss: 2.149461269378662 *\n",
      "Epoch: 3, Train_Loss: 2.6205596923828125, Test_Loss: 2.146859884262085 *\n",
      "Epoch: 3, Train_Loss: 2.138592004776001, Test_Loss: 2.1452391147613525 *\n",
      "Epoch: 3, Train_Loss: 2.160256862640381, Test_Loss: 2.1592049598693848\n",
      "Epoch: 3, Train_Loss: 2.3516499996185303, Test_Loss: 6.538875579833984\n",
      "Epoch: 3, Train_Loss: 2.6457009315490723, Test_Loss: 3.083794593811035 *\n",
      "Epoch: 3, Train_Loss: 2.3339552879333496, Test_Loss: 2.1325557231903076 *\n",
      "Epoch: 3, Train_Loss: 2.1213009357452393, Test_Loss: 2.1228115558624268 *\n",
      "Epoch: 3, Train_Loss: 2.436957836151123, Test_Loss: 2.1224513053894043 *\n",
      "Epoch: 3, Train_Loss: 2.7826335430145264, Test_Loss: 2.128326416015625\n",
      "Epoch: 3, Train_Loss: 2.639976739883423, Test_Loss: 2.116548538208008 *\n",
      "Epoch: 3, Train_Loss: 2.151371479034424, Test_Loss: 2.117609977722168\n",
      "Epoch: 3, Train_Loss: 2.1279847621917725, Test_Loss: 2.111013412475586 *\n",
      "Epoch: 3, Train_Loss: 2.153991222381592, Test_Loss: 2.1119515895843506\n",
      "Epoch: 3, Train_Loss: 3.510690689086914, Test_Loss: 2.1108956336975098 *\n",
      "Epoch: 3, Train_Loss: 2.8857781887054443, Test_Loss: 2.1112687587738037\n",
      "Epoch: 3, Train_Loss: 2.104372024536133, Test_Loss: 2.1075456142425537 *\n",
      "Epoch: 3, Train_Loss: 2.117812395095825, Test_Loss: 2.1255903244018555\n",
      "Epoch: 3, Train_Loss: 2.098273515701294, Test_Loss: 2.113022804260254 *\n",
      "Epoch: 3, Train_Loss: 2.2649998664855957, Test_Loss: 2.0972063541412354 *\n",
      "Epoch: 3, Train_Loss: 2.4407787322998047, Test_Loss: 2.0918431282043457 *\n",
      "Epoch: 3, Train_Loss: 2.1137499809265137, Test_Loss: 2.0922865867614746\n",
      "Epoch: 3, Train_Loss: 2.3967409133911133, Test_Loss: 2.0892274379730225 *\n",
      "Epoch: 3, Train_Loss: 2.110208749771118, Test_Loss: 2.0858523845672607 *\n",
      "Epoch: 3, Train_Loss: 2.125837802886963, Test_Loss: 2.0842652320861816 *\n",
      "Epoch: 3, Train_Loss: 2.170790195465088, Test_Loss: 2.0809407234191895 *\n",
      "Epoch: 3, Train_Loss: 2.4169609546661377, Test_Loss: 2.081533193588257\n",
      "Epoch: 3, Train_Loss: 2.20967698097229, Test_Loss: 2.082836627960205\n",
      "Epoch: 3, Train_Loss: 2.116783618927002, Test_Loss: 2.0783448219299316 *\n",
      "Epoch: 3, Train_Loss: 2.2106971740722656, Test_Loss: 2.075465202331543 *\n",
      "Epoch: 3, Train_Loss: 2.2702677249908447, Test_Loss: 2.071553945541382 *\n",
      "Epoch: 3, Train_Loss: 2.0824978351593018, Test_Loss: 2.070401906967163 *\n",
      "Epoch: 3, Train_Loss: 2.1132144927978516, Test_Loss: 2.0690178871154785 *\n",
      "Epoch: 3, Train_Loss: 2.071042776107788, Test_Loss: 2.068533420562744 *\n",
      "Epoch: 3, Train_Loss: 2.091597318649292, Test_Loss: 2.1205077171325684\n",
      "Epoch: 3, Train_Loss: 2.1009976863861084, Test_Loss: 2.2491092681884766\n",
      "Epoch: 3, Train_Loss: 2.689861536026001, Test_Loss: 7.224312782287598\n",
      "Epoch: 3, Train_Loss: 2.226308822631836, Test_Loss: 2.0628836154937744 *\n",
      "Epoch: 3, Train_Loss: 2.860185146331787, Test_Loss: 2.0619280338287354 *\n",
      "Epoch: 3, Train_Loss: 2.6510870456695557, Test_Loss: 2.093449592590332\n",
      "Epoch: 3, Train_Loss: 2.3053181171417236, Test_Loss: 2.0921216011047363 *\n",
      "Epoch: 3, Train_Loss: 2.341590404510498, Test_Loss: 2.1025075912475586\n",
      "Epoch: 3, Train_Loss: 2.0850369930267334, Test_Loss: 2.0490546226501465 *\n",
      "Epoch: 3, Train_Loss: 2.0535531044006348, Test_Loss: 2.1365766525268555\n",
      "Epoch: 3, Train_Loss: 2.0603928565979004, Test_Loss: 2.0715646743774414 *\n",
      "Epoch: 3, Train_Loss: 2.1937460899353027, Test_Loss: 2.0561366081237793 *\n",
      "Epoch: 3, Train_Loss: 2.4314193725585938, Test_Loss: 2.054919958114624 *\n",
      "Epoch: 3, Train_Loss: 2.4188520908355713, Test_Loss: 2.155139207839966\n",
      "Epoch: 3, Train_Loss: 3.5945348739624023, Test_Loss: 2.059767484664917 *\n",
      "Epoch: 3, Train_Loss: 3.0907998085021973, Test_Loss: 2.116744041442871\n",
      "Epoch: 3, Train_Loss: 2.8582680225372314, Test_Loss: 2.1255667209625244\n",
      "Epoch: 3, Train_Loss: 2.2885217666625977, Test_Loss: 2.082632064819336 *\n",
      "Epoch: 3, Train_Loss: 2.027282476425171, Test_Loss: 2.049173593521118 *\n",
      "Epoch: 3, Train_Loss: 2.10320782661438, Test_Loss: 2.0667357444763184\n",
      "Epoch: 3, Train_Loss: 3.009634494781494, Test_Loss: 2.0939178466796875\n",
      "Epoch: 3, Train_Loss: 3.1323928833007812, Test_Loss: 2.024658203125 *\n",
      "Epoch: 3, Train_Loss: 2.0672271251678467, Test_Loss: 2.019583225250244 *\n",
      "Epoch: 3, Train_Loss: 2.077772617340088, Test_Loss: 2.0184922218322754 *\n",
      "Epoch: 3, Train_Loss: 2.0449936389923096, Test_Loss: 2.0168864727020264 *\n",
      "Epoch: 3, Train_Loss: 2.328415870666504, Test_Loss: 2.013697862625122 *\n",
      "Epoch: 3, Train_Loss: 2.1843178272247314, Test_Loss: 2.0113601684570312 *\n",
      "Epoch: 3, Train_Loss: 2.905803918838501, Test_Loss: 2.0110974311828613 *\n",
      "Epoch: 3, Train_Loss: 2.858707904815674, Test_Loss: 2.009680986404419 *\n",
      "Epoch: 3, Train_Loss: 2.230170965194702, Test_Loss: 2.012834072113037\n",
      "Epoch: 3, Train_Loss: 2.010035991668701, Test_Loss: 2.006885051727295 *\n",
      "Epoch: 3, Train_Loss: 2.006042242050171, Test_Loss: 2.0047976970672607 *\n",
      "Epoch: 3, Train_Loss: 2.001864433288574, Test_Loss: 2.0425448417663574\n",
      "Epoch: 3, Train_Loss: 2.031656503677368, Test_Loss: 2.008897066116333 *\n",
      "Epoch: 3, Train_Loss: 1.9970825910568237, Test_Loss: 2.2970476150512695\n",
      "Epoch: 3, Train_Loss: 2.0121939182281494, Test_Loss: 2.5197560787200928\n",
      "Epoch: 3, Train_Loss: 19.236351013183594, Test_Loss: 2.1710760593414307 *\n",
      "Epoch: 3, Train_Loss: 2.000377655029297, Test_Loss: 2.026909589767456 *\n",
      "Epoch: 3, Train_Loss: 4.031724452972412, Test_Loss: 2.003061294555664 *\n",
      "Epoch: 3, Train_Loss: 4.161844253540039, Test_Loss: 2.0076451301574707\n",
      "Epoch: 3, Train_Loss: 1.9879951477050781, Test_Loss: 2.088372230529785\n",
      "Epoch: 3, Train_Loss: 2.0526375770568848, Test_Loss: 2.6943764686584473\n",
      "Epoch: 3, Train_Loss: 5.668068885803223, Test_Loss: 3.041473388671875\n",
      "Epoch: 3, Train_Loss: 10.56776237487793, Test_Loss: 2.0809271335601807 *\n",
      "Epoch: 3, Train_Loss: 2.0417776107788086, Test_Loss: 2.039430856704712 *\n",
      "Epoch: 3, Train_Loss: 1.9947878122329712, Test_Loss: 1.9755054712295532 *\n",
      "Epoch: 3, Train_Loss: 8.042877197265625, Test_Loss: 1.9999788999557495\n",
      "Epoch: 3, Train_Loss: 2.007295608520508, Test_Loss: 2.002437114715576\n",
      "Epoch: 3, Train_Loss: 2.0169873237609863, Test_Loss: 2.0310869216918945\n",
      "Epoch: 3, Train_Loss: 1.979178786277771, Test_Loss: 2.09138822555542\n",
      "Epoch: 3, Train_Loss: 1.9782975912094116, Test_Loss: 2.048398971557617 *\n",
      "Epoch: 3, Train_Loss: 1.985265851020813, Test_Loss: 2.0254440307617188 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 1.9861040115356445, Test_Loss: 1.9929614067077637 *\n",
      "Epoch: 3, Train_Loss: 1.9923292398452759, Test_Loss: 2.2877073287963867\n",
      "Epoch: 3, Train_Loss: 1.9801816940307617, Test_Loss: 2.007312536239624 *\n",
      "Epoch: 3, Train_Loss: 1.9705722332000732, Test_Loss: 2.036888360977173\n",
      "Epoch: 3, Train_Loss: 1.9794175624847412, Test_Loss: 1.9540210962295532 *\n",
      "Epoch: 3, Train_Loss: 1.9686758518218994, Test_Loss: 1.9499338865280151 *\n",
      "Epoch: 3, Train_Loss: 1.960479974746704, Test_Loss: 1.9463483095169067 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 1.992725133895874, Test_Loss: 1.9435395002365112 *\n",
      "Epoch: 3, Train_Loss: 2.0030081272125244, Test_Loss: 1.9782472848892212\n",
      "Epoch: 3, Train_Loss: 1.9658690690994263, Test_Loss: 7.564077854156494\n",
      "Epoch: 3, Train_Loss: 1.9472250938415527, Test_Loss: 2.2220687866210938 *\n",
      "Epoch: 3, Train_Loss: 1.9426782131195068, Test_Loss: 1.9538978338241577 *\n",
      "Epoch: 3, Train_Loss: 1.9347447156906128, Test_Loss: 1.946244239807129 *\n",
      "Epoch: 3, Train_Loss: 1.9325840473175049, Test_Loss: 1.9359556436538696 *\n",
      "Epoch: 3, Train_Loss: 1.9328727722167969, Test_Loss: 1.938597321510315\n",
      "Epoch: 3, Train_Loss: 1.9309701919555664, Test_Loss: 1.9392368793487549\n",
      "Epoch: 3, Train_Loss: 1.9319648742675781, Test_Loss: 1.960031509399414\n",
      "Epoch: 3, Train_Loss: 1.9265937805175781, Test_Loss: 1.9438470602035522 *\n",
      "Epoch: 3, Train_Loss: 1.924780249595642, Test_Loss: 1.945462703704834\n",
      "Epoch: 3, Train_Loss: 1.9225785732269287, Test_Loss: 1.9464426040649414\n",
      "Epoch: 3, Train_Loss: 1.9211194515228271, Test_Loss: 1.9482059478759766\n",
      "Epoch: 3, Train_Loss: 1.9317911863327026, Test_Loss: 1.9223709106445312 *\n",
      "Epoch: 3, Train_Loss: 1.944269061088562, Test_Loss: 1.9222321510314941 *\n",
      "Epoch: 3, Train_Loss: 1.9329760074615479, Test_Loss: 1.9278018474578857\n",
      "Epoch: 3, Train_Loss: 1.9276237487792969, Test_Loss: 1.9332215785980225\n",
      "Epoch: 3, Train_Loss: 2.9354262351989746, Test_Loss: 1.9152981042861938 *\n",
      "Epoch: 3, Train_Loss: 10.21491527557373, Test_Loss: 1.920898199081421\n",
      "Epoch: 3, Train_Loss: 1.9656051397323608, Test_Loss: 1.9160990715026855 *\n",
      "Epoch: 3, Train_Loss: 1.9096262454986572, Test_Loss: 1.921112060546875\n",
      "Epoch: 3, Train_Loss: 1.9194023609161377, Test_Loss: 1.9295077323913574\n",
      "Epoch: 3, Train_Loss: 1.9209632873535156, Test_Loss: 1.923058271408081 *\n",
      "Epoch: 3, Train_Loss: 1.923252820968628, Test_Loss: 1.9323067665100098\n",
      "Epoch: 3, Train_Loss: 1.9119175672531128, Test_Loss: 1.935666799545288\n",
      "Epoch: 3, Train_Loss: 1.9501110315322876, Test_Loss: 1.9155306816101074 *\n",
      "Epoch: 3, Train_Loss: 2.1199495792388916, Test_Loss: 1.9136035442352295 *\n",
      "Epoch: 3, Train_Loss: 2.062830924987793, Test_Loss: 1.9129246473312378 *\n",
      "Epoch: 3, Train_Loss: 1.9911757707595825, Test_Loss: 1.9052022695541382 *\n",
      "Epoch: 3, Train_Loss: 1.9185336828231812, Test_Loss: 1.9033015966415405 *\n",
      "Epoch: 3, Train_Loss: 2.0362467765808105, Test_Loss: 1.9099948406219482\n",
      "Epoch: 3, Train_Loss: 1.993387222290039, Test_Loss: 1.9535983800888062\n",
      "Epoch: 3, Train_Loss: 2.0491902828216553, Test_Loss: 3.1797361373901367\n",
      "Epoch: 3, Train_Loss: 2.0181267261505127, Test_Loss: 6.3365936279296875\n",
      "Epoch: 3, Train_Loss: 2.0027480125427246, Test_Loss: 1.8970555067062378 *\n",
      "Epoch: 3, Train_Loss: 1.8854509592056274, Test_Loss: 1.8845072984695435 *\n",
      "Epoch: 3, Train_Loss: 1.9184585809707642, Test_Loss: 1.904496431350708\n",
      "Epoch: 3, Train_Loss: 1.933018684387207, Test_Loss: 1.9071778059005737\n",
      "Epoch: 3, Train_Loss: 1.8934862613677979, Test_Loss: 1.9114179611206055\n",
      "Epoch: 3, Train_Loss: 1.8768750429153442, Test_Loss: 1.9006706476211548 *\n",
      "Epoch: 3, Train_Loss: 1.8744525909423828, Test_Loss: 2.04274582862854\n",
      "Epoch: 3, Train_Loss: 1.8723481893539429, Test_Loss: 1.895247220993042 *\n",
      "Epoch: 3, Train_Loss: 3.3422775268554688, Test_Loss: 1.8795591592788696 *\n",
      "Epoch: 3, Train_Loss: 6.2039794921875, Test_Loss: 1.924910306930542\n",
      "Epoch: 3, Train_Loss: 1.8714600801467896, Test_Loss: 1.8687340021133423 *\n",
      "Epoch: 3, Train_Loss: 1.8823143243789673, Test_Loss: 1.8863139152526855\n",
      "Epoch: 3, Train_Loss: 1.8803733587265015, Test_Loss: 1.9048012495040894\n",
      "Epoch: 3, Train_Loss: 1.8757033348083496, Test_Loss: 1.9328539371490479\n",
      "Epoch: 3, Train_Loss: 1.8677029609680176, Test_Loss: 1.951420545578003\n",
      "Epoch: 3, Train_Loss: 1.8616222143173218, Test_Loss: 2.0220096111297607\n",
      "Epoch: 3, Train_Loss: 1.8632038831710815, Test_Loss: 1.8899304866790771 *\n",
      "Epoch: 3, Train_Loss: 1.8830832242965698, Test_Loss: 1.867201328277588 *\n",
      "Epoch: 3, Train_Loss: 1.865065097808838, Test_Loss: 1.853798270225525 *\n",
      "Epoch: 3, Train_Loss: 1.8568178415298462, Test_Loss: 1.8499912023544312 *\n",
      "Epoch: 3, Train_Loss: 1.8538926839828491, Test_Loss: 1.8489090204238892 *\n",
      "Epoch: 3, Train_Loss: 1.8502322435379028, Test_Loss: 1.84749174118042 *\n",
      "Epoch: 3, Train_Loss: 1.8661538362503052, Test_Loss: 1.8456023931503296 *\n",
      "Epoch: 3, Train_Loss: 1.8462456464767456, Test_Loss: 1.843870997428894 *\n",
      "Epoch: 3, Train_Loss: 1.8423265218734741, Test_Loss: 1.8484441041946411\n",
      "Epoch: 3, Train_Loss: 1.897258996963501, Test_Loss: 1.8425419330596924 *\n",
      "Epoch: 3, Train_Loss: 1.9041235446929932, Test_Loss: 1.84514319896698\n",
      "Epoch: 3, Train_Loss: 1.844149112701416, Test_Loss: 1.839431881904602 *\n",
      "Epoch: 3, Train_Loss: 1.8368901014328003, Test_Loss: 1.8410145044326782\n",
      "Epoch: 3, Train_Loss: 1.8430038690567017, Test_Loss: 1.8833354711532593\n",
      "Epoch: 3, Train_Loss: 1.9313820600509644, Test_Loss: 1.8366179466247559 *\n",
      "Epoch: 3, Train_Loss: 1.9013631343841553, Test_Loss: 2.2162108421325684\n",
      "Epoch: 3, Train_Loss: 1.9128450155258179, Test_Loss: 2.321823835372925\n",
      "Epoch: 3, Train_Loss: 1.8638771772384644, Test_Loss: 1.9674216508865356 *\n",
      "Epoch: 3, Train_Loss: 1.8925105333328247, Test_Loss: 1.8383231163024902 *\n",
      "Epoch: 3, Train_Loss: 1.8918572664260864, Test_Loss: 1.8362572193145752 *\n",
      "Epoch: 3, Train_Loss: 1.88975989818573, Test_Loss: 1.8530396223068237\n",
      "Epoch: 3, Train_Loss: 1.8269270658493042, Test_Loss: 1.9973912239074707\n",
      "Epoch: 3, Train_Loss: 1.9263347387313843, Test_Loss: 3.0535950660705566\n",
      "Epoch: 3, Train_Loss: 1.8317136764526367, Test_Loss: 2.985818386077881 *\n",
      "Epoch: 3, Train_Loss: 1.8135441541671753, Test_Loss: 1.8562363386154175 *\n",
      "Epoch: 3, Train_Loss: 1.8144514560699463, Test_Loss: 1.870469093322754\n",
      "Epoch: 3, Train_Loss: 1.8128575086593628, Test_Loss: 1.8109517097473145 *\n",
      "Epoch: 3, Train_Loss: 1.811002254486084, Test_Loss: 1.8169499635696411\n",
      "Epoch: 3, Train_Loss: 1.808950424194336, Test_Loss: 1.811697244644165 *\n",
      "Epoch: 3, Train_Loss: 3.339461326599121, Test_Loss: 1.839609146118164\n",
      "Epoch: 3, Train_Loss: 5.25318717956543, Test_Loss: 1.8815280199050903\n",
      "Epoch: 3, Train_Loss: 1.7997491359710693, Test_Loss: 1.8159021139144897 *\n",
      "Epoch: 3, Train_Loss: 1.8246183395385742, Test_Loss: 1.8128395080566406 *\n",
      "Epoch: 3, Train_Loss: 1.8181262016296387, Test_Loss: 1.896462321281433\n",
      "Epoch: 3, Train_Loss: 1.792573094367981, Test_Loss: 2.1742115020751953\n",
      "Epoch: 3, Train_Loss: 1.7927273511886597, Test_Loss: 1.993098258972168 *\n",
      "Epoch: 3, Train_Loss: 1.7909897565841675, Test_Loss: 1.8043956756591797 *\n",
      "Epoch: 3, Train_Loss: 1.7897526025772095, Test_Loss: 1.792134165763855 *\n",
      "Epoch: 3, Train_Loss: 1.786773920059204, Test_Loss: 1.7910677194595337 *\n",
      "Epoch: 3, Train_Loss: 1.7930927276611328, Test_Loss: 1.7901614904403687 *\n",
      "Epoch: 3, Train_Loss: 1.8859647512435913, Test_Loss: 1.7901711463928223\n",
      "Epoch: 3, Train_Loss: 1.8530185222625732, Test_Loss: 2.1086339950561523\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 1.8896363973617554, Test_Loss: 6.941780090332031\n",
      "Epoch: 3, Train_Loss: 1.8251264095306396, Test_Loss: 1.8470698595046997 *\n",
      "Epoch: 3, Train_Loss: 1.7753729820251465, Test_Loss: 1.7819818258285522 *\n",
      "Epoch: 3, Train_Loss: 1.9361854791641235, Test_Loss: 1.7750005722045898 *\n",
      "Epoch: 3, Train_Loss: 1.9808326959609985, Test_Loss: 1.777362585067749\n",
      "Epoch: 3, Train_Loss: 1.9578944444656372, Test_Loss: 1.7750020027160645 *\n",
      "Epoch: 3, Train_Loss: 1.8682036399841309, Test_Loss: 1.7704166173934937 *\n",
      "Epoch: 3, Train_Loss: 1.7686598300933838, Test_Loss: 1.7769417762756348\n",
      "Epoch: 3, Train_Loss: 1.766899824142456, Test_Loss: 1.7728431224822998 *\n",
      "Epoch: 3, Train_Loss: 1.7646725177764893, Test_Loss: 1.7713468074798584 *\n",
      "Epoch: 3, Train_Loss: 1.765359878540039, Test_Loss: 1.7700077295303345 *\n",
      "Epoch: 3, Train_Loss: 1.7673276662826538, Test_Loss: 1.7713199853897095\n",
      "Epoch: 3, Train_Loss: 1.760581374168396, Test_Loss: 1.7714194059371948\n",
      "Epoch: 3, Train_Loss: 1.761029601097107, Test_Loss: 1.7741405963897705\n",
      "Epoch: 3, Train_Loss: 1.7592748403549194, Test_Loss: 1.7675777673721313 *\n",
      "Epoch: 3, Train_Loss: 1.756231427192688, Test_Loss: 1.7603272199630737 *\n",
      "Epoch: 3, Train_Loss: 1.7908778190612793, Test_Loss: 1.7554069757461548 *\n",
      "Epoch: 3, Train_Loss: 1.920228362083435, Test_Loss: 1.7561912536621094\n",
      "Epoch: 3, Train_Loss: 1.902738332748413, Test_Loss: 1.754102110862732 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 1.876930832862854, Test_Loss: 1.7514249086380005 *\n",
      "Epoch: 3, Train_Loss: 1.8573188781738281, Test_Loss: 1.7526055574417114\n",
      "Epoch: 3, Train_Loss: 1.92425537109375, Test_Loss: 1.747813105583191 *\n",
      "Epoch: 3, Train_Loss: 1.8172273635864258, Test_Loss: 1.7496341466903687\n",
      "Epoch: 3, Train_Loss: 1.8964016437530518, Test_Loss: 1.7476767301559448 *\n",
      "Epoch: 3, Train_Loss: 1.8812575340270996, Test_Loss: 1.745364785194397 *\n",
      "Epoch: 3, Train_Loss: 2.066697120666504, Test_Loss: 1.7432602643966675 *\n",
      "Epoch: 3, Train_Loss: 1.7465178966522217, Test_Loss: 1.7426079511642456 *\n",
      "Epoch: 3, Train_Loss: 1.77618408203125, Test_Loss: 1.7386252880096436 *\n",
      "Epoch: 3, Train_Loss: 4.834329605102539, Test_Loss: 1.7395962476730347\n",
      "Epoch: 3, Train_Loss: 2.0042436122894287, Test_Loss: 1.7480664253234863\n",
      "Epoch: 3, Train_Loss: 1.7591662406921387, Test_Loss: 1.790101408958435\n",
      "Epoch: 3, Train_Loss: 1.7710621356964111, Test_Loss: 4.210219860076904\n",
      "Epoch: 3, Train_Loss: 1.7684974670410156, Test_Loss: 4.849490642547607\n",
      "Epoch: 3, Train_Loss: 1.7359358072280884, Test_Loss: 1.7304316759109497 *\n",
      "Epoch: 3, Train_Loss: 1.7305723428726196, Test_Loss: 1.7238502502441406 *\n",
      "Epoch: 3, Train_Loss: 1.81658935546875, Test_Loss: 1.7574461698532104\n",
      "Epoch: 3, Train_Loss: 1.8844386339187622, Test_Loss: 1.7571381330490112 *\n",
      "Epoch: 3, Train_Loss: 1.8491020202636719, Test_Loss: 1.7547132968902588 *\n",
      "Epoch: 3, Train_Loss: 1.8154650926589966, Test_Loss: 1.7703876495361328\n",
      "Epoch: 3, Train_Loss: 1.818607211112976, Test_Loss: 1.8511009216308594\n",
      "Epoch: 3, Train_Loss: 1.7506885528564453, Test_Loss: 1.7167295217514038 *\n",
      "Epoch: 3, Train_Loss: 1.754120111465454, Test_Loss: 1.7365374565124512\n",
      "Epoch: 3, Train_Loss: 1.7149999141693115, Test_Loss: 1.7328808307647705 *\n",
      "Epoch: 3, Train_Loss: 1.7437832355499268, Test_Loss: 1.7228819131851196 *\n",
      "Epoch: 3, Train_Loss: 1.7246111631393433, Test_Loss: 1.715470314025879 *\n",
      "Epoch: 3, Train_Loss: 1.7040735483169556, Test_Loss: 1.773838758468628\n",
      "Epoch: 3, Train_Loss: 1.7252171039581299, Test_Loss: 1.7730299234390259 *\n",
      "Epoch: 3, Train_Loss: 1.7648416757583618, Test_Loss: 1.7956377267837524\n",
      "Epoch: 3, Train_Loss: 1.7526785135269165, Test_Loss: 1.8155103921890259\n",
      "Epoch: 3, Train_Loss: 1.6976341009140015, Test_Loss: 1.7148851156234741 *\n",
      "Epoch: 3, Train_Loss: 1.6961264610290527, Test_Loss: 1.7215940952301025\n",
      "Epoch: 3, Train_Loss: 1.6939477920532227, Test_Loss: 1.704927682876587 *\n",
      "Epoch: 3, Train_Loss: 1.6922298669815063, Test_Loss: 1.7014933824539185 *\n",
      "Epoch: 3, Train_Loss: 1.6913094520568848, Test_Loss: 1.701685905456543\n",
      "Epoch: 3, Train_Loss: 1.6898480653762817, Test_Loss: 1.7040036916732788\n",
      "Epoch: 3, Train_Loss: 1.6881810426712036, Test_Loss: 1.7004961967468262 *\n",
      "Epoch: 3, Train_Loss: 1.6866191625595093, Test_Loss: 1.696344256401062 *\n",
      "Epoch: 3, Train_Loss: 1.6850165128707886, Test_Loss: 1.7015366554260254\n",
      "Epoch: 3, Train_Loss: 1.6830288171768188, Test_Loss: 1.696743130683899 *\n",
      "Epoch: 3, Train_Loss: 1.6846368312835693, Test_Loss: 1.6958105564117432 *\n",
      "Epoch: 3, Train_Loss: 1.690952181816101, Test_Loss: 1.6859354972839355 *\n",
      "Epoch: 3, Train_Loss: 1.6930869817733765, Test_Loss: 1.6991071701049805\n",
      "Epoch: 3, Train_Loss: 1.6932575702667236, Test_Loss: 1.7347805500030518\n",
      "Epoch: 3, Train_Loss: 1.6837096214294434, Test_Loss: 1.696513295173645 *\n",
      "Epoch: 3, Train_Loss: 1.6792588233947754, Test_Loss: 2.1668331623077393\n",
      "Epoch: 3, Train_Loss: 1.6719149351119995, Test_Loss: 2.1516239643096924 *\n",
      "Epoch: 3, Train_Loss: 1.6702009439468384, Test_Loss: 1.8098722696304321 *\n",
      "Epoch: 3, Train_Loss: 1.680281162261963, Test_Loss: 1.69329035282135 *\n",
      "Epoch: 3, Train_Loss: 1.6822259426116943, Test_Loss: 1.685882806777954 *\n",
      "Epoch: 3, Train_Loss: 1.6657168865203857, Test_Loss: 1.704479455947876\n",
      "Epoch: 3, Train_Loss: 1.6642979383468628, Test_Loss: 1.9457521438598633\n",
      "Epoch: 3, Train_Loss: 1.6632099151611328, Test_Loss: 3.0081558227539062\n",
      "Epoch: 3, Train_Loss: 1.7366970777511597, Test_Loss: 2.46791410446167 *\n",
      "Epoch: 3, Train_Loss: 1.6999545097351074, Test_Loss: 1.7208921909332275 *\n",
      "Epoch: 3, Train_Loss: 1.6975300312042236, Test_Loss: 1.6931754350662231 *\n",
      "Epoch: 3, Train_Loss: 1.6653273105621338, Test_Loss: 1.6643942594528198 *\n",
      "Epoch: 3, Train_Loss: 1.6580955982208252, Test_Loss: 1.662741780281067 *\n",
      "Epoch: 3, Train_Loss: 1.7057560682296753, Test_Loss: 1.6632591485977173\n",
      "Epoch: 3, Train_Loss: 1.654496192932129, Test_Loss: 1.685444951057434\n",
      "Epoch: 3, Train_Loss: 1.6700037717819214, Test_Loss: 1.7347440719604492\n",
      "Epoch: 3, Train_Loss: 1.691131353378296, Test_Loss: 1.650720477104187 *\n",
      "Epoch: 3, Train_Loss: 1.672231912612915, Test_Loss: 1.6874598264694214\n",
      "Epoch: 3, Train_Loss: 1.8201415538787842, Test_Loss: 1.7729641199111938\n",
      "Epoch: 3, Train_Loss: 1.7491413354873657, Test_Loss: 1.9812052249908447\n",
      "Epoch: 3, Train_Loss: 1.6938409805297852, Test_Loss: 1.8490190505981445 *\n",
      "Epoch: 3, Train_Loss: 1.6547454595565796, Test_Loss: 1.6521272659301758 *\n",
      "Epoch: 3, Train_Loss: 1.671345829963684, Test_Loss: 1.64369797706604 *\n",
      "Epoch: 3, Train_Loss: 1.6451239585876465, Test_Loss: 1.6421794891357422 *\n",
      "Epoch: 3, Train_Loss: 1.647249698638916, Test_Loss: 1.6407980918884277 *\n",
      "Epoch: 3, Train_Loss: 1.643534541130066, Test_Loss: 1.642567753791809\n",
      "Epoch: 3, Train_Loss: 1.641618013381958, Test_Loss: 2.788299083709717\n",
      "Epoch: 3, Train_Loss: 1.6677772998809814, Test_Loss: 6.080508708953857\n",
      "Epoch: 3, Train_Loss: 1.6880253553390503, Test_Loss: 1.6467312574386597 *\n",
      "Epoch: 3, Train_Loss: 1.6754677295684814, Test_Loss: 1.6318609714508057 *\n",
      "Epoch: 3, Train_Loss: 1.682066559791565, Test_Loss: 1.6283254623413086 *\n",
      "Epoch: 3, Train_Loss: 1.6466959714889526, Test_Loss: 1.628973126411438\n",
      "Epoch: 3, Train_Loss: 1.6309940814971924, Test_Loss: 1.627287745475769 *\n",
      "Epoch: 3, Train_Loss: 1.7542411088943481, Test_Loss: 1.6256245374679565 *\n",
      "Epoch: 3, Train_Loss: 1.8115062713623047, Test_Loss: 1.6304011344909668\n",
      "Epoch: 3, Train_Loss: 1.6194839477539062, Test_Loss: 1.6260417699813843 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 1.649160623550415, Test_Loss: 1.6269947290420532\n",
      "Epoch: 3, Train_Loss: 1.6186833381652832, Test_Loss: 1.6217314004898071 *\n",
      "Epoch: 3, Train_Loss: 1.616723895072937, Test_Loss: 1.6242611408233643\n",
      "Epoch: 3, Train_Loss: 1.617522954940796, Test_Loss: 1.6320686340332031\n",
      "Epoch: 3, Train_Loss: 1.6163583993911743, Test_Loss: 1.630851149559021 *\n",
      "Epoch: 3, Train_Loss: 1.635276198387146, Test_Loss: 1.620746374130249 *\n",
      "Epoch: 3, Train_Loss: 1.6427148580551147, Test_Loss: 1.6129095554351807 *\n",
      "Epoch: 3, Train_Loss: 1.6239452362060547, Test_Loss: 1.6112536191940308 *\n",
      "Epoch: 3, Train_Loss: 1.6240395307540894, Test_Loss: 1.609357476234436 *\n",
      "Epoch: 3, Train_Loss: 1.642134666442871, Test_Loss: 1.6091562509536743 *\n",
      "Epoch: 3, Train_Loss: 1.6064783334732056, Test_Loss: 1.6075913906097412 *\n",
      "Epoch: 3, Train_Loss: 1.607168197631836, Test_Loss: 1.6048943996429443 *\n",
      "Epoch: 3, Train_Loss: 1.6028603315353394, Test_Loss: 1.603845477104187 *\n",
      "Epoch: 3, Train_Loss: 1.619383692741394, Test_Loss: 1.6053394079208374\n",
      "Epoch: 3, Train_Loss: 1.6184195280075073, Test_Loss: 1.6011325120925903 *\n",
      "Epoch: 3, Train_Loss: 1.6079022884368896, Test_Loss: 1.6013411283493042\n",
      "Epoch: 3, Train_Loss: 1.612127661705017, Test_Loss: 1.599003791809082 *\n",
      "Epoch: 3, Train_Loss: 1.6709492206573486, Test_Loss: 1.597710371017456 *\n",
      "Epoch: 3, Train_Loss: 1.6532038450241089, Test_Loss: 1.5952955484390259 *\n",
      "Epoch: 3, Train_Loss: 1.6119893789291382, Test_Loss: 1.5947750806808472 *\n",
      "Epoch: 3, Train_Loss: 1.5997151136398315, Test_Loss: 1.626132607460022\n",
      "Epoch: 3, Train_Loss: 1.602687120437622, Test_Loss: 1.6290228366851807\n",
      "Epoch: 3, Train_Loss: 1.5995436906814575, Test_Loss: 5.046029090881348\n",
      "Epoch: 3, Train_Loss: 1.5994408130645752, Test_Loss: 3.7232513427734375 *\n",
      "Epoch: 3, Train_Loss: 1.6045689582824707, Test_Loss: 1.584886074066162 *\n",
      "Epoch: 3, Train_Loss: 1.6276588439941406, Test_Loss: 1.5843271017074585 *\n",
      "Epoch: 3, Train_Loss: 3.83052921295166, Test_Loss: 1.6238163709640503\n",
      "Epoch: 3, Train_Loss: 4.762286186218262, Test_Loss: 1.6260945796966553\n",
      "Epoch: 3, Train_Loss: 1.5901172161102295, Test_Loss: 1.5991979837417603 *\n",
      "Epoch: 3, Train_Loss: 1.5824167728424072, Test_Loss: 1.6501657962799072\n",
      "Epoch: 3, Train_Loss: 1.6259979009628296, Test_Loss: 1.6886484622955322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 1.7556599378585815, Test_Loss: 1.572906732559204 *\n",
      "Epoch: 3, Train_Loss: 1.5934829711914062, Test_Loss: 1.607405662536621\n",
      "Epoch: 3, Train_Loss: 1.5729682445526123, Test_Loss: 1.5870269536972046 *\n",
      "Epoch: 3, Train_Loss: 1.5728240013122559, Test_Loss: 1.581585168838501 *\n",
      "Epoch: 3, Train_Loss: 1.636046051979065, Test_Loss: 1.5716571807861328 *\n",
      "Epoch: 3, Train_Loss: 1.5729273557662964, Test_Loss: 1.6541093587875366\n",
      "Epoch: 3, Train_Loss: 1.575879693031311, Test_Loss: 1.6150575876235962 *\n",
      "Epoch: 3, Train_Loss: 2.4022908210754395, Test_Loss: 1.6687958240509033\n",
      "Epoch: 3, Train_Loss: 2.9432358741760254, Test_Loss: 1.6364526748657227 *\n",
      "Epoch: 3, Train_Loss: 2.156038761138916, Test_Loss: 1.600843906402588 *\n",
      "Epoch: 3, Train_Loss: 1.6682026386260986, Test_Loss: 1.584222674369812 *\n",
      "Epoch: 3, Train_Loss: 2.3546369075775146, Test_Loss: 1.574493169784546 *\n",
      "Epoch: 3, Train_Loss: 3.8335890769958496, Test_Loss: 1.5731092691421509 *\n",
      "Epoch: 3, Train_Loss: 1.9094858169555664, Test_Loss: 1.5790537595748901\n",
      "Epoch: 3, Train_Loss: 1.611539363861084, Test_Loss: 1.5845988988876343\n",
      "Epoch: 3, Train_Loss: 1.5749092102050781, Test_Loss: 1.576846718788147 *\n",
      "Epoch: 3, Train_Loss: 2.851130485534668, Test_Loss: 1.5580805540084839 *\n",
      "Epoch: 3, Train_Loss: 2.894888401031494, Test_Loss: 1.5633373260498047\n",
      "Epoch: 3, Train_Loss: 1.686912178993225, Test_Loss: 1.5592113733291626 *\n",
      "Epoch: 3, Train_Loss: 1.549830675125122, Test_Loss: 1.5545237064361572 *\n",
      "Epoch: 3, Train_Loss: 1.545000672340393, Test_Loss: 1.5500856637954712 *\n",
      "Epoch: 3, Train_Loss: 2.261521577835083, Test_Loss: 1.5697555541992188\n",
      "Epoch: 3, Train_Loss: 1.7151145935058594, Test_Loss: 1.5843697786331177\n",
      "Epoch: 3, Train_Loss: 1.5817514657974243, Test_Loss: 1.6261969804763794\n",
      "Epoch: 3, Train_Loss: 1.5658557415008545, Test_Loss: 2.0660934448242188\n",
      "Epoch: 3, Train_Loss: 1.6562360525131226, Test_Loss: 1.9361393451690674 *\n",
      "Epoch: 3, Train_Loss: 1.688868761062622, Test_Loss: 1.640657901763916 *\n",
      "Epoch: 3, Train_Loss: 1.6711976528167725, Test_Loss: 1.5538197755813599 *\n",
      "Epoch: 3, Train_Loss: 1.9140071868896484, Test_Loss: 1.5473319292068481 *\n",
      "Epoch: 3, Train_Loss: 1.6469534635543823, Test_Loss: 1.5993120670318604\n",
      "Epoch: 3, Train_Loss: 1.6007462739944458, Test_Loss: 1.9797163009643555\n",
      "Epoch: 3, Train_Loss: 1.7392793893814087, Test_Loss: 2.6799728870391846\n",
      "Epoch: 3, Train_Loss: 1.9057375192642212, Test_Loss: 2.1004574298858643 *\n",
      "Epoch: 3, Train_Loss: 1.9088008403778076, Test_Loss: 1.598524570465088 *\n",
      "Epoch: 3, Train_Loss: 1.576448917388916, Test_Loss: 1.5350013971328735 *\n",
      "Epoch: 3, Train_Loss: 1.687436819076538, Test_Loss: 1.5315568447113037 *\n",
      "Epoch: 3, Train_Loss: 1.7011750936508179, Test_Loss: 1.5238337516784668 *\n",
      "Epoch: 3, Train_Loss: 1.5542261600494385, Test_Loss: 1.5300744771957397\n",
      "Epoch: 3, Train_Loss: 1.5403493642807007, Test_Loss: 1.5223374366760254 *\n",
      "Epoch: 3, Train_Loss: 1.517960548400879, Test_Loss: 1.5498173236846924\n",
      "Epoch: 3, Train_Loss: 1.5170458555221558, Test_Loss: 1.5258173942565918 *\n",
      "Epoch: 3, Train_Loss: 1.5216395854949951, Test_Loss: 1.588123083114624\n",
      "Epoch: 3, Train_Loss: 1.5195438861846924, Test_Loss: 1.6884779930114746\n",
      "Epoch: 3, Train_Loss: 1.579879879951477, Test_Loss: 1.8832454681396484\n",
      "Epoch: 3, Train_Loss: 1.5497498512268066, Test_Loss: 1.8310037851333618 *\n",
      "Epoch: 3, Train_Loss: 1.5770609378814697, Test_Loss: 1.5284343957901 *\n",
      "Epoch: 3, Train_Loss: 1.6040170192718506, Test_Loss: 1.5191935300827026 *\n",
      "Epoch: 3, Train_Loss: 2.0176186561584473, Test_Loss: 1.5170354843139648 *\n",
      "Epoch: 3, Train_Loss: 1.5102332830429077, Test_Loss: 1.5167415142059326 *\n",
      "Epoch: 3, Train_Loss: 1.5605671405792236, Test_Loss: 1.5554395914077759\n",
      "Epoch: 3, Train_Loss: 1.6550482511520386, Test_Loss: 3.773317575454712\n",
      "Epoch: 3, Train_Loss: 1.9283288717269897, Test_Loss: 4.480203628540039\n",
      "Epoch: 3, Train_Loss: 1.7685704231262207, Test_Loss: 1.5065127611160278 *\n",
      "Epoch: 3, Train_Loss: 1.4985660314559937, Test_Loss: 1.4991090297698975 *\n",
      "Epoch: 3, Train_Loss: 1.5810056924819946, Test_Loss: 1.4956544637680054 *\n",
      "Epoch: 3, Train_Loss: 2.137554168701172, Test_Loss: 1.5025864839553833\n",
      "Epoch: 3, Train_Loss: 2.074869155883789, Test_Loss: 1.4967516660690308 *\n",
      "Epoch: 3, Train_Loss: 1.5635935068130493, Test_Loss: 1.4986389875411987\n",
      "Epoch: 3, Train_Loss: 1.5193125009536743, Test_Loss: 1.4949641227722168 *\n",
      "Epoch: 3, Train_Loss: 1.501297950744629, Test_Loss: 1.4953727722167969\n",
      "Epoch: 3, Train_Loss: 2.346816301345825, Test_Loss: 1.4971108436584473\n",
      "Epoch: 3, Train_Loss: 2.3274848461151123, Test_Loss: 1.4928858280181885 *\n",
      "Epoch: 3, Train_Loss: 1.5040189027786255, Test_Loss: 1.4982658624649048\n",
      "Epoch: 3, Train_Loss: 1.5048834085464478, Test_Loss: 1.4961795806884766 *\n",
      "Epoch: 3, Train_Loss: 1.484695553779602, Test_Loss: 1.4889476299285889 *\n",
      "Epoch: 3, Train_Loss: 1.5105862617492676, Test_Loss: 1.4888614416122437 *\n",
      "Epoch: 3, Train_Loss: 2.009647846221924, Test_Loss: 1.4812490940093994 *\n",
      "Epoch: 3, Train_Loss: 1.4855093955993652, Test_Loss: 1.4837231636047363\n",
      "Epoch: 3, Train_Loss: 1.6574617624282837, Test_Loss: 1.4844183921813965\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 1.5887452363967896, Test_Loss: 1.4819676876068115 *\n",
      "Epoch: 3, Train_Loss: 1.4951404333114624, Test_Loss: 1.4768768548965454 *\n",
      "Epoch: 3, Train_Loss: 1.5314691066741943, Test_Loss: 1.4772908687591553\n",
      "Epoch: 3, Train_Loss: 1.6775836944580078, Test_Loss: 1.4809215068817139\n",
      "Epoch: 3, Train_Loss: 1.6802730560302734, Test_Loss: 1.4926936626434326\n",
      "Epoch: 3, Train_Loss: 1.4884998798370361, Test_Loss: 1.4800851345062256 *\n",
      "Epoch: 3, Train_Loss: 1.4827934503555298, Test_Loss: 1.478880763053894 *\n",
      "Epoch: 3, Train_Loss: 1.5207078456878662, Test_Loss: 1.4718585014343262 *\n",
      "Epoch: 3, Train_Loss: 1.520070195198059, Test_Loss: 1.4750021696090698\n",
      "Epoch: 3, Train_Loss: 1.535889744758606, Test_Loss: 1.4741142988204956 *\n",
      "Epoch: 3, Train_Loss: 1.4723352193832397, Test_Loss: 1.4681472778320312 *\n",
      "Epoch: 3, Train_Loss: 1.479241132736206, Test_Loss: 1.5403307676315308\n",
      "Epoch: 3, Train_Loss: 1.4753305912017822, Test_Loss: 1.5190269947052002 *\n",
      "Epoch: 3, Train_Loss: 1.8494077920913696, Test_Loss: 6.218912124633789\n",
      "Epoch: 3, Train_Loss: 1.7097551822662354, Test_Loss: 2.262169361114502 *\n",
      "Epoch: 3, Train_Loss: 2.0596485137939453, Test_Loss: 1.4973883628845215 *\n",
      "Epoch: 3, Train_Loss: 2.031369209289551, Test_Loss: 1.5030710697174072\n",
      "Epoch: 3, Train_Loss: 1.6443578004837036, Test_Loss: 1.481269121170044 *\n",
      "Epoch: 3, Train_Loss: 1.6847580671310425, Test_Loss: 1.4672340154647827 *\n",
      "Epoch: 3, Train_Loss: 1.5174225568771362, Test_Loss: 1.459049940109253 *\n",
      "Epoch: 3, Train_Loss: 1.456477165222168, Test_Loss: 1.509987711906433\n",
      "Epoch: 3, Train_Loss: 1.4541404247283936, Test_Loss: 1.5006643533706665 *\n",
      "Epoch: 3, Train_Loss: 1.511831283569336, Test_Loss: 1.4597550630569458 *\n",
      "Epoch: 3, Train_Loss: 1.7170491218566895, Test_Loss: 1.4843999147415161\n",
      "Epoch: 3, Train_Loss: 1.8732173442840576, Test_Loss: 1.5870530605316162\n",
      "Epoch: 3, Train_Loss: 2.167607307434082, Test_Loss: 1.5284373760223389 *\n",
      "Epoch: 3, Train_Loss: 2.7671403884887695, Test_Loss: 1.5022879838943481 *\n",
      "Epoch: 3, Train_Loss: 1.8883109092712402, Test_Loss: 1.4630842208862305 *\n",
      "Epoch: 3, Train_Loss: 1.7269822359085083, Test_Loss: 1.5205861330032349\n",
      "Epoch: 3, Train_Loss: 1.4766901731491089, Test_Loss: 1.4630799293518066 *\n",
      "Epoch: 3, Train_Loss: 1.4552186727523804, Test_Loss: 1.5267324447631836\n",
      "Epoch: 3, Train_Loss: 1.9008674621582031, Test_Loss: 1.6620628833770752\n",
      "Epoch: 3, Train_Loss: 2.6680688858032227, Test_Loss: 1.45707106590271 *\n",
      "Epoch: 3, Train_Loss: 1.5809739828109741, Test_Loss: 1.5022274255752563\n",
      "Epoch: 3, Train_Loss: 1.5811537504196167, Test_Loss: 1.4806798696517944 *\n",
      "Epoch: 3, Train_Loss: 1.4773344993591309, Test_Loss: 1.4932024478912354\n",
      "Epoch: 3, Train_Loss: 1.5048309564590454, Test_Loss: 1.4678932428359985 *\n",
      "Epoch: 3, Train_Loss: 1.6715184450149536, Test_Loss: 1.488356590270996\n",
      "Epoch: 3, Train_Loss: 2.0685389041900635, Test_Loss: 1.5440964698791504\n",
      "Epoch: 3, Train_Loss: 2.1944832801818848, Test_Loss: 1.4957860708236694 *\n",
      "Epoch: 3, Train_Loss: 1.8199421167373657, Test_Loss: 1.4721747636795044 *\n",
      "Epoch: 3, Train_Loss: 1.4588545560836792, Test_Loss: 1.4793691635131836\n",
      "Epoch: 3, Train_Loss: 1.4284652471542358, Test_Loss: 1.501205563545227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 1.4336353540420532, Test_Loss: 1.4263652563095093 *\n",
      "Epoch: 3, Train_Loss: 1.4987295866012573, Test_Loss: 1.4446609020233154\n",
      "Epoch: 3, Train_Loss: 1.4243254661560059, Test_Loss: 1.5519318580627441\n",
      "Epoch: 3, Train_Loss: 1.4516360759735107, Test_Loss: 1.6893011331558228\n",
      "Epoch: 3, Train_Loss: 17.050994873046875, Test_Loss: 1.5589594841003418 *\n",
      "Epoch: 3, Train_Loss: 2.4027411937713623, Test_Loss: 1.4631905555725098 *\n",
      "Epoch: 3, Train_Loss: 2.6220221519470215, Test_Loss: 1.4192289113998413 *\n",
      "Epoch: 3, Train_Loss: 3.630192279815674, Test_Loss: 1.4419766664505005\n",
      "Epoch: 3, Train_Loss: 1.4486033916473389, Test_Loss: 1.454993486404419\n",
      "Epoch: 3, Train_Loss: 1.46339750289917, Test_Loss: 1.6258962154388428\n",
      "Epoch: 3, Train_Loss: 2.8642945289611816, Test_Loss: 1.8918119668960571\n",
      "Epoch: 4, Train_Loss: 10.806530952453613, Test_Loss: 1.854583740234375 *\n",
      "Epoch: 4, Train_Loss: 1.842220664024353, Test_Loss: 1.4339104890823364 *\n",
      "Epoch: 4, Train_Loss: 1.4328941106796265, Test_Loss: 1.4558804035186768\n",
      "Epoch: 4, Train_Loss: 6.241604804992676, Test_Loss: 1.5217325687408447\n",
      "Epoch: 4, Train_Loss: 2.073622703552246, Test_Loss: 1.5638258457183838\n",
      "Epoch: 4, Train_Loss: 1.544247031211853, Test_Loss: 1.5839484930038452\n",
      "Epoch: 4, Train_Loss: 1.4255359172821045, Test_Loss: 1.5755220651626587 *\n",
      "Epoch: 4, Train_Loss: 1.4133623838424683, Test_Loss: 1.7267130613327026\n",
      "Epoch: 4, Train_Loss: 1.4258151054382324, Test_Loss: 1.6297646760940552 *\n",
      "Epoch: 4, Train_Loss: 1.4390555620193481, Test_Loss: 1.510716438293457 *\n",
      "Epoch: 4, Train_Loss: 1.459736943244934, Test_Loss: 1.8377857208251953\n",
      "Epoch: 4, Train_Loss: 1.414833426475525, Test_Loss: 1.4425878524780273 *\n",
      "Epoch: 4, Train_Loss: 1.4086438417434692, Test_Loss: 1.4851429462432861\n",
      "Epoch: 4, Train_Loss: 1.4291573762893677, Test_Loss: 1.459818959236145 *\n",
      "Epoch: 4, Train_Loss: 1.4332959651947021, Test_Loss: 1.430983304977417 *\n",
      "Epoch: 4, Train_Loss: 1.4033706188201904, Test_Loss: 1.405228614807129 *\n",
      "Epoch: 4, Train_Loss: 1.4205502271652222, Test_Loss: 1.3927180767059326 *\n",
      "Epoch: 4, Train_Loss: 1.433760404586792, Test_Loss: 1.3984506130218506\n",
      "Epoch: 4, Train_Loss: 1.4214946031570435, Test_Loss: 5.4437642097473145\n",
      "Epoch: 4, Train_Loss: 1.395895004272461, Test_Loss: 3.210578203201294 *\n",
      "Epoch: 4, Train_Loss: 1.3903722763061523, Test_Loss: 1.4234931468963623 *\n",
      "Epoch: 4, Train_Loss: 1.388075351715088, Test_Loss: 1.4198946952819824 *\n",
      "Epoch: 4, Train_Loss: 1.385140299797058, Test_Loss: 1.4059484004974365 *\n",
      "Epoch: 4, Train_Loss: 1.3841010332107544, Test_Loss: 1.387422800064087 *\n",
      "Epoch: 4, Train_Loss: 1.3828378915786743, Test_Loss: 1.4015867710113525\n",
      "Epoch: 4, Train_Loss: 1.3831393718719482, Test_Loss: 1.4092707633972168\n",
      "Epoch: 4, Train_Loss: 1.380772590637207, Test_Loss: 1.3887079954147339 *\n",
      "Epoch: 4, Train_Loss: 1.379459023475647, Test_Loss: 1.3903213739395142\n",
      "Epoch: 4, Train_Loss: 1.3783245086669922, Test_Loss: 1.3914291858673096\n",
      "Epoch: 4, Train_Loss: 1.3786157369613647, Test_Loss: 1.3896323442459106 *\n",
      "Epoch: 4, Train_Loss: 1.3891254663467407, Test_Loss: 1.3905059099197388\n",
      "Epoch: 4, Train_Loss: 1.4053562879562378, Test_Loss: 1.3888810873031616 *\n",
      "Epoch: 4, Train_Loss: 1.4079089164733887, Test_Loss: 1.381188154220581 *\n",
      "Epoch: 4, Train_Loss: 1.3848726749420166, Test_Loss: 1.3913880586624146\n",
      "Epoch: 4, Train_Loss: 1.3913642168045044, Test_Loss: 1.3759791851043701 *\n",
      "Epoch: 4, Train_Loss: 10.675705909729004, Test_Loss: 1.383363127708435\n",
      "Epoch: 4, Train_Loss: 1.5753705501556396, Test_Loss: 1.3784254789352417 *\n",
      "Epoch: 4, Train_Loss: 1.37740957736969, Test_Loss: 1.3870899677276611\n",
      "Epoch: 4, Train_Loss: 1.3831760883331299, Test_Loss: 1.3810060024261475 *\n",
      "Epoch: 4, Train_Loss: 1.3893314599990845, Test_Loss: 1.3841700553894043\n",
      "Epoch: 4, Train_Loss: 1.3855998516082764, Test_Loss: 1.4143245220184326\n",
      "Epoch: 4, Train_Loss: 1.377167820930481, Test_Loss: 1.4308699369430542\n",
      "Epoch: 4, Train_Loss: 1.4227454662322998, Test_Loss: 1.4015790224075317 *\n",
      "Epoch: 4, Train_Loss: 1.5467300415039062, Test_Loss: 1.3913323879241943 *\n",
      "Epoch: 4, Train_Loss: 1.554179072380066, Test_Loss: 1.3788647651672363 *\n",
      "Epoch: 4, Train_Loss: 1.51882004737854, Test_Loss: 1.386539101600647\n",
      "Epoch: 4, Train_Loss: 1.3708409070968628, Test_Loss: 1.3822624683380127 *\n",
      "Epoch: 4, Train_Loss: 1.4753433465957642, Test_Loss: 1.3763755559921265 *\n",
      "Epoch: 4, Train_Loss: 1.461304783821106, Test_Loss: 1.4337960481643677\n",
      "Epoch: 4, Train_Loss: 1.5123274326324463, Test_Loss: 1.4062771797180176 *\n",
      "Epoch: 4, Train_Loss: 1.4908394813537598, Test_Loss: 6.957294464111328\n",
      "Epoch: 4, Train_Loss: 1.481865644454956, Test_Loss: 1.474025011062622 *\n",
      "Epoch: 4, Train_Loss: 1.389951467514038, Test_Loss: 1.359578013420105 *\n",
      "Epoch: 4, Train_Loss: 1.378179669380188, Test_Loss: 1.3726139068603516\n",
      "Epoch: 4, Train_Loss: 1.4230589866638184, Test_Loss: 1.3901594877243042\n",
      "Epoch: 4, Train_Loss: 1.3760570287704468, Test_Loss: 1.3940349817276\n",
      "Epoch: 4, Train_Loss: 1.357945203781128, Test_Loss: 1.3557661771774292 *\n",
      "Epoch: 4, Train_Loss: 1.353642463684082, Test_Loss: 1.4940299987792969\n",
      "Epoch: 4, Train_Loss: 1.3527424335479736, Test_Loss: 1.4270826578140259 *\n",
      "Epoch: 4, Train_Loss: 1.5819905996322632, Test_Loss: 1.3494281768798828 *\n",
      "Epoch: 4, Train_Loss: 6.8489508628845215, Test_Loss: 1.4167486429214478\n",
      "Epoch: 4, Train_Loss: 1.3614815473556519, Test_Loss: 1.3506718873977661 *\n",
      "Epoch: 4, Train_Loss: 1.364044189453125, Test_Loss: 1.3703677654266357\n",
      "Epoch: 4, Train_Loss: 1.367182731628418, Test_Loss: 1.3624274730682373 *\n",
      "Epoch: 4, Train_Loss: 1.361785888671875, Test_Loss: 1.436766266822815\n",
      "Epoch: 4, Train_Loss: 1.356177806854248, Test_Loss: 1.3897761106491089 *\n",
      "Epoch: 4, Train_Loss: 1.347507119178772, Test_Loss: 1.510739803314209\n",
      "Epoch: 4, Train_Loss: 1.3504524230957031, Test_Loss: 1.4225895404815674 *\n",
      "Epoch: 4, Train_Loss: 1.37045419216156, Test_Loss: 1.3572925329208374 *\n",
      "Epoch: 4, Train_Loss: 1.3538810014724731, Test_Loss: 1.3434579372406006 *\n",
      "Epoch: 4, Train_Loss: 1.355238437652588, Test_Loss: 1.3406800031661987 *\n",
      "Epoch: 4, Train_Loss: 1.3456248044967651, Test_Loss: 1.3407237529754639\n",
      "Epoch: 4, Train_Loss: 1.3418009281158447, Test_Loss: 1.3400171995162964 *\n",
      "Epoch: 4, Train_Loss: 1.3601632118225098, Test_Loss: 1.339066982269287 *\n",
      "Epoch: 4, Train_Loss: 1.3383458852767944, Test_Loss: 1.3383502960205078 *\n",
      "Epoch: 4, Train_Loss: 1.3362951278686523, Test_Loss: 1.3365654945373535 *\n",
      "Epoch: 4, Train_Loss: 1.3758723735809326, Test_Loss: 1.341788649559021\n",
      "Epoch: 4, Train_Loss: 1.3983595371246338, Test_Loss: 1.3420225381851196\n",
      "Epoch: 4, Train_Loss: 1.3548638820648193, Test_Loss: 1.3355443477630615 *\n",
      "Epoch: 4, Train_Loss: 1.3326820135116577, Test_Loss: 1.3361756801605225\n",
      "Epoch: 4, Train_Loss: 1.3327707052230835, Test_Loss: 1.3718119859695435\n",
      "Epoch: 4, Train_Loss: 1.4047584533691406, Test_Loss: 1.3459023237228394 *\n",
      "Epoch: 4, Train_Loss: 1.4072093963623047, Test_Loss: 1.5745607614517212\n",
      "Epoch: 4, Train_Loss: 1.400245189666748, Test_Loss: 1.8326826095581055\n",
      "Epoch: 4, Train_Loss: 1.337643027305603, Test_Loss: 1.5275663137435913 *\n",
      "Epoch: 4, Train_Loss: 1.3854246139526367, Test_Loss: 1.376221776008606 *\n",
      "Epoch: 4, Train_Loss: 1.4051592350006104, Test_Loss: 1.3331633806228638 *\n",
      "Epoch: 4, Train_Loss: 1.4042028188705444, Test_Loss: 1.3424965143203735\n",
      "Epoch: 4, Train_Loss: 1.337148666381836, Test_Loss: 1.4569145441055298\n",
      "Epoch: 4, Train_Loss: 1.4628007411956787, Test_Loss: 2.232759714126587\n",
      "Epoch: 4, Train_Loss: 1.329833745956421, Test_Loss: 2.828728199005127\n",
      "Epoch: 4, Train_Loss: 1.3276516199111938, Test_Loss: 1.488586664199829 *\n",
      "Epoch: 4, Train_Loss: 1.3210397958755493, Test_Loss: 1.3913791179656982 *\n",
      "Epoch: 4, Train_Loss: 1.3201245069503784, Test_Loss: 1.3163758516311646 *\n",
      "Epoch: 4, Train_Loss: 1.3189823627471924, Test_Loss: 1.32692551612854\n",
      "Epoch: 4, Train_Loss: 1.3183128833770752, Test_Loss: 1.3206688165664673 *\n",
      "Epoch: 4, Train_Loss: 1.5702030658721924, Test_Loss: 1.3348912000656128\n",
      "Epoch: 4, Train_Loss: 6.011312961578369, Test_Loss: 1.369005560874939\n",
      "Epoch: 4, Train_Loss: 1.3480297327041626, Test_Loss: 1.362313151359558 *\n",
      "Epoch: 4, Train_Loss: 1.329844355583191, Test_Loss: 1.3204444646835327 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.3352357149124146, Test_Loss: 1.4022928476333618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.3082295656204224, Test_Loss: 1.692135214805603\n",
      "Epoch: 4, Train_Loss: 1.3064827919006348, Test_Loss: 1.3787155151367188 *\n",
      "Epoch: 4, Train_Loss: 1.3076698780059814, Test_Loss: 1.4474925994873047\n",
      "Epoch: 4, Train_Loss: 1.3070493936538696, Test_Loss: 1.307657241821289 *\n",
      "Epoch: 4, Train_Loss: 1.3043855428695679, Test_Loss: 1.3068830966949463 *\n",
      "Epoch: 4, Train_Loss: 1.3031120300292969, Test_Loss: 1.3060604333877563 *\n",
      "Epoch: 4, Train_Loss: 1.4006747007369995, Test_Loss: 1.3050143718719482 *\n",
      "Epoch: 4, Train_Loss: 1.4040303230285645, Test_Loss: 1.3143690824508667\n",
      "Epoch: 4, Train_Loss: 1.4250675439834595, Test_Loss: 6.372708320617676\n",
      "Epoch: 4, Train_Loss: 1.382394552230835, Test_Loss: 1.866121530532837 *\n",
      "Epoch: 4, Train_Loss: 1.2975852489471436, Test_Loss: 1.304157018661499 *\n",
      "Epoch: 4, Train_Loss: 1.4025425910949707, Test_Loss: 1.296688199043274 *\n",
      "Epoch: 4, Train_Loss: 1.4746538400650024, Test_Loss: 1.2961167097091675 *\n",
      "Epoch: 4, Train_Loss: 1.4730205535888672, Test_Loss: 1.2999703884124756\n",
      "Epoch: 4, Train_Loss: 1.4333055019378662, Test_Loss: 1.2949858903884888 *\n",
      "Epoch: 4, Train_Loss: 1.297860860824585, Test_Loss: 1.3006975650787354\n",
      "Epoch: 4, Train_Loss: 1.292233943939209, Test_Loss: 1.2963531017303467 *\n",
      "Epoch: 4, Train_Loss: 1.2917563915252686, Test_Loss: 1.2965164184570312\n",
      "Epoch: 4, Train_Loss: 1.292021632194519, Test_Loss: 1.2984658479690552\n",
      "Epoch: 4, Train_Loss: 1.2943079471588135, Test_Loss: 1.300797700881958\n",
      "Epoch: 4, Train_Loss: 1.2899373769760132, Test_Loss: 1.2939274311065674 *\n",
      "Epoch: 4, Train_Loss: 1.2890901565551758, Test_Loss: 1.3063149452209473\n",
      "Epoch: 4, Train_Loss: 1.2888444662094116, Test_Loss: 1.2976397275924683 *\n",
      "Epoch: 4, Train_Loss: 1.2868746519088745, Test_Loss: 1.292305588722229 *\n",
      "Epoch: 4, Train_Loss: 1.295333981513977, Test_Loss: 1.2855873107910156 *\n",
      "Epoch: 4, Train_Loss: 1.4336600303649902, Test_Loss: 1.2894048690795898\n",
      "Epoch: 4, Train_Loss: 1.438867211341858, Test_Loss: 1.28560471534729 *\n",
      "Epoch: 4, Train_Loss: 1.4471426010131836, Test_Loss: 1.2836105823516846 *\n",
      "Epoch: 4, Train_Loss: 1.3405284881591797, Test_Loss: 1.2882927656173706\n",
      "Epoch: 4, Train_Loss: 1.4501787424087524, Test_Loss: 1.2817765474319458 *\n",
      "Epoch: 4, Train_Loss: 1.4014227390289307, Test_Loss: 1.2842738628387451\n",
      "Epoch: 4, Train_Loss: 1.3845345973968506, Test_Loss: 1.2869014739990234\n",
      "Epoch: 4, Train_Loss: 1.4238492250442505, Test_Loss: 1.281923770904541 *\n",
      "Epoch: 4, Train_Loss: 1.6244525909423828, Test_Loss: 1.2801822423934937 *\n",
      "Epoch: 4, Train_Loss: 1.2903074026107788, Test_Loss: 1.2797679901123047 *\n",
      "Epoch: 4, Train_Loss: 1.2912952899932861, Test_Loss: 1.2783280611038208 *\n",
      "Epoch: 4, Train_Loss: 3.8990249633789062, Test_Loss: 1.2781552076339722 *\n",
      "Epoch: 4, Train_Loss: 2.0244193077087402, Test_Loss: 1.2817682027816772\n",
      "Epoch: 4, Train_Loss: 1.2961393594741821, Test_Loss: 1.3338671922683716\n",
      "Epoch: 4, Train_Loss: 1.3111674785614014, Test_Loss: 1.868340253829956\n",
      "Epoch: 4, Train_Loss: 1.315434217453003, Test_Loss: 6.291974067687988\n",
      "Epoch: 4, Train_Loss: 1.293198585510254, Test_Loss: 1.2818578481674194 *\n",
      "Epoch: 4, Train_Loss: 1.272039532661438, Test_Loss: 1.2682856321334839 *\n",
      "Epoch: 4, Train_Loss: 1.3222932815551758, Test_Loss: 1.2950305938720703\n",
      "Epoch: 4, Train_Loss: 1.4329859018325806, Test_Loss: 1.3098136186599731\n",
      "Epoch: 4, Train_Loss: 1.3946678638458252, Test_Loss: 1.31277596950531\n",
      "Epoch: 4, Train_Loss: 1.3716559410095215, Test_Loss: 1.2728818655014038 *\n",
      "Epoch: 4, Train_Loss: 1.3814945220947266, Test_Loss: 1.4094346761703491\n",
      "Epoch: 4, Train_Loss: 1.3176778554916382, Test_Loss: 1.2993507385253906 *\n",
      "Epoch: 4, Train_Loss: 1.3112655878067017, Test_Loss: 1.262468934059143 *\n",
      "Epoch: 4, Train_Loss: 1.2737581729888916, Test_Loss: 1.310731053352356\n",
      "Epoch: 4, Train_Loss: 1.2846544981002808, Test_Loss: 1.2684367895126343 *\n",
      "Epoch: 4, Train_Loss: 1.2734990119934082, Test_Loss: 1.2726823091506958\n",
      "Epoch: 4, Train_Loss: 1.25735604763031, Test_Loss: 1.303210735321045\n",
      "Epoch: 4, Train_Loss: 1.2697675228118896, Test_Loss: 1.354845643043518\n",
      "Epoch: 4, Train_Loss: 1.3240355253219604, Test_Loss: 1.3215147256851196 *\n",
      "Epoch: 4, Train_Loss: 1.3291730880737305, Test_Loss: 1.3888342380523682\n",
      "Epoch: 4, Train_Loss: 1.2604221105575562, Test_Loss: 1.2935130596160889 *\n",
      "Epoch: 4, Train_Loss: 1.2531343698501587, Test_Loss: 1.2833006381988525 *\n",
      "Epoch: 4, Train_Loss: 1.2509739398956299, Test_Loss: 1.2643882036209106 *\n",
      "Epoch: 4, Train_Loss: 1.2501318454742432, Test_Loss: 1.2601585388183594 *\n",
      "Epoch: 4, Train_Loss: 1.2491343021392822, Test_Loss: 1.2617757320404053\n",
      "Epoch: 4, Train_Loss: 1.249024748802185, Test_Loss: 1.2614375352859497 *\n",
      "Epoch: 4, Train_Loss: 1.2466739416122437, Test_Loss: 1.2604111433029175 *\n",
      "Epoch: 4, Train_Loss: 1.246538758277893, Test_Loss: 1.2597894668579102 *\n",
      "Epoch: 4, Train_Loss: 1.2453207969665527, Test_Loss: 1.2633278369903564\n",
      "Epoch: 4, Train_Loss: 1.2440001964569092, Test_Loss: 1.2615278959274292 *\n",
      "Epoch: 4, Train_Loss: 1.2438677549362183, Test_Loss: 1.2672673463821411\n",
      "Epoch: 4, Train_Loss: 1.2500863075256348, Test_Loss: 1.2476335763931274 *\n",
      "Epoch: 4, Train_Loss: 1.2479097843170166, Test_Loss: 1.256589651107788\n",
      "Epoch: 4, Train_Loss: 1.2471240758895874, Test_Loss: 1.3094605207443237\n",
      "Epoch: 4, Train_Loss: 1.2529634237289429, Test_Loss: 1.252772569656372 *\n",
      "Epoch: 4, Train_Loss: 1.241702675819397, Test_Loss: 1.6141793727874756\n",
      "Epoch: 4, Train_Loss: 1.2381978034973145, Test_Loss: 1.8128420114517212\n",
      "Epoch: 4, Train_Loss: 1.2368824481964111, Test_Loss: 1.435941457748413 *\n",
      "Epoch: 4, Train_Loss: 1.241684079170227, Test_Loss: 1.2766228914260864 *\n",
      "Epoch: 4, Train_Loss: 1.2500675916671753, Test_Loss: 1.2629469633102417 *\n",
      "Epoch: 4, Train_Loss: 1.2352070808410645, Test_Loss: 1.255066990852356 *\n",
      "Epoch: 4, Train_Loss: 1.2335232496261597, Test_Loss: 1.3711509704589844\n",
      "Epoch: 4, Train_Loss: 1.2331503629684448, Test_Loss: 2.3278300762176514\n",
      "Epoch: 4, Train_Loss: 1.2775763273239136, Test_Loss: 2.5189008712768555\n",
      "Epoch: 4, Train_Loss: 1.2975046634674072, Test_Loss: 1.2939012050628662 *\n",
      "Epoch: 4, Train_Loss: 1.2728698253631592, Test_Loss: 1.3156684637069702\n",
      "Epoch: 4, Train_Loss: 1.24526047706604, Test_Loss: 1.230035424232483 *\n",
      "Epoch: 4, Train_Loss: 1.2285066843032837, Test_Loss: 1.236670732498169\n",
      "Epoch: 4, Train_Loss: 1.276135802268982, Test_Loss: 1.2353346347808838 *\n",
      "Epoch: 4, Train_Loss: 1.23582124710083, Test_Loss: 1.2504991292953491\n",
      "Epoch: 4, Train_Loss: 1.2358201742172241, Test_Loss: 1.2820392847061157\n",
      "Epoch: 4, Train_Loss: 1.2659558057785034, Test_Loss: 1.2526311874389648 *\n",
      "Epoch: 4, Train_Loss: 1.2404224872589111, Test_Loss: 1.2330992221832275 *\n",
      "Epoch: 4, Train_Loss: 1.374254822731018, Test_Loss: 1.3301607370376587\n",
      "Epoch: 4, Train_Loss: 1.3327289819717407, Test_Loss: 1.6262047290802002\n",
      "Epoch: 4, Train_Loss: 1.2791146039962769, Test_Loss: 1.3780847787857056 *\n",
      "Epoch: 4, Train_Loss: 1.2390209436416626, Test_Loss: 1.28520929813385 *\n",
      "Epoch: 4, Train_Loss: 1.2314648628234863, Test_Loss: 1.2266356945037842 *\n",
      "Epoch: 4, Train_Loss: 1.2401721477508545, Test_Loss: 1.2258391380310059 *\n",
      "Epoch: 4, Train_Loss: 1.221238613128662, Test_Loss: 1.22513747215271 *\n",
      "Epoch: 4, Train_Loss: 1.2268983125686646, Test_Loss: 1.2233399152755737 *\n",
      "Epoch: 4, Train_Loss: 1.229627013206482, Test_Loss: 1.3311066627502441\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.234375238418579, Test_Loss: 6.54737663269043\n",
      "Epoch: 4, Train_Loss: 1.298980474472046, Test_Loss: 1.350776195526123 *\n",
      "Epoch: 4, Train_Loss: 1.2299526929855347, Test_Loss: 1.220444917678833 *\n",
      "Epoch: 4, Train_Loss: 1.296648621559143, Test_Loss: 1.2133694887161255 *\n",
      "Epoch: 4, Train_Loss: 1.228482961654663, Test_Loss: 1.215691089630127\n",
      "Epoch: 4, Train_Loss: 1.2304646968841553, Test_Loss: 1.2165987491607666\n",
      "Epoch: 4, Train_Loss: 1.32162606716156, Test_Loss: 1.2114499807357788 *\n",
      "Epoch: 4, Train_Loss: 1.418476939201355, Test_Loss: 1.2168523073196411\n",
      "Epoch: 4, Train_Loss: 1.2150684595108032, Test_Loss: 1.2122085094451904 *\n",
      "Epoch: 4, Train_Loss: 1.2374544143676758, Test_Loss: 1.2120062112808228 *\n",
      "Epoch: 4, Train_Loss: 1.2077081203460693, Test_Loss: 1.2132725715637207\n",
      "Epoch: 4, Train_Loss: 1.207767367362976, Test_Loss: 1.2148884534835815\n",
      "Epoch: 4, Train_Loss: 1.2078813314437866, Test_Loss: 1.2173722982406616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.205315113067627, Test_Loss: 1.2257925271987915\n",
      "Epoch: 4, Train_Loss: 1.227555751800537, Test_Loss: 1.2162485122680664 *\n",
      "Epoch: 4, Train_Loss: 1.2289118766784668, Test_Loss: 1.2067267894744873 *\n",
      "Epoch: 4, Train_Loss: 1.226006031036377, Test_Loss: 1.202484130859375 *\n",
      "Epoch: 4, Train_Loss: 1.2215955257415771, Test_Loss: 1.204129934310913\n",
      "Epoch: 4, Train_Loss: 1.2290199995040894, Test_Loss: 1.2020782232284546 *\n",
      "Epoch: 4, Train_Loss: 1.2091550827026367, Test_Loss: 1.200218915939331 *\n",
      "Epoch: 4, Train_Loss: 1.2020148038864136, Test_Loss: 1.202581524848938\n",
      "Epoch: 4, Train_Loss: 1.197696328163147, Test_Loss: 1.1987730264663696 *\n",
      "Epoch: 4, Train_Loss: 1.216488242149353, Test_Loss: 1.2004430294036865\n",
      "Epoch: 4, Train_Loss: 1.2196779251098633, Test_Loss: 1.200524926185608\n",
      "Epoch: 4, Train_Loss: 1.2151085138320923, Test_Loss: 1.1976357698440552 *\n",
      "Epoch: 4, Train_Loss: 1.1983708143234253, Test_Loss: 1.1966018676757812 *\n",
      "Epoch: 4, Train_Loss: 1.2678147554397583, Test_Loss: 1.196126103401184 *\n",
      "Epoch: 4, Train_Loss: 1.2572529315948486, Test_Loss: 1.1940109729766846 *\n",
      "Epoch: 4, Train_Loss: 1.2311339378356934, Test_Loss: 1.1943517923355103\n",
      "Epoch: 4, Train_Loss: 1.1959294080734253, Test_Loss: 1.200304388999939\n",
      "Epoch: 4, Train_Loss: 1.2137724161148071, Test_Loss: 1.24942147731781\n",
      "Epoch: 4, Train_Loss: 1.1934001445770264, Test_Loss: 2.963397979736328\n",
      "Epoch: 4, Train_Loss: 1.2106010913848877, Test_Loss: 4.976131439208984\n",
      "Epoch: 4, Train_Loss: 1.2003742456436157, Test_Loss: 1.1949677467346191 *\n",
      "Epoch: 4, Train_Loss: 1.215360403060913, Test_Loss: 1.1870098114013672 *\n",
      "Epoch: 4, Train_Loss: 2.793196678161621, Test_Loss: 1.2284759283065796\n",
      "Epoch: 4, Train_Loss: 4.979877948760986, Test_Loss: 1.2298043966293335\n",
      "Epoch: 4, Train_Loss: 1.2686305046081543, Test_Loss: 1.2342278957366943\n",
      "Epoch: 4, Train_Loss: 1.2021771669387817, Test_Loss: 1.2175827026367188 *\n",
      "Epoch: 4, Train_Loss: 1.202700138092041, Test_Loss: 1.3226137161254883\n",
      "Epoch: 4, Train_Loss: 1.3915233612060547, Test_Loss: 1.1898633241653442 *\n",
      "Epoch: 4, Train_Loss: 1.2423992156982422, Test_Loss: 1.1967195272445679\n",
      "Epoch: 4, Train_Loss: 1.1956132650375366, Test_Loss: 1.2167761325836182\n",
      "Epoch: 4, Train_Loss: 1.179822564125061, Test_Loss: 1.1890040636062622 *\n",
      "Epoch: 4, Train_Loss: 1.261397361755371, Test_Loss: 1.1915104389190674\n",
      "Epoch: 4, Train_Loss: 1.1949313879013062, Test_Loss: 1.2435457706451416\n",
      "Epoch: 4, Train_Loss: 1.1909964084625244, Test_Loss: 1.2661205530166626\n",
      "Epoch: 4, Train_Loss: 1.6783735752105713, Test_Loss: 1.2585529088974 *\n",
      "Epoch: 4, Train_Loss: 2.6660571098327637, Test_Loss: 1.3098512887954712\n",
      "Epoch: 4, Train_Loss: 2.195716381072998, Test_Loss: 1.192786693572998 *\n",
      "Epoch: 4, Train_Loss: 1.3249261379241943, Test_Loss: 1.2048243284225464\n",
      "Epoch: 4, Train_Loss: 1.4502911567687988, Test_Loss: 1.1897271871566772 *\n",
      "Epoch: 4, Train_Loss: 3.526555061340332, Test_Loss: 1.1863324642181396 *\n",
      "Epoch: 4, Train_Loss: 1.9932937622070312, Test_Loss: 1.186079502105713 *\n",
      "Epoch: 4, Train_Loss: 1.2126619815826416, Test_Loss: 1.185312271118164 *\n",
      "Epoch: 4, Train_Loss: 1.1924145221710205, Test_Loss: 1.183091163635254 *\n",
      "Epoch: 4, Train_Loss: 2.1659011840820312, Test_Loss: 1.1821634769439697 *\n",
      "Epoch: 4, Train_Loss: 2.8496110439300537, Test_Loss: 1.1898993253707886\n",
      "Epoch: 4, Train_Loss: 1.7994680404663086, Test_Loss: 1.1827785968780518 *\n",
      "Epoch: 4, Train_Loss: 1.190359354019165, Test_Loss: 1.183488130569458\n",
      "Epoch: 4, Train_Loss: 1.1772981882095337, Test_Loss: 1.1673439741134644 *\n",
      "Epoch: 4, Train_Loss: 1.6084387302398682, Test_Loss: 1.1839854717254639\n",
      "Epoch: 4, Train_Loss: 1.4813443422317505, Test_Loss: 1.2304883003234863\n",
      "Epoch: 4, Train_Loss: 1.177142858505249, Test_Loss: 1.170668601989746 *\n",
      "Epoch: 4, Train_Loss: 1.216341257095337, Test_Loss: 1.6414762735366821\n",
      "Epoch: 4, Train_Loss: 1.2926687002182007, Test_Loss: 1.707769751548767\n",
      "Epoch: 4, Train_Loss: 1.3033045530319214, Test_Loss: 1.3357610702514648 *\n",
      "Epoch: 4, Train_Loss: 1.2383161783218384, Test_Loss: 1.1851009130477905 *\n",
      "Epoch: 4, Train_Loss: 1.5773208141326904, Test_Loss: 1.179387092590332 *\n",
      "Epoch: 4, Train_Loss: 1.2928003072738647, Test_Loss: 1.1794416904449463\n",
      "Epoch: 4, Train_Loss: 1.2276777029037476, Test_Loss: 1.3281986713409424\n",
      "Epoch: 4, Train_Loss: 1.3931463956832886, Test_Loss: 2.3252573013305664\n",
      "Epoch: 4, Train_Loss: 1.4783004522323608, Test_Loss: 2.0341989994049072 *\n",
      "Epoch: 4, Train_Loss: 1.5844862461090088, Test_Loss: 1.2054470777511597 *\n",
      "Epoch: 4, Train_Loss: 1.3435540199279785, Test_Loss: 1.2083244323730469\n",
      "Epoch: 4, Train_Loss: 1.2395583391189575, Test_Loss: 1.1541863679885864 *\n",
      "Epoch: 4, Train_Loss: 1.2851192951202393, Test_Loss: 1.1551378965377808\n",
      "Epoch: 4, Train_Loss: 1.2133913040161133, Test_Loss: 1.1581578254699707\n",
      "Epoch: 4, Train_Loss: 1.164701223373413, Test_Loss: 1.1626713275909424\n",
      "Epoch: 4, Train_Loss: 1.1480903625488281, Test_Loss: 1.20821213722229\n",
      "Epoch: 4, Train_Loss: 1.149012565612793, Test_Loss: 1.150541067123413 *\n",
      "Epoch: 4, Train_Loss: 1.1487199068069458, Test_Loss: 1.1703579425811768\n",
      "Epoch: 4, Train_Loss: 1.1494803428649902, Test_Loss: 1.2702147960662842\n",
      "Epoch: 4, Train_Loss: 1.1851850748062134, Test_Loss: 1.537846565246582\n",
      "Epoch: 4, Train_Loss: 1.2176461219787598, Test_Loss: 1.4123691320419312 *\n",
      "Epoch: 4, Train_Loss: 1.2259773015975952, Test_Loss: 1.1635515689849854 *\n",
      "Epoch: 4, Train_Loss: 1.2431602478027344, Test_Loss: 1.1534305810928345 *\n",
      "Epoch: 4, Train_Loss: 1.5754914283752441, Test_Loss: 1.1521883010864258 *\n",
      "Epoch: 4, Train_Loss: 1.2426034212112427, Test_Loss: 1.151524305343628 *\n",
      "Epoch: 4, Train_Loss: 1.1637182235717773, Test_Loss: 1.154348611831665\n",
      "Epoch: 4, Train_Loss: 1.237149715423584, Test_Loss: 1.720627784729004\n",
      "Epoch: 4, Train_Loss: 1.5823875665664673, Test_Loss: 6.011505603790283\n",
      "Epoch: 4, Train_Loss: 1.5755685567855835, Test_Loss: 1.178601861000061 *\n",
      "Epoch: 4, Train_Loss: 1.1397197246551514, Test_Loss: 1.1424235105514526 *\n",
      "Epoch: 4, Train_Loss: 1.140210509300232, Test_Loss: 1.1371173858642578 *\n",
      "Epoch: 4, Train_Loss: 1.712990164756775, Test_Loss: 1.1396903991699219\n",
      "Epoch: 4, Train_Loss: 1.7801333665847778, Test_Loss: 1.138136386871338 *\n",
      "Epoch: 4, Train_Loss: 1.274616003036499, Test_Loss: 1.1359550952911377 *\n",
      "Epoch: 4, Train_Loss: 1.162637710571289, Test_Loss: 1.1437746286392212\n",
      "Epoch: 4, Train_Loss: 1.1471418142318726, Test_Loss: 1.1383548974990845 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.8101189136505127, Test_Loss: 1.1401370763778687\n",
      "Epoch: 4, Train_Loss: 2.4118213653564453, Test_Loss: 1.1391472816467285 *\n",
      "Epoch: 4, Train_Loss: 1.228556752204895, Test_Loss: 1.148665189743042\n",
      "Epoch: 4, Train_Loss: 1.156631350517273, Test_Loss: 1.1373250484466553 *\n",
      "Epoch: 4, Train_Loss: 1.1333372592926025, Test_Loss: 1.1346241235733032 *\n",
      "Epoch: 4, Train_Loss: 1.1327283382415771, Test_Loss: 1.143815279006958\n",
      "Epoch: 4, Train_Loss: 1.6081273555755615, Test_Loss: 1.1331769227981567 *\n",
      "Epoch: 4, Train_Loss: 1.1488107442855835, Test_Loss: 1.1290767192840576 *\n",
      "Epoch: 4, Train_Loss: 1.2371927499771118, Test_Loss: 1.1278653144836426 *\n",
      "Epoch: 4, Train_Loss: 1.385955810546875, Test_Loss: 1.1308282613754272\n",
      "Epoch: 4, Train_Loss: 1.1363118886947632, Test_Loss: 1.1260565519332886 *\n",
      "Epoch: 4, Train_Loss: 1.1834962368011475, Test_Loss: 1.133148193359375\n",
      "Epoch: 4, Train_Loss: 1.2373319864273071, Test_Loss: 1.1271262168884277 *\n",
      "Epoch: 4, Train_Loss: 1.4311339855194092, Test_Loss: 1.1359801292419434\n",
      "Epoch: 4, Train_Loss: 1.1448578834533691, Test_Loss: 1.1346125602722168 *\n",
      "Epoch: 4, Train_Loss: 1.1743682622909546, Test_Loss: 1.1254132986068726 *\n",
      "Epoch: 4, Train_Loss: 1.2092984914779663, Test_Loss: 1.1236928701400757 *\n",
      "Epoch: 4, Train_Loss: 1.2142245769500732, Test_Loss: 1.1261394023895264\n",
      "Epoch: 4, Train_Loss: 1.180600881576538, Test_Loss: 1.1219854354858398 *\n",
      "Epoch: 4, Train_Loss: 1.121437668800354, Test_Loss: 1.1198457479476929 *\n",
      "Epoch: 4, Train_Loss: 1.1334878206253052, Test_Loss: 1.1417431831359863\n",
      "Epoch: 4, Train_Loss: 1.1391829252243042, Test_Loss: 1.1827843189239502\n",
      "Epoch: 4, Train_Loss: 1.4983116388320923, Test_Loss: 3.9377951622009277\n",
      "Epoch: 4, Train_Loss: 1.4193708896636963, Test_Loss: 3.493593215942383 *\n",
      "Epoch: 4, Train_Loss: 1.5711718797683716, Test_Loss: 1.145262598991394 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.8446288108825684, Test_Loss: 1.164015293121338\n",
      "Epoch: 4, Train_Loss: 1.302607774734497, Test_Loss: 1.1886752843856812\n",
      "Epoch: 4, Train_Loss: 1.3968966007232666, Test_Loss: 1.1283732652664185 *\n",
      "Epoch: 4, Train_Loss: 1.205418348312378, Test_Loss: 1.1448676586151123\n",
      "Epoch: 4, Train_Loss: 1.122864007949829, Test_Loss: 1.159613847732544\n",
      "Epoch: 4, Train_Loss: 1.1159939765930176, Test_Loss: 1.1735702753067017\n",
      "Epoch: 4, Train_Loss: 1.1453602313995361, Test_Loss: 1.1328779458999634 *\n",
      "Epoch: 4, Train_Loss: 1.3371179103851318, Test_Loss: 1.1545544862747192\n",
      "Epoch: 4, Train_Loss: 1.5983383655548096, Test_Loss: 1.2234761714935303\n",
      "Epoch: 4, Train_Loss: 1.463581919670105, Test_Loss: 1.259028673171997\n",
      "Epoch: 4, Train_Loss: 3.0446219444274902, Test_Loss: 1.199295163154602 *\n",
      "Epoch: 4, Train_Loss: 1.547921895980835, Test_Loss: 1.1810784339904785 *\n",
      "Epoch: 4, Train_Loss: 1.9897217750549316, Test_Loss: 1.1390339136123657 *\n",
      "Epoch: 4, Train_Loss: 1.1758660078048706, Test_Loss: 1.1559948921203613\n",
      "Epoch: 4, Train_Loss: 1.1122254133224487, Test_Loss: 1.1116598844528198 *\n",
      "Epoch: 4, Train_Loss: 1.5189619064331055, Test_Loss: 1.3670575618743896\n",
      "Epoch: 4, Train_Loss: 2.398704767227173, Test_Loss: 1.1508018970489502 *\n",
      "Epoch: 4, Train_Loss: 1.5354900360107422, Test_Loss: 1.0999733209609985 *\n",
      "Epoch: 4, Train_Loss: 1.2301621437072754, Test_Loss: 1.11054265499115\n",
      "Epoch: 4, Train_Loss: 1.1699399948120117, Test_Loss: 1.1025131940841675 *\n",
      "Epoch: 4, Train_Loss: 1.1730577945709229, Test_Loss: 1.1032330989837646\n",
      "Epoch: 4, Train_Loss: 1.4256516695022583, Test_Loss: 1.1068741083145142\n",
      "Epoch: 4, Train_Loss: 1.4050570726394653, Test_Loss: 1.1039625406265259 *\n",
      "Epoch: 4, Train_Loss: 2.0179953575134277, Test_Loss: 1.1229974031448364\n",
      "Epoch: 4, Train_Loss: 1.4233429431915283, Test_Loss: 1.1180540323257446 *\n",
      "Epoch: 4, Train_Loss: 1.1268129348754883, Test_Loss: 1.1098405122756958 *\n",
      "Epoch: 4, Train_Loss: 1.1000007390975952, Test_Loss: 1.152298927307129\n",
      "Epoch: 4, Train_Loss: 1.0960749387741089, Test_Loss: 1.1015527248382568 *\n",
      "Epoch: 4, Train_Loss: 1.129917860031128, Test_Loss: 1.1004983186721802 *\n",
      "Epoch: 4, Train_Loss: 1.1289311647415161, Test_Loss: 1.1418018341064453\n",
      "Epoch: 4, Train_Loss: 1.115210771560669, Test_Loss: 1.41431725025177\n",
      "Epoch: 4, Train_Loss: 6.179856777191162, Test_Loss: 1.3742656707763672 *\n",
      "Epoch: 4, Train_Loss: 13.171114921569824, Test_Loss: 1.1627273559570312 *\n",
      "Epoch: 4, Train_Loss: 1.6245083808898926, Test_Loss: 1.0920250415802002 *\n",
      "Epoch: 4, Train_Loss: 4.508218288421631, Test_Loss: 1.1098142862319946\n",
      "Epoch: 4, Train_Loss: 1.6023280620574951, Test_Loss: 1.157309651374817\n",
      "Epoch: 4, Train_Loss: 1.1883424520492554, Test_Loss: 1.327280044555664\n",
      "Epoch: 4, Train_Loss: 1.1897028684616089, Test_Loss: 1.9713160991668701\n",
      "Epoch: 4, Train_Loss: 10.645463943481445, Test_Loss: 1.640594244003296 *\n",
      "Epoch: 4, Train_Loss: 3.2198681831359863, Test_Loss: 1.126711130142212 *\n",
      "Epoch: 4, Train_Loss: 1.1160705089569092, Test_Loss: 1.1183232069015503 *\n",
      "Epoch: 4, Train_Loss: 3.2944753170013428, Test_Loss: 1.1223846673965454\n",
      "Epoch: 4, Train_Loss: 4.691835403442383, Test_Loss: 1.1431875228881836\n",
      "Epoch: 4, Train_Loss: 1.1764490604400635, Test_Loss: 1.175035834312439\n",
      "Epoch: 4, Train_Loss: 1.0988123416900635, Test_Loss: 1.1137921810150146 *\n",
      "Epoch: 4, Train_Loss: 1.085360050201416, Test_Loss: 1.3216148614883423\n",
      "Epoch: 4, Train_Loss: 1.090556025505066, Test_Loss: 1.1373494863510132 *\n",
      "Epoch: 4, Train_Loss: 1.0895683765411377, Test_Loss: 1.1786632537841797\n",
      "Epoch: 4, Train_Loss: 1.0782525539398193, Test_Loss: 1.3297735452651978\n",
      "Epoch: 4, Train_Loss: 1.081610918045044, Test_Loss: 1.2807382345199585 *\n",
      "Epoch: 4, Train_Loss: 1.077998399734497, Test_Loss: 1.1632486581802368 *\n",
      "Epoch: 4, Train_Loss: 1.1182143688201904, Test_Loss: 1.1783912181854248\n",
      "Epoch: 4, Train_Loss: 1.150538444519043, Test_Loss: 1.1922144889831543\n",
      "Epoch: 4, Train_Loss: 1.1509995460510254, Test_Loss: 1.1818939447402954 *\n",
      "Epoch: 4, Train_Loss: 1.1774214506149292, Test_Loss: 1.162119746208191 *\n",
      "Epoch: 4, Train_Loss: 1.2111382484436035, Test_Loss: 1.1103315353393555 *\n",
      "Epoch: 4, Train_Loss: 1.2012791633605957, Test_Loss: 3.149245500564575\n",
      "Epoch: 4, Train_Loss: 1.0992337465286255, Test_Loss: 5.430334091186523\n",
      "Epoch: 4, Train_Loss: 1.0893186330795288, Test_Loss: 1.1485778093338013 *\n",
      "Epoch: 4, Train_Loss: 1.0804930925369263, Test_Loss: 1.1637605428695679\n",
      "Epoch: 4, Train_Loss: 1.072938323020935, Test_Loss: 1.1368478536605835 *\n",
      "Epoch: 4, Train_Loss: 1.0753896236419678, Test_Loss: 1.0957574844360352 *\n",
      "Epoch: 4, Train_Loss: 1.0683034658432007, Test_Loss: 1.1477725505828857\n",
      "Epoch: 4, Train_Loss: 1.0700383186340332, Test_Loss: 1.1393259763717651 *\n",
      "Epoch: 4, Train_Loss: 1.067886471748352, Test_Loss: 1.1378153562545776 *\n",
      "Epoch: 4, Train_Loss: 1.0643091201782227, Test_Loss: 1.1079879999160767 *\n",
      "Epoch: 4, Train_Loss: 1.064448356628418, Test_Loss: 1.1198039054870605\n",
      "Epoch: 4, Train_Loss: 1.0638511180877686, Test_Loss: 1.117638111114502 *\n",
      "Epoch: 4, Train_Loss: 1.063742995262146, Test_Loss: 1.147655725479126\n",
      "Epoch: 4, Train_Loss: 1.0649197101593018, Test_Loss: 1.0669645071029663 *\n",
      "Epoch: 4, Train_Loss: 1.1025313138961792, Test_Loss: 1.0683408975601196\n",
      "Epoch: 4, Train_Loss: 1.0764424800872803, Test_Loss: 1.106455683708191\n",
      "Epoch: 4, Train_Loss: 1.0767756700515747, Test_Loss: 1.0935865640640259 *\n",
      "Epoch: 4, Train_Loss: 9.140503883361816, Test_Loss: 1.1021071672439575\n",
      "Epoch: 4, Train_Loss: 2.0831046104431152, Test_Loss: 1.0854579210281372 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 1.0717889070510864, Test_Loss: 1.1280224323272705\n",
      "Epoch: 4, Train_Loss: 1.1054372787475586, Test_Loss: 1.085058331489563 *\n",
      "Epoch: 4, Train_Loss: 1.1363904476165771, Test_Loss: 1.0955760478973389\n",
      "Epoch: 4, Train_Loss: 1.05886709690094, Test_Loss: 1.113610029220581\n",
      "Epoch: 4, Train_Loss: 1.0597798824310303, Test_Loss: 1.1559035778045654\n",
      "Epoch: 4, Train_Loss: 1.1090140342712402, Test_Loss: 1.1415622234344482 *\n",
      "Epoch: 4, Train_Loss: 1.2359743118286133, Test_Loss: 1.1130082607269287 *\n",
      "Epoch: 4, Train_Loss: 1.2854101657867432, Test_Loss: 1.0958119630813599 *\n",
      "Epoch: 4, Train_Loss: 1.2115392684936523, Test_Loss: 1.0923688411712646 *\n",
      "Epoch: 4, Train_Loss: 1.0726261138916016, Test_Loss: 1.0910720825195312 *\n",
      "Epoch: 4, Train_Loss: 1.1493637561798096, Test_Loss: 1.0738338232040405 *\n",
      "Epoch: 4, Train_Loss: 1.1610982418060303, Test_Loss: 1.170436143875122\n",
      "Epoch: 4, Train_Loss: 1.2136648893356323, Test_Loss: 1.078662395477295 *\n",
      "Epoch: 4, Train_Loss: 1.1714122295379639, Test_Loss: 5.294407844543457\n",
      "Epoch: 4, Train_Loss: 1.173711895942688, Test_Loss: 2.7467639446258545 *\n",
      "Epoch: 4, Train_Loss: 1.1121770143508911, Test_Loss: 1.053663730621338 *\n",
      "Epoch: 4, Train_Loss: 1.0570167303085327, Test_Loss: 1.0551023483276367\n",
      "Epoch: 4, Train_Loss: 1.1060435771942139, Test_Loss: 1.0639935731887817\n",
      "Epoch: 4, Train_Loss: 1.067590355873108, Test_Loss: 1.060447096824646 *\n",
      "Epoch: 4, Train_Loss: 1.0570813417434692, Test_Loss: 1.054654598236084 *\n",
      "Epoch: 4, Train_Loss: 1.049428939819336, Test_Loss: 1.1430197954177856\n",
      "Epoch: 4, Train_Loss: 1.0500624179840088, Test_Loss: 1.1608021259307861\n",
      "Epoch: 4, Train_Loss: 1.0874998569488525, Test_Loss: 1.043159008026123 *\n",
      "Epoch: 4, Train_Loss: 6.497043132781982, Test_Loss: 1.0926423072814941\n",
      "Epoch: 4, Train_Loss: 1.1142594814300537, Test_Loss: 1.0556719303131104 *\n",
      "Epoch: 4, Train_Loss: 1.0632632970809937, Test_Loss: 1.0580050945281982\n",
      "Epoch: 4, Train_Loss: 1.0769320726394653, Test_Loss: 1.0469179153442383 *\n",
      "Epoch: 4, Train_Loss: 1.0633984804153442, Test_Loss: 1.0892393589019775\n",
      "Epoch: 4, Train_Loss: 1.05392587184906, Test_Loss: 1.0758123397827148 *\n",
      "Epoch: 4, Train_Loss: 1.0462156534194946, Test_Loss: 1.1793768405914307\n",
      "Epoch: 4, Train_Loss: 1.0552794933319092, Test_Loss: 1.1501089334487915 *\n",
      "Epoch: 4, Train_Loss: 1.050060749053955, Test_Loss: 1.051697850227356 *\n",
      "Epoch: 4, Train_Loss: 1.0434331893920898, Test_Loss: 1.0374813079833984 *\n",
      "Epoch: 4, Train_Loss: 1.0684767961502075, Test_Loss: 1.034570336341858 *\n",
      "Epoch: 4, Train_Loss: 1.0486197471618652, Test_Loss: 1.035149097442627\n",
      "Epoch: 4, Train_Loss: 1.0474549531936646, Test_Loss: 1.034810185432434 *\n",
      "Epoch: 4, Train_Loss: 1.0650540590286255, Test_Loss: 1.0361998081207275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 1.0373528003692627, Test_Loss: 1.0345708131790161 *\n",
      "Epoch: 4, Train_Loss: 1.0384012460708618, Test_Loss: 1.0310853719711304 *\n",
      "Epoch: 4, Train_Loss: 1.0578923225402832, Test_Loss: 1.0339176654815674\n",
      "Epoch: 4, Train_Loss: 1.0851603746414185, Test_Loss: 1.0296361446380615 *\n",
      "Epoch: 4, Train_Loss: 1.0633488893508911, Test_Loss: 1.0336133241653442\n",
      "Epoch: 4, Train_Loss: 1.0353832244873047, Test_Loss: 1.0455001592636108\n",
      "Epoch: 4, Train_Loss: 1.0355136394500732, Test_Loss: 1.038604736328125 *\n",
      "Epoch: 4, Train_Loss: 1.0774314403533936, Test_Loss: 1.0479122400283813\n",
      "Epoch: 4, Train_Loss: 1.0972448587417603, Test_Loss: 1.140357494354248\n",
      "Epoch: 4, Train_Loss: 1.0749632120132446, Test_Loss: 1.4397406578063965\n",
      "Epoch: 4, Train_Loss: 1.0536775588989258, Test_Loss: 1.3101608753204346 *\n",
      "Epoch: 4, Train_Loss: 1.0754942893981934, Test_Loss: 1.1063916683197021 *\n",
      "Epoch: 4, Train_Loss: 1.109972596168518, Test_Loss: 1.0315611362457275 *\n",
      "Epoch: 4, Train_Loss: 1.0978038311004639, Test_Loss: 1.039246916770935\n",
      "Epoch: 4, Train_Loss: 1.0743409395217896, Test_Loss: 1.1150555610656738\n",
      "Epoch: 4, Train_Loss: 1.1796976327896118, Test_Loss: 1.5137913227081299\n",
      "Epoch: 4, Train_Loss: 1.0480107069015503, Test_Loss: 2.239464282989502\n",
      "Epoch: 5, Train_Loss: 1.0427436828613281, Test_Loss: 1.475422978401184 *\n",
      "Epoch: 5, Train_Loss: 1.0275934934616089, Test_Loss: 1.0872570276260376 *\n",
      "Epoch: 5, Train_Loss: 1.0241035223007202, Test_Loss: 1.0271693468093872 *\n",
      "Epoch: 5, Train_Loss: 1.0234991312026978, Test_Loss: 1.0310981273651123\n",
      "Epoch: 5, Train_Loss: 1.0226632356643677, Test_Loss: 1.0265647172927856 *\n",
      "Epoch: 5, Train_Loss: 1.022687315940857, Test_Loss: 1.0325829982757568\n",
      "Epoch: 5, Train_Loss: 5.5139875411987305, Test_Loss: 1.0535355806350708\n",
      "Epoch: 5, Train_Loss: 1.4258439540863037, Test_Loss: 1.0759657621383667\n",
      "Epoch: 5, Train_Loss: 1.0203956365585327, Test_Loss: 1.0203709602355957 *\n",
      "Epoch: 5, Train_Loss: 1.0357965230941772, Test_Loss: 1.091825008392334\n",
      "Epoch: 5, Train_Loss: 1.0164004564285278, Test_Loss: 1.2714208364486694\n",
      "Epoch: 5, Train_Loss: 1.0118041038513184, Test_Loss: 1.2166204452514648 *\n",
      "Epoch: 5, Train_Loss: 1.013204574584961, Test_Loss: 1.2063570022583008 *\n",
      "Epoch: 5, Train_Loss: 1.0121649503707886, Test_Loss: 1.0172364711761475 *\n",
      "Epoch: 5, Train_Loss: 1.0105977058410645, Test_Loss: 1.0162121057510376 *\n",
      "Epoch: 5, Train_Loss: 1.0094548463821411, Test_Loss: 1.015708565711975 *\n",
      "Epoch: 5, Train_Loss: 1.067702293395996, Test_Loss: 1.0154712200164795 *\n",
      "Epoch: 5, Train_Loss: 1.0937917232513428, Test_Loss: 1.0243078470230103\n",
      "Epoch: 5, Train_Loss: 1.1087979078292847, Test_Loss: 4.021132469177246\n",
      "Epoch: 5, Train_Loss: 1.0971722602844238, Test_Loss: 3.5066018104553223 *\n",
      "Epoch: 5, Train_Loss: 1.007127046585083, Test_Loss: 1.014153242111206 *\n",
      "Epoch: 5, Train_Loss: 1.0662455558776855, Test_Loss: 1.009089469909668 *\n",
      "Epoch: 5, Train_Loss: 1.1938800811767578, Test_Loss: 1.0062437057495117 *\n",
      "Epoch: 5, Train_Loss: 1.1997904777526855, Test_Loss: 1.011115550994873\n",
      "Epoch: 5, Train_Loss: 1.1814924478530884, Test_Loss: 1.010934591293335 *\n",
      "Epoch: 5, Train_Loss: 1.010823130607605, Test_Loss: 1.0163671970367432\n",
      "Epoch: 5, Train_Loss: 1.0021132230758667, Test_Loss: 1.0118799209594727 *\n",
      "Epoch: 5, Train_Loss: 1.0033812522888184, Test_Loss: 1.0134482383728027\n",
      "Epoch: 5, Train_Loss: 1.0042874813079834, Test_Loss: 1.0170767307281494\n",
      "Epoch: 5, Train_Loss: 1.0069584846496582, Test_Loss: 1.0168377161026 *\n",
      "Epoch: 5, Train_Loss: 1.0030848979949951, Test_Loss: 1.0206588506698608\n",
      "Epoch: 5, Train_Loss: 1.0021559000015259, Test_Loss: 1.0064404010772705 *\n",
      "Epoch: 5, Train_Loss: 1.0026586055755615, Test_Loss: 1.0027772188186646 *\n",
      "Epoch: 5, Train_Loss: 1.0005319118499756, Test_Loss: 1.0144717693328857\n",
      "Epoch: 5, Train_Loss: 1.0053458213806152, Test_Loss: 1.0002310276031494 *\n",
      "Epoch: 5, Train_Loss: 1.1102049350738525, Test_Loss: 1.0089737176895142\n",
      "Epoch: 5, Train_Loss: 1.1865299940109253, Test_Loss: 1.0013058185577393 *\n",
      "Epoch: 5, Train_Loss: 1.1901493072509766, Test_Loss: 1.007262945175171\n",
      "Epoch: 5, Train_Loss: 1.0477403402328491, Test_Loss: 1.0030674934387207 *\n",
      "Epoch: 5, Train_Loss: 1.1637237071990967, Test_Loss: 1.0015480518341064 *\n",
      "Epoch: 5, Train_Loss: 1.154905915260315, Test_Loss: 1.0076736211776733\n",
      "Epoch: 5, Train_Loss: 1.0371193885803223, Test_Loss: 1.021315574645996\n",
      "Epoch: 5, Train_Loss: 1.2030731439590454, Test_Loss: 1.0054454803466797 *\n",
      "Epoch: 5, Train_Loss: 1.1891415119171143, Test_Loss: 1.0043095350265503 *\n",
      "Epoch: 5, Train_Loss: 1.207858920097351, Test_Loss: 0.9974563717842102 *\n",
      "Epoch: 5, Train_Loss: 1.006441354751587, Test_Loss: 0.9987326264381409\n",
      "Epoch: 5, Train_Loss: 2.65203595161438, Test_Loss: 0.9981161952018738 *\n",
      "Epoch: 5, Train_Loss: 2.620521068572998, Test_Loss: 0.9948809146881104 *\n",
      "Epoch: 5, Train_Loss: 1.0211806297302246, Test_Loss: 1.042187213897705\n",
      "Epoch: 5, Train_Loss: 1.0241146087646484, Test_Loss: 1.018538475036621 *\n",
      "Epoch: 5, Train_Loss: 1.0271799564361572, Test_Loss: 6.196167945861816\n",
      "Epoch: 5, Train_Loss: 1.0205669403076172, Test_Loss: 1.4016211032867432 *\n",
      "Epoch: 5, Train_Loss: 0.9892576932907104, Test_Loss: 0.9866241812705994 *\n",
      "Epoch: 5, Train_Loss: 1.0117610692977905, Test_Loss: 1.0055640935897827\n",
      "Epoch: 5, Train_Loss: 1.1463520526885986, Test_Loss: 1.0321623086929321\n",
      "Epoch: 5, Train_Loss: 1.1048780679702759, Test_Loss: 1.0389384031295776\n",
      "Epoch: 5, Train_Loss: 1.089045524597168, Test_Loss: 0.9898477792739868 *\n",
      "Epoch: 5, Train_Loss: 1.0782644748687744, Test_Loss: 1.0771572589874268\n",
      "Epoch: 5, Train_Loss: 1.0409928560256958, Test_Loss: 1.0501773357391357 *\n",
      "Epoch: 5, Train_Loss: 1.0185350179672241, Test_Loss: 0.9828392267227173 *\n",
      "Epoch: 5, Train_Loss: 1.0023468732833862, Test_Loss: 1.0134236812591553\n",
      "Epoch: 5, Train_Loss: 1.0007548332214355, Test_Loss: 1.0020157098770142 *\n",
      "Epoch: 5, Train_Loss: 1.0044677257537842, Test_Loss: 0.9870606660842896 *\n",
      "Epoch: 5, Train_Loss: 0.9869351387023926, Test_Loss: 0.9916336536407471\n",
      "Epoch: 5, Train_Loss: 0.9789708852767944, Test_Loss: 1.1075198650360107\n",
      "Epoch: 5, Train_Loss: 1.0299310684204102, Test_Loss: 1.0135303735733032 *\n",
      "Epoch: 5, Train_Loss: 1.036156415939331, Test_Loss: 1.0707783699035645\n",
      "Epoch: 5, Train_Loss: 0.9975698590278625, Test_Loss: 1.0231684446334839 *\n",
      "Epoch: 5, Train_Loss: 0.9751016497612, Test_Loss: 1.028580665588379\n",
      "Epoch: 5, Train_Loss: 0.9746425747871399, Test_Loss: 0.9954786896705627 *\n",
      "Epoch: 5, Train_Loss: 0.9735214114189148, Test_Loss: 0.9945932030677795 *\n",
      "Epoch: 5, Train_Loss: 0.9735956192016602, Test_Loss: 0.9979184865951538\n",
      "Epoch: 5, Train_Loss: 0.9728602170944214, Test_Loss: 0.9983194470405579\n",
      "Epoch: 5, Train_Loss: 0.9728440046310425, Test_Loss: 0.9990522265434265\n",
      "Epoch: 5, Train_Loss: 0.972798764705658, Test_Loss: 1.0003297328948975\n",
      "Epoch: 5, Train_Loss: 0.9715425372123718, Test_Loss: 0.9911235570907593 *\n",
      "Epoch: 5, Train_Loss: 0.9700372219085693, Test_Loss: 1.001244068145752\n",
      "Epoch: 5, Train_Loss: 0.9745370745658875, Test_Loss: 0.9955182075500488 *\n",
      "Epoch: 5, Train_Loss: 0.982345461845398, Test_Loss: 0.9787740707397461 *\n",
      "Epoch: 5, Train_Loss: 0.982715904712677, Test_Loss: 0.9805543422698975\n",
      "Epoch: 5, Train_Loss: 0.9785128235816956, Test_Loss: 1.0325502157211304\n",
      "Epoch: 5, Train_Loss: 0.9881672859191895, Test_Loss: 0.9933826327323914 *\n",
      "Epoch: 5, Train_Loss: 0.9727649092674255, Test_Loss: 1.2292171716690063\n",
      "Epoch: 5, Train_Loss: 0.9705228805541992, Test_Loss: 1.5562562942504883\n",
      "Epoch: 5, Train_Loss: 0.965442419052124, Test_Loss: 1.2584484815597534 *\n",
      "Epoch: 5, Train_Loss: 0.9687690734863281, Test_Loss: 1.0575697422027588 *\n",
      "Epoch: 5, Train_Loss: 0.9880840182304382, Test_Loss: 0.9994954466819763 *\n",
      "Epoch: 5, Train_Loss: 0.9709600210189819, Test_Loss: 0.9727708101272583 *\n",
      "Epoch: 5, Train_Loss: 0.9648520350456238, Test_Loss: 1.0509395599365234\n",
      "Epoch: 5, Train_Loss: 0.961845338344574, Test_Loss: 1.614267110824585\n",
      "Epoch: 5, Train_Loss: 0.9761605858802795, Test_Loss: 2.2295987606048584\n",
      "Epoch: 5, Train_Loss: 1.034536361694336, Test_Loss: 1.1921257972717285 *\n",
      "Epoch: 5, Train_Loss: 1.0028716325759888, Test_Loss: 1.0646811723709106 *\n",
      "Epoch: 5, Train_Loss: 0.9956445693969727, Test_Loss: 0.960724949836731 *\n",
      "Epoch: 5, Train_Loss: 0.9588373899459839, Test_Loss: 0.9653686285018921\n",
      "Epoch: 5, Train_Loss: 1.005581259727478, Test_Loss: 0.9614009261131287 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 0.979209840297699, Test_Loss: 0.9729281067848206\n",
      "Epoch: 5, Train_Loss: 0.9600542783737183, Test_Loss: 0.9922748804092407\n",
      "Epoch: 5, Train_Loss: 0.9788265824317932, Test_Loss: 1.001932144165039\n",
      "Epoch: 5, Train_Loss: 0.9858710169792175, Test_Loss: 0.963203489780426 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.0707497596740723, Test_Loss: 1.0652433633804321\n",
      "Epoch: 5, Train_Loss: 1.0562894344329834, Test_Loss: 1.3339247703552246\n",
      "Epoch: 5, Train_Loss: 1.0244148969650269, Test_Loss: 1.048904299736023 *\n",
      "Epoch: 5, Train_Loss: 0.9809485673904419, Test_Loss: 1.1280250549316406\n",
      "Epoch: 5, Train_Loss: 0.9566421508789062, Test_Loss: 0.9602880477905273 *\n",
      "Epoch: 5, Train_Loss: 0.9800764918327332, Test_Loss: 0.9596743583679199 *\n",
      "Epoch: 5, Train_Loss: 0.952814519405365, Test_Loss: 0.9590665102005005 *\n",
      "Epoch: 5, Train_Loss: 0.9594809412956238, Test_Loss: 0.9584826231002808 *\n",
      "Epoch: 5, Train_Loss: 0.9639618396759033, Test_Loss: 0.9697147607803345\n",
      "Epoch: 5, Train_Loss: 0.9674826264381409, Test_Loss: 5.339369773864746\n",
      "Epoch: 5, Train_Loss: 1.0461369752883911, Test_Loss: 2.0687966346740723 *\n",
      "Epoch: 5, Train_Loss: 0.952052652835846, Test_Loss: 0.9572414755821228 *\n",
      "Epoch: 5, Train_Loss: 1.0291435718536377, Test_Loss: 0.9498082399368286 *\n",
      "Epoch: 5, Train_Loss: 0.957152783870697, Test_Loss: 0.9494025111198425 *\n",
      "Epoch: 5, Train_Loss: 0.9798334836959839, Test_Loss: 0.9554373025894165\n",
      "Epoch: 5, Train_Loss: 0.9895077347755432, Test_Loss: 0.9495097994804382 *\n",
      "Epoch: 5, Train_Loss: 1.223043441772461, Test_Loss: 0.9542315006256104\n",
      "Epoch: 5, Train_Loss: 0.9587976932525635, Test_Loss: 0.9480726718902588 *\n",
      "Epoch: 5, Train_Loss: 0.9775816798210144, Test_Loss: 0.9494936466217041\n",
      "Epoch: 5, Train_Loss: 0.9444640278816223, Test_Loss: 0.9520307183265686\n",
      "Epoch: 5, Train_Loss: 0.9447718262672424, Test_Loss: 0.9550437927246094\n",
      "Epoch: 5, Train_Loss: 0.9462804794311523, Test_Loss: 0.9500969648361206 *\n",
      "Epoch: 5, Train_Loss: 0.9438158273696899, Test_Loss: 0.9619185924530029\n",
      "Epoch: 5, Train_Loss: 0.9578703045845032, Test_Loss: 0.955493152141571 *\n",
      "Epoch: 5, Train_Loss: 0.9604716897010803, Test_Loss: 0.9475190043449402 *\n",
      "Epoch: 5, Train_Loss: 0.9635686278343201, Test_Loss: 0.9406525492668152 *\n",
      "Epoch: 5, Train_Loss: 0.9530119299888611, Test_Loss: 0.9440790414810181\n",
      "Epoch: 5, Train_Loss: 0.9584366083145142, Test_Loss: 0.9410860538482666 *\n",
      "Epoch: 5, Train_Loss: 0.9578222632408142, Test_Loss: 0.9401707649230957 *\n",
      "Epoch: 5, Train_Loss: 0.9423717856407166, Test_Loss: 0.9414432644844055\n",
      "Epoch: 5, Train_Loss: 0.9374507069587708, Test_Loss: 0.9385189414024353 *\n",
      "Epoch: 5, Train_Loss: 0.9535084366798401, Test_Loss: 0.9405283331871033\n",
      "Epoch: 5, Train_Loss: 0.9583594799041748, Test_Loss: 0.9440402388572693\n",
      "Epoch: 5, Train_Loss: 0.9649840593338013, Test_Loss: 0.9394140839576721 *\n",
      "Epoch: 5, Train_Loss: 0.9352876543998718, Test_Loss: 0.9387937784194946 *\n",
      "Epoch: 5, Train_Loss: 0.9895909428596497, Test_Loss: 0.9367539286613464 *\n",
      "Epoch: 5, Train_Loss: 1.0014138221740723, Test_Loss: 0.9366471767425537 *\n",
      "Epoch: 5, Train_Loss: 0.9793907999992371, Test_Loss: 0.9365221858024597 *\n",
      "Epoch: 5, Train_Loss: 0.9332345128059387, Test_Loss: 0.9377999305725098\n",
      "Epoch: 5, Train_Loss: 0.9630951285362244, Test_Loss: 0.9892801642417908\n",
      "Epoch: 5, Train_Loss: 0.934043288230896, Test_Loss: 1.0705065727233887\n",
      "Epoch: 5, Train_Loss: 0.9547134041786194, Test_Loss: 6.342855930328369\n",
      "Epoch: 5, Train_Loss: 0.9333264231681824, Test_Loss: 0.9535734057426453 *\n",
      "Epoch: 5, Train_Loss: 0.9516880512237549, Test_Loss: 0.9302698969841003 *\n",
      "Epoch: 5, Train_Loss: 1.154258131980896, Test_Loss: 0.9593693614006042\n",
      "Epoch: 5, Train_Loss: 4.608773708343506, Test_Loss: 0.9825531244277954\n",
      "Epoch: 5, Train_Loss: 2.5105929374694824, Test_Loss: 0.9815463423728943 *\n",
      "Epoch: 5, Train_Loss: 0.9476364850997925, Test_Loss: 0.9317202568054199 *\n",
      "Epoch: 5, Train_Loss: 0.9299732446670532, Test_Loss: 1.049185037612915\n",
      "Epoch: 5, Train_Loss: 1.090721607208252, Test_Loss: 0.9751190543174744 *\n",
      "Epoch: 5, Train_Loss: 1.0308456420898438, Test_Loss: 0.92621910572052 *\n",
      "Epoch: 5, Train_Loss: 0.9419938325881958, Test_Loss: 0.9682562351226807\n",
      "Epoch: 5, Train_Loss: 0.9246516823768616, Test_Loss: 0.9389483332633972 *\n",
      "Epoch: 5, Train_Loss: 0.9853342771530151, Test_Loss: 0.933436930179596 *\n",
      "Epoch: 5, Train_Loss: 0.9503999948501587, Test_Loss: 0.9613178968429565\n",
      "Epoch: 5, Train_Loss: 0.9378215074539185, Test_Loss: 1.028403401374817\n",
      "Epoch: 5, Train_Loss: 1.1134039163589478, Test_Loss: 0.9719757437705994 *\n",
      "Epoch: 5, Train_Loss: 2.23702335357666, Test_Loss: 1.0247241258621216\n",
      "Epoch: 5, Train_Loss: 2.234861373901367, Test_Loss: 0.9620927572250366 *\n",
      "Epoch: 5, Train_Loss: 1.0489128828048706, Test_Loss: 0.9656411409378052\n",
      "Epoch: 5, Train_Loss: 0.9929513931274414, Test_Loss: 0.937938392162323 *\n",
      "Epoch: 5, Train_Loss: 3.000865936279297, Test_Loss: 0.9324632883071899 *\n",
      "Epoch: 5, Train_Loss: 2.192279815673828, Test_Loss: 0.9372394680976868\n",
      "Epoch: 5, Train_Loss: 0.9603642225265503, Test_Loss: 0.9389692544937134\n",
      "Epoch: 5, Train_Loss: 0.9433485865592957, Test_Loss: 0.9378406405448914 *\n",
      "Epoch: 5, Train_Loss: 1.352431297302246, Test_Loss: 0.935467004776001 *\n",
      "Epoch: 5, Train_Loss: 2.4498255252838135, Test_Loss: 0.9283665418624878 *\n",
      "Epoch: 5, Train_Loss: 1.881218671798706, Test_Loss: 0.9276508688926697 *\n",
      "Epoch: 5, Train_Loss: 0.929483950138092, Test_Loss: 0.9249616861343384 *\n",
      "Epoch: 5, Train_Loss: 0.9393337368965149, Test_Loss: 0.9233779311180115 *\n",
      "Epoch: 5, Train_Loss: 1.118821144104004, Test_Loss: 0.9207071661949158 *\n",
      "Epoch: 5, Train_Loss: 1.5681066513061523, Test_Loss: 0.9487884640693665\n",
      "Epoch: 5, Train_Loss: 0.9572957158088684, Test_Loss: 0.9305369257926941 *\n",
      "Epoch: 5, Train_Loss: 1.0039472579956055, Test_Loss: 1.2114133834838867\n",
      "Epoch: 5, Train_Loss: 1.0422866344451904, Test_Loss: 1.4347784519195557\n",
      "Epoch: 5, Train_Loss: 1.027806043624878, Test_Loss: 1.1247923374176025 *\n",
      "Epoch: 5, Train_Loss: 1.0225328207015991, Test_Loss: 0.9662578701972961 *\n",
      "Epoch: 5, Train_Loss: 1.1882874965667725, Test_Loss: 0.9336185455322266 *\n",
      "Epoch: 5, Train_Loss: 1.0939158201217651, Test_Loss: 0.9225406646728516 *\n",
      "Epoch: 5, Train_Loss: 0.9735437631607056, Test_Loss: 0.9991458058357239\n",
      "Epoch: 5, Train_Loss: 1.087754249572754, Test_Loss: 1.5856101512908936\n",
      "Epoch: 5, Train_Loss: 1.128173589706421, Test_Loss: 1.9395605325698853\n",
      "Epoch: 5, Train_Loss: 1.3020853996276855, Test_Loss: 1.0128517150878906 *\n",
      "Epoch: 5, Train_Loss: 1.1707926988601685, Test_Loss: 0.9780259132385254 *\n",
      "Epoch: 5, Train_Loss: 0.9584465026855469, Test_Loss: 0.9090756177902222 *\n",
      "Epoch: 5, Train_Loss: 1.0562576055526733, Test_Loss: 0.9144686460494995\n",
      "Epoch: 5, Train_Loss: 1.0079032182693481, Test_Loss: 0.9168691039085388\n",
      "Epoch: 5, Train_Loss: 0.918209433555603, Test_Loss: 0.9344260096549988\n",
      "Epoch: 5, Train_Loss: 0.9096232056617737, Test_Loss: 0.9315285086631775 *\n",
      "Epoch: 5, Train_Loss: 0.9077263474464417, Test_Loss: 0.9293005466461182 *\n",
      "Epoch: 5, Train_Loss: 0.9069495797157288, Test_Loss: 0.9123870730400085 *\n",
      "Epoch: 5, Train_Loss: 0.9106213450431824, Test_Loss: 1.0165330171585083\n",
      "Epoch: 5, Train_Loss: 0.923454999923706, Test_Loss: 1.3109830617904663\n",
      "Epoch: 5, Train_Loss: 0.9605069756507874, Test_Loss: 1.0856082439422607 *\n",
      "Epoch: 5, Train_Loss: 0.9304730892181396, Test_Loss: 1.0609047412872314 *\n",
      "Epoch: 5, Train_Loss: 0.9917548298835754, Test_Loss: 0.9032624363899231 *\n",
      "Epoch: 5, Train_Loss: 1.062722086906433, Test_Loss: 0.9027511477470398 *\n",
      "Epoch: 5, Train_Loss: 1.2207568883895874, Test_Loss: 0.9024549722671509 *\n",
      "Epoch: 5, Train_Loss: 0.9272885322570801, Test_Loss: 0.9069319367408752\n",
      "Epoch: 5, Train_Loss: 0.9391227960586548, Test_Loss: 0.9604586362838745\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 1.205298900604248, Test_Loss: 6.114500045776367\n",
      "Epoch: 5, Train_Loss: 1.2808926105499268, Test_Loss: 1.1900099515914917 *\n",
      "Epoch: 5, Train_Loss: 0.9585824012756348, Test_Loss: 0.9066430330276489 *\n",
      "Epoch: 5, Train_Loss: 0.9080855846405029, Test_Loss: 0.903536856174469 *\n",
      "Epoch: 5, Train_Loss: 1.2857742309570312, Test_Loss: 0.8970204591751099 *\n",
      "Epoch: 5, Train_Loss: 1.450082540512085, Test_Loss: 0.9031151533126831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 1.0863112211227417, Test_Loss: 0.9072427153587341\n",
      "Epoch: 5, Train_Loss: 0.9124305844306946, Test_Loss: 0.9325046539306641\n",
      "Epoch: 5, Train_Loss: 0.9150851368904114, Test_Loss: 0.9063350558280945 *\n",
      "Epoch: 5, Train_Loss: 1.1384351253509521, Test_Loss: 0.9084081649780273\n",
      "Epoch: 5, Train_Loss: 2.105607748031616, Test_Loss: 0.9227797389030457\n",
      "Epoch: 5, Train_Loss: 1.231830358505249, Test_Loss: 0.9498629570007324\n",
      "Epoch: 5, Train_Loss: 0.9253501296043396, Test_Loss: 0.9002337455749512 *\n",
      "Epoch: 5, Train_Loss: 0.9022744297981262, Test_Loss: 0.8918172121047974 *\n",
      "Epoch: 5, Train_Loss: 0.8970263600349426, Test_Loss: 0.9051477313041687\n",
      "Epoch: 5, Train_Loss: 1.283841609954834, Test_Loss: 0.9082046747207642\n",
      "Epoch: 5, Train_Loss: 1.0290411710739136, Test_Loss: 0.8994919657707214 *\n",
      "Epoch: 5, Train_Loss: 0.8991098999977112, Test_Loss: 0.915698766708374\n",
      "Epoch: 5, Train_Loss: 1.2122961282730103, Test_Loss: 0.9187982678413391\n",
      "Epoch: 5, Train_Loss: 0.907523512840271, Test_Loss: 0.9026637673377991 *\n",
      "Epoch: 5, Train_Loss: 0.9037454724311829, Test_Loss: 0.9013136029243469 *\n",
      "Epoch: 5, Train_Loss: 0.960608184337616, Test_Loss: 0.9088155031204224\n",
      "Epoch: 5, Train_Loss: 1.1098339557647705, Test_Loss: 0.941498339176178\n",
      "Epoch: 5, Train_Loss: 0.9267165660858154, Test_Loss: 0.9420014023780823\n",
      "Epoch: 5, Train_Loss: 0.9913568496704102, Test_Loss: 0.9192284345626831 *\n",
      "Epoch: 5, Train_Loss: 0.897081196308136, Test_Loss: 0.9121329188346863 *\n",
      "Epoch: 5, Train_Loss: 1.0575282573699951, Test_Loss: 0.9038869738578796 *\n",
      "Epoch: 5, Train_Loss: 0.944449782371521, Test_Loss: 0.9009791612625122 *\n",
      "Epoch: 5, Train_Loss: 0.9092857241630554, Test_Loss: 0.9060590863227844\n",
      "Epoch: 5, Train_Loss: 0.892116367816925, Test_Loss: 0.9145047068595886\n",
      "Epoch: 5, Train_Loss: 0.9046388864517212, Test_Loss: 0.9737129211425781\n",
      "Epoch: 5, Train_Loss: 1.054516315460205, Test_Loss: 1.9485740661621094\n",
      "Epoch: 5, Train_Loss: 1.2894337177276611, Test_Loss: 5.1772847175598145\n",
      "Epoch: 5, Train_Loss: 1.1355570554733276, Test_Loss: 0.9085472822189331 *\n",
      "Epoch: 5, Train_Loss: 1.475843906402588, Test_Loss: 0.9512705206871033\n",
      "Epoch: 5, Train_Loss: 1.1847317218780518, Test_Loss: 0.9559136629104614\n",
      "Epoch: 5, Train_Loss: 1.078805923461914, Test_Loss: 0.8814665079116821 *\n",
      "Epoch: 5, Train_Loss: 0.983775794506073, Test_Loss: 0.8961668014526367\n",
      "Epoch: 5, Train_Loss: 0.9098638296127319, Test_Loss: 0.9165793061256409\n",
      "Epoch: 5, Train_Loss: 0.8889750838279724, Test_Loss: 0.962165892124176\n",
      "Epoch: 5, Train_Loss: 0.8822616934776306, Test_Loss: 0.9032725691795349 *\n",
      "Epoch: 5, Train_Loss: 1.004844307899475, Test_Loss: 0.9230193495750427\n",
      "Epoch: 5, Train_Loss: 1.2635071277618408, Test_Loss: 0.9448575973510742\n",
      "Epoch: 5, Train_Loss: 1.1493619680404663, Test_Loss: 1.0582571029663086\n",
      "Epoch: 5, Train_Loss: 2.532801628112793, Test_Loss: 0.9503730535507202 *\n",
      "Epoch: 5, Train_Loss: 1.531519889831543, Test_Loss: 0.9708591103553772\n",
      "Epoch: 5, Train_Loss: 1.6411420106887817, Test_Loss: 0.8814414143562317 *\n",
      "Epoch: 5, Train_Loss: 1.0143051147460938, Test_Loss: 0.9827427268028259\n",
      "Epoch: 5, Train_Loss: 0.8876339793205261, Test_Loss: 0.8787083029747009 *\n",
      "Epoch: 5, Train_Loss: 1.033937931060791, Test_Loss: 1.0303722620010376\n",
      "Epoch: 5, Train_Loss: 1.7250404357910156, Test_Loss: 0.9574724435806274 *\n",
      "Epoch: 5, Train_Loss: 1.7023003101348877, Test_Loss: 0.8970315456390381 *\n",
      "Epoch: 5, Train_Loss: 0.9707989692687988, Test_Loss: 0.9237648844718933\n",
      "Epoch: 5, Train_Loss: 0.9457143545150757, Test_Loss: 0.9100232124328613 *\n",
      "Epoch: 5, Train_Loss: 0.9470638036727905, Test_Loss: 0.9023371338844299 *\n",
      "Epoch: 5, Train_Loss: 1.173383355140686, Test_Loss: 0.9019380211830139 *\n",
      "Epoch: 5, Train_Loss: 1.0841877460479736, Test_Loss: 0.9245812296867371\n",
      "Epoch: 5, Train_Loss: 1.4381870031356812, Test_Loss: 0.9335713386535645\n",
      "Epoch: 5, Train_Loss: 1.1622231006622314, Test_Loss: 0.9263371825218201 *\n",
      "Epoch: 5, Train_Loss: 0.9914150834083557, Test_Loss: 0.8834472894668579 *\n",
      "Epoch: 5, Train_Loss: 0.8900706171989441, Test_Loss: 0.9456213712692261\n",
      "Epoch: 5, Train_Loss: 0.9004355072975159, Test_Loss: 0.8914523124694824 *\n",
      "Epoch: 5, Train_Loss: 0.8946609497070312, Test_Loss: 0.8657388091087341 *\n",
      "Epoch: 5, Train_Loss: 0.9420226216316223, Test_Loss: 0.9250389337539673\n",
      "Epoch: 5, Train_Loss: 0.8860941529273987, Test_Loss: 1.0461825132369995\n",
      "Epoch: 5, Train_Loss: 0.9146027565002441, Test_Loss: 1.1114094257354736\n",
      "Epoch: 5, Train_Loss: 17.162334442138672, Test_Loss: 0.9123409986495972 *\n",
      "Epoch: 5, Train_Loss: 0.9324934482574463, Test_Loss: 0.8700960874557495 *\n",
      "Epoch: 5, Train_Loss: 3.4137091636657715, Test_Loss: 0.8819587230682373\n",
      "Epoch: 5, Train_Loss: 2.0199761390686035, Test_Loss: 0.929867148399353\n",
      "Epoch: 5, Train_Loss: 0.8876767158508301, Test_Loss: 0.9636529684066772\n",
      "Epoch: 5, Train_Loss: 0.9524198174476624, Test_Loss: 1.3407448530197144\n",
      "Epoch: 5, Train_Loss: 7.303599834442139, Test_Loss: 1.6655364036560059\n",
      "Epoch: 5, Train_Loss: 5.286538600921631, Test_Loss: 0.9987359046936035 *\n",
      "Epoch: 5, Train_Loss: 0.8809887766838074, Test_Loss: 0.9277552366256714 *\n",
      "Epoch: 5, Train_Loss: 1.1802093982696533, Test_Loss: 0.9542858600616455\n",
      "Epoch: 5, Train_Loss: 5.250194549560547, Test_Loss: 1.2250456809997559\n",
      "Epoch: 5, Train_Loss: 1.3662112951278687, Test_Loss: 1.3184770345687866\n",
      "Epoch: 5, Train_Loss: 1.1163136959075928, Test_Loss: 1.4522227048873901\n",
      "Epoch: 5, Train_Loss: 0.9024271368980408, Test_Loss: 1.896483063697815\n",
      "Epoch: 5, Train_Loss: 1.0117512941360474, Test_Loss: 1.6068177223205566 *\n",
      "Epoch: 5, Train_Loss: 1.1028478145599365, Test_Loss: 1.46731698513031 *\n",
      "Epoch: 5, Train_Loss: 1.013941764831543, Test_Loss: 1.0981786251068115 *\n",
      "Epoch: 5, Train_Loss: 0.9531022310256958, Test_Loss: 1.1897706985473633\n",
      "Epoch: 5, Train_Loss: 0.8837957978248596, Test_Loss: 0.9083578586578369 *\n",
      "Epoch: 5, Train_Loss: 0.8719419240951538, Test_Loss: 0.9291488528251648\n",
      "Epoch: 5, Train_Loss: 0.8863600492477417, Test_Loss: 0.8918863534927368 *\n",
      "Epoch: 5, Train_Loss: 0.8783026337623596, Test_Loss: 0.8752629160881042 *\n",
      "Epoch: 5, Train_Loss: 0.8951345682144165, Test_Loss: 0.8630830645561218 *\n",
      "Epoch: 5, Train_Loss: 0.8966538310050964, Test_Loss: 0.8553793430328369 *\n",
      "Epoch: 5, Train_Loss: 0.9034132957458496, Test_Loss: 1.1234276294708252\n",
      "Epoch: 5, Train_Loss: 0.8818349838256836, Test_Loss: 6.354475021362305\n",
      "Epoch: 5, Train_Loss: 0.8766041994094849, Test_Loss: 0.9935407638549805 *\n",
      "Epoch: 5, Train_Loss: 0.889453649520874, Test_Loss: 0.882046103477478 *\n",
      "Epoch: 5, Train_Loss: 0.8620039224624634, Test_Loss: 0.881689190864563 *\n",
      "Epoch: 5, Train_Loss: 0.8509904742240906, Test_Loss: 0.8591840863227844 *\n",
      "Epoch: 5, Train_Loss: 0.8473316431045532, Test_Loss: 0.8697136044502258\n",
      "Epoch: 5, Train_Loss: 0.8473432660102844, Test_Loss: 0.8852720260620117\n",
      "Epoch: 5, Train_Loss: 0.8489560484886169, Test_Loss: 0.9010207056999207\n",
      "Epoch: 5, Train_Loss: 0.8468564748764038, Test_Loss: 0.8638626933097839 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 0.8459473848342896, Test_Loss: 0.8680700063705444\n",
      "Epoch: 5, Train_Loss: 0.8453575968742371, Test_Loss: 0.8734312057495117\n",
      "Epoch: 5, Train_Loss: 0.8578826189041138, Test_Loss: 0.9038255214691162\n",
      "Epoch: 5, Train_Loss: 0.8578956127166748, Test_Loss: 0.849192202091217 *\n",
      "Epoch: 5, Train_Loss: 0.962830126285553, Test_Loss: 0.8458862900733948 *\n",
      "Epoch: 5, Train_Loss: 0.8706468343734741, Test_Loss: 0.8666077852249146\n",
      "Epoch: 5, Train_Loss: 0.8710309267044067, Test_Loss: 0.8697750568389893\n",
      "Epoch: 5, Train_Loss: 5.549979209899902, Test_Loss: 0.866984486579895 *\n",
      "Epoch: 5, Train_Loss: 5.586979389190674, Test_Loss: 0.8687403202056885\n",
      "Epoch: 5, Train_Loss: 0.8542920351028442, Test_Loss: 0.9085217118263245\n",
      "Epoch: 5, Train_Loss: 0.8676667809486389, Test_Loss: 0.8632247447967529 *\n",
      "Epoch: 5, Train_Loss: 0.9056048393249512, Test_Loss: 0.8703847527503967\n",
      "Epoch: 5, Train_Loss: 0.8568532466888428, Test_Loss: 0.8772886991500854\n",
      "Epoch: 5, Train_Loss: 0.8425653576850891, Test_Loss: 0.9270488023757935\n",
      "Epoch: 5, Train_Loss: 0.864812970161438, Test_Loss: 0.9249951839447021 *\n",
      "Epoch: 5, Train_Loss: 0.9176581501960754, Test_Loss: 0.9066644310951233 *\n",
      "Epoch: 5, Train_Loss: 1.077185869216919, Test_Loss: 0.8807870745658875 *\n",
      "Epoch: 5, Train_Loss: 0.9623129367828369, Test_Loss: 0.8746776580810547 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 0.886085569858551, Test_Loss: 0.8727913498878479 *\n",
      "Epoch: 5, Train_Loss: 0.889352560043335, Test_Loss: 0.8703376054763794 *\n",
      "Epoch: 5, Train_Loss: 0.9560815095901489, Test_Loss: 0.8946738243103027\n",
      "Epoch: 5, Train_Loss: 0.959111750125885, Test_Loss: 0.9201880097389221\n",
      "Epoch: 5, Train_Loss: 0.9789755344390869, Test_Loss: 3.4076123237609863\n",
      "Epoch: 5, Train_Loss: 0.9535163640975952, Test_Loss: 4.385313987731934\n",
      "Epoch: 5, Train_Loss: 0.9371790885925293, Test_Loss: 0.8539432287216187 *\n",
      "Epoch: 5, Train_Loss: 0.8372573852539062, Test_Loss: 0.8415240049362183 *\n",
      "Epoch: 5, Train_Loss: 0.8838939070701599, Test_Loss: 0.8427243828773499\n",
      "Epoch: 5, Train_Loss: 0.8738500475883484, Test_Loss: 0.8359030485153198 *\n",
      "Epoch: 5, Train_Loss: 0.8493480682373047, Test_Loss: 0.8510249257087708\n",
      "Epoch: 5, Train_Loss: 0.8376495242118835, Test_Loss: 0.9037185907363892\n",
      "Epoch: 5, Train_Loss: 0.837859034538269, Test_Loss: 1.0064678192138672\n",
      "Epoch: 5, Train_Loss: 0.8449974060058594, Test_Loss: 0.8387846946716309 *\n",
      "Epoch: 5, Train_Loss: 4.722935676574707, Test_Loss: 0.8575724959373474\n",
      "Epoch: 5, Train_Loss: 2.680978298187256, Test_Loss: 0.8617757558822632\n",
      "Epoch: 5, Train_Loss: 0.8448957204818726, Test_Loss: 0.8439344763755798 *\n",
      "Epoch: 5, Train_Loss: 0.8578466773033142, Test_Loss: 0.8447026014328003\n",
      "Epoch: 5, Train_Loss: 0.8541034460067749, Test_Loss: 0.8642602562904358\n",
      "Epoch: 5, Train_Loss: 0.8444812297821045, Test_Loss: 0.8593816161155701 *\n",
      "Epoch: 5, Train_Loss: 0.8395628333091736, Test_Loss: 0.9345719218254089\n",
      "Epoch: 5, Train_Loss: 0.8416839838027954, Test_Loss: 0.9368239045143127\n",
      "Epoch: 5, Train_Loss: 0.8448992371559143, Test_Loss: 0.8414876461029053 *\n",
      "Epoch: 5, Train_Loss: 0.8437607884407043, Test_Loss: 0.8340885043144226 *\n",
      "Epoch: 5, Train_Loss: 0.8562264442443848, Test_Loss: 0.8307502865791321 *\n",
      "Epoch: 5, Train_Loss: 0.8390840291976929, Test_Loss: 0.830326497554779 *\n",
      "Epoch: 5, Train_Loss: 0.8386646509170532, Test_Loss: 0.8307494521141052\n",
      "Epoch: 5, Train_Loss: 0.8547317981719971, Test_Loss: 0.8280993700027466 *\n",
      "Epoch: 5, Train_Loss: 0.8309998512268066, Test_Loss: 0.8283604383468628\n",
      "Epoch: 5, Train_Loss: 0.8303739428520203, Test_Loss: 0.8306487798690796\n",
      "Epoch: 5, Train_Loss: 0.8346247673034668, Test_Loss: 0.829135000705719 *\n",
      "Epoch: 5, Train_Loss: 0.8642534613609314, Test_Loss: 0.828346848487854 *\n",
      "Epoch: 5, Train_Loss: 0.8575122356414795, Test_Loss: 0.8252755403518677 *\n",
      "Epoch: 5, Train_Loss: 0.8292931914329529, Test_Loss: 0.8455871343612671\n",
      "Epoch: 5, Train_Loss: 0.8306095600128174, Test_Loss: 0.8232770562171936 *\n",
      "Epoch: 5, Train_Loss: 0.8641407489776611, Test_Loss: 0.8393746018409729\n",
      "Epoch: 5, Train_Loss: 0.9187542200088501, Test_Loss: 0.8449728488922119\n",
      "Epoch: 5, Train_Loss: 0.8683537244796753, Test_Loss: 1.1648752689361572\n",
      "Epoch: 5, Train_Loss: 0.8689422011375427, Test_Loss: 1.1660950183868408\n",
      "Epoch: 5, Train_Loss: 0.8609209656715393, Test_Loss: 0.9120448231697083 *\n",
      "Epoch: 5, Train_Loss: 0.9062254428863525, Test_Loss: 0.8299675583839417 *\n",
      "Epoch: 5, Train_Loss: 0.8855680227279663, Test_Loss: 0.8354288935661316\n",
      "Epoch: 5, Train_Loss: 0.8914670348167419, Test_Loss: 0.8803856372833252\n",
      "Epoch: 5, Train_Loss: 0.951095700263977, Test_Loss: 1.0788986682891846\n",
      "Epoch: 5, Train_Loss: 0.8609780073165894, Test_Loss: 1.9167306423187256\n",
      "Epoch: 5, Train_Loss: 0.8552711009979248, Test_Loss: 1.5739116668701172 *\n",
      "Epoch: 5, Train_Loss: 0.8260316848754883, Test_Loss: 0.8682112097740173 *\n",
      "Epoch: 5, Train_Loss: 0.8197339177131653, Test_Loss: 0.8495429158210754 *\n",
      "Epoch: 5, Train_Loss: 0.8167043924331665, Test_Loss: 0.8316728472709656 *\n",
      "Epoch: 5, Train_Loss: 0.8168933391571045, Test_Loss: 0.8387999534606934\n",
      "Epoch: 5, Train_Loss: 0.8215207457542419, Test_Loss: 0.8392228484153748\n",
      "Epoch: 5, Train_Loss: 4.420500755310059, Test_Loss: 0.8302955627441406 *\n",
      "Epoch: 5, Train_Loss: 2.2221717834472656, Test_Loss: 0.9122627973556519\n",
      "Epoch: 5, Train_Loss: 0.8151659965515137, Test_Loss: 0.8154037594795227 *\n",
      "Epoch: 5, Train_Loss: 0.8340123891830444, Test_Loss: 0.8511443138122559\n",
      "Epoch: 5, Train_Loss: 0.8200331926345825, Test_Loss: 0.9431094527244568\n",
      "Epoch: 5, Train_Loss: 0.8116349577903748, Test_Loss: 1.1541764736175537\n",
      "Epoch: 5, Train_Loss: 0.8125907182693481, Test_Loss: 1.0455853939056396 *\n",
      "Epoch: 5, Train_Loss: 0.8112354278564453, Test_Loss: 0.8202425837516785 *\n",
      "Epoch: 5, Train_Loss: 0.810339629650116, Test_Loss: 0.8138881921768188 *\n",
      "Epoch: 5, Train_Loss: 0.809989333152771, Test_Loss: 0.8134003281593323 *\n",
      "Epoch: 5, Train_Loss: 0.8362399339675903, Test_Loss: 0.8133206367492676 *\n",
      "Epoch: 5, Train_Loss: 0.8883647918701172, Test_Loss: 0.8230379223823547\n",
      "Epoch: 5, Train_Loss: 0.8863451480865479, Test_Loss: 1.792362928390503\n",
      "Epoch: 5, Train_Loss: 0.8973507881164551, Test_Loss: 5.2936553955078125\n",
      "Epoch: 5, Train_Loss: 0.8243504166603088, Test_Loss: 0.8296725153923035 *\n",
      "Epoch: 5, Train_Loss: 0.8291769623756409, Test_Loss: 0.8120822906494141 *\n",
      "Epoch: 5, Train_Loss: 0.9880123138427734, Test_Loss: 0.8091433644294739 *\n",
      "Epoch: 5, Train_Loss: 0.9756268262863159, Test_Loss: 0.8107825517654419\n",
      "Epoch: 5, Train_Loss: 0.9829054474830627, Test_Loss: 0.8117950558662415\n",
      "Epoch: 5, Train_Loss: 0.8526484966278076, Test_Loss: 0.8111002445220947 *\n",
      "Epoch: 5, Train_Loss: 0.8057929277420044, Test_Loss: 0.8179256319999695\n",
      "Epoch: 5, Train_Loss: 0.8059611916542053, Test_Loss: 0.8098341822624207 *\n",
      "Epoch: 5, Train_Loss: 0.806544840335846, Test_Loss: 0.8128846883773804\n",
      "Epoch: 5, Train_Loss: 0.8101283311843872, Test_Loss: 0.8127537965774536 *\n",
      "Epoch: 5, Train_Loss: 0.8117719888687134, Test_Loss: 0.8241389989852905\n",
      "Epoch: 5, Train_Loss: 0.8055869340896606, Test_Loss: 0.8136702179908752 *\n",
      "Epoch: 5, Train_Loss: 0.8064770102500916, Test_Loss: 0.8108040690422058 *\n",
      "Epoch: 5, Train_Loss: 0.8064819574356079, Test_Loss: 0.8152419924736023\n",
      "Epoch: 5, Train_Loss: 0.8083116412162781, Test_Loss: 0.8110630512237549 *\n",
      "Epoch: 5, Train_Loss: 0.8885210156440735, Test_Loss: 0.8160378336906433\n",
      "Epoch: 5, Train_Loss: 1.0125954151153564, Test_Loss: 0.8101987838745117 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 0.9803153276443481, Test_Loss: 0.830767035484314\n",
      "Epoch: 5, Train_Loss: 0.876746654510498, Test_Loss: 0.8076444864273071 *\n",
      "Epoch: 5, Train_Loss: 0.9607084393501282, Test_Loss: 0.8131110072135925\n",
      "Epoch: 5, Train_Loss: 0.9959088563919067, Test_Loss: 0.8169254064559937\n",
      "Epoch: 5, Train_Loss: 0.8225147128105164, Test_Loss: 0.8408209085464478\n",
      "Epoch: 5, Train_Loss: 1.0261086225509644, Test_Loss: 0.8320152163505554 *\n",
      "Epoch: 5, Train_Loss: 0.9645949006080627, Test_Loss: 0.8225171566009521 *\n",
      "Epoch: 5, Train_Loss: 1.0815435647964478, Test_Loss: 0.8122568130493164 *\n",
      "Epoch: 5, Train_Loss: 0.8068047761917114, Test_Loss: 0.8108610510826111 *\n",
      "Epoch: 5, Train_Loss: 1.4788963794708252, Test_Loss: 0.8107102513313293 *\n",
      "Epoch: 5, Train_Loss: 3.3319692611694336, Test_Loss: 0.8044353127479553 *\n",
      "Epoch: 5, Train_Loss: 0.840365469455719, Test_Loss: 0.8463313579559326\n",
      "Epoch: 5, Train_Loss: 0.8346129655838013, Test_Loss: 0.8427650928497314 *\n",
      "Epoch: 5, Train_Loss: 0.8347616791725159, Test_Loss: 4.212375164031982\n",
      "Epoch: 5, Train_Loss: 0.837565541267395, Test_Loss: 3.1000754833221436 *\n",
      "Epoch: 5, Train_Loss: 0.7954241633415222, Test_Loss: 0.7983204126358032 *\n",
      "Epoch: 5, Train_Loss: 0.7990753054618835, Test_Loss: 0.797945499420166 *\n",
      "Epoch: 5, Train_Loss: 0.9130318760871887, Test_Loss: 0.8290594816207886\n",
      "Epoch: 5, Train_Loss: 0.9143149256706238, Test_Loss: 0.8239282965660095 *\n",
      "Epoch: 5, Train_Loss: 0.88688063621521, Test_Loss: 0.8152933716773987 *\n",
      "Epoch: 5, Train_Loss: 0.8564321398735046, Test_Loss: 0.8678395748138428\n",
      "Epoch: 5, Train_Loss: 0.8391667008399963, Test_Loss: 0.8961973190307617\n",
      "Epoch: 5, Train_Loss: 0.8122196197509766, Test_Loss: 0.7937090396881104 *\n",
      "Epoch: 5, Train_Loss: 0.8235726952552795, Test_Loss: 0.8156789541244507\n",
      "Epoch: 5, Train_Loss: 0.8031883239746094, Test_Loss: 0.8103946447372437 *\n",
      "Epoch: 5, Train_Loss: 0.8153161406517029, Test_Loss: 0.8077601790428162 *\n",
      "Epoch: 5, Train_Loss: 0.7993497848510742, Test_Loss: 0.795208215713501 *\n",
      "Epoch: 5, Train_Loss: 0.7898364067077637, Test_Loss: 0.8681732416152954\n",
      "Epoch: 5, Train_Loss: 0.8186058402061462, Test_Loss: 0.8471393585205078 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 0.835486650466919, Test_Loss: 0.8467857837677002 *\n",
      "Epoch: 5, Train_Loss: 0.8131672739982605, Test_Loss: 0.8473978042602539\n",
      "Epoch: 5, Train_Loss: 0.7905633449554443, Test_Loss: 0.8283769488334656 *\n",
      "Epoch: 5, Train_Loss: 0.7884080410003662, Test_Loss: 0.8086348176002502 *\n",
      "Epoch: 5, Train_Loss: 0.7866233587265015, Test_Loss: 0.7939760088920593 *\n",
      "Epoch: 5, Train_Loss: 0.7869794368743896, Test_Loss: 0.8008852005004883\n",
      "Epoch: 5, Train_Loss: 0.7912107706069946, Test_Loss: 0.8005385398864746 *\n",
      "Epoch: 5, Train_Loss: 0.7925333380699158, Test_Loss: 0.80720055103302\n",
      "Epoch: 5, Train_Loss: 0.7934053540229797, Test_Loss: 0.8032155632972717 *\n",
      "Epoch: 5, Train_Loss: 0.785171627998352, Test_Loss: 0.7899588942527771 *\n",
      "Epoch: 5, Train_Loss: 0.7837748527526855, Test_Loss: 0.7983427047729492\n",
      "Epoch: 5, Train_Loss: 0.7927532196044922, Test_Loss: 0.7934912443161011 *\n",
      "Epoch: 5, Train_Loss: 0.8056122660636902, Test_Loss: 0.7994948029518127\n",
      "Epoch: 5, Train_Loss: 0.8125370144844055, Test_Loss: 0.790000855922699 *\n",
      "Epoch: 5, Train_Loss: 0.806209921836853, Test_Loss: 0.8118801712989807\n",
      "Epoch: 5, Train_Loss: 0.8114529252052307, Test_Loss: 0.8416082859039307\n",
      "Epoch: 5, Train_Loss: 0.7891784906387329, Test_Loss: 0.8700629472732544\n",
      "Epoch: 5, Train_Loss: 0.7945088148117065, Test_Loss: 1.3161349296569824\n",
      "Epoch: 5, Train_Loss: 0.7806082367897034, Test_Loss: 1.1960246562957764 *\n",
      "Epoch: 5, Train_Loss: 0.7803990840911865, Test_Loss: 0.9106041789054871 *\n",
      "Epoch: 5, Train_Loss: 0.7933441996574402, Test_Loss: 0.8109526038169861 *\n",
      "Epoch: 5, Train_Loss: 0.7910244464874268, Test_Loss: 0.7977231740951538 *\n",
      "Epoch: 5, Train_Loss: 0.782139241695404, Test_Loss: 0.8363364934921265\n",
      "Epoch: 5, Train_Loss: 0.7790212035179138, Test_Loss: 1.1606074571609497\n",
      "Epoch: 5, Train_Loss: 0.784105658531189, Test_Loss: 1.9264013767242432\n",
      "Epoch: 6, Train_Loss: 0.8378579020500183, Test_Loss: 1.3009998798370361 *\n",
      "Epoch: 6, Train_Loss: 0.799744188785553, Test_Loss: 0.8746250867843628 *\n",
      "Epoch: 6, Train_Loss: 0.8159275650978088, Test_Loss: 0.792411208152771 *\n",
      "Epoch: 6, Train_Loss: 0.7778404951095581, Test_Loss: 0.7872406840324402 *\n",
      "Epoch: 6, Train_Loss: 0.805476725101471, Test_Loss: 0.7825690507888794 *\n",
      "Epoch: 6, Train_Loss: 0.8136473298072815, Test_Loss: 0.7855153679847717\n",
      "Epoch: 6, Train_Loss: 0.7789450883865356, Test_Loss: 0.8069082498550415\n",
      "Epoch: 6, Train_Loss: 0.7898198962211609, Test_Loss: 0.8436412215232849\n",
      "Epoch: 6, Train_Loss: 0.8142886161804199, Test_Loss: 0.7766985297203064 *\n",
      "Epoch: 6, Train_Loss: 0.8673719167709351, Test_Loss: 0.8396524786949158\n",
      "Epoch: 6, Train_Loss: 0.8926736116409302, Test_Loss: 0.9287655353546143\n",
      "Epoch: 6, Train_Loss: 0.8603909611701965, Test_Loss: 1.0779778957366943\n",
      "Epoch: 6, Train_Loss: 0.8140906095504761, Test_Loss: 0.9643169641494751 *\n",
      "Epoch: 6, Train_Loss: 0.7753483653068542, Test_Loss: 0.7772443294525146 *\n",
      "Epoch: 6, Train_Loss: 0.8046824932098389, Test_Loss: 0.774807870388031 *\n",
      "Epoch: 6, Train_Loss: 0.774223804473877, Test_Loss: 0.7743316292762756 *\n",
      "Epoch: 6, Train_Loss: 0.7839990854263306, Test_Loss: 0.7741662859916687 *\n",
      "Epoch: 6, Train_Loss: 0.7777310013771057, Test_Loss: 0.7867081165313721\n",
      "Epoch: 6, Train_Loss: 0.7803093791007996, Test_Loss: 2.973475456237793\n",
      "Epoch: 6, Train_Loss: 0.8427806496620178, Test_Loss: 4.106732368469238\n",
      "Epoch: 6, Train_Loss: 0.7831730842590332, Test_Loss: 0.7787916660308838 *\n",
      "Epoch: 6, Train_Loss: 0.8440427184104919, Test_Loss: 0.7716349959373474 *\n",
      "Epoch: 6, Train_Loss: 0.775985598564148, Test_Loss: 0.7697268724441528 *\n",
      "Epoch: 6, Train_Loss: 0.7906522154808044, Test_Loss: 0.7733729481697083\n",
      "Epoch: 6, Train_Loss: 0.7763810753822327, Test_Loss: 0.7705475687980652 *\n",
      "Epoch: 6, Train_Loss: 1.027592658996582, Test_Loss: 0.7731391191482544\n",
      "Epoch: 6, Train_Loss: 0.8305110931396484, Test_Loss: 0.7708318829536438 *\n",
      "Epoch: 6, Train_Loss: 0.7859507203102112, Test_Loss: 0.7700194120407104 *\n",
      "Epoch: 6, Train_Loss: 0.7790641188621521, Test_Loss: 0.7698565125465393 *\n",
      "Epoch: 6, Train_Loss: 0.768454372882843, Test_Loss: 0.7696595191955566 *\n",
      "Epoch: 6, Train_Loss: 0.7678828239440918, Test_Loss: 0.7744147181510925\n",
      "Epoch: 6, Train_Loss: 0.764666736125946, Test_Loss: 0.7901731729507446\n",
      "Epoch: 6, Train_Loss: 0.7699321508407593, Test_Loss: 0.7862292528152466 *\n",
      "Epoch: 6, Train_Loss: 0.7746419310569763, Test_Loss: 0.7689113020896912 *\n",
      "Epoch: 6, Train_Loss: 0.7889168858528137, Test_Loss: 0.7658601999282837 *\n",
      "Epoch: 6, Train_Loss: 0.7744197249412537, Test_Loss: 0.7692578434944153\n",
      "Epoch: 6, Train_Loss: 0.7771854996681213, Test_Loss: 0.7657853364944458 *\n",
      "Epoch: 6, Train_Loss: 0.7811123132705688, Test_Loss: 0.7693800330162048\n",
      "Epoch: 6, Train_Loss: 0.7647188901901245, Test_Loss: 0.7650337815284729 *\n",
      "Epoch: 6, Train_Loss: 0.7618156671524048, Test_Loss: 0.7636236548423767 *\n",
      "Epoch: 6, Train_Loss: 0.7683818340301514, Test_Loss: 0.7661784887313843\n",
      "Epoch: 6, Train_Loss: 0.7783132195472717, Test_Loss: 0.7734948396682739\n",
      "Epoch: 6, Train_Loss: 0.7867514491081238, Test_Loss: 0.7676272988319397 *\n",
      "Epoch: 6, Train_Loss: 0.7651729583740234, Test_Loss: 0.7683870792388916\n",
      "Epoch: 6, Train_Loss: 0.7920690178871155, Test_Loss: 0.7641012668609619 *\n",
      "Epoch: 6, Train_Loss: 0.8291604518890381, Test_Loss: 0.7636798620223999 *\n",
      "Epoch: 6, Train_Loss: 0.7977467179298401, Test_Loss: 0.7643685340881348\n",
      "Epoch: 6, Train_Loss: 0.7586155533790588, Test_Loss: 0.7623989582061768 *\n",
      "Epoch: 6, Train_Loss: 0.7866474390029907, Test_Loss: 0.813869059085846\n",
      "Epoch: 6, Train_Loss: 0.7668235301971436, Test_Loss: 0.7885476350784302 *\n",
      "Epoch: 6, Train_Loss: 0.7696686387062073, Test_Loss: 5.404404640197754\n",
      "Epoch: 6, Train_Loss: 0.7583373188972473, Test_Loss: 1.833177089691162 *\n",
      "Epoch: 6, Train_Loss: 0.7815942764282227, Test_Loss: 0.7576856017112732 *\n",
      "Epoch: 6, Train_Loss: 0.8103586435317993, Test_Loss: 0.7692562341690063\n",
      "Epoch: 6, Train_Loss: 3.1474547386169434, Test_Loss: 0.7927126288414001\n",
      "Epoch: 6, Train_Loss: 3.727229595184326, Test_Loss: 0.7977070212364197\n",
      "Epoch: 6, Train_Loss: 0.7898403406143188, Test_Loss: 0.7634244561195374 *\n",
      "Epoch: 6, Train_Loss: 0.7588576674461365, Test_Loss: 0.8518388271331787\n",
      "Epoch: 6, Train_Loss: 0.8843270540237427, Test_Loss: 0.8400325179100037 *\n",
      "Epoch: 6, Train_Loss: 0.9275273084640503, Test_Loss: 0.7549214959144592 *\n",
      "Epoch: 6, Train_Loss: 0.7873245477676392, Test_Loss: 0.7881349325180054\n",
      "Epoch: 6, Train_Loss: 0.7556073665618896, Test_Loss: 0.7669803500175476 *\n",
      "Epoch: 6, Train_Loss: 0.7920506596565247, Test_Loss: 0.7640331387519836 *\n",
      "Epoch: 6, Train_Loss: 0.8030570149421692, Test_Loss: 0.756104588508606 *\n",
      "Epoch: 6, Train_Loss: 0.761424720287323, Test_Loss: 0.8383353352546692\n",
      "Epoch: 6, Train_Loss: 0.7835545539855957, Test_Loss: 0.7905086278915405 *\n",
      "Epoch: 6, Train_Loss: 1.850104570388794, Test_Loss: 0.8369861841201782\n",
      "Epoch: 6, Train_Loss: 2.073441743850708, Test_Loss: 0.798225462436676 *\n",
      "Epoch: 6, Train_Loss: 0.9795608520507812, Test_Loss: 0.7980366945266724 *\n",
      "Epoch: 6, Train_Loss: 0.8336277604103088, Test_Loss: 0.7624919414520264 *\n",
      "Epoch: 6, Train_Loss: 2.2414438724517822, Test_Loss: 0.7559400796890259 *\n",
      "Epoch: 6, Train_Loss: 2.5582308769226074, Test_Loss: 0.7637665867805481\n",
      "Epoch: 6, Train_Loss: 0.7856258749961853, Test_Loss: 0.7614870071411133 *\n",
      "Epoch: 6, Train_Loss: 0.763253390789032, Test_Loss: 0.7677757740020752\n",
      "Epoch: 6, Train_Loss: 0.9319087862968445, Test_Loss: 0.7635045051574707 *\n",
      "Epoch: 6, Train_Loss: 2.1320693492889404, Test_Loss: 0.7480104565620422 *\n",
      "Epoch: 6, Train_Loss: 1.857525110244751, Test_Loss: 0.7536495923995972\n",
      "Epoch: 6, Train_Loss: 0.7619853615760803, Test_Loss: 0.7491418123245239 *\n",
      "Epoch: 6, Train_Loss: 0.7710850238800049, Test_Loss: 0.7514315843582153\n",
      "Epoch: 6, Train_Loss: 0.7868592143058777, Test_Loss: 0.761344313621521\n",
      "Epoch: 6, Train_Loss: 1.6008477210998535, Test_Loss: 0.7601021528244019 *\n",
      "Epoch: 6, Train_Loss: 0.8069674968719482, Test_Loss: 0.7700437307357788\n",
      "Epoch: 6, Train_Loss: 0.8447074890136719, Test_Loss: 0.9171084761619568\n",
      "Epoch: 6, Train_Loss: 0.8014507293701172, Test_Loss: 1.2040650844573975\n",
      "Epoch: 6, Train_Loss: 0.9357139468193054, Test_Loss: 1.0434319972991943 *\n",
      "Epoch: 6, Train_Loss: 0.9106782078742981, Test_Loss: 0.8571327924728394 *\n",
      "Epoch: 6, Train_Loss: 0.9325873255729675, Test_Loss: 0.7752713561058044 *\n",
      "Epoch: 6, Train_Loss: 0.956188440322876, Test_Loss: 0.7535905241966248 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 0.7890705466270447, Test_Loss: 0.7968235611915588\n",
      "Epoch: 6, Train_Loss: 0.8924740552902222, Test_Loss: 1.1564922332763672\n",
      "Epoch: 6, Train_Loss: 0.935642421245575, Test_Loss: 1.6335257291793823\n",
      "Epoch: 6, Train_Loss: 1.1203374862670898, Test_Loss: 1.0581587553024292 *\n",
      "Epoch: 6, Train_Loss: 1.0365874767303467, Test_Loss: 0.8204143047332764 *\n",
      "Epoch: 6, Train_Loss: 0.7686932682991028, Test_Loss: 0.7568976283073425 *\n",
      "Epoch: 6, Train_Loss: 0.85367751121521, Test_Loss: 0.7747324705123901\n",
      "Epoch: 6, Train_Loss: 0.8923325538635254, Test_Loss: 0.7710159420967102 *\n",
      "Epoch: 6, Train_Loss: 0.7637659311294556, Test_Loss: 0.7941330671310425\n",
      "Epoch: 6, Train_Loss: 0.7477734088897705, Test_Loss: 0.7612575888633728 *\n",
      "Epoch: 6, Train_Loss: 0.7424144744873047, Test_Loss: 0.8058006167411804\n",
      "Epoch: 6, Train_Loss: 0.7388767600059509, Test_Loss: 0.7568453550338745 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 0.7460758090019226, Test_Loss: 0.8463740944862366\n",
      "Epoch: 6, Train_Loss: 0.7481767535209656, Test_Loss: 1.0533084869384766\n",
      "Epoch: 6, Train_Loss: 0.8027869462966919, Test_Loss: 0.9234892129898071 *\n",
      "Epoch: 6, Train_Loss: 0.749282956123352, Test_Loss: 1.0847887992858887\n",
      "Epoch: 6, Train_Loss: 0.8235107064247131, Test_Loss: 0.7713761329650879 *\n",
      "Epoch: 6, Train_Loss: 0.8485571146011353, Test_Loss: 0.7636321187019348 *\n",
      "Epoch: 6, Train_Loss: 1.0417200326919556, Test_Loss: 0.7620890736579895 *\n",
      "Epoch: 6, Train_Loss: 0.7545160055160522, Test_Loss: 0.766596257686615\n",
      "Epoch: 6, Train_Loss: 0.7777128219604492, Test_Loss: 0.850425124168396\n",
      "Epoch: 6, Train_Loss: 0.9239332675933838, Test_Loss: 4.201608180999756\n",
      "Epoch: 6, Train_Loss: 1.0371801853179932, Test_Loss: 2.596071481704712 *\n",
      "Epoch: 6, Train_Loss: 0.9150217771530151, Test_Loss: 0.7504639029502869 *\n",
      "Epoch: 6, Train_Loss: 0.7490010857582092, Test_Loss: 0.7579795122146606\n",
      "Epoch: 6, Train_Loss: 0.9742398858070374, Test_Loss: 0.7473430633544922 *\n",
      "Epoch: 6, Train_Loss: 1.1540776491165161, Test_Loss: 0.734366774559021 *\n",
      "Epoch: 6, Train_Loss: 0.9784709215164185, Test_Loss: 0.7715820074081421\n",
      "Epoch: 6, Train_Loss: 0.7442002296447754, Test_Loss: 0.786292314529419\n",
      "Epoch: 6, Train_Loss: 0.7446739673614502, Test_Loss: 0.7573849558830261 *\n",
      "Epoch: 6, Train_Loss: 0.7811229825019836, Test_Loss: 0.7602518796920776\n",
      "Epoch: 6, Train_Loss: 1.5774378776550293, Test_Loss: 0.7668246030807495\n",
      "Epoch: 6, Train_Loss: 1.2631821632385254, Test_Loss: 0.7807549238204956\n",
      "Epoch: 6, Train_Loss: 0.7664051055908203, Test_Loss: 0.7856284976005554\n",
      "Epoch: 6, Train_Loss: 0.7876759767532349, Test_Loss: 0.7312833070755005 *\n",
      "Epoch: 6, Train_Loss: 0.7412919402122498, Test_Loss: 0.7377315759658813\n",
      "Epoch: 6, Train_Loss: 0.921239972114563, Test_Loss: 0.7610573768615723\n",
      "Epoch: 6, Train_Loss: 1.0448529720306396, Test_Loss: 0.7619588375091553\n",
      "Epoch: 6, Train_Loss: 0.7377345561981201, Test_Loss: 0.7869329452514648\n",
      "Epoch: 6, Train_Loss: 0.9894118309020996, Test_Loss: 0.7909558415412903\n",
      "Epoch: 6, Train_Loss: 0.7636895179748535, Test_Loss: 0.7796498537063599 *\n",
      "Epoch: 6, Train_Loss: 0.7410696148872375, Test_Loss: 0.7422290444374084 *\n",
      "Epoch: 6, Train_Loss: 0.7693799734115601, Test_Loss: 0.7570746541023254\n",
      "Epoch: 6, Train_Loss: 0.8822110891342163, Test_Loss: 0.794077455997467\n",
      "Epoch: 6, Train_Loss: 0.8038551807403564, Test_Loss: 0.8207936882972717\n",
      "Epoch: 6, Train_Loss: 0.853474497795105, Test_Loss: 0.8121647834777832 *\n",
      "Epoch: 6, Train_Loss: 0.733557403087616, Test_Loss: 0.7871982455253601 *\n",
      "Epoch: 6, Train_Loss: 0.8114698529243469, Test_Loss: 0.7576291561126709 *\n",
      "Epoch: 6, Train_Loss: 0.7637348175048828, Test_Loss: 0.7587248682975769\n",
      "Epoch: 6, Train_Loss: 0.7697533369064331, Test_Loss: 0.7583872079849243 *\n",
      "Epoch: 6, Train_Loss: 0.7573369145393372, Test_Loss: 0.7462973594665527 *\n",
      "Epoch: 6, Train_Loss: 0.7304736971855164, Test_Loss: 0.8356156349182129\n",
      "Epoch: 6, Train_Loss: 0.7985312342643738, Test_Loss: 0.7852612733840942 *\n",
      "Epoch: 6, Train_Loss: 1.021459937095642, Test_Loss: 6.357017993927002\n",
      "Epoch: 6, Train_Loss: 0.8901323080062866, Test_Loss: 0.8417342901229858 *\n",
      "Epoch: 6, Train_Loss: 1.2590432167053223, Test_Loss: 0.8192219734191895 *\n",
      "Epoch: 6, Train_Loss: 1.0910519361495972, Test_Loss: 0.8323803544044495\n",
      "Epoch: 6, Train_Loss: 0.8990792632102966, Test_Loss: 0.7241395711898804 *\n",
      "Epoch: 6, Train_Loss: 0.8480088710784912, Test_Loss: 0.7324434518814087\n",
      "Epoch: 6, Train_Loss: 0.7806738615036011, Test_Loss: 0.7478361129760742\n",
      "Epoch: 6, Train_Loss: 0.730672299861908, Test_Loss: 0.7712817788124084\n",
      "Epoch: 6, Train_Loss: 0.7273937463760376, Test_Loss: 0.7571011185646057 *\n",
      "Epoch: 6, Train_Loss: 0.8015179634094238, Test_Loss: 0.757227897644043\n",
      "Epoch: 6, Train_Loss: 1.0808732509613037, Test_Loss: 0.7663798332214355\n",
      "Epoch: 6, Train_Loss: 1.116560935974121, Test_Loss: 0.9324023723602295\n",
      "Epoch: 6, Train_Loss: 1.9818851947784424, Test_Loss: 0.815260648727417 *\n",
      "Epoch: 6, Train_Loss: 1.6426937580108643, Test_Loss: 0.822477400302887\n",
      "Epoch: 6, Train_Loss: 1.248878836631775, Test_Loss: 0.7178874015808105 *\n",
      "Epoch: 6, Train_Loss: 0.9632747769355774, Test_Loss: 0.8270395994186401\n",
      "Epoch: 6, Train_Loss: 0.7270148992538452, Test_Loss: 0.7363525032997131 *\n",
      "Epoch: 6, Train_Loss: 0.793414831161499, Test_Loss: 0.8224025964736938\n",
      "Epoch: 6, Train_Loss: 1.2510817050933838, Test_Loss: 0.9393693804740906\n",
      "Epoch: 6, Train_Loss: 1.7730576992034912, Test_Loss: 0.7446685433387756 *\n",
      "Epoch: 6, Train_Loss: 0.7751345634460449, Test_Loss: 0.7915647029876709\n",
      "Epoch: 6, Train_Loss: 0.7736691236495972, Test_Loss: 0.7716485261917114 *\n",
      "Epoch: 6, Train_Loss: 0.8179967403411865, Test_Loss: 0.7797608971595764\n",
      "Epoch: 6, Train_Loss: 0.9161725044250488, Test_Loss: 0.7684187293052673 *\n",
      "Epoch: 6, Train_Loss: 0.9049506187438965, Test_Loss: 0.7854078412055969\n",
      "Epoch: 6, Train_Loss: 1.1097395420074463, Test_Loss: 0.8417339324951172\n",
      "Epoch: 6, Train_Loss: 0.9829602241516113, Test_Loss: 0.8063337206840515 *\n",
      "Epoch: 6, Train_Loss: 1.0514580011367798, Test_Loss: 0.7499248385429382 *\n",
      "Epoch: 6, Train_Loss: 0.7345053553581238, Test_Loss: 0.795859694480896\n",
      "Epoch: 6, Train_Loss: 0.7334060668945312, Test_Loss: 0.7944705486297607 *\n",
      "Epoch: 6, Train_Loss: 0.7327700853347778, Test_Loss: 0.7229735255241394 *\n",
      "Epoch: 6, Train_Loss: 0.8062119483947754, Test_Loss: 0.7652844190597534\n",
      "Epoch: 6, Train_Loss: 0.7269991636276245, Test_Loss: 0.8251941204071045\n",
      "Epoch: 6, Train_Loss: 0.7731997966766357, Test_Loss: 0.9478119611740112\n",
      "Epoch: 6, Train_Loss: 16.593278884887695, Test_Loss: 0.7981152534484863 *\n",
      "Epoch: 6, Train_Loss: 0.7984786629676819, Test_Loss: 0.7429448962211609 *\n",
      "Epoch: 6, Train_Loss: 2.636683702468872, Test_Loss: 0.7140623331069946 *\n",
      "Epoch: 6, Train_Loss: 2.2785794734954834, Test_Loss: 0.7373461723327637\n",
      "Epoch: 6, Train_Loss: 0.7326942086219788, Test_Loss: 0.7825257182121277\n",
      "Epoch: 6, Train_Loss: 0.7721469402313232, Test_Loss: 0.9799260497093201\n",
      "Epoch: 6, Train_Loss: 4.528301239013672, Test_Loss: 1.2043770551681519\n",
      "Epoch: 6, Train_Loss: 7.872975826263428, Test_Loss: 0.8989713191986084 *\n",
      "Epoch: 6, Train_Loss: 0.9035082459449768, Test_Loss: 0.7404428720474243 *\n",
      "Epoch: 6, Train_Loss: 0.8066352605819702, Test_Loss: 0.7904163599014282\n",
      "Epoch: 6, Train_Loss: 6.371402263641357, Test_Loss: 0.9659997820854187\n",
      "Epoch: 6, Train_Loss: 0.777152955532074, Test_Loss: 0.9893732666969299\n",
      "Epoch: 6, Train_Loss: 0.8252978324890137, Test_Loss: 0.9723176956176758 *\n",
      "Epoch: 6, Train_Loss: 0.7360809445381165, Test_Loss: 0.9192070364952087 *\n",
      "Epoch: 6, Train_Loss: 0.7659561634063721, Test_Loss: 0.9588598012924194\n",
      "Epoch: 6, Train_Loss: 0.7868486642837524, Test_Loss: 0.9400529265403748 *\n",
      "Epoch: 6, Train_Loss: 0.7106046676635742, Test_Loss: 0.8374536037445068 *\n",
      "Epoch: 6, Train_Loss: 0.778833270072937, Test_Loss: 1.2104694843292236\n",
      "Epoch: 6, Train_Loss: 0.71470707654953, Test_Loss: 0.7963820099830627 *\n",
      "Epoch: 6, Train_Loss: 0.7126128673553467, Test_Loss: 0.8078673481941223\n",
      "Epoch: 6, Train_Loss: 0.7760980725288391, Test_Loss: 0.8004204630851746 *\n",
      "Epoch: 6, Train_Loss: 0.7764724493026733, Test_Loss: 0.7851463556289673 *\n",
      "Epoch: 6, Train_Loss: 0.7618637084960938, Test_Loss: 0.7639334201812744 *\n",
      "Epoch: 6, Train_Loss: 0.7842786312103271, Test_Loss: 0.7339308261871338 *\n",
      "Epoch: 6, Train_Loss: 0.7414983510971069, Test_Loss: 0.7199320793151855 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 0.7101311087608337, Test_Loss: 6.315985202789307\n",
      "Epoch: 6, Train_Loss: 0.7174745202064514, Test_Loss: 1.6225281953811646 *\n",
      "Epoch: 6, Train_Loss: 0.7107029557228088, Test_Loss: 0.817376971244812 *\n",
      "Epoch: 6, Train_Loss: 0.7039458155632019, Test_Loss: 0.8229770064353943\n",
      "Epoch: 6, Train_Loss: 0.7017920017242432, Test_Loss: 0.7714282274246216 *\n",
      "Epoch: 6, Train_Loss: 0.7007063627243042, Test_Loss: 0.7344408631324768 *\n",
      "Epoch: 6, Train_Loss: 0.7001476883888245, Test_Loss: 0.831145167350769\n",
      "Epoch: 6, Train_Loss: 0.700312077999115, Test_Loss: 0.8384920358657837\n",
      "Epoch: 6, Train_Loss: 0.7013972401618958, Test_Loss: 0.7657808065414429 *\n",
      "Epoch: 6, Train_Loss: 0.6991403698921204, Test_Loss: 0.7753802537918091\n",
      "Epoch: 6, Train_Loss: 0.6987125277519226, Test_Loss: 0.7693748474121094 *\n",
      "Epoch: 6, Train_Loss: 0.7064764499664307, Test_Loss: 0.8017611503601074\n",
      "Epoch: 6, Train_Loss: 0.7146203517913818, Test_Loss: 0.7639657855033875 *\n",
      "Epoch: 6, Train_Loss: 0.745800256729126, Test_Loss: 0.7060341238975525 *\n",
      "Epoch: 6, Train_Loss: 0.7491344809532166, Test_Loss: 0.7413942217826843\n",
      "Epoch: 6, Train_Loss: 0.7249937653541565, Test_Loss: 0.7533305883407593\n",
      "Epoch: 6, Train_Loss: 2.196113109588623, Test_Loss: 0.7384151816368103 *\n",
      "Epoch: 6, Train_Loss: 7.743829250335693, Test_Loss: 0.7592214345932007\n",
      "Epoch: 6, Train_Loss: 0.7063737511634827, Test_Loss: 0.7892389297485352\n",
      "Epoch: 6, Train_Loss: 0.7560431361198425, Test_Loss: 0.7716142535209656 *\n",
      "Epoch: 6, Train_Loss: 0.8089298009872437, Test_Loss: 0.7593250274658203 *\n",
      "Epoch: 6, Train_Loss: 0.7718189358711243, Test_Loss: 0.772413969039917\n",
      "Epoch: 6, Train_Loss: 0.7109095454216003, Test_Loss: 0.873042106628418\n",
      "Epoch: 6, Train_Loss: 0.7514703869819641, Test_Loss: 0.8437051773071289 *\n",
      "Epoch: 6, Train_Loss: 0.8015367388725281, Test_Loss: 0.8403941988945007 *\n",
      "Epoch: 6, Train_Loss: 0.9549002647399902, Test_Loss: 0.7814118266105652 *\n",
      "Epoch: 6, Train_Loss: 0.8042750954627991, Test_Loss: 0.7502248883247375 *\n",
      "Epoch: 6, Train_Loss: 0.757797122001648, Test_Loss: 0.755817711353302\n",
      "Epoch: 6, Train_Loss: 0.732762336730957, Test_Loss: 0.751610279083252 *\n",
      "Epoch: 6, Train_Loss: 0.9048412442207336, Test_Loss: 0.7348223924636841 *\n",
      "Epoch: 6, Train_Loss: 0.823753833770752, Test_Loss: 0.8311523199081421\n",
      "Epoch: 6, Train_Loss: 0.9137675762176514, Test_Loss: 1.1907732486724854\n",
      "Epoch: 6, Train_Loss: 0.8241665959358215, Test_Loss: 5.823390007019043\n",
      "Epoch: 6, Train_Loss: 0.7818366885185242, Test_Loss: 0.7075608968734741 *\n",
      "Epoch: 6, Train_Loss: 0.693629264831543, Test_Loss: 0.6910223364830017 *\n",
      "Epoch: 6, Train_Loss: 0.7358580231666565, Test_Loss: 0.7153140306472778\n",
      "Epoch: 6, Train_Loss: 0.7680309414863586, Test_Loss: 0.6975980997085571 *\n",
      "Epoch: 6, Train_Loss: 0.7124269604682922, Test_Loss: 0.7094016671180725\n",
      "Epoch: 6, Train_Loss: 0.6898247003555298, Test_Loss: 0.7081503868103027 *\n",
      "Epoch: 6, Train_Loss: 0.6894708871841431, Test_Loss: 0.8129778504371643\n",
      "Epoch: 6, Train_Loss: 0.6947966814041138, Test_Loss: 0.7290791869163513 *\n",
      "Epoch: 6, Train_Loss: 2.336642026901245, Test_Loss: 0.689948558807373 *\n",
      "Epoch: 6, Train_Loss: 4.808320045471191, Test_Loss: 0.7268389463424683\n",
      "Epoch: 6, Train_Loss: 0.6889045834541321, Test_Loss: 0.7015153765678406 *\n",
      "Epoch: 6, Train_Loss: 0.7167998552322388, Test_Loss: 0.6978327035903931 *\n",
      "Epoch: 6, Train_Loss: 0.7048837542533875, Test_Loss: 0.7268739938735962\n",
      "Epoch: 6, Train_Loss: 0.69432532787323, Test_Loss: 0.6948556303977966 *\n",
      "Epoch: 6, Train_Loss: 0.6906450390815735, Test_Loss: 0.7795565724372864\n",
      "Epoch: 6, Train_Loss: 0.6934975981712341, Test_Loss: 0.8016177415847778\n",
      "Epoch: 6, Train_Loss: 0.6990385055541992, Test_Loss: 0.7236616015434265 *\n",
      "Epoch: 6, Train_Loss: 0.6987059116363525, Test_Loss: 0.7124819159507751 *\n",
      "Epoch: 6, Train_Loss: 0.7316189408302307, Test_Loss: 0.6914162039756775 *\n",
      "Epoch: 6, Train_Loss: 0.7005549669265747, Test_Loss: 0.6896029710769653 *\n",
      "Epoch: 6, Train_Loss: 0.6876757740974426, Test_Loss: 0.6869738698005676 *\n",
      "Epoch: 6, Train_Loss: 0.6857572197914124, Test_Loss: 0.6877639293670654\n",
      "Epoch: 6, Train_Loss: 0.7035877108573914, Test_Loss: 0.689093828201294\n",
      "Epoch: 6, Train_Loss: 0.6860223412513733, Test_Loss: 0.6861844658851624 *\n",
      "Epoch: 6, Train_Loss: 0.6860568523406982, Test_Loss: 0.6971175670623779\n",
      "Epoch: 6, Train_Loss: 0.7119176983833313, Test_Loss: 0.6868674755096436 *\n",
      "Epoch: 6, Train_Loss: 0.7122400999069214, Test_Loss: 0.6837297677993774 *\n",
      "Epoch: 6, Train_Loss: 0.6899704337120056, Test_Loss: 0.7261499166488647\n",
      "Epoch: 6, Train_Loss: 0.682067334651947, Test_Loss: 0.6960361003875732 *\n",
      "Epoch: 6, Train_Loss: 0.7015902996063232, Test_Loss: 0.6911033391952515 *\n",
      "Epoch: 6, Train_Loss: 0.7602641582489014, Test_Loss: 0.7236568331718445\n",
      "Epoch: 6, Train_Loss: 0.7046059966087341, Test_Loss: 0.8800531625747681\n",
      "Epoch: 6, Train_Loss: 0.7175299525260925, Test_Loss: 1.016011357307434\n",
      "Epoch: 6, Train_Loss: 0.718479335308075, Test_Loss: 0.8477218747138977 *\n",
      "Epoch: 6, Train_Loss: 0.7983070611953735, Test_Loss: 0.711889386177063 *\n",
      "Epoch: 6, Train_Loss: 0.7286013960838318, Test_Loss: 0.6968459486961365 *\n",
      "Epoch: 6, Train_Loss: 0.7511284947395325, Test_Loss: 0.6968605518341064\n",
      "Epoch: 6, Train_Loss: 0.7689028978347778, Test_Loss: 0.7792374491691589\n",
      "Epoch: 6, Train_Loss: 0.8421717286109924, Test_Loss: 1.3934446573257446\n",
      "Epoch: 6, Train_Loss: 0.7072378993034363, Test_Loss: 1.6295323371887207\n",
      "Epoch: 6, Train_Loss: 0.6810595989227295, Test_Loss: 0.7462404370307922 *\n",
      "Epoch: 6, Train_Loss: 0.6786792278289795, Test_Loss: 0.7255145311355591 *\n",
      "Epoch: 6, Train_Loss: 0.6763861179351807, Test_Loss: 0.6821670532226562 *\n",
      "Epoch: 6, Train_Loss: 0.6758957505226135, Test_Loss: 0.6928067803382874\n",
      "Epoch: 6, Train_Loss: 0.6789814233779907, Test_Loss: 0.6929419040679932\n",
      "Epoch: 6, Train_Loss: 2.5315804481506348, Test_Loss: 0.7098408341407776\n",
      "Epoch: 6, Train_Loss: 3.7502481937408447, Test_Loss: 0.7160384654998779\n",
      "Epoch: 6, Train_Loss: 0.676841676235199, Test_Loss: 0.6967345476150513 *\n",
      "Epoch: 6, Train_Loss: 0.6852455735206604, Test_Loss: 0.683884859085083 *\n",
      "Epoch: 6, Train_Loss: 0.6817528009414673, Test_Loss: 0.781528651714325\n",
      "Epoch: 6, Train_Loss: 0.6757704615592957, Test_Loss: 1.0729182958602905\n",
      "Epoch: 6, Train_Loss: 0.6742157936096191, Test_Loss: 0.8860382437705994 *\n",
      "Epoch: 6, Train_Loss: 0.6736630797386169, Test_Loss: 0.7727881073951721 *\n",
      "Epoch: 6, Train_Loss: 0.6726552844047546, Test_Loss: 0.6891787052154541 *\n",
      "Epoch: 6, Train_Loss: 0.672479510307312, Test_Loss: 0.6888757944107056 *\n",
      "Epoch: 6, Train_Loss: 0.6802382469177246, Test_Loss: 0.6892757415771484\n",
      "Epoch: 6, Train_Loss: 0.7398884296417236, Test_Loss: 0.690447986125946\n",
      "Epoch: 6, Train_Loss: 0.715344250202179, Test_Loss: 0.7656773328781128\n",
      "Epoch: 6, Train_Loss: 0.7575767636299133, Test_Loss: 5.862672805786133\n",
      "Epoch: 6, Train_Loss: 0.7026052474975586, Test_Loss: 0.8184173107147217 *\n",
      "Epoch: 6, Train_Loss: 0.6733747720718384, Test_Loss: 0.6825796365737915 *\n",
      "Epoch: 6, Train_Loss: 0.8589361906051636, Test_Loss: 0.6736688613891602 *\n",
      "Epoch: 6, Train_Loss: 0.8975749611854553, Test_Loss: 0.6758921146392822\n",
      "Epoch: 6, Train_Loss: 0.8643726110458374, Test_Loss: 0.6783811450004578\n",
      "Epoch: 6, Train_Loss: 0.7632547616958618, Test_Loss: 0.6769257187843323 *\n",
      "Epoch: 6, Train_Loss: 0.6697620749473572, Test_Loss: 0.6958271265029907\n",
      "Epoch: 6, Train_Loss: 0.6688314080238342, Test_Loss: 0.6737711429595947 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 0.6723324656486511, Test_Loss: 0.6803446412086487\n",
      "Epoch: 6, Train_Loss: 0.6794631481170654, Test_Loss: 0.6806264519691467\n",
      "Epoch: 6, Train_Loss: 0.6859511733055115, Test_Loss: 0.7037959098815918\n",
      "Epoch: 6, Train_Loss: 0.674852728843689, Test_Loss: 0.6728909015655518 *\n",
      "Epoch: 6, Train_Loss: 0.6683149933815002, Test_Loss: 0.6722185611724854 *\n",
      "Epoch: 6, Train_Loss: 0.667366087436676, Test_Loss: 0.6811184883117676\n",
      "Epoch: 6, Train_Loss: 0.6765205264091492, Test_Loss: 0.6744131445884705 *\n",
      "Epoch: 6, Train_Loss: 0.7279136776924133, Test_Loss: 0.6729159951210022 *\n",
      "Epoch: 6, Train_Loss: 0.8666266798973083, Test_Loss: 0.6718078851699829 *\n",
      "Epoch: 6, Train_Loss: 0.846557080745697, Test_Loss: 0.6843945980072021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 0.7941904664039612, Test_Loss: 0.6746917366981506 *\n",
      "Epoch: 6, Train_Loss: 0.7720955014228821, Test_Loss: 0.6775758266448975\n",
      "Epoch: 6, Train_Loss: 0.8132624626159668, Test_Loss: 0.677314281463623 *\n",
      "Epoch: 6, Train_Loss: 0.7231320738792419, Test_Loss: 0.6998346447944641\n",
      "Epoch: 6, Train_Loss: 0.8092750310897827, Test_Loss: 0.6951728463172913 *\n",
      "Epoch: 6, Train_Loss: 0.8094565272331238, Test_Loss: 0.6817004084587097 *\n",
      "Epoch: 6, Train_Loss: 0.9738726615905762, Test_Loss: 0.6750562787055969 *\n",
      "Epoch: 6, Train_Loss: 0.6736750602722168, Test_Loss: 0.6755015850067139\n",
      "Epoch: 6, Train_Loss: 0.7147237658500671, Test_Loss: 0.6723125576972961 *\n",
      "Epoch: 6, Train_Loss: 3.682683229446411, Test_Loss: 0.6693244576454163 *\n",
      "Epoch: 6, Train_Loss: 0.8599804043769836, Test_Loss: 0.6757866740226746\n",
      "Epoch: 6, Train_Loss: 0.69393390417099, Test_Loss: 0.7285550236701965\n",
      "Epoch: 6, Train_Loss: 0.7003824710845947, Test_Loss: 2.3326501846313477\n",
      "Epoch: 6, Train_Loss: 0.6923346519470215, Test_Loss: 4.395094871520996\n",
      "Epoch: 6, Train_Loss: 0.6710576415061951, Test_Loss: 0.6674482822418213 *\n",
      "Epoch: 6, Train_Loss: 0.6673159599304199, Test_Loss: 0.6611998081207275 *\n",
      "Epoch: 6, Train_Loss: 0.7399935722351074, Test_Loss: 0.7079206705093384\n",
      "Epoch: 6, Train_Loss: 0.7962895631790161, Test_Loss: 0.686336100101471 *\n",
      "Epoch: 6, Train_Loss: 0.7611503005027771, Test_Loss: 0.7099794745445251\n",
      "Epoch: 6, Train_Loss: 0.7308410406112671, Test_Loss: 0.6876831650733948 *\n",
      "Epoch: 6, Train_Loss: 0.7313299775123596, Test_Loss: 0.7679431438446045\n",
      "Epoch: 6, Train_Loss: 0.6815837621688843, Test_Loss: 0.668967068195343 *\n",
      "Epoch: 6, Train_Loss: 0.6887093782424927, Test_Loss: 0.6707133650779724\n",
      "Epoch: 6, Train_Loss: 0.6669337749481201, Test_Loss: 0.6829191446304321\n",
      "Epoch: 6, Train_Loss: 0.7011443972587585, Test_Loss: 0.6833329796791077\n",
      "Epoch: 6, Train_Loss: 0.6791552305221558, Test_Loss: 0.6645482182502747 *\n",
      "Epoch: 6, Train_Loss: 0.6584413051605225, Test_Loss: 0.7312278151512146\n",
      "Epoch: 6, Train_Loss: 0.6761914491653442, Test_Loss: 0.7389715313911438\n",
      "Epoch: 6, Train_Loss: 0.7058099508285522, Test_Loss: 0.720632791519165 *\n",
      "Epoch: 6, Train_Loss: 0.6910823583602905, Test_Loss: 0.7326317429542542\n",
      "Epoch: 6, Train_Loss: 0.6583214402198792, Test_Loss: 0.6851919889450073 *\n",
      "Epoch: 6, Train_Loss: 0.6565704345703125, Test_Loss: 0.7062847018241882\n",
      "Epoch: 6, Train_Loss: 0.6563146710395813, Test_Loss: 0.6719375252723694 *\n",
      "Epoch: 6, Train_Loss: 0.6562885642051697, Test_Loss: 0.6679866909980774 *\n",
      "Epoch: 6, Train_Loss: 0.6570872664451599, Test_Loss: 0.6720895171165466\n",
      "Epoch: 6, Train_Loss: 0.6587339639663696, Test_Loss: 0.6755459904670715\n",
      "Epoch: 6, Train_Loss: 0.6575554609298706, Test_Loss: 0.6718875765800476 *\n",
      "Epoch: 6, Train_Loss: 0.6556497812271118, Test_Loss: 0.6717327833175659 *\n",
      "Epoch: 6, Train_Loss: 0.6544955372810364, Test_Loss: 0.674526572227478\n",
      "Epoch: 6, Train_Loss: 0.6554720401763916, Test_Loss: 0.668910801410675 *\n",
      "Epoch: 6, Train_Loss: 0.6659246683120728, Test_Loss: 0.6750176548957825\n",
      "Epoch: 6, Train_Loss: 0.6721806526184082, Test_Loss: 0.6565542817115784 *\n",
      "Epoch: 6, Train_Loss: 0.6767728924751282, Test_Loss: 0.6733413338661194\n",
      "Epoch: 6, Train_Loss: 0.6765968799591064, Test_Loss: 0.7267543077468872\n",
      "Epoch: 6, Train_Loss: 0.6657905578613281, Test_Loss: 0.6620416641235352 *\n",
      "Epoch: 6, Train_Loss: 0.6700542569160461, Test_Loss: 1.131281852722168\n",
      "Epoch: 6, Train_Loss: 0.6540663838386536, Test_Loss: 1.1838715076446533\n",
      "Epoch: 6, Train_Loss: 0.6547670364379883, Test_Loss: 0.8358914852142334 *\n",
      "Epoch: 6, Train_Loss: 0.6751152276992798, Test_Loss: 0.6858430504798889 *\n",
      "Epoch: 6, Train_Loss: 0.6808769702911377, Test_Loss: 0.6779817938804626 *\n",
      "Epoch: 6, Train_Loss: 0.6523987054824829, Test_Loss: 0.671820878982544 *\n",
      "Epoch: 6, Train_Loss: 0.6550513505935669, Test_Loss: 0.8087120652198792\n",
      "Epoch: 6, Train_Loss: 0.6515632271766663, Test_Loss: 1.6113007068634033\n",
      "Epoch: 6, Train_Loss: 0.6971558332443237, Test_Loss: 1.435471773147583 *\n",
      "Epoch: 6, Train_Loss: 0.6872490644454956, Test_Loss: 0.7010294198989868 *\n",
      "Epoch: 6, Train_Loss: 0.7075647115707397, Test_Loss: 0.7150018215179443\n",
      "Epoch: 6, Train_Loss: 0.6605809330940247, Test_Loss: 0.6509131789207458 *\n",
      "Epoch: 6, Train_Loss: 0.6565688848495483, Test_Loss: 0.6522623896598816\n",
      "Epoch: 6, Train_Loss: 0.7172220349311829, Test_Loss: 0.6561511754989624\n",
      "Epoch: 6, Train_Loss: 0.6491777896881104, Test_Loss: 0.662002682685852\n",
      "Epoch: 6, Train_Loss: 0.6602883338928223, Test_Loss: 0.7106302380561829\n",
      "Epoch: 6, Train_Loss: 0.674168586730957, Test_Loss: 0.6516585946083069 *\n",
      "Epoch: 6, Train_Loss: 0.6794756650924683, Test_Loss: 0.6674318313598633\n",
      "Epoch: 6, Train_Loss: 0.7843141555786133, Test_Loss: 0.7665414810180664\n",
      "Epoch: 6, Train_Loss: 0.7238224744796753, Test_Loss: 1.0339586734771729\n",
      "Epoch: 6, Train_Loss: 0.6772976517677307, Test_Loss: 0.8852054476737976 *\n",
      "Epoch: 6, Train_Loss: 0.6529328227043152, Test_Loss: 0.6608688235282898 *\n",
      "Epoch: 6, Train_Loss: 0.6714019775390625, Test_Loss: 0.6541539430618286 *\n",
      "Epoch: 6, Train_Loss: 0.6467875242233276, Test_Loss: 0.6537076830863953 *\n",
      "Epoch: 6, Train_Loss: 0.6508979201316833, Test_Loss: 0.6535530686378479 *\n",
      "Epoch: 6, Train_Loss: 0.6540629267692566, Test_Loss: 0.654096782207489\n",
      "Epoch: 6, Train_Loss: 0.6590613126754761, Test_Loss: 1.16055428981781\n",
      "Epoch: 6, Train_Loss: 0.6980838179588318, Test_Loss: 5.549606800079346\n",
      "Epoch: 6, Train_Loss: 0.7018441557884216, Test_Loss: 0.6870498657226562 *\n",
      "Epoch: 6, Train_Loss: 0.6836498379707336, Test_Loss: 0.6495195627212524 *\n",
      "Epoch: 6, Train_Loss: 0.6785121560096741, Test_Loss: 0.6449611186981201 *\n",
      "Epoch: 6, Train_Loss: 0.6689075231552124, Test_Loss: 0.6491512060165405\n",
      "Epoch: 6, Train_Loss: 0.6538227200508118, Test_Loss: 0.6471098065376282 *\n",
      "Epoch: 6, Train_Loss: 0.7806384563446045, Test_Loss: 0.644985020160675 *\n",
      "Epoch: 6, Train_Loss: 0.8361720442771912, Test_Loss: 0.6511240601539612\n",
      "Epoch: 6, Train_Loss: 0.6429053544998169, Test_Loss: 0.6432397961616516 *\n",
      "Epoch: 6, Train_Loss: 0.6748937964439392, Test_Loss: 0.6460452079772949\n",
      "Epoch: 6, Train_Loss: 0.6413736939430237, Test_Loss: 0.6438850164413452 *\n",
      "Epoch: 6, Train_Loss: 0.6403927206993103, Test_Loss: 0.6555185317993164\n",
      "Epoch: 6, Train_Loss: 0.6408290266990662, Test_Loss: 0.6552894115447998 *\n",
      "Epoch: 6, Train_Loss: 0.6419918537139893, Test_Loss: 0.6577482223510742\n",
      "Epoch: 6, Train_Loss: 0.6515503525733948, Test_Loss: 0.651829719543457 *\n",
      "Epoch: 6, Train_Loss: 0.6620792746543884, Test_Loss: 0.6406059861183167 *\n",
      "Epoch: 6, Train_Loss: 0.6476782560348511, Test_Loss: 0.640507161617279 *\n",
      "Epoch: 6, Train_Loss: 0.6513885259628296, Test_Loss: 0.6394459009170532 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 0.6569352746009827, Test_Loss: 0.6461060643196106\n",
      "Epoch: 6, Train_Loss: 0.6381024718284607, Test_Loss: 0.6384651064872742 *\n",
      "Epoch: 6, Train_Loss: 0.6386597752571106, Test_Loss: 0.6418859362602234\n",
      "Epoch: 6, Train_Loss: 0.6372295022010803, Test_Loss: 0.6382603645324707 *\n",
      "Epoch: 6, Train_Loss: 0.6656126379966736, Test_Loss: 0.6465794444084167\n",
      "Epoch: 6, Train_Loss: 0.6647684574127197, Test_Loss: 0.6438597440719604 *\n",
      "Epoch: 6, Train_Loss: 0.6500677466392517, Test_Loss: 0.6412138342857361 *\n",
      "Epoch: 6, Train_Loss: 0.6529232859611511, Test_Loss: 0.638214111328125 *\n",
      "Epoch: 6, Train_Loss: 0.7048441171646118, Test_Loss: 0.6384320855140686\n",
      "Epoch: 6, Train_Loss: 0.6840605735778809, Test_Loss: 0.6368921995162964 *\n",
      "Epoch: 6, Train_Loss: 0.6499099731445312, Test_Loss: 0.6366255283355713 *\n",
      "Epoch: 6, Train_Loss: 0.652717113494873, Test_Loss: 0.6505240201950073\n",
      "Epoch: 6, Train_Loss: 0.6517366170883179, Test_Loss: 0.6936544179916382\n",
      "Epoch: 6, Train_Loss: 0.6452520489692688, Test_Loss: 3.399045944213867\n",
      "Epoch: 6, Train_Loss: 0.6436885595321655, Test_Loss: 3.3780293464660645 *\n",
      "Epoch: 6, Train_Loss: 0.6606955528259277, Test_Loss: 0.6360311508178711 *\n",
      "Epoch: 6, Train_Loss: 0.6835274696350098, Test_Loss: 0.6338126063346863 *\n",
      "Epoch: 6, Train_Loss: 2.8866004943847656, Test_Loss: 0.6763272881507874\n",
      "Epoch: 6, Train_Loss: 3.768923044204712, Test_Loss: 0.6558462381362915 *\n",
      "Epoch: 6, Train_Loss: 0.6526973843574524, Test_Loss: 0.669713020324707\n",
      "Epoch: 6, Train_Loss: 0.6416826844215393, Test_Loss: 0.6866441965103149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 0.6934847235679626, Test_Loss: 0.7431064248085022\n",
      "Epoch: 6, Train_Loss: 0.8149499297142029, Test_Loss: 0.6340166330337524 *\n",
      "Epoch: 6, Train_Loss: 0.6624510884284973, Test_Loss: 0.6571372151374817\n",
      "Epoch: 6, Train_Loss: 0.6378861665725708, Test_Loss: 0.6521068215370178 *\n",
      "Epoch: 6, Train_Loss: 0.6430593132972717, Test_Loss: 0.6465298533439636 *\n",
      "Epoch: 6, Train_Loss: 0.706809401512146, Test_Loss: 0.6384867429733276 *\n",
      "Epoch: 6, Train_Loss: 0.6389200091362, Test_Loss: 0.6966286301612854\n",
      "Epoch: 6, Train_Loss: 0.6417317986488342, Test_Loss: 0.6822603940963745 *\n",
      "Epoch: 6, Train_Loss: 1.4740676879882812, Test_Loss: 0.7161035537719727\n",
      "Epoch: 6, Train_Loss: 1.889291524887085, Test_Loss: 0.7215093374252319\n",
      "Epoch: 6, Train_Loss: 1.1934425830841064, Test_Loss: 0.6523652672767639 *\n",
      "Epoch: 6, Train_Loss: 0.743651270866394, Test_Loss: 0.6506909728050232 *\n",
      "Epoch: 6, Train_Loss: 1.4842901229858398, Test_Loss: 0.6383554935455322 *\n",
      "Epoch: 6, Train_Loss: 2.7443251609802246, Test_Loss: 0.6358979940414429 *\n",
      "Epoch: 6, Train_Loss: 0.8757194876670837, Test_Loss: 0.6367542147636414\n",
      "Epoch: 6, Train_Loss: 0.6467761397361755, Test_Loss: 0.6411837339401245\n",
      "Epoch: 6, Train_Loss: 0.6483265161514282, Test_Loss: 0.6399025321006775 *\n",
      "Epoch: 6, Train_Loss: 1.865562081336975, Test_Loss: 0.6333994269371033 *\n",
      "Epoch: 6, Train_Loss: 1.8600268363952637, Test_Loss: 0.6342885494232178\n",
      "Epoch: 6, Train_Loss: 0.7151839137077332, Test_Loss: 0.6273154020309448 *\n",
      "Epoch: 6, Train_Loss: 0.6396000385284424, Test_Loss: 0.6297442317008972\n",
      "Epoch: 6, Train_Loss: 0.6281939148902893, Test_Loss: 0.6461473703384399\n",
      "Epoch: 6, Train_Loss: 1.3293557167053223, Test_Loss: 0.6282883882522583 *\n",
      "Epoch: 6, Train_Loss: 0.7770417928695679, Test_Loss: 0.6498972177505493\n",
      "Epoch: 6, Train_Loss: 0.6871559023857117, Test_Loss: 0.659337043762207\n",
      "Epoch: 6, Train_Loss: 0.6626367568969727, Test_Loss: 1.0325361490249634\n",
      "Epoch: 6, Train_Loss: 0.7969945669174194, Test_Loss: 1.0294798612594604 *\n",
      "Epoch: 6, Train_Loss: 0.782246470451355, Test_Loss: 0.7612724304199219 *\n",
      "Epoch: 6, Train_Loss: 0.7519631385803223, Test_Loss: 0.6497024893760681 *\n",
      "Epoch: 6, Train_Loss: 0.9177863597869873, Test_Loss: 0.6441968083381653 *\n",
      "Epoch: 6, Train_Loss: 0.6884917616844177, Test_Loss: 0.6672857999801636\n",
      "Epoch: 6, Train_Loss: 0.7066789269447327, Test_Loss: 0.8521147966384888\n",
      "Epoch: 6, Train_Loss: 0.82793128490448, Test_Loss: 1.4015002250671387\n",
      "Epoch: 7, Train_Loss: 0.9466502666473389, Test_Loss: 1.1488615274429321 *\n",
      "Epoch: 7, Train_Loss: 0.9375859498977661, Test_Loss: 0.6849722266197205 *\n",
      "Epoch: 7, Train_Loss: 0.6858436465263367, Test_Loss: 0.6535586714744568 *\n",
      "Epoch: 7, Train_Loss: 0.7513065934181213, Test_Loss: 0.633716881275177 *\n",
      "Epoch: 7, Train_Loss: 0.7621579766273499, Test_Loss: 0.6281348466873169 *\n",
      "Epoch: 7, Train_Loss: 0.6520050764083862, Test_Loss: 0.6486518383026123\n",
      "Epoch: 7, Train_Loss: 0.6285735964775085, Test_Loss: 0.6358361840248108 *\n",
      "Epoch: 7, Train_Loss: 0.6227301955223083, Test_Loss: 0.6668364405632019\n",
      "Epoch: 7, Train_Loss: 0.6223620176315308, Test_Loss: 0.6403762102127075 *\n",
      "Epoch: 7, Train_Loss: 0.625581681728363, Test_Loss: 0.6849964261054993\n",
      "Epoch: 7, Train_Loss: 0.6274211406707764, Test_Loss: 0.751331090927124\n",
      "Epoch: 7, Train_Loss: 0.6674883365631104, Test_Loss: 1.0270401239395142\n",
      "Epoch: 7, Train_Loss: 0.6372975707054138, Test_Loss: 1.0254268646240234 *\n",
      "Epoch: 7, Train_Loss: 0.667352557182312, Test_Loss: 0.6558201313018799 *\n",
      "Epoch: 7, Train_Loss: 0.7477341294288635, Test_Loss: 0.6342637538909912 *\n",
      "Epoch: 7, Train_Loss: 0.9555853605270386, Test_Loss: 0.63161700963974 *\n",
      "Epoch: 7, Train_Loss: 0.6236111521720886, Test_Loss: 0.6326582431793213\n",
      "Epoch: 7, Train_Loss: 0.6661636233329773, Test_Loss: 0.6793311238288879\n",
      "Epoch: 7, Train_Loss: 0.7431038618087769, Test_Loss: 2.046982526779175\n",
      "Epoch: 7, Train_Loss: 0.9328129291534424, Test_Loss: 4.4731059074401855\n",
      "Epoch: 7, Train_Loss: 0.83582603931427, Test_Loss: 0.6427573561668396 *\n",
      "Epoch: 7, Train_Loss: 0.6322601437568665, Test_Loss: 0.6386899352073669 *\n",
      "Epoch: 7, Train_Loss: 0.7191606760025024, Test_Loss: 0.6343473792076111 *\n",
      "Epoch: 7, Train_Loss: 1.0360164642333984, Test_Loss: 0.6201884746551514 *\n",
      "Epoch: 7, Train_Loss: 0.9090406894683838, Test_Loss: 0.6551101207733154\n",
      "Epoch: 7, Train_Loss: 0.6451311707496643, Test_Loss: 0.6587764024734497\n",
      "Epoch: 7, Train_Loss: 0.628746747970581, Test_Loss: 0.6638550758361816\n",
      "Epoch: 7, Train_Loss: 0.634900689125061, Test_Loss: 0.6306315660476685 *\n",
      "Epoch: 7, Train_Loss: 1.276656150817871, Test_Loss: 0.6524715423583984\n",
      "Epoch: 7, Train_Loss: 1.2538173198699951, Test_Loss: 0.6540383696556091\n",
      "Epoch: 7, Train_Loss: 0.6532509326934814, Test_Loss: 0.7041406035423279\n",
      "Epoch: 7, Train_Loss: 0.653455913066864, Test_Loss: 0.6224294900894165 *\n",
      "Epoch: 7, Train_Loss: 0.6177078485488892, Test_Loss: 0.624370276927948\n",
      "Epoch: 7, Train_Loss: 0.6451797485351562, Test_Loss: 0.6535178422927856\n",
      "Epoch: 7, Train_Loss: 0.9778482913970947, Test_Loss: 0.6402499079704285 *\n",
      "Epoch: 7, Train_Loss: 0.6236268281936646, Test_Loss: 0.6534299254417419\n",
      "Epoch: 7, Train_Loss: 0.8160457611083984, Test_Loss: 0.6314219236373901 *\n",
      "Epoch: 7, Train_Loss: 0.70912766456604, Test_Loss: 0.718419075012207\n",
      "Epoch: 7, Train_Loss: 0.6336939930915833, Test_Loss: 0.6233420968055725 *\n",
      "Epoch: 7, Train_Loss: 0.6306776404380798, Test_Loss: 0.6521003246307373\n",
      "Epoch: 7, Train_Loss: 0.7287822365760803, Test_Loss: 0.6596031785011292\n",
      "Epoch: 7, Train_Loss: 0.7055639624595642, Test_Loss: 0.7513216733932495\n",
      "Epoch: 7, Train_Loss: 0.6666889786720276, Test_Loss: 0.7329162359237671 *\n",
      "Epoch: 7, Train_Loss: 0.6412526965141296, Test_Loss: 0.6855229139328003 *\n",
      "Epoch: 7, Train_Loss: 0.6340779662132263, Test_Loss: 0.6391923427581787 *\n",
      "Epoch: 7, Train_Loss: 0.704120934009552, Test_Loss: 0.6479150056838989\n",
      "Epoch: 7, Train_Loss: 0.6663345694541931, Test_Loss: 0.6559985280036926\n",
      "Epoch: 7, Train_Loss: 0.6395494341850281, Test_Loss: 0.6241521835327148 *\n",
      "Epoch: 7, Train_Loss: 0.6185174584388733, Test_Loss: 0.7588209509849548\n",
      "Epoch: 7, Train_Loss: 0.6541164517402649, Test_Loss: 0.680145800113678 *\n",
      "Epoch: 7, Train_Loss: 0.916750431060791, Test_Loss: 4.538933753967285\n",
      "Epoch: 7, Train_Loss: 0.8848460912704468, Test_Loss: 2.16694974899292 *\n",
      "Epoch: 7, Train_Loss: 1.075430989265442, Test_Loss: 0.6614440679550171 *\n",
      "Epoch: 7, Train_Loss: 0.9803292155265808, Test_Loss: 0.6598310470581055 *\n",
      "Epoch: 7, Train_Loss: 0.8404648303985596, Test_Loss: 0.6274334192276001 *\n",
      "Epoch: 7, Train_Loss: 0.7869480848312378, Test_Loss: 0.6244104504585266 *\n",
      "Epoch: 7, Train_Loss: 0.6780261397361755, Test_Loss: 0.6268994212150574\n",
      "Epoch: 7, Train_Loss: 0.6150540113449097, Test_Loss: 0.6593925356864929\n",
      "Epoch: 7, Train_Loss: 0.6208197474479675, Test_Loss: 0.6492159366607666 *\n",
      "Epoch: 7, Train_Loss: 0.6655427813529968, Test_Loss: 0.6184837222099304 *\n",
      "Epoch: 7, Train_Loss: 0.8753278851509094, Test_Loss: 0.6464707255363464\n",
      "Epoch: 7, Train_Loss: 1.008408546447754, Test_Loss: 0.7098323106765747\n",
      "Epoch: 7, Train_Loss: 1.3317852020263672, Test_Loss: 0.6862321496009827 *\n",
      "Epoch: 7, Train_Loss: 1.7519361972808838, Test_Loss: 0.6459716558456421 *\n",
      "Epoch: 7, Train_Loss: 0.9691736698150635, Test_Loss: 0.6295864582061768 *\n",
      "Epoch: 7, Train_Loss: 0.9016784429550171, Test_Loss: 0.6895287036895752\n",
      "Epoch: 7, Train_Loss: 0.6230593323707581, Test_Loss: 0.6518996357917786 *\n",
      "Epoch: 7, Train_Loss: 0.6322781443595886, Test_Loss: 0.6323432922363281 *\n",
      "Epoch: 7, Train_Loss: 1.0196728706359863, Test_Loss: 0.7790713906288147\n",
      "Epoch: 7, Train_Loss: 1.862175464630127, Test_Loss: 0.6380068063735962 *\n",
      "Epoch: 7, Train_Loss: 0.6969987750053406, Test_Loss: 0.67253577709198\n",
      "Epoch: 7, Train_Loss: 0.6447358727455139, Test_Loss: 0.6655185222625732 *\n",
      "Epoch: 7, Train_Loss: 0.6943923234939575, Test_Loss: 0.6743204593658447\n",
      "Epoch: 7, Train_Loss: 0.7129676938056946, Test_Loss: 0.6406234502792358 *\n",
      "Epoch: 7, Train_Loss: 0.912790060043335, Test_Loss: 0.680317759513855\n",
      "Epoch: 7, Train_Loss: 0.8889961242675781, Test_Loss: 0.7736035585403442\n",
      "Epoch: 7, Train_Loss: 0.8961924910545349, Test_Loss: 0.7378194332122803 *\n",
      "Epoch: 7, Train_Loss: 0.9531234502792358, Test_Loss: 0.6913031339645386 *\n",
      "Epoch: 7, Train_Loss: 0.6397197246551514, Test_Loss: 0.6648311614990234 *\n",
      "Epoch: 7, Train_Loss: 0.6150918006896973, Test_Loss: 0.7222661972045898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.6412943601608276, Test_Loss: 0.6452518105506897 *\n",
      "Epoch: 7, Train_Loss: 0.7114792466163635, Test_Loss: 0.6371207237243652 *\n",
      "Epoch: 7, Train_Loss: 0.6058128476142883, Test_Loss: 0.7041089534759521\n",
      "Epoch: 7, Train_Loss: 0.6408801674842834, Test_Loss: 0.772024929523468\n",
      "Epoch: 7, Train_Loss: 16.22124481201172, Test_Loss: 0.7541913986206055 *\n",
      "Epoch: 7, Train_Loss: 1.046217918395996, Test_Loss: 0.6485607028007507 *\n",
      "Epoch: 7, Train_Loss: 1.8280922174453735, Test_Loss: 0.6050261855125427 *\n",
      "Epoch: 7, Train_Loss: 2.7395119667053223, Test_Loss: 0.6252869963645935\n",
      "Epoch: 7, Train_Loss: 0.6320836544036865, Test_Loss: 0.6797754764556885\n",
      "Epoch: 7, Train_Loss: 0.7231746912002563, Test_Loss: 0.7904738783836365\n",
      "Epoch: 7, Train_Loss: 2.1659348011016846, Test_Loss: 0.9258596897125244\n",
      "Epoch: 7, Train_Loss: 9.393366813659668, Test_Loss: 0.993277907371521\n",
      "Epoch: 7, Train_Loss: 0.8993561267852783, Test_Loss: 0.6401993036270142 *\n",
      "Epoch: 7, Train_Loss: 0.6553936004638672, Test_Loss: 0.770667314529419\n",
      "Epoch: 7, Train_Loss: 5.328963279724121, Test_Loss: 1.026753306388855\n",
      "Epoch: 7, Train_Loss: 1.070528268814087, Test_Loss: 1.1247000694274902\n",
      "Epoch: 7, Train_Loss: 0.7625534534454346, Test_Loss: 1.20145845413208\n",
      "Epoch: 7, Train_Loss: 0.6250014305114746, Test_Loss: 0.7982103824615479 *\n",
      "Epoch: 7, Train_Loss: 0.6706814765930176, Test_Loss: 1.13985276222229\n",
      "Epoch: 7, Train_Loss: 0.6737889051437378, Test_Loss: 0.7317224144935608 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 0.6472201347351074, Test_Loss: 0.9648304581642151\n",
      "Epoch: 7, Train_Loss: 0.6814758777618408, Test_Loss: 1.5099612474441528\n",
      "Epoch: 7, Train_Loss: 0.599389374256134, Test_Loss: 0.8270135521888733 *\n",
      "Epoch: 7, Train_Loss: 0.5970351099967957, Test_Loss: 0.8529856204986572\n",
      "Epoch: 7, Train_Loss: 0.5993176102638245, Test_Loss: 0.6410855054855347 *\n",
      "Epoch: 7, Train_Loss: 0.643785297870636, Test_Loss: 0.6427175402641296\n",
      "Epoch: 7, Train_Loss: 0.6198892593383789, Test_Loss: 0.6345019340515137 *\n",
      "Epoch: 7, Train_Loss: 0.6270997524261475, Test_Loss: 0.6183924674987793 *\n",
      "Epoch: 7, Train_Loss: 0.6098262071609497, Test_Loss: 0.6245721578598022\n",
      "Epoch: 7, Train_Loss: 0.6051287055015564, Test_Loss: 3.833895683288574\n",
      "Epoch: 7, Train_Loss: 0.6121990084648132, Test_Loss: 3.808166265487671 *\n",
      "Epoch: 7, Train_Loss: 0.6092288494110107, Test_Loss: 0.8516858816146851 *\n",
      "Epoch: 7, Train_Loss: 0.5945616364479065, Test_Loss: 0.8894317150115967\n",
      "Epoch: 7, Train_Loss: 0.5959808826446533, Test_Loss: 0.8630028963088989 *\n",
      "Epoch: 7, Train_Loss: 0.592811107635498, Test_Loss: 0.6500239372253418 *\n",
      "Epoch: 7, Train_Loss: 0.5920053124427795, Test_Loss: 0.9638579487800598\n",
      "Epoch: 7, Train_Loss: 0.593035101890564, Test_Loss: 0.9107033014297485 *\n",
      "Epoch: 7, Train_Loss: 0.592068076133728, Test_Loss: 0.8116167783737183 *\n",
      "Epoch: 7, Train_Loss: 0.5915864706039429, Test_Loss: 0.7346646189689636 *\n",
      "Epoch: 7, Train_Loss: 0.5908194780349731, Test_Loss: 0.7884498834609985\n",
      "Epoch: 7, Train_Loss: 0.5938723683357239, Test_Loss: 0.7636229991912842 *\n",
      "Epoch: 7, Train_Loss: 0.6087061762809753, Test_Loss: 0.9765807390213013\n",
      "Epoch: 7, Train_Loss: 0.6099522113800049, Test_Loss: 0.7126179933547974 *\n",
      "Epoch: 7, Train_Loss: 0.6834316253662109, Test_Loss: 0.7968428134918213\n",
      "Epoch: 7, Train_Loss: 0.600162923336029, Test_Loss: 0.7881152033805847 *\n",
      "Epoch: 7, Train_Loss: 0.6883276700973511, Test_Loss: 0.6928009986877441 *\n",
      "Epoch: 7, Train_Loss: 8.382124900817871, Test_Loss: 0.7703298330307007\n",
      "Epoch: 7, Train_Loss: 0.6471920609474182, Test_Loss: 0.7167853116989136 *\n",
      "Epoch: 7, Train_Loss: 0.7309936285018921, Test_Loss: 0.87801593542099\n",
      "Epoch: 7, Train_Loss: 0.8375390768051147, Test_Loss: 0.701987087726593 *\n",
      "Epoch: 7, Train_Loss: 0.8486881256103516, Test_Loss: 0.7561575174331665\n",
      "Epoch: 7, Train_Loss: 0.667926013469696, Test_Loss: 0.8053779006004333\n",
      "Epoch: 7, Train_Loss: 0.6692501902580261, Test_Loss: 0.9239289164543152\n",
      "Epoch: 7, Train_Loss: 0.7311674356460571, Test_Loss: 0.816092312335968 *\n",
      "Epoch: 7, Train_Loss: 0.8186304569244385, Test_Loss: 0.7140698432922363 *\n",
      "Epoch: 7, Train_Loss: 0.7317094802856445, Test_Loss: 0.6397197246551514 *\n",
      "Epoch: 7, Train_Loss: 0.6735713481903076, Test_Loss: 0.653144896030426\n",
      "Epoch: 7, Train_Loss: 0.5912448167800903, Test_Loss: 0.6341352462768555 *\n",
      "Epoch: 7, Train_Loss: 0.7306641340255737, Test_Loss: 0.6013822555541992 *\n",
      "Epoch: 7, Train_Loss: 0.6773496270179749, Test_Loss: 0.7045177221298218\n",
      "Epoch: 7, Train_Loss: 0.7521918416023254, Test_Loss: 0.6159316897392273 *\n",
      "Epoch: 7, Train_Loss: 0.6829997301101685, Test_Loss: 5.694113731384277\n",
      "Epoch: 7, Train_Loss: 0.678486168384552, Test_Loss: 1.0953855514526367 *\n",
      "Epoch: 7, Train_Loss: 0.6044957041740417, Test_Loss: 0.586259126663208 *\n",
      "Epoch: 7, Train_Loss: 0.6223052144050598, Test_Loss: 0.6037084460258484\n",
      "Epoch: 7, Train_Loss: 0.6865657567977905, Test_Loss: 0.6227911114692688\n",
      "Epoch: 7, Train_Loss: 0.6233899593353271, Test_Loss: 0.6358280777931213\n",
      "Epoch: 7, Train_Loss: 0.5852404236793518, Test_Loss: 0.592330276966095 *\n",
      "Epoch: 7, Train_Loss: 0.5835053324699402, Test_Loss: 0.659590482711792\n",
      "Epoch: 7, Train_Loss: 0.5839682221412659, Test_Loss: 0.653264582157135 *\n",
      "Epoch: 7, Train_Loss: 0.9991824626922607, Test_Loss: 0.5843101739883423 *\n",
      "Epoch: 7, Train_Loss: 6.183659553527832, Test_Loss: 0.6307351589202881\n",
      "Epoch: 7, Train_Loss: 0.5867424011230469, Test_Loss: 0.593675971031189 *\n",
      "Epoch: 7, Train_Loss: 0.5890752673149109, Test_Loss: 0.5986224412918091\n",
      "Epoch: 7, Train_Loss: 0.5921587347984314, Test_Loss: 0.5884177088737488 *\n",
      "Epoch: 7, Train_Loss: 0.5911506414413452, Test_Loss: 0.6932233572006226\n",
      "Epoch: 7, Train_Loss: 0.5870023369789124, Test_Loss: 0.6185731887817383 *\n",
      "Epoch: 7, Train_Loss: 0.5839117765426636, Test_Loss: 0.7237199544906616\n",
      "Epoch: 7, Train_Loss: 0.5884344577789307, Test_Loss: 0.6631304621696472 *\n",
      "Epoch: 7, Train_Loss: 0.6213856935501099, Test_Loss: 0.6057419180870056 *\n",
      "Epoch: 7, Train_Loss: 0.5972297191619873, Test_Loss: 0.5880199670791626 *\n",
      "Epoch: 7, Train_Loss: 0.5907703638076782, Test_Loss: 0.5870447158813477 *\n",
      "Epoch: 7, Train_Loss: 0.5869908332824707, Test_Loss: 0.58701491355896 *\n",
      "Epoch: 7, Train_Loss: 0.5839582085609436, Test_Loss: 0.5859045386314392 *\n",
      "Epoch: 7, Train_Loss: 0.6027170419692993, Test_Loss: 0.5864916443824768\n",
      "Epoch: 7, Train_Loss: 0.5834113359451294, Test_Loss: 0.5866475701332092\n",
      "Epoch: 7, Train_Loss: 0.5826321244239807, Test_Loss: 0.5831884741783142 *\n",
      "Epoch: 7, Train_Loss: 0.6171906590461731, Test_Loss: 0.5926470756530762\n",
      "Epoch: 7, Train_Loss: 0.6324830651283264, Test_Loss: 0.5945573449134827\n",
      "Epoch: 7, Train_Loss: 0.597261369228363, Test_Loss: 0.5854702591896057 *\n",
      "Epoch: 7, Train_Loss: 0.5800977349281311, Test_Loss: 0.5884358286857605\n",
      "Epoch: 7, Train_Loss: 0.5815046429634094, Test_Loss: 0.6220225095748901\n",
      "Epoch: 7, Train_Loss: 0.6764785051345825, Test_Loss: 0.5981746315956116 *\n",
      "Epoch: 7, Train_Loss: 0.6628251075744629, Test_Loss: 0.782812774181366\n",
      "Epoch: 7, Train_Loss: 0.6685742735862732, Test_Loss: 1.061513900756836\n",
      "Epoch: 7, Train_Loss: 0.6132351160049438, Test_Loss: 0.8313623666763306 *\n",
      "Epoch: 7, Train_Loss: 0.6281917691230774, Test_Loss: 0.6528949737548828 *\n",
      "Epoch: 7, Train_Loss: 0.643052875995636, Test_Loss: 0.5976204872131348 *\n",
      "Epoch: 7, Train_Loss: 0.655312180519104, Test_Loss: 0.5896414518356323 *\n",
      "Epoch: 7, Train_Loss: 0.6025776863098145, Test_Loss: 0.6912209987640381\n",
      "Epoch: 7, Train_Loss: 0.7213553190231323, Test_Loss: 1.2386775016784668\n",
      "Epoch: 7, Train_Loss: 0.5929139256477356, Test_Loss: 1.836240530014038\n",
      "Epoch: 7, Train_Loss: 0.5877723097801208, Test_Loss: 0.853998064994812 *\n",
      "Epoch: 7, Train_Loss: 0.5834590792655945, Test_Loss: 0.6499987244606018 *\n",
      "Epoch: 7, Train_Loss: 0.5815718173980713, Test_Loss: 0.5773177146911621 *\n",
      "Epoch: 7, Train_Loss: 0.5822041034698486, Test_Loss: 0.5851604342460632\n",
      "Epoch: 7, Train_Loss: 0.5832217335700989, Test_Loss: 0.5803865790367126 *\n",
      "Epoch: 7, Train_Loss: 0.9665101766586304, Test_Loss: 0.5903266072273254\n",
      "Epoch: 7, Train_Loss: 5.019552707672119, Test_Loss: 0.6135909557342529\n",
      "Epoch: 7, Train_Loss: 0.5929012298583984, Test_Loss: 0.6229208111763\n",
      "Epoch: 7, Train_Loss: 0.5880588889122009, Test_Loss: 0.5816903710365295 *\n",
      "Epoch: 7, Train_Loss: 0.5911869406700134, Test_Loss: 0.6744710803031921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.5728720426559448, Test_Loss: 0.9420011639595032\n",
      "Epoch: 7, Train_Loss: 0.572273850440979, Test_Loss: 0.6722152829170227 *\n",
      "Epoch: 7, Train_Loss: 0.5734373331069946, Test_Loss: 0.7567694187164307\n",
      "Epoch: 7, Train_Loss: 0.5723854303359985, Test_Loss: 0.5809619426727295 *\n",
      "Epoch: 7, Train_Loss: 0.5716885924339294, Test_Loss: 0.5808966755867004 *\n",
      "Epoch: 7, Train_Loss: 0.5716155171394348, Test_Loss: 0.5808352828025818 *\n",
      "Epoch: 7, Train_Loss: 0.6567516326904297, Test_Loss: 0.5810575485229492\n",
      "Epoch: 7, Train_Loss: 0.6514115333557129, Test_Loss: 0.5945801138877869\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 0.6757055521011353, Test_Loss: 4.7880024909973145\n",
      "Epoch: 7, Train_Loss: 0.6327775120735168, Test_Loss: 1.810287356376648 *\n",
      "Epoch: 7, Train_Loss: 0.5708450078964233, Test_Loss: 0.5796394944190979 *\n",
      "Epoch: 7, Train_Loss: 0.7038955092430115, Test_Loss: 0.5724696516990662 *\n",
      "Epoch: 7, Train_Loss: 0.7734220623970032, Test_Loss: 0.5729222893714905\n",
      "Epoch: 7, Train_Loss: 0.7687100768089294, Test_Loss: 0.5810379981994629\n",
      "Epoch: 7, Train_Loss: 0.7122600078582764, Test_Loss: 0.5740635395050049 *\n",
      "Epoch: 7, Train_Loss: 0.5713725686073303, Test_Loss: 0.5780045986175537\n",
      "Epoch: 7, Train_Loss: 0.5687416791915894, Test_Loss: 0.5719774961471558 *\n",
      "Epoch: 7, Train_Loss: 0.5697318911552429, Test_Loss: 0.5733942985534668\n",
      "Epoch: 7, Train_Loss: 0.5772960782051086, Test_Loss: 0.5736002922058105\n",
      "Epoch: 7, Train_Loss: 0.5798799991607666, Test_Loss: 0.5758818984031677\n",
      "Epoch: 7, Train_Loss: 0.5747369527816772, Test_Loss: 0.5774390697479248\n",
      "Epoch: 7, Train_Loss: 0.5702043771743774, Test_Loss: 0.5899443626403809\n",
      "Epoch: 7, Train_Loss: 0.5682220458984375, Test_Loss: 0.5784117579460144 *\n",
      "Epoch: 7, Train_Loss: 0.5724283456802368, Test_Loss: 0.5734214186668396 *\n",
      "Epoch: 7, Train_Loss: 0.587959885597229, Test_Loss: 0.5682130455970764 *\n",
      "Epoch: 7, Train_Loss: 0.7391824722290039, Test_Loss: 0.5733574628829956\n",
      "Epoch: 7, Train_Loss: 0.7495162487030029, Test_Loss: 0.5713390707969666 *\n",
      "Epoch: 7, Train_Loss: 0.7437443137168884, Test_Loss: 0.5725606083869934\n",
      "Epoch: 7, Train_Loss: 0.626123309135437, Test_Loss: 0.5697611570358276 *\n",
      "Epoch: 7, Train_Loss: 0.7088243961334229, Test_Loss: 0.5676385760307312 *\n",
      "Epoch: 7, Train_Loss: 0.6752364635467529, Test_Loss: 0.5771178603172302\n",
      "Epoch: 7, Train_Loss: 0.6702638864517212, Test_Loss: 0.5849832892417908\n",
      "Epoch: 7, Train_Loss: 0.7199753522872925, Test_Loss: 0.5777648091316223 *\n",
      "Epoch: 7, Train_Loss: 0.8936721682548523, Test_Loss: 0.5739145278930664 *\n",
      "Epoch: 7, Train_Loss: 0.5761640071868896, Test_Loss: 0.5684637427330017 *\n",
      "Epoch: 7, Train_Loss: 0.5737849473953247, Test_Loss: 0.5705054998397827\n",
      "Epoch: 7, Train_Loss: 3.2102811336517334, Test_Loss: 0.5687515735626221 *\n",
      "Epoch: 7, Train_Loss: 1.134157657623291, Test_Loss: 0.5683632493019104 *\n",
      "Epoch: 7, Train_Loss: 0.5973941087722778, Test_Loss: 0.6219269633293152\n",
      "Epoch: 7, Train_Loss: 0.6087558269500732, Test_Loss: 0.6605909466743469\n",
      "Epoch: 7, Train_Loss: 0.6027848720550537, Test_Loss: 5.901942253112793\n",
      "Epoch: 7, Train_Loss: 0.5906184911727905, Test_Loss: 0.5930644273757935 *\n",
      "Epoch: 7, Train_Loss: 0.566032886505127, Test_Loss: 0.5635434985160828 *\n",
      "Epoch: 7, Train_Loss: 0.6087271571159363, Test_Loss: 0.5945228934288025\n",
      "Epoch: 7, Train_Loss: 0.7013823986053467, Test_Loss: 0.614626944065094\n",
      "Epoch: 7, Train_Loss: 0.6706187725067139, Test_Loss: 0.6230791211128235\n",
      "Epoch: 7, Train_Loss: 0.6395970582962036, Test_Loss: 0.5667071342468262 *\n",
      "Epoch: 7, Train_Loss: 0.6489081978797913, Test_Loss: 0.6487147808074951\n",
      "Epoch: 7, Train_Loss: 0.5925890803337097, Test_Loss: 0.6042519211769104 *\n",
      "Epoch: 7, Train_Loss: 0.5913383960723877, Test_Loss: 0.5646647214889526 *\n",
      "Epoch: 7, Train_Loss: 0.5698147416114807, Test_Loss: 0.5926281213760376\n",
      "Epoch: 7, Train_Loss: 0.6136426329612732, Test_Loss: 0.5865493416786194 *\n",
      "Epoch: 7, Train_Loss: 0.5940366983413696, Test_Loss: 0.5656330585479736 *\n",
      "Epoch: 7, Train_Loss: 0.5625720024108887, Test_Loss: 0.6023009419441223\n",
      "Epoch: 7, Train_Loss: 0.5691510438919067, Test_Loss: 0.6862946152687073\n",
      "Epoch: 7, Train_Loss: 0.6035497784614563, Test_Loss: 0.6011073589324951 *\n",
      "Epoch: 7, Train_Loss: 0.6051585674285889, Test_Loss: 0.6528621912002563\n",
      "Epoch: 7, Train_Loss: 0.563072144985199, Test_Loss: 0.5996678471565247 *\n",
      "Epoch: 7, Train_Loss: 0.558849573135376, Test_Loss: 0.6118313074111938\n",
      "Epoch: 7, Train_Loss: 0.5595385432243347, Test_Loss: 0.5763620734214783 *\n",
      "Epoch: 7, Train_Loss: 0.5586614608764648, Test_Loss: 0.5711054801940918 *\n",
      "Epoch: 7, Train_Loss: 0.5592751502990723, Test_Loss: 0.5754050612449646\n",
      "Epoch: 7, Train_Loss: 0.5594867467880249, Test_Loss: 0.5752807855606079 *\n",
      "Epoch: 7, Train_Loss: 0.5594664812088013, Test_Loss: 0.5742536187171936 *\n",
      "Epoch: 7, Train_Loss: 0.5592993497848511, Test_Loss: 0.5748513340950012\n",
      "Epoch: 7, Train_Loss: 0.5593565106391907, Test_Loss: 0.5724878907203674 *\n",
      "Epoch: 7, Train_Loss: 0.5584793090820312, Test_Loss: 0.5778366923332214\n",
      "Epoch: 7, Train_Loss: 0.5603363513946533, Test_Loss: 0.5920026898384094\n",
      "Epoch: 7, Train_Loss: 0.5772566795349121, Test_Loss: 0.5632455348968506 *\n",
      "Epoch: 7, Train_Loss: 0.5750597715377808, Test_Loss: 0.5744420886039734\n",
      "Epoch: 7, Train_Loss: 0.5730781555175781, Test_Loss: 0.6374106407165527\n",
      "Epoch: 7, Train_Loss: 0.5787916779518127, Test_Loss: 0.5782756209373474 *\n",
      "Epoch: 7, Train_Loss: 0.5684316754341125, Test_Loss: 0.8845887780189514\n",
      "Epoch: 7, Train_Loss: 0.5558004379272461, Test_Loss: 1.138634204864502\n",
      "Epoch: 7, Train_Loss: 0.5563535690307617, Test_Loss: 0.8093757629394531 *\n",
      "Epoch: 7, Train_Loss: 0.5683292150497437, Test_Loss: 0.6279008984565735 *\n",
      "Epoch: 7, Train_Loss: 0.5828936100006104, Test_Loss: 0.5921141505241394 *\n",
      "Epoch: 7, Train_Loss: 0.5561257004737854, Test_Loss: 0.5632920861244202 *\n",
      "Epoch: 7, Train_Loss: 0.557589054107666, Test_Loss: 0.6508349180221558\n",
      "Epoch: 7, Train_Loss: 0.5548028945922852, Test_Loss: 1.2152416706085205\n",
      "Epoch: 7, Train_Loss: 0.5900407433509827, Test_Loss: 1.601524829864502\n",
      "Epoch: 7, Train_Loss: 0.6079335808753967, Test_Loss: 0.6579331755638123 *\n",
      "Epoch: 7, Train_Loss: 0.6107697486877441, Test_Loss: 0.654719352722168 *\n",
      "Epoch: 7, Train_Loss: 0.5744694471359253, Test_Loss: 0.5545989871025085 *\n",
      "Epoch: 7, Train_Loss: 0.5527988076210022, Test_Loss: 0.5584469437599182\n",
      "Epoch: 7, Train_Loss: 0.616517961025238, Test_Loss: 0.5598747730255127\n",
      "Epoch: 7, Train_Loss: 0.5626782774925232, Test_Loss: 0.5645631551742554\n",
      "Epoch: 7, Train_Loss: 0.5593797564506531, Test_Loss: 0.5851519107818604\n",
      "Epoch: 7, Train_Loss: 0.5799979567527771, Test_Loss: 0.5846076607704163 *\n",
      "Epoch: 7, Train_Loss: 0.5657839775085449, Test_Loss: 0.5573939085006714 *\n",
      "Epoch: 7, Train_Loss: 0.6857297420501709, Test_Loss: 0.6695013046264648\n",
      "Epoch: 7, Train_Loss: 0.6371088027954102, Test_Loss: 0.9633097648620605\n",
      "Epoch: 7, Train_Loss: 0.5897658467292786, Test_Loss: 0.6620901226997375 *\n",
      "Epoch: 7, Train_Loss: 0.5635193586349487, Test_Loss: 0.6989951133728027\n",
      "Epoch: 7, Train_Loss: 0.5651496052742004, Test_Loss: 0.5616046786308289 *\n",
      "Epoch: 7, Train_Loss: 0.5642916560173035, Test_Loss: 0.561373770236969 *\n",
      "Epoch: 7, Train_Loss: 0.5530611276626587, Test_Loss: 0.5612212419509888 *\n",
      "Epoch: 7, Train_Loss: 0.5608463883399963, Test_Loss: 0.5609506964683533 *\n",
      "Epoch: 7, Train_Loss: 0.5675511360168457, Test_Loss: 0.5802739858627319\n",
      "Epoch: 7, Train_Loss: 0.5793530941009521, Test_Loss: 5.658829212188721\n",
      "Epoch: 7, Train_Loss: 0.644167423248291, Test_Loss: 0.865415096282959 *\n",
      "Epoch: 7, Train_Loss: 0.5602712035179138, Test_Loss: 0.5585753321647644 *\n",
      "Epoch: 7, Train_Loss: 0.6116690635681152, Test_Loss: 0.5508288741111755 *\n",
      "Epoch: 7, Train_Loss: 0.570101797580719, Test_Loss: 0.5541667342185974\n",
      "Epoch: 7, Train_Loss: 0.5729349851608276, Test_Loss: 0.558108925819397\n",
      "Epoch: 7, Train_Loss: 0.6477355360984802, Test_Loss: 0.5501150488853455 *\n",
      "Epoch: 7, Train_Loss: 0.7799896001815796, Test_Loss: 0.5543447136878967\n",
      "Epoch: 7, Train_Loss: 0.5579047203063965, Test_Loss: 0.5495104789733887 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 0.5841180682182312, Test_Loss: 0.5494092106819153 *\n",
      "Epoch: 7, Train_Loss: 0.5474784970283508, Test_Loss: 0.5510146617889404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.5473079681396484, Test_Loss: 0.5599958300590515\n",
      "Epoch: 7, Train_Loss: 0.5475426316261292, Test_Loss: 0.5555049180984497 *\n",
      "Epoch: 7, Train_Loss: 0.5462968349456787, Test_Loss: 0.5733460187911987\n",
      "Epoch: 7, Train_Loss: 0.5599943995475769, Test_Loss: 0.5613866448402405 *\n",
      "Epoch: 7, Train_Loss: 0.5630356073379517, Test_Loss: 0.5479448437690735 *\n",
      "Epoch: 7, Train_Loss: 0.5580891966819763, Test_Loss: 0.5462228059768677 *\n",
      "Epoch: 7, Train_Loss: 0.556476891040802, Test_Loss: 0.5482004284858704\n",
      "Epoch: 7, Train_Loss: 0.5647603273391724, Test_Loss: 0.5496311187744141\n",
      "Epoch: 7, Train_Loss: 0.551624596118927, Test_Loss: 0.5460102558135986 *\n",
      "Epoch: 7, Train_Loss: 0.5468164682388306, Test_Loss: 0.5466620326042175\n",
      "Epoch: 7, Train_Loss: 0.5441571474075317, Test_Loss: 0.5447668433189392 *\n",
      "Epoch: 7, Train_Loss: 0.5689294338226318, Test_Loss: 0.5520615577697754\n",
      "Epoch: 7, Train_Loss: 0.5738315582275391, Test_Loss: 0.5523568391799927\n",
      "Epoch: 7, Train_Loss: 0.5695139765739441, Test_Loss: 0.549133837223053 *\n",
      "Epoch: 7, Train_Loss: 0.5472453832626343, Test_Loss: 0.5469012260437012 *\n",
      "Epoch: 7, Train_Loss: 0.6022154092788696, Test_Loss: 0.5457315444946289 *\n",
      "Epoch: 7, Train_Loss: 0.5880140662193298, Test_Loss: 0.5447725653648376 *\n",
      "Epoch: 7, Train_Loss: 0.5702189803123474, Test_Loss: 0.5450159907341003\n",
      "Epoch: 7, Train_Loss: 0.5526490807533264, Test_Loss: 0.5484845042228699\n",
      "Epoch: 7, Train_Loss: 0.5741267800331116, Test_Loss: 0.604799747467041\n",
      "Epoch: 7, Train_Loss: 0.544564962387085, Test_Loss: 1.4733631610870361\n",
      "Epoch: 7, Train_Loss: 0.5577525496482849, Test_Loss: 5.0655598640441895\n",
      "Epoch: 7, Train_Loss: 0.5608230233192444, Test_Loss: 0.5485690832138062 *\n",
      "Epoch: 7, Train_Loss: 0.5781808495521545, Test_Loss: 0.5418710112571716 *\n",
      "Epoch: 7, Train_Loss: 2.3115463256835938, Test_Loss: 0.5837693214416504\n",
      "Epoch: 7, Train_Loss: 4.2215704917907715, Test_Loss: 0.5821247100830078 *\n",
      "Epoch: 7, Train_Loss: 0.5768129229545593, Test_Loss: 0.5958285927772522\n",
      "Epoch: 7, Train_Loss: 0.5561263561248779, Test_Loss: 0.5537360906600952 *\n",
      "Epoch: 7, Train_Loss: 0.5606313943862915, Test_Loss: 0.6537188291549683\n",
      "Epoch: 7, Train_Loss: 0.724214494228363, Test_Loss: 0.563281238079071 *\n",
      "Epoch: 7, Train_Loss: 0.5812445282936096, Test_Loss: 0.545534074306488 *\n",
      "Epoch: 7, Train_Loss: 0.5493766069412231, Test_Loss: 0.5767407417297363\n",
      "Epoch: 7, Train_Loss: 0.5396491885185242, Test_Loss: 0.5567442178726196 *\n",
      "Epoch: 7, Train_Loss: 0.6070809960365295, Test_Loss: 0.5478965640068054 *\n",
      "Epoch: 7, Train_Loss: 0.5531160235404968, Test_Loss: 0.5969763398170471\n",
      "Epoch: 7, Train_Loss: 0.5563229322433472, Test_Loss: 0.6254928708076477\n",
      "Epoch: 7, Train_Loss: 1.05611252784729, Test_Loss: 0.5990976095199585 *\n",
      "Epoch: 7, Train_Loss: 1.8596689701080322, Test_Loss: 0.6311526894569397\n",
      "Epoch: 7, Train_Loss: 1.4406986236572266, Test_Loss: 0.5670087337493896 *\n",
      "Epoch: 7, Train_Loss: 0.6535599231719971, Test_Loss: 0.585673451423645\n",
      "Epoch: 7, Train_Loss: 0.864760160446167, Test_Loss: 0.5485921502113342 *\n",
      "Epoch: 7, Train_Loss: 2.8722167015075684, Test_Loss: 0.5428088903427124 *\n",
      "Epoch: 7, Train_Loss: 1.2112786769866943, Test_Loss: 0.5488592982292175\n",
      "Epoch: 7, Train_Loss: 0.5751422643661499, Test_Loss: 0.5482706427574158 *\n",
      "Epoch: 7, Train_Loss: 0.5520325899124146, Test_Loss: 0.5461306571960449 *\n",
      "Epoch: 7, Train_Loss: 1.39304518699646, Test_Loss: 0.5470145344734192\n",
      "Epoch: 7, Train_Loss: 1.779179334640503, Test_Loss: 0.541304349899292 *\n",
      "Epoch: 7, Train_Loss: 0.9869803190231323, Test_Loss: 0.537478506565094 *\n",
      "Epoch: 7, Train_Loss: 0.5388948917388916, Test_Loss: 0.5445589423179626\n",
      "Epoch: 7, Train_Loss: 0.5460621118545532, Test_Loss: 0.550653338432312\n",
      "Epoch: 7, Train_Loss: 1.033900260925293, Test_Loss: 0.5393068194389343 *\n",
      "Epoch: 7, Train_Loss: 0.8349930047988892, Test_Loss: 0.5643665790557861\n",
      "Epoch: 7, Train_Loss: 0.5591506958007812, Test_Loss: 0.5545856952667236 *\n",
      "Epoch: 7, Train_Loss: 0.5826014280319214, Test_Loss: 0.862005352973938\n",
      "Epoch: 7, Train_Loss: 0.6350123882293701, Test_Loss: 0.9675346612930298\n",
      "Epoch: 7, Train_Loss: 0.6446704864501953, Test_Loss: 0.6950797438621521 *\n",
      "Epoch: 7, Train_Loss: 0.6214195489883423, Test_Loss: 0.5538932681083679 *\n",
      "Epoch: 7, Train_Loss: 0.941208004951477, Test_Loss: 0.5550732612609863\n",
      "Epoch: 7, Train_Loss: 0.6462418437004089, Test_Loss: 0.5651211738586426\n",
      "Epoch: 7, Train_Loss: 0.6071640253067017, Test_Loss: 0.6912263631820679\n",
      "Epoch: 7, Train_Loss: 0.7026172876358032, Test_Loss: 1.0499393939971924\n",
      "Epoch: 7, Train_Loss: 0.7768059968948364, Test_Loss: 1.235945224761963\n",
      "Epoch: 7, Train_Loss: 0.8183908462524414, Test_Loss: 0.5977994799613953 *\n",
      "Epoch: 7, Train_Loss: 0.6884368658065796, Test_Loss: 0.5750855207443237 *\n",
      "Epoch: 7, Train_Loss: 0.6890084743499756, Test_Loss: 0.5426622629165649 *\n",
      "Epoch: 7, Train_Loss: 0.7461674213409424, Test_Loss: 0.5414936542510986 *\n",
      "Epoch: 7, Train_Loss: 0.6072320342063904, Test_Loss: 0.552314043045044\n",
      "Epoch: 7, Train_Loss: 0.5463299751281738, Test_Loss: 0.5728014707565308\n",
      "Epoch: 7, Train_Loss: 0.5349196195602417, Test_Loss: 0.5728197693824768\n",
      "Epoch: 7, Train_Loss: 0.5449826717376709, Test_Loss: 0.5487469434738159 *\n",
      "Epoch: 7, Train_Loss: 0.5455742478370667, Test_Loss: 0.5609610676765442\n",
      "Epoch: 7, Train_Loss: 0.541969895362854, Test_Loss: 0.6174854636192322\n",
      "Epoch: 7, Train_Loss: 0.5662669539451599, Test_Loss: 0.9131283760070801\n",
      "Epoch: 7, Train_Loss: 0.5688529014587402, Test_Loss: 0.9146261215209961\n",
      "Epoch: 7, Train_Loss: 0.5682005286216736, Test_Loss: 0.6018784046173096 *\n",
      "Epoch: 7, Train_Loss: 0.6269320249557495, Test_Loss: 0.534629225730896 *\n",
      "Epoch: 7, Train_Loss: 0.7892900705337524, Test_Loss: 0.5342458486557007 *\n",
      "Epoch: 7, Train_Loss: 0.5902612805366516, Test_Loss: 0.5364917516708374\n",
      "Epoch: 7, Train_Loss: 0.5773719549179077, Test_Loss: 0.5562995076179504\n",
      "Epoch: 7, Train_Loss: 0.5956350564956665, Test_Loss: 0.7515226602554321\n",
      "Epoch: 7, Train_Loss: 0.8668598532676697, Test_Loss: 5.839864730834961\n",
      "Epoch: 7, Train_Loss: 0.7329878807067871, Test_Loss: 0.63910311460495 *\n",
      "Epoch: 7, Train_Loss: 0.5463022589683533, Test_Loss: 0.5474140048027039 *\n",
      "Epoch: 7, Train_Loss: 0.5496435761451721, Test_Loss: 0.5428657531738281 *\n",
      "Epoch: 7, Train_Loss: 0.918755292892456, Test_Loss: 0.5353971123695374 *\n",
      "Epoch: 7, Train_Loss: 0.8908265829086304, Test_Loss: 0.5407989621162415\n",
      "Epoch: 7, Train_Loss: 0.6201005578041077, Test_Loss: 0.5553175210952759\n",
      "Epoch: 7, Train_Loss: 0.5595313310623169, Test_Loss: 0.5852288603782654\n",
      "Epoch: 7, Train_Loss: 0.5665225386619568, Test_Loss: 0.5358614921569824 *\n",
      "Epoch: 7, Train_Loss: 0.9774419069290161, Test_Loss: 0.5546690821647644\n",
      "Epoch: 7, Train_Loss: 1.1734645366668701, Test_Loss: 0.5604988932609558\n",
      "Epoch: 7, Train_Loss: 0.6424139738082886, Test_Loss: 0.6197817325592041\n",
      "Epoch: 7, Train_Loss: 0.5590232610702515, Test_Loss: 0.5391679406166077 *\n",
      "Epoch: 7, Train_Loss: 0.5351166725158691, Test_Loss: 0.5354486107826233 *\n",
      "Epoch: 7, Train_Loss: 0.5325144529342651, Test_Loss: 0.5586427450180054\n",
      "Epoch: 7, Train_Loss: 0.917494535446167, Test_Loss: 0.5492018461227417 *\n",
      "Epoch: 7, Train_Loss: 0.5656589269638062, Test_Loss: 0.554499626159668\n",
      "Epoch: 7, Train_Loss: 0.5839542150497437, Test_Loss: 0.554131031036377 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 0.6835134029388428, Test_Loss: 0.5984402894973755\n",
      "Epoch: 7, Train_Loss: 0.5653753876686096, Test_Loss: 0.5474421381950378 *\n",
      "Epoch: 7, Train_Loss: 0.5411102175712585, Test_Loss: 0.5524841547012329\n",
      "Epoch: 7, Train_Loss: 0.5875834226608276, Test_Loss: 0.5405444502830505 *\n",
      "Epoch: 7, Train_Loss: 0.6533653736114502, Test_Loss: 0.6315170526504517\n",
      "Epoch: 7, Train_Loss: 0.5612581372261047, Test_Loss: 0.6180403232574463 *\n",
      "Epoch: 7, Train_Loss: 0.6108003854751587, Test_Loss: 0.5929207801818848 *\n",
      "Epoch: 7, Train_Loss: 0.5413463115692139, Test_Loss: 0.5610285401344299 *\n",
      "Epoch: 7, Train_Loss: 0.7118239998817444, Test_Loss: 0.5549818873405457 *\n",
      "Epoch: 7, Train_Loss: 0.5928562879562378, Test_Loss: 0.5604550838470459\n",
      "Epoch: 7, Train_Loss: 0.5739704966545105, Test_Loss: 0.5481684803962708 *\n",
      "Epoch: 7, Train_Loss: 0.5364131927490234, Test_Loss: 0.5661705732345581\n",
      "Epoch: 7, Train_Loss: 0.5512281060218811, Test_Loss: 0.6475934982299805\n",
      "Epoch: 7, Train_Loss: 0.8311153650283813, Test_Loss: 2.693035125732422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 0.8294771313667297, Test_Loss: 3.5796611309051514\n",
      "Epoch: 7, Train_Loss: 0.9355980157852173, Test_Loss: 0.5718790888786316 *\n",
      "Epoch: 7, Train_Loss: 0.9811805486679077, Test_Loss: 0.5848191380500793\n",
      "Epoch: 7, Train_Loss: 0.7730211019515991, Test_Loss: 0.5511387586593628 *\n",
      "Epoch: 7, Train_Loss: 0.6719388961791992, Test_Loss: 0.5362517833709717 *\n",
      "Epoch: 7, Train_Loss: 0.5780060887336731, Test_Loss: 0.5413827300071716\n",
      "Epoch: 7, Train_Loss: 0.5314804315567017, Test_Loss: 0.5735109448432922\n",
      "Epoch: 7, Train_Loss: 0.5305705070495605, Test_Loss: 0.541545569896698 *\n",
      "Epoch: 7, Train_Loss: 0.5574777126312256, Test_Loss: 0.5373719930648804 *\n",
      "Epoch: 7, Train_Loss: 0.7970925569534302, Test_Loss: 0.5655810236930847\n",
      "Epoch: 7, Train_Loss: 0.8689072132110596, Test_Loss: 0.6008815169334412\n",
      "Epoch: 7, Train_Loss: 0.923163890838623, Test_Loss: 0.642360508441925\n",
      "Epoch: 7, Train_Loss: 1.8703043460845947, Test_Loss: 0.5604039430618286 *\n",
      "Epoch: 7, Train_Loss: 0.8388346433639526, Test_Loss: 0.5844236016273499\n",
      "Epoch: 7, Train_Loss: 0.7559460997581482, Test_Loss: 0.5534143447875977 *\n",
      "Epoch: 7, Train_Loss: 0.5518903136253357, Test_Loss: 0.5950062274932861\n",
      "Epoch: 7, Train_Loss: 0.5389341711997986, Test_Loss: 0.5299047827720642 *\n",
      "Epoch: 7, Train_Loss: 0.8757779598236084, Test_Loss: 0.6634947061538696\n",
      "Epoch: 7, Train_Loss: 1.50589919090271, Test_Loss: 0.5607126951217651 *\n",
      "Epoch: 7, Train_Loss: 0.8973425030708313, Test_Loss: 0.5628240704536438\n",
      "Epoch: 7, Train_Loss: 0.6020035743713379, Test_Loss: 0.5784639716148376\n",
      "Epoch: 7, Train_Loss: 0.6015394926071167, Test_Loss: 0.5796921253204346\n",
      "Epoch: 7, Train_Loss: 0.5913866758346558, Test_Loss: 0.5465347170829773 *\n",
      "Epoch: 7, Train_Loss: 0.8382127285003662, Test_Loss: 0.5657885074615479\n",
      "Epoch: 7, Train_Loss: 0.7457298040390015, Test_Loss: 0.5912443995475769\n",
      "Epoch: 7, Train_Loss: 0.7414993047714233, Test_Loss: 0.6095041036605835\n",
      "Epoch: 7, Train_Loss: 0.7156510353088379, Test_Loss: 0.609778881072998\n",
      "Epoch: 7, Train_Loss: 0.5728192329406738, Test_Loss: 0.5524848699569702 *\n",
      "Epoch: 7, Train_Loss: 0.5262272357940674, Test_Loss: 0.6175363063812256\n",
      "Epoch: 7, Train_Loss: 0.5550552606582642, Test_Loss: 0.5592120289802551 *\n",
      "Epoch: 7, Train_Loss: 0.566831111907959, Test_Loss: 0.530319094657898 *\n",
      "Epoch: 7, Train_Loss: 0.569560170173645, Test_Loss: 0.6115056872367859\n",
      "Epoch: 7, Train_Loss: 0.555145263671875, Test_Loss: 0.6531245112419128\n",
      "Epoch: 7, Train_Loss: 7.471283912658691, Test_Loss: 0.6741786003112793\n",
      "Epoch: 7, Train_Loss: 10.152260780334473, Test_Loss: 0.5551173686981201 *\n",
      "Epoch: 7, Train_Loss: 1.1592273712158203, Test_Loss: 0.5296127796173096 *\n",
      "Epoch: 7, Train_Loss: 2.3518519401550293, Test_Loss: 0.5541507601737976\n",
      "Epoch: 7, Train_Loss: 0.8827146291732788, Test_Loss: 0.5883708000183105\n",
      "Epoch: 7, Train_Loss: 0.6569566130638123, Test_Loss: 0.5725899338722229 *\n",
      "Epoch: 7, Train_Loss: 0.7354289293289185, Test_Loss: 0.5925461649894714\n",
      "Epoch: 8, Train_Loss: 9.123085021972656, Test_Loss: 0.7793549299240112 *\n",
      "Epoch: 8, Train_Loss: 2.1967287063598633, Test_Loss: 0.5786932706832886 *\n",
      "Epoch: 8, Train_Loss: 0.6233141422271729, Test_Loss: 0.6249367594718933\n",
      "Epoch: 8, Train_Loss: 3.151622772216797, Test_Loss: 0.6619067788124084\n",
      "Epoch: 8, Train_Loss: 3.507648468017578, Test_Loss: 0.7615062594413757\n",
      "Epoch: 8, Train_Loss: 0.6950476169586182, Test_Loss: 0.8889347314834595\n",
      "Epoch: 8, Train_Loss: 0.541089653968811, Test_Loss: 0.5753982663154602 *\n",
      "Epoch: 8, Train_Loss: 0.5541049838066101, Test_Loss: 0.9579153060913086\n",
      "Epoch: 8, Train_Loss: 0.5462732315063477, Test_Loss: 0.5518412590026855 *\n",
      "Epoch: 8, Train_Loss: 0.5487058758735657, Test_Loss: 0.8038054704666138\n",
      "Epoch: 8, Train_Loss: 0.5269147753715515, Test_Loss: 1.1041555404663086\n",
      "Epoch: 8, Train_Loss: 0.5290104746818542, Test_Loss: 0.7340357303619385 *\n",
      "Epoch: 8, Train_Loss: 0.52037113904953, Test_Loss: 0.6174393892288208 *\n",
      "Epoch: 8, Train_Loss: 0.5447708964347839, Test_Loss: 0.6062546968460083 *\n",
      "Epoch: 8, Train_Loss: 0.5966955423355103, Test_Loss: 0.6685779094696045\n",
      "Epoch: 8, Train_Loss: 0.5485473871231079, Test_Loss: 0.6579482555389404 *\n",
      "Epoch: 8, Train_Loss: 0.6026544570922852, Test_Loss: 0.6309311389923096 *\n",
      "Epoch: 8, Train_Loss: 0.6337065100669861, Test_Loss: 0.5634473562240601 *\n",
      "Epoch: 8, Train_Loss: 0.5857364535331726, Test_Loss: 1.6603949069976807\n",
      "Epoch: 8, Train_Loss: 0.5306222438812256, Test_Loss: 6.4004807472229\n",
      "Epoch: 8, Train_Loss: 0.5276750922203064, Test_Loss: 0.6856015920639038 *\n",
      "Epoch: 8, Train_Loss: 0.5186687111854553, Test_Loss: 0.7483250498771667\n",
      "Epoch: 8, Train_Loss: 0.5119960308074951, Test_Loss: 0.717600405216217 *\n",
      "Epoch: 8, Train_Loss: 0.5128519535064697, Test_Loss: 0.5707836747169495 *\n",
      "Epoch: 8, Train_Loss: 0.5113109946250916, Test_Loss: 0.6477630138397217\n",
      "Epoch: 8, Train_Loss: 0.5122430920600891, Test_Loss: 0.727296769618988\n",
      "Epoch: 8, Train_Loss: 0.5111820697784424, Test_Loss: 0.7045053243637085 *\n",
      "Epoch: 8, Train_Loss: 0.5105006098747253, Test_Loss: 0.5647444725036621 *\n",
      "Epoch: 8, Train_Loss: 0.5101596117019653, Test_Loss: 0.6404826045036316\n",
      "Epoch: 8, Train_Loss: 0.5103570222854614, Test_Loss: 0.6102627515792847 *\n",
      "Epoch: 8, Train_Loss: 0.5199595093727112, Test_Loss: 0.7480086088180542\n",
      "Epoch: 8, Train_Loss: 0.5306195616722107, Test_Loss: 0.5802490711212158 *\n",
      "Epoch: 8, Train_Loss: 0.5627822279930115, Test_Loss: 0.6166740655899048\n",
      "Epoch: 8, Train_Loss: 0.5193963050842285, Test_Loss: 0.6559450030326843\n",
      "Epoch: 8, Train_Loss: 0.5394829511642456, Test_Loss: 0.5756085515022278 *\n",
      "Epoch: 8, Train_Loss: 8.668266296386719, Test_Loss: 0.5833849906921387\n",
      "Epoch: 8, Train_Loss: 1.1430130004882812, Test_Loss: 0.5535368323326111 *\n",
      "Epoch: 8, Train_Loss: 0.5367458462715149, Test_Loss: 0.7186829447746277\n",
      "Epoch: 8, Train_Loss: 0.5953132510185242, Test_Loss: 0.5615544319152832 *\n",
      "Epoch: 8, Train_Loss: 0.6497911810874939, Test_Loss: 0.6379565000534058\n",
      "Epoch: 8, Train_Loss: 0.5360121130943298, Test_Loss: 0.6154646277427673 *\n",
      "Epoch: 8, Train_Loss: 0.5475448369979858, Test_Loss: 0.7678000926971436\n",
      "Epoch: 8, Train_Loss: 0.6321513056755066, Test_Loss: 0.7346985340118408 *\n",
      "Epoch: 8, Train_Loss: 0.7112116813659668, Test_Loss: 0.6259348392486572 *\n",
      "Epoch: 8, Train_Loss: 0.6868850588798523, Test_Loss: 0.5741874575614929 *\n",
      "Epoch: 8, Train_Loss: 0.6342198252677917, Test_Loss: 0.5874706506729126\n",
      "Epoch: 8, Train_Loss: 0.5232104659080505, Test_Loss: 0.5797176957130432 *\n",
      "Epoch: 8, Train_Loss: 0.627845287322998, Test_Loss: 0.5397775173187256 *\n",
      "Epoch: 8, Train_Loss: 0.6337113380432129, Test_Loss: 0.622973620891571\n",
      "Epoch: 8, Train_Loss: 0.7368912696838379, Test_Loss: 0.5711508393287659 *\n",
      "Epoch: 8, Train_Loss: 0.648190975189209, Test_Loss: 3.8201544284820557\n",
      "Epoch: 8, Train_Loss: 0.6025211811065674, Test_Loss: 2.8794727325439453 *\n",
      "Epoch: 8, Train_Loss: 0.5443036556243896, Test_Loss: 0.5107033848762512 *\n",
      "Epoch: 8, Train_Loss: 0.523637592792511, Test_Loss: 0.5092687606811523 *\n",
      "Epoch: 8, Train_Loss: 0.5798410177230835, Test_Loss: 0.5338549613952637\n",
      "Epoch: 8, Train_Loss: 0.5350016355514526, Test_Loss: 0.5153999924659729 *\n",
      "Epoch: 8, Train_Loss: 0.5121183395385742, Test_Loss: 0.5295268297195435\n",
      "Epoch: 8, Train_Loss: 0.5073357224464417, Test_Loss: 0.5731154084205627\n",
      "Epoch: 8, Train_Loss: 0.5104936361312866, Test_Loss: 0.6130162477493286\n",
      "Epoch: 8, Train_Loss: 0.5641975998878479, Test_Loss: 0.5071750283241272 *\n",
      "Epoch: 8, Train_Loss: 6.06016731262207, Test_Loss: 0.5375657081604004\n",
      "Epoch: 8, Train_Loss: 0.5433724522590637, Test_Loss: 0.5246227979660034 *\n",
      "Epoch: 8, Train_Loss: 0.5089688301086426, Test_Loss: 0.5198323726654053 *\n",
      "Epoch: 8, Train_Loss: 0.5245023369789124, Test_Loss: 0.5122771263122559 *\n",
      "Epoch: 8, Train_Loss: 0.5163565874099731, Test_Loss: 0.5542332530021667\n",
      "Epoch: 8, Train_Loss: 0.5102775692939758, Test_Loss: 0.5464057922363281 *\n",
      "Epoch: 8, Train_Loss: 0.507297158241272, Test_Loss: 0.6168009638786316\n",
      "Epoch: 8, Train_Loss: 0.512416660785675, Test_Loss: 0.6144472360610962 *\n",
      "Epoch: 8, Train_Loss: 0.5325719714164734, Test_Loss: 0.5235793590545654 *\n",
      "Epoch: 8, Train_Loss: 0.5128685235977173, Test_Loss: 0.5121163129806519 *\n",
      "Epoch: 8, Train_Loss: 0.5402716398239136, Test_Loss: 0.5049954652786255 *\n",
      "Epoch: 8, Train_Loss: 0.5072574615478516, Test_Loss: 0.5061994791030884\n",
      "Epoch: 8, Train_Loss: 0.5055100321769714, Test_Loss: 0.5048257112503052 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.5224503874778748, Test_Loss: 0.505923867225647\n",
      "Epoch: 8, Train_Loss: 0.5046694874763489, Test_Loss: 0.5067383646965027\n",
      "Epoch: 8, Train_Loss: 0.5061396956443787, Test_Loss: 0.5038569569587708 *\n",
      "Epoch: 8, Train_Loss: 0.5256466269493103, Test_Loss: 0.5072401165962219\n",
      "Epoch: 8, Train_Loss: 0.5426822900772095, Test_Loss: 0.5028092265129089 *\n",
      "Epoch: 8, Train_Loss: 0.5244348645210266, Test_Loss: 0.5061706304550171\n",
      "Epoch: 8, Train_Loss: 0.5021631121635437, Test_Loss: 0.5207741856575012\n",
      "Epoch: 8, Train_Loss: 0.5016745328903198, Test_Loss: 0.5075759887695312 *\n",
      "Epoch: 8, Train_Loss: 0.5747413635253906, Test_Loss: 0.522165060043335\n",
      "Epoch: 8, Train_Loss: 0.5651155710220337, Test_Loss: 0.56118243932724\n",
      "Epoch: 8, Train_Loss: 0.5450226068496704, Test_Loss: 0.8485571146011353\n",
      "Epoch: 8, Train_Loss: 0.5188301801681519, Test_Loss: 0.8150230646133423 *\n",
      "Epoch: 8, Train_Loss: 0.5712546110153198, Test_Loss: 0.603990912437439 *\n",
      "Epoch: 8, Train_Loss: 0.5731440782546997, Test_Loss: 0.5083435773849487 *\n",
      "Epoch: 8, Train_Loss: 0.5669651627540588, Test_Loss: 0.5218949913978577\n",
      "Epoch: 8, Train_Loss: 0.5679901838302612, Test_Loss: 0.5564188361167908\n",
      "Epoch: 8, Train_Loss: 0.7349447011947632, Test_Loss: 0.7306144833564758\n",
      "Epoch: 8, Train_Loss: 0.5260918140411377, Test_Loss: 1.2305063009262085\n",
      "Epoch: 8, Train_Loss: 0.5383180379867554, Test_Loss: 0.9665037393569946 *\n",
      "Epoch: 8, Train_Loss: 0.5176712274551392, Test_Loss: 0.5614297389984131 *\n",
      "Epoch: 8, Train_Loss: 0.5047986507415771, Test_Loss: 0.5209998488426208 *\n",
      "Epoch: 8, Train_Loss: 0.5052455067634583, Test_Loss: 0.5116131901741028 *\n",
      "Epoch: 8, Train_Loss: 0.507540762424469, Test_Loss: 0.5086144208908081 *\n",
      "Epoch: 8, Train_Loss: 0.5125669836997986, Test_Loss: 0.5143190026283264\n",
      "Epoch: 8, Train_Loss: 4.950921535491943, Test_Loss: 0.5184248089790344\n",
      "Epoch: 8, Train_Loss: 0.7811163663864136, Test_Loss: 0.5541253685951233\n",
      "Epoch: 8, Train_Loss: 0.5003342032432556, Test_Loss: 0.49959754943847656 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.5049957633018494, Test_Loss: 0.5647221207618713\n",
      "Epoch: 8, Train_Loss: 0.5000994801521301, Test_Loss: 0.6388087868690491\n",
      "Epoch: 8, Train_Loss: 0.5010740160942078, Test_Loss: 0.8620590567588806\n",
      "Epoch: 8, Train_Loss: 0.49749937653541565, Test_Loss: 0.7792149782180786 *\n",
      "Epoch: 8, Train_Loss: 0.4963875710964203, Test_Loss: 0.5225790143013 *\n",
      "Epoch: 8, Train_Loss: 0.4964001476764679, Test_Loss: 0.5140743851661682 *\n",
      "Epoch: 8, Train_Loss: 0.49671030044555664, Test_Loss: 0.5139593482017517 *\n",
      "Epoch: 8, Train_Loss: 0.536435604095459, Test_Loss: 0.5148199200630188\n",
      "Epoch: 8, Train_Loss: 0.5180038213729858, Test_Loss: 0.5441718697547913\n",
      "Epoch: 8, Train_Loss: 0.5528371334075928, Test_Loss: 2.427609443664551\n",
      "Epoch: 8, Train_Loss: 0.5391247272491455, Test_Loss: 3.661504030227661\n",
      "Epoch: 8, Train_Loss: 0.49965110421180725, Test_Loss: 0.5099990963935852 *\n",
      "Epoch: 8, Train_Loss: 0.585475742816925, Test_Loss: 0.49919259548187256 *\n",
      "Epoch: 8, Train_Loss: 0.7218029499053955, Test_Loss: 0.49923568964004517\n",
      "Epoch: 8, Train_Loss: 0.7157361507415771, Test_Loss: 0.5095539093017578\n",
      "Epoch: 8, Train_Loss: 0.7120107412338257, Test_Loss: 0.4991176724433899 *\n",
      "Epoch: 8, Train_Loss: 0.49655744433403015, Test_Loss: 0.5015725493431091\n",
      "Epoch: 8, Train_Loss: 0.4948914647102356, Test_Loss: 0.4993866980075836 *\n",
      "Epoch: 8, Train_Loss: 0.4948524534702301, Test_Loss: 0.4964977204799652 *\n",
      "Epoch: 8, Train_Loss: 0.5050296783447266, Test_Loss: 0.5005232095718384\n",
      "Epoch: 8, Train_Loss: 0.5084154009819031, Test_Loss: 0.49839356541633606 *\n",
      "Epoch: 8, Train_Loss: 0.5036324858665466, Test_Loss: 0.5077999234199524\n",
      "Epoch: 8, Train_Loss: 0.49833598732948303, Test_Loss: 0.5100505948066711\n",
      "Epoch: 8, Train_Loss: 0.49364760518074036, Test_Loss: 0.5050525665283203 *\n",
      "Epoch: 8, Train_Loss: 0.49644699692726135, Test_Loss: 0.5041477084159851 *\n",
      "Epoch: 8, Train_Loss: 0.506243884563446, Test_Loss: 0.493882417678833 *\n",
      "Epoch: 8, Train_Loss: 0.6269228458404541, Test_Loss: 0.4977259635925293\n",
      "Epoch: 8, Train_Loss: 0.6908408403396606, Test_Loss: 0.494308203458786 *\n",
      "Epoch: 8, Train_Loss: 0.6917632818222046, Test_Loss: 0.5014286637306213\n",
      "Epoch: 8, Train_Loss: 0.5404173731803894, Test_Loss: 0.49599123001098633 *\n",
      "Epoch: 8, Train_Loss: 0.6393702030181885, Test_Loss: 0.496145099401474\n",
      "Epoch: 8, Train_Loss: 0.6330320835113525, Test_Loss: 0.49918580055236816\n",
      "Epoch: 8, Train_Loss: 0.5384458899497986, Test_Loss: 0.5120140314102173\n",
      "Epoch: 8, Train_Loss: 0.6697613596916199, Test_Loss: 0.5005530118942261 *\n",
      "Epoch: 8, Train_Loss: 0.6809549331665039, Test_Loss: 0.4991803467273712 *\n",
      "Epoch: 8, Train_Loss: 0.6684297323226929, Test_Loss: 0.49439874291419983 *\n",
      "Epoch: 8, Train_Loss: 0.5049511194229126, Test_Loss: 0.497066468000412\n",
      "Epoch: 8, Train_Loss: 2.278986692428589, Test_Loss: 0.4949548840522766 *\n",
      "Epoch: 8, Train_Loss: 1.901587724685669, Test_Loss: 0.49212732911109924 *\n",
      "Epoch: 8, Train_Loss: 0.5262341499328613, Test_Loss: 0.5371664762496948\n",
      "Epoch: 8, Train_Loss: 0.5390026569366455, Test_Loss: 0.5238905549049377 *\n",
      "Epoch: 8, Train_Loss: 0.5451257824897766, Test_Loss: 4.781426906585693\n",
      "Epoch: 8, Train_Loss: 0.5328432321548462, Test_Loss: 1.6582391262054443 *\n",
      "Epoch: 8, Train_Loss: 0.4906596541404724, Test_Loss: 0.4907347857952118 *\n",
      "Epoch: 8, Train_Loss: 0.5097183585166931, Test_Loss: 0.504385232925415\n",
      "Epoch: 8, Train_Loss: 0.6232504844665527, Test_Loss: 0.5520293116569519\n",
      "Epoch: 8, Train_Loss: 0.5859812498092651, Test_Loss: 0.5556637048721313\n",
      "Epoch: 8, Train_Loss: 0.5826301574707031, Test_Loss: 0.5056371092796326 *\n",
      "Epoch: 8, Train_Loss: 0.581799328327179, Test_Loss: 0.5635802149772644\n",
      "Epoch: 8, Train_Loss: 0.5450104475021362, Test_Loss: 0.5647199153900146\n",
      "Epoch: 8, Train_Loss: 0.5201149582862854, Test_Loss: 0.49226799607276917 *\n",
      "Epoch: 8, Train_Loss: 0.5067593455314636, Test_Loss: 0.5242779850959778\n",
      "Epoch: 8, Train_Loss: 0.5165261626243591, Test_Loss: 0.5071390271186829 *\n",
      "Epoch: 8, Train_Loss: 0.517593502998352, Test_Loss: 0.500124990940094 *\n",
      "Epoch: 8, Train_Loss: 0.4958013594150543, Test_Loss: 0.49275147914886475 *\n",
      "Epoch: 8, Train_Loss: 0.48936185240745544, Test_Loss: 0.6259356737136841\n",
      "Epoch: 8, Train_Loss: 0.5354171395301819, Test_Loss: 0.5358496904373169 *\n",
      "Epoch: 8, Train_Loss: 0.5426440238952637, Test_Loss: 0.5943712592124939\n",
      "Epoch: 8, Train_Loss: 0.5045994520187378, Test_Loss: 0.5561549067497253 *\n",
      "Epoch: 8, Train_Loss: 0.4866028428077698, Test_Loss: 0.5303052663803101 *\n",
      "Epoch: 8, Train_Loss: 0.48664045333862305, Test_Loss: 0.5111531019210815 *\n",
      "Epoch: 8, Train_Loss: 0.48601233959198, Test_Loss: 0.5089895129203796 *\n",
      "Epoch: 8, Train_Loss: 0.48637571930885315, Test_Loss: 0.5095725655555725\n",
      "Epoch: 8, Train_Loss: 0.4861249029636383, Test_Loss: 0.5094305872917175 *\n",
      "Epoch: 8, Train_Loss: 0.48651576042175293, Test_Loss: 0.510994017124176\n",
      "Epoch: 8, Train_Loss: 0.4868755638599396, Test_Loss: 0.5102286338806152 *\n",
      "Epoch: 8, Train_Loss: 0.4862772822380066, Test_Loss: 0.5038769841194153 *\n",
      "Epoch: 8, Train_Loss: 0.48543694615364075, Test_Loss: 0.5188025236129761\n",
      "Epoch: 8, Train_Loss: 0.4868852496147156, Test_Loss: 0.5126641392707825 *\n",
      "Epoch: 8, Train_Loss: 0.497263103723526, Test_Loss: 0.5019421577453613 *\n",
      "Epoch: 8, Train_Loss: 0.4987110495567322, Test_Loss: 0.49696704745292664 *\n",
      "Epoch: 8, Train_Loss: 0.49691060185432434, Test_Loss: 0.5476294755935669\n",
      "Epoch: 8, Train_Loss: 0.5063204169273376, Test_Loss: 0.529022753238678 *\n",
      "Epoch: 8, Train_Loss: 0.49097564816474915, Test_Loss: 0.6918278932571411\n",
      "Epoch: 8, Train_Loss: 0.48743486404418945, Test_Loss: 1.0697835683822632\n",
      "Epoch: 8, Train_Loss: 0.4844430387020111, Test_Loss: 0.8638750314712524 *\n",
      "Epoch: 8, Train_Loss: 0.4885992407798767, Test_Loss: 0.6067601442337036 *\n",
      "Epoch: 8, Train_Loss: 0.5099591612815857, Test_Loss: 0.5263516306877136 *\n",
      "Epoch: 8, Train_Loss: 0.4909925162792206, Test_Loss: 0.49483180046081543 *\n",
      "Epoch: 8, Train_Loss: 0.4858699440956116, Test_Loss: 0.5532636642456055\n",
      "Epoch: 8, Train_Loss: 0.4831005036830902, Test_Loss: 0.9237916469573975\n",
      "Epoch: 8, Train_Loss: 0.49777793884277344, Test_Loss: 1.470322608947754\n",
      "Epoch: 8, Train_Loss: 0.548984169960022, Test_Loss: 0.8123173713684082 *\n",
      "Epoch: 8, Train_Loss: 0.5365515947341919, Test_Loss: 0.5887755155563354 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.5211772322654724, Test_Loss: 0.4889542758464813 *\n",
      "Epoch: 8, Train_Loss: 0.4820652902126312, Test_Loss: 0.4874245822429657 *\n",
      "Epoch: 8, Train_Loss: 0.5365727543830872, Test_Loss: 0.482871413230896 *\n",
      "Epoch: 8, Train_Loss: 0.5046231746673584, Test_Loss: 0.4940340518951416\n",
      "Epoch: 8, Train_Loss: 0.48457762598991394, Test_Loss: 0.5096267461776733\n",
      "Epoch: 8, Train_Loss: 0.5026023983955383, Test_Loss: 0.5256244540214539\n",
      "Epoch: 8, Train_Loss: 0.5071449875831604, Test_Loss: 0.48559942841529846 *\n",
      "Epoch: 8, Train_Loss: 0.5884693264961243, Test_Loss: 0.5888713598251343\n",
      "Epoch: 8, Train_Loss: 0.5690529942512512, Test_Loss: 0.8170449137687683\n",
      "Epoch: 8, Train_Loss: 0.5367165207862854, Test_Loss: 0.6362548470497131 *\n",
      "Epoch: 8, Train_Loss: 0.5005070567131042, Test_Loss: 0.6839487552642822\n",
      "Epoch: 8, Train_Loss: 0.4859342575073242, Test_Loss: 0.49305954575538635 *\n",
      "Epoch: 8, Train_Loss: 0.5024973154067993, Test_Loss: 0.49322834610939026\n",
      "Epoch: 8, Train_Loss: 0.4814647138118744, Test_Loss: 0.49278008937835693 *\n",
      "Epoch: 8, Train_Loss: 0.4859430193901062, Test_Loss: 0.49273496866226196 *\n",
      "Epoch: 8, Train_Loss: 0.4987833797931671, Test_Loss: 0.5026708841323853\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.5030460357666016, Test_Loss: 3.919679641723633\n",
      "Epoch: 8, Train_Loss: 0.5861895084381104, Test_Loss: 2.4575002193450928 *\n",
      "Epoch: 8, Train_Loss: 0.48178765177726746, Test_Loss: 0.4904283285140991 *\n",
      "Epoch: 8, Train_Loss: 0.5495290756225586, Test_Loss: 0.4828006625175476 *\n",
      "Epoch: 8, Train_Loss: 0.48972946405410767, Test_Loss: 0.48239830136299133 *\n",
      "Epoch: 8, Train_Loss: 0.5171534419059753, Test_Loss: 0.49293071031570435\n",
      "Epoch: 8, Train_Loss: 0.5308158993721008, Test_Loss: 0.48220348358154297 *\n",
      "Epoch: 8, Train_Loss: 0.7538317441940308, Test_Loss: 0.4850615859031677\n",
      "Epoch: 8, Train_Loss: 0.49373674392700195, Test_Loss: 0.48012489080429077 *\n",
      "Epoch: 8, Train_Loss: 0.5147601962089539, Test_Loss: 0.4811471104621887\n",
      "Epoch: 8, Train_Loss: 0.47808486223220825, Test_Loss: 0.4826245605945587\n",
      "Epoch: 8, Train_Loss: 0.47827261686325073, Test_Loss: 0.48189279437065125 *\n",
      "Epoch: 8, Train_Loss: 0.4787951409816742, Test_Loss: 0.48897215723991394\n",
      "Epoch: 8, Train_Loss: 0.4772728383541107, Test_Loss: 0.5019234418869019\n",
      "Epoch: 8, Train_Loss: 0.48917514085769653, Test_Loss: 0.49440497159957886 *\n",
      "Epoch: 8, Train_Loss: 0.49133235216140747, Test_Loss: 0.48221588134765625 *\n",
      "Epoch: 8, Train_Loss: 0.49408912658691406, Test_Loss: 0.47715210914611816 *\n",
      "Epoch: 8, Train_Loss: 0.4843508005142212, Test_Loss: 0.4797879755496979\n",
      "Epoch: 8, Train_Loss: 0.49183782935142517, Test_Loss: 0.47833630442619324 *\n",
      "Epoch: 8, Train_Loss: 0.4907979369163513, Test_Loss: 0.479728639125824\n",
      "Epoch: 8, Train_Loss: 0.47891753911972046, Test_Loss: 0.47804883122444153 *\n",
      "Epoch: 8, Train_Loss: 0.47552797198295593, Test_Loss: 0.47636404633522034 *\n",
      "Epoch: 8, Train_Loss: 0.49556905031204224, Test_Loss: 0.4788917005062103\n",
      "Epoch: 8, Train_Loss: 0.5025280714035034, Test_Loss: 0.4875895082950592\n",
      "Epoch: 8, Train_Loss: 0.5099506974220276, Test_Loss: 0.47962743043899536 *\n",
      "Epoch: 8, Train_Loss: 0.4750484228134155, Test_Loss: 0.47933322191238403 *\n",
      "Epoch: 8, Train_Loss: 0.5248932838439941, Test_Loss: 0.47687795758247375 *\n",
      "Epoch: 8, Train_Loss: 0.529712975025177, Test_Loss: 0.4767764210700989 *\n",
      "Epoch: 8, Train_Loss: 0.5127098560333252, Test_Loss: 0.4770117700099945\n",
      "Epoch: 8, Train_Loss: 0.4760170876979828, Test_Loss: 0.4766339957714081 *\n",
      "Epoch: 8, Train_Loss: 0.5120238065719604, Test_Loss: 0.5219616889953613\n",
      "Epoch: 8, Train_Loss: 0.475301057100296, Test_Loss: 0.506425678730011 *\n",
      "Epoch: 8, Train_Loss: 0.4928359389305115, Test_Loss: 5.770107746124268\n",
      "Epoch: 8, Train_Loss: 0.4778514504432678, Test_Loss: 0.6549098491668701 *\n",
      "Epoch: 8, Train_Loss: 0.5003741979598999, Test_Loss: 0.4745180606842041 *\n",
      "Epoch: 8, Train_Loss: 0.8572744131088257, Test_Loss: 0.4991149604320526\n",
      "Epoch: 8, Train_Loss: 4.352890491485596, Test_Loss: 0.5278614163398743\n",
      "Epoch: 8, Train_Loss: 1.715890645980835, Test_Loss: 0.5303004384040833\n",
      "Epoch: 8, Train_Loss: 0.49215543270111084, Test_Loss: 0.4792996048927307 *\n",
      "Epoch: 8, Train_Loss: 0.47646114230155945, Test_Loss: 0.5707499980926514\n",
      "Epoch: 8, Train_Loss: 0.6245324611663818, Test_Loss: 0.5353373885154724 *\n",
      "Epoch: 8, Train_Loss: 0.5586441159248352, Test_Loss: 0.4735930562019348 *\n",
      "Epoch: 8, Train_Loss: 0.486899733543396, Test_Loss: 0.5118847489356995\n",
      "Epoch: 8, Train_Loss: 0.47229617834091187, Test_Loss: 0.48902082443237305 *\n",
      "Epoch: 8, Train_Loss: 0.5292380452156067, Test_Loss: 0.48095399141311646 *\n",
      "Epoch: 8, Train_Loss: 0.49492666125297546, Test_Loss: 0.4913714826107025\n",
      "Epoch: 8, Train_Loss: 0.4877411425113678, Test_Loss: 0.5839350819587708\n",
      "Epoch: 8, Train_Loss: 0.6860052347183228, Test_Loss: 0.5081297755241394 *\n",
      "Epoch: 8, Train_Loss: 1.6664992570877075, Test_Loss: 0.5648967623710632\n",
      "Epoch: 8, Train_Loss: 1.6886194944381714, Test_Loss: 0.5122072696685791 *\n",
      "Epoch: 8, Train_Loss: 0.5645269155502319, Test_Loss: 0.5265694260597229\n",
      "Epoch: 8, Train_Loss: 0.5564960241317749, Test_Loss: 0.48671814799308777 *\n",
      "Epoch: 8, Train_Loss: 2.5836892127990723, Test_Loss: 0.48069339990615845 *\n",
      "Epoch: 8, Train_Loss: 1.5705713033676147, Test_Loss: 0.48438891768455505\n",
      "Epoch: 8, Train_Loss: 0.5112943053245544, Test_Loss: 0.4836576282978058 *\n",
      "Epoch: 8, Train_Loss: 0.49018192291259766, Test_Loss: 0.48367565870285034\n",
      "Epoch: 8, Train_Loss: 0.879249095916748, Test_Loss: 0.4840889871120453\n",
      "Epoch: 8, Train_Loss: 1.4903770685195923, Test_Loss: 0.4727712869644165 *\n",
      "Epoch: 8, Train_Loss: 1.1454284191131592, Test_Loss: 0.4740716218948364\n",
      "Epoch: 8, Train_Loss: 0.48675280809402466, Test_Loss: 0.47336769104003906 *\n",
      "Epoch: 8, Train_Loss: 0.4936617612838745, Test_Loss: 0.48480626940727234\n",
      "Epoch: 8, Train_Loss: 0.7184855937957764, Test_Loss: 0.4845806956291199 *\n",
      "Epoch: 8, Train_Loss: 0.942733645439148, Test_Loss: 0.4843498766422272 *\n",
      "Epoch: 8, Train_Loss: 0.48427560925483704, Test_Loss: 0.4916786849498749\n",
      "Epoch: 8, Train_Loss: 0.5025991797447205, Test_Loss: 0.665350079536438\n",
      "Epoch: 8, Train_Loss: 0.5241767168045044, Test_Loss: 0.8617614507675171\n",
      "Epoch: 8, Train_Loss: 0.5167537927627563, Test_Loss: 0.6499234437942505 *\n",
      "Epoch: 8, Train_Loss: 0.5475606322288513, Test_Loss: 0.5034593343734741 *\n",
      "Epoch: 8, Train_Loss: 0.8121181726455688, Test_Loss: 0.4700387716293335 *\n",
      "Epoch: 8, Train_Loss: 0.6991445422172546, Test_Loss: 0.5000780820846558\n",
      "Epoch: 8, Train_Loss: 0.5740391612052917, Test_Loss: 0.6296441555023193\n",
      "Epoch: 8, Train_Loss: 0.5691637992858887, Test_Loss: 0.8233286142349243\n",
      "Epoch: 8, Train_Loss: 0.6045684814453125, Test_Loss: 0.8714288473129272\n",
      "Epoch: 8, Train_Loss: 0.7290029525756836, Test_Loss: 0.6972203254699707 *\n",
      "Epoch: 8, Train_Loss: 0.6655734181404114, Test_Loss: 0.4972888231277466 *\n",
      "Epoch: 8, Train_Loss: 0.5587255954742432, Test_Loss: 0.4903661906719208 *\n",
      "Epoch: 8, Train_Loss: 0.7135650515556335, Test_Loss: 0.4810640811920166 *\n",
      "Epoch: 8, Train_Loss: 0.5898641347885132, Test_Loss: 0.47945132851600647 *\n",
      "Epoch: 8, Train_Loss: 0.48969483375549316, Test_Loss: 0.5707933306694031\n",
      "Epoch: 8, Train_Loss: 0.4741846024990082, Test_Loss: 0.4972759485244751 *\n",
      "Epoch: 8, Train_Loss: 0.47324666380882263, Test_Loss: 0.5044868588447571\n",
      "Epoch: 8, Train_Loss: 0.47499874234199524, Test_Loss: 0.48384082317352295 *\n",
      "Epoch: 8, Train_Loss: 0.475039005279541, Test_Loss: 0.6036669015884399\n",
      "Epoch: 8, Train_Loss: 0.47956475615501404, Test_Loss: 0.8317759037017822\n",
      "Epoch: 8, Train_Loss: 0.4922342002391815, Test_Loss: 0.7382603287696838 *\n",
      "Epoch: 8, Train_Loss: 0.4735974371433258, Test_Loss: 0.9038868546485901\n",
      "Epoch: 8, Train_Loss: 0.530210554599762, Test_Loss: 0.502630889415741 *\n",
      "Epoch: 8, Train_Loss: 0.5971435308456421, Test_Loss: 0.49734601378440857 *\n",
      "Epoch: 8, Train_Loss: 0.7336214780807495, Test_Loss: 0.49650853872299194 *\n",
      "Epoch: 8, Train_Loss: 0.5184392929077148, Test_Loss: 0.5267693996429443\n",
      "Epoch: 8, Train_Loss: 0.5056989192962646, Test_Loss: 0.6381081342697144\n",
      "Epoch: 8, Train_Loss: 0.680438220500946, Test_Loss: 5.385295867919922\n",
      "Epoch: 8, Train_Loss: 0.559670627117157, Test_Loss: 1.2009347677230835 *\n",
      "Epoch: 8, Train_Loss: 0.5254672169685364, Test_Loss: 0.4849664568901062 *\n",
      "Epoch: 8, Train_Loss: 0.49562177062034607, Test_Loss: 0.4826561510562897 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.7797656059265137, Test_Loss: 0.4745592176914215 *\n",
      "Epoch: 8, Train_Loss: 0.871797502040863, Test_Loss: 0.4716585576534271 *\n",
      "Epoch: 8, Train_Loss: 0.5927521586418152, Test_Loss: 0.4985538423061371\n",
      "Epoch: 8, Train_Loss: 0.4862788915634155, Test_Loss: 0.5187212228775024\n",
      "Epoch: 8, Train_Loss: 0.5039806365966797, Test_Loss: 0.4795941710472107 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.6853979229927063, Test_Loss: 0.4874088764190674\n",
      "Epoch: 8, Train_Loss: 1.1054701805114746, Test_Loss: 0.4944068491458893\n",
      "Epoch: 8, Train_Loss: 0.7578164339065552, Test_Loss: 0.5295178294181824\n",
      "Epoch: 8, Train_Loss: 0.5123156309127808, Test_Loss: 0.500065267086029 *\n",
      "Epoch: 8, Train_Loss: 0.477341890335083, Test_Loss: 0.46952518820762634 *\n",
      "Epoch: 8, Train_Loss: 0.467022180557251, Test_Loss: 0.4833105206489563\n",
      "Epoch: 8, Train_Loss: 0.837303876876831, Test_Loss: 0.4938886761665344\n",
      "Epoch: 8, Train_Loss: 0.5920669436454773, Test_Loss: 0.47658059000968933 *\n",
      "Epoch: 8, Train_Loss: 0.4795517921447754, Test_Loss: 0.492024302482605\n",
      "Epoch: 8, Train_Loss: 0.7284905314445496, Test_Loss: 0.5180867314338684\n",
      "Epoch: 8, Train_Loss: 0.5046510696411133, Test_Loss: 0.5014339089393616 *\n",
      "Epoch: 8, Train_Loss: 0.4708804190158844, Test_Loss: 0.4781828224658966 *\n",
      "Epoch: 8, Train_Loss: 0.5012139678001404, Test_Loss: 0.48147428035736084\n",
      "Epoch: 8, Train_Loss: 0.6306606531143188, Test_Loss: 0.531135618686676\n",
      "Epoch: 8, Train_Loss: 0.5009530782699585, Test_Loss: 0.537179708480835\n",
      "Epoch: 8, Train_Loss: 0.6307888627052307, Test_Loss: 0.5348404049873352 *\n",
      "Epoch: 8, Train_Loss: 0.47376030683517456, Test_Loss: 0.5036613941192627 *\n",
      "Epoch: 8, Train_Loss: 0.6858317255973816, Test_Loss: 0.4871155619621277 *\n",
      "Epoch: 8, Train_Loss: 0.5297122001647949, Test_Loss: 0.4910067021846771\n",
      "Epoch: 8, Train_Loss: 0.48553168773651123, Test_Loss: 0.48577335476875305 *\n",
      "Epoch: 8, Train_Loss: 0.47220465540885925, Test_Loss: 0.4767567217350006 *\n",
      "Epoch: 8, Train_Loss: 0.47853153944015503, Test_Loss: 0.5750057697296143\n",
      "Epoch: 8, Train_Loss: 0.6677091121673584, Test_Loss: 0.8211147785186768\n",
      "Epoch: 8, Train_Loss: 0.7627432346343994, Test_Loss: 5.4724321365356445\n",
      "Epoch: 8, Train_Loss: 0.7153019905090332, Test_Loss: 0.4929397702217102 *\n",
      "Epoch: 8, Train_Loss: 0.9985793828964233, Test_Loss: 0.5421366095542908\n",
      "Epoch: 8, Train_Loss: 0.7083204388618469, Test_Loss: 0.5144281983375549 *\n",
      "Epoch: 8, Train_Loss: 0.5953433513641357, Test_Loss: 0.4639660120010376 *\n",
      "Epoch: 8, Train_Loss: 0.5321592688560486, Test_Loss: 0.47069889307022095\n",
      "Epoch: 8, Train_Loss: 0.4815993309020996, Test_Loss: 0.47756969928741455\n",
      "Epoch: 8, Train_Loss: 0.46566784381866455, Test_Loss: 0.5063751935958862\n",
      "Epoch: 8, Train_Loss: 0.46979641914367676, Test_Loss: 0.49146538972854614 *\n",
      "Epoch: 8, Train_Loss: 0.614642322063446, Test_Loss: 0.4779905378818512 *\n",
      "Epoch: 8, Train_Loss: 0.7415614724159241, Test_Loss: 0.5165681838989258\n",
      "Epoch: 8, Train_Loss: 0.7210158109664917, Test_Loss: 0.6077672839164734\n",
      "Epoch: 8, Train_Loss: 1.8988498449325562, Test_Loss: 0.5164608955383301 *\n",
      "Epoch: 8, Train_Loss: 1.2702460289001465, Test_Loss: 0.5298795104026794\n",
      "Epoch: 8, Train_Loss: 0.6763753890991211, Test_Loss: 0.46485376358032227 *\n",
      "Epoch: 8, Train_Loss: 0.5618061423301697, Test_Loss: 0.5437875986099243\n",
      "Epoch: 8, Train_Loss: 0.46911680698394775, Test_Loss: 0.4744119942188263 *\n",
      "Epoch: 8, Train_Loss: 0.6460428833961487, Test_Loss: 0.5750241875648499\n",
      "Epoch: 8, Train_Loss: 1.244978666305542, Test_Loss: 0.5541841387748718 *\n",
      "Epoch: 8, Train_Loss: 1.274338722229004, Test_Loss: 0.4680742621421814 *\n",
      "Epoch: 8, Train_Loss: 0.5187860131263733, Test_Loss: 0.4944192171096802\n",
      "Epoch: 8, Train_Loss: 0.5135928988456726, Test_Loss: 0.4776145815849304 *\n",
      "Epoch: 8, Train_Loss: 0.5229153633117676, Test_Loss: 0.47848814725875854\n",
      "Epoch: 8, Train_Loss: 0.793236494064331, Test_Loss: 0.47395798563957214 *\n",
      "Epoch: 8, Train_Loss: 0.6320828795433044, Test_Loss: 0.48041850328445435\n",
      "Epoch: 8, Train_Loss: 0.6221330165863037, Test_Loss: 0.5049132704734802\n",
      "Epoch: 8, Train_Loss: 0.603561520576477, Test_Loss: 0.4987276494503021 *\n",
      "Epoch: 8, Train_Loss: 0.5284756422042847, Test_Loss: 0.4709607660770416 *\n",
      "Epoch: 8, Train_Loss: 0.4676657021045685, Test_Loss: 0.5197969675064087\n",
      "Epoch: 8, Train_Loss: 0.491606205701828, Test_Loss: 0.48783063888549805 *\n",
      "Epoch: 8, Train_Loss: 0.4792989492416382, Test_Loss: 0.4584085941314697 *\n",
      "Epoch: 8, Train_Loss: 0.5206054449081421, Test_Loss: 0.5218971967697144\n",
      "Epoch: 8, Train_Loss: 0.4773958921432495, Test_Loss: 0.6041833758354187\n",
      "Epoch: 8, Train_Loss: 0.5072653889656067, Test_Loss: 0.6609225273132324\n",
      "Epoch: 8, Train_Loss: 16.97307777404785, Test_Loss: 0.5178185105323792 *\n",
      "Epoch: 8, Train_Loss: 0.4891262352466583, Test_Loss: 0.4723908305168152 *\n",
      "Epoch: 8, Train_Loss: 2.176318645477295, Test_Loss: 0.4712473154067993 *\n",
      "Epoch: 8, Train_Loss: 1.2624139785766602, Test_Loss: 0.5299780964851379\n",
      "Epoch: 8, Train_Loss: 0.5139665603637695, Test_Loss: 0.577215850353241\n",
      "Epoch: 8, Train_Loss: 0.6319981813430786, Test_Loss: 0.586482048034668\n",
      "Epoch: 8, Train_Loss: 6.694843292236328, Test_Loss: 0.6339482069015503\n",
      "Epoch: 8, Train_Loss: 4.234881401062012, Test_Loss: 0.6554631590843201\n",
      "Epoch: 8, Train_Loss: 0.6117963194847107, Test_Loss: 0.5220947265625 *\n",
      "Epoch: 8, Train_Loss: 0.884781539440155, Test_Loss: 0.5062171220779419 *\n",
      "Epoch: 8, Train_Loss: 5.4421467781066895, Test_Loss: 0.6349833607673645\n",
      "Epoch: 8, Train_Loss: 0.7547304630279541, Test_Loss: 0.7303774356842041\n",
      "Epoch: 8, Train_Loss: 0.5221932530403137, Test_Loss: 0.7163411974906921 *\n",
      "Epoch: 8, Train_Loss: 0.4919261336326599, Test_Loss: 1.0023045539855957\n",
      "Epoch: 8, Train_Loss: 0.5412710309028625, Test_Loss: 1.0498733520507812\n",
      "Epoch: 8, Train_Loss: 0.5018666386604309, Test_Loss: 1.2030121088027954\n",
      "Epoch: 8, Train_Loss: 0.46348798274993896, Test_Loss: 0.9005780220031738 *\n",
      "Epoch: 8, Train_Loss: 0.5092334151268005, Test_Loss: 1.124176025390625\n",
      "Epoch: 8, Train_Loss: 0.4656926095485687, Test_Loss: 0.6842818260192871 *\n",
      "Epoch: 8, Train_Loss: 0.5196915864944458, Test_Loss: 0.9865584969520569\n",
      "Epoch: 8, Train_Loss: 0.5807577967643738, Test_Loss: 0.9720644950866699 *\n",
      "Epoch: 8, Train_Loss: 0.571488082408905, Test_Loss: 0.8032740354537964 *\n",
      "Epoch: 8, Train_Loss: 0.5666643977165222, Test_Loss: 0.6531426310539246 *\n",
      "Epoch: 8, Train_Loss: 0.5295047760009766, Test_Loss: 0.5254284739494324 *\n",
      "Epoch: 8, Train_Loss: 0.5101368427276611, Test_Loss: 0.5303770899772644\n",
      "Epoch: 8, Train_Loss: 0.4656205475330353, Test_Loss: 6.894212245941162\n",
      "Epoch: 8, Train_Loss: 0.473291277885437, Test_Loss: 0.7949310541152954 *\n",
      "Epoch: 8, Train_Loss: 0.4572015106678009, Test_Loss: 0.6002577543258667 *\n",
      "Epoch: 8, Train_Loss: 0.4526050388813019, Test_Loss: 0.5992860794067383 *\n",
      "Epoch: 8, Train_Loss: 0.4550066292285919, Test_Loss: 0.5592172741889954 *\n",
      "Epoch: 8, Train_Loss: 0.452674925327301, Test_Loss: 0.5024901628494263 *\n",
      "Epoch: 8, Train_Loss: 0.45365503430366516, Test_Loss: 0.6443491578102112\n",
      "Epoch: 8, Train_Loss: 0.45567864179611206, Test_Loss: 0.6346248388290405 *\n",
      "Epoch: 8, Train_Loss: 0.4515000283718109, Test_Loss: 0.48495936393737793 *\n",
      "Epoch: 8, Train_Loss: 0.4521467983722687, Test_Loss: 0.5146178007125854\n",
      "Epoch: 8, Train_Loss: 0.452034592628479, Test_Loss: 0.5336368083953857\n",
      "Epoch: 8, Train_Loss: 0.4528690576553345, Test_Loss: 0.6385759115219116\n",
      "Epoch: 8, Train_Loss: 0.4708097577095032, Test_Loss: 0.5287396907806396 *\n",
      "Epoch: 8, Train_Loss: 0.5188491344451904, Test_Loss: 0.5377020835876465\n",
      "Epoch: 8, Train_Loss: 0.4695127308368683, Test_Loss: 0.583379864692688\n",
      "Epoch: 8, Train_Loss: 0.5004807114601135, Test_Loss: 0.5237789750099182 *\n",
      "Epoch: 8, Train_Loss: 5.563729763031006, Test_Loss: 0.5136573314666748 *\n",
      "Epoch: 8, Train_Loss: 4.395723819732666, Test_Loss: 0.5378664135932922\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 0.46332186460494995, Test_Loss: 0.61451256275177\n",
      "Epoch: 8, Train_Loss: 0.5068067908287048, Test_Loss: 0.5491980910301208 *\n",
      "Epoch: 8, Train_Loss: 0.5656182169914246, Test_Loss: 0.5043233036994934 *\n",
      "Epoch: 8, Train_Loss: 0.47346609830856323, Test_Loss: 0.5430880188941956\n",
      "Epoch: 8, Train_Loss: 0.4585159122943878, Test_Loss: 0.6579101085662842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 0.5216607451438904, Test_Loss: 0.6479328870773315 *\n",
      "Epoch: 8, Train_Loss: 0.587403416633606, Test_Loss: 0.5947980880737305 *\n",
      "Epoch: 8, Train_Loss: 0.6831418871879578, Test_Loss: 0.5453593134880066 *\n",
      "Epoch: 8, Train_Loss: 0.6063748002052307, Test_Loss: 0.5268716812133789 *\n",
      "Epoch: 8, Train_Loss: 0.5003591179847717, Test_Loss: 0.5299215316772461\n",
      "Epoch: 8, Train_Loss: 0.5031214952468872, Test_Loss: 0.5200549364089966 *\n",
      "Epoch: 8, Train_Loss: 0.5862483978271484, Test_Loss: 0.519118070602417 *\n",
      "Epoch: 8, Train_Loss: 0.611942708492279, Test_Loss: 0.5477339029312134\n",
      "Epoch: 8, Train_Loss: 0.6156821250915527, Test_Loss: 2.0925381183624268\n",
      "Epoch: 8, Train_Loss: 0.5648679137229919, Test_Loss: 4.557774066925049\n",
      "Epoch: 8, Train_Loss: 0.5318873524665833, Test_Loss: 0.465046763420105 *\n",
      "Epoch: 8, Train_Loss: 0.45189037919044495, Test_Loss: 0.45130541920661926 *\n",
      "Epoch: 8, Train_Loss: 0.49152871966362, Test_Loss: 0.47122374176979065\n",
      "Epoch: 8, Train_Loss: 0.47752925753593445, Test_Loss: 0.4523114860057831 *\n",
      "Epoch: 8, Train_Loss: 0.4666319787502289, Test_Loss: 0.4723649322986603\n",
      "Epoch: 8, Train_Loss: 0.4497382938861847, Test_Loss: 0.48440515995025635\n",
      "Epoch: 8, Train_Loss: 0.45284393429756165, Test_Loss: 0.609868586063385\n",
      "Epoch: 8, Train_Loss: 0.46584102511405945, Test_Loss: 0.46350017189979553 *\n",
      "Epoch: 8, Train_Loss: 4.661388874053955, Test_Loss: 0.46069955825805664 *\n",
      "Epoch: 8, Train_Loss: 1.8728477954864502, Test_Loss: 0.491301953792572\n",
      "Epoch: 8, Train_Loss: 0.45039549469947815, Test_Loss: 0.4554034471511841 *\n",
      "Epoch: 8, Train_Loss: 0.4662446975708008, Test_Loss: 0.46384361386299133\n",
      "Epoch: 8, Train_Loss: 0.46446818113327026, Test_Loss: 0.490314245223999\n",
      "Epoch: 8, Train_Loss: 0.45390594005584717, Test_Loss: 0.4771285653114319 *\n",
      "Epoch: 8, Train_Loss: 0.4525664448738098, Test_Loss: 0.5439897775650024\n",
      "Epoch: 8, Train_Loss: 0.45349806547164917, Test_Loss: 0.5920866131782532\n",
      "Epoch: 8, Train_Loss: 0.4701974391937256, Test_Loss: 0.47137540578842163 *\n",
      "Epoch: 8, Train_Loss: 0.46375858783721924, Test_Loss: 0.46456509828567505 *\n",
      "Epoch: 8, Train_Loss: 0.4954635798931122, Test_Loss: 0.4463481903076172 *\n",
      "Epoch: 8, Train_Loss: 0.45002132654190063, Test_Loss: 0.44562530517578125 *\n",
      "Epoch: 8, Train_Loss: 0.4483483135700226, Test_Loss: 0.44626519083976746\n",
      "Epoch: 8, Train_Loss: 0.4634090065956116, Test_Loss: 0.4475466310977936\n",
      "Epoch: 8, Train_Loss: 0.44668394327163696, Test_Loss: 0.44659626483917236 *\n",
      "Epoch: 8, Train_Loss: 0.4461808204650879, Test_Loss: 0.4458128809928894 *\n",
      "Epoch: 8, Train_Loss: 0.4544471204280853, Test_Loss: 0.4491070806980133\n",
      "Epoch: 8, Train_Loss: 0.49463823437690735, Test_Loss: 0.44622108340263367 *\n",
      "Epoch: 8, Train_Loss: 0.47922906279563904, Test_Loss: 0.44664260745048523\n",
      "Epoch: 8, Train_Loss: 0.4451572597026825, Test_Loss: 0.46733036637306213\n",
      "Epoch: 8, Train_Loss: 0.4453172981739044, Test_Loss: 0.4452866017818451 *\n",
      "Epoch: 8, Train_Loss: 0.49393585324287415, Test_Loss: 0.46815574169158936\n",
      "Epoch: 8, Train_Loss: 0.5223023891448975, Test_Loss: 0.4742368161678314\n",
      "Epoch: 8, Train_Loss: 0.4706366956233978, Test_Loss: 0.7178478240966797\n",
      "Epoch: 8, Train_Loss: 0.47078391909599304, Test_Loss: 0.7581642270088196\n",
      "Epoch: 8, Train_Loss: 0.5290919542312622, Test_Loss: 0.5599863529205322 *\n",
      "Epoch: 8, Train_Loss: 0.5330548882484436, Test_Loss: 0.4531959593296051 *\n",
      "Epoch: 8, Train_Loss: 0.4987574815750122, Test_Loss: 0.46156126260757446\n",
      "Epoch: 8, Train_Loss: 0.5110549330711365, Test_Loss: 0.48189887404441833\n",
      "Epoch: 8, Train_Loss: 0.6711172461509705, Test_Loss: 0.6001077890396118\n",
      "Epoch: 8, Train_Loss: 0.48630788922309875, Test_Loss: 1.139013409614563\n",
      "Epoch: 9, Train_Loss: 0.48244911432266235, Test_Loss: 1.189244031906128 *\n",
      "Epoch: 9, Train_Loss: 0.44745075702667236, Test_Loss: 0.48906856775283813 *\n",
      "Epoch: 9, Train_Loss: 0.4440290033817291, Test_Loss: 0.48086947202682495 *\n",
      "Epoch: 9, Train_Loss: 0.4426969587802887, Test_Loss: 0.4503265619277954 *\n",
      "Epoch: 9, Train_Loss: 0.4433143734931946, Test_Loss: 0.454923540353775\n",
      "Epoch: 9, Train_Loss: 0.4450206160545349, Test_Loss: 0.454516738653183 *\n",
      "Epoch: 9, Train_Loss: 4.276397705078125, Test_Loss: 0.4668252170085907\n",
      "Epoch: 9, Train_Loss: 1.6307759284973145, Test_Loss: 0.5130708813667297\n",
      "Epoch: 9, Train_Loss: 0.44320571422576904, Test_Loss: 0.44732868671417236 *\n",
      "Epoch: 9, Train_Loss: 0.46339941024780273, Test_Loss: 0.460137277841568\n",
      "Epoch: 9, Train_Loss: 0.4471164047718048, Test_Loss: 0.5527120232582092\n",
      "Epoch: 9, Train_Loss: 0.4395207166671753, Test_Loss: 0.8197329640388489\n",
      "Epoch: 9, Train_Loss: 0.4403458833694458, Test_Loss: 0.68418949842453 *\n",
      "Epoch: 9, Train_Loss: 0.43989452719688416, Test_Loss: 0.45858892798423767 *\n",
      "Epoch: 9, Train_Loss: 0.43928495049476624, Test_Loss: 0.45304322242736816 *\n",
      "Epoch: 9, Train_Loss: 0.43922489881515503, Test_Loss: 0.4529745280742645 *\n",
      "Epoch: 9, Train_Loss: 0.47130343317985535, Test_Loss: 0.4537111818790436\n",
      "Epoch: 9, Train_Loss: 0.5254192352294922, Test_Loss: 0.4548702538013458\n",
      "Epoch: 9, Train_Loss: 0.525998592376709, Test_Loss: 0.8771305084228516\n",
      "Epoch: 9, Train_Loss: 0.5268955826759338, Test_Loss: 5.481377601623535\n",
      "Epoch: 9, Train_Loss: 0.451763391494751, Test_Loss: 0.48857808113098145 *\n",
      "Epoch: 9, Train_Loss: 0.47108954191207886, Test_Loss: 0.4485945999622345 *\n",
      "Epoch: 9, Train_Loss: 0.642435610294342, Test_Loss: 0.4431486427783966 *\n",
      "Epoch: 9, Train_Loss: 0.6609100103378296, Test_Loss: 0.4447813332080841\n",
      "Epoch: 9, Train_Loss: 0.630932092666626, Test_Loss: 0.44397884607315063 *\n",
      "Epoch: 9, Train_Loss: 0.4783179759979248, Test_Loss: 0.44359463453292847 *\n",
      "Epoch: 9, Train_Loss: 0.43761834502220154, Test_Loss: 0.45319926738739014\n",
      "Epoch: 9, Train_Loss: 0.43799200654029846, Test_Loss: 0.44143712520599365 *\n",
      "Epoch: 9, Train_Loss: 0.44167447090148926, Test_Loss: 0.44431987404823303\n",
      "Epoch: 9, Train_Loss: 0.44741612672805786, Test_Loss: 0.44856783747673035\n",
      "Epoch: 9, Train_Loss: 0.4468412697315216, Test_Loss: 0.4664076566696167\n",
      "Epoch: 9, Train_Loss: 0.44327130913734436, Test_Loss: 0.44413888454437256 *\n",
      "Epoch: 9, Train_Loss: 0.43834617733955383, Test_Loss: 0.44156262278556824 *\n",
      "Epoch: 9, Train_Loss: 0.4377117156982422, Test_Loss: 0.4562743902206421\n",
      "Epoch: 9, Train_Loss: 0.44852858781814575, Test_Loss: 0.4449359178543091 *\n",
      "Epoch: 9, Train_Loss: 0.5392741560935974, Test_Loss: 0.44343888759613037 *\n",
      "Epoch: 9, Train_Loss: 0.6216792464256287, Test_Loss: 0.44226744771003723 *\n",
      "Epoch: 9, Train_Loss: 0.5785325169563293, Test_Loss: 0.4558188319206238\n",
      "Epoch: 9, Train_Loss: 0.5103627443313599, Test_Loss: 0.4426094889640808 *\n",
      "Epoch: 9, Train_Loss: 0.5782117247581482, Test_Loss: 0.44432318210601807\n",
      "Epoch: 9, Train_Loss: 0.5980174541473389, Test_Loss: 0.44453540444374084\n",
      "Epoch: 9, Train_Loss: 0.44575873017311096, Test_Loss: 0.4613422751426697\n",
      "Epoch: 9, Train_Loss: 0.6170804500579834, Test_Loss: 0.4599457383155823 *\n",
      "Epoch: 9, Train_Loss: 0.5699068307876587, Test_Loss: 0.44990092515945435 *\n",
      "Epoch: 9, Train_Loss: 0.6938666105270386, Test_Loss: 0.44410279393196106 *\n",
      "Epoch: 9, Train_Loss: 0.44470152258872986, Test_Loss: 0.44750067591667175\n",
      "Epoch: 9, Train_Loss: 1.2360790967941284, Test_Loss: 0.44483518600463867 *\n",
      "Epoch: 9, Train_Loss: 2.865649461746216, Test_Loss: 0.4417414367198944 *\n",
      "Epoch: 9, Train_Loss: 0.4754526615142822, Test_Loss: 0.4586063623428345\n",
      "Epoch: 9, Train_Loss: 0.47385159134864807, Test_Loss: 0.4957001209259033\n",
      "Epoch: 9, Train_Loss: 0.4731605350971222, Test_Loss: 3.0932388305664062\n",
      "Epoch: 9, Train_Loss: 0.4703182280063629, Test_Loss: 3.2732863426208496\n",
      "Epoch: 9, Train_Loss: 0.43742862343788147, Test_Loss: 0.4370672106742859 *\n",
      "Epoch: 9, Train_Loss: 0.4482029676437378, Test_Loss: 0.43443334102630615 *\n",
      "Epoch: 9, Train_Loss: 0.570068359375, Test_Loss: 0.48446524143218994\n",
      "Epoch: 9, Train_Loss: 0.58675217628479, Test_Loss: 0.4689602255821228 *\n",
      "Epoch: 9, Train_Loss: 0.5435966849327087, Test_Loss: 0.4763041138648987\n",
      "Epoch: 9, Train_Loss: 0.5162187218666077, Test_Loss: 0.48377883434295654\n",
      "Epoch: 9, Train_Loss: 0.5083432197570801, Test_Loss: 0.5409789681434631\n",
      "Epoch: 9, Train_Loss: 0.4614781141281128, Test_Loss: 0.43764540553092957 *\n",
      "Epoch: 9, Train_Loss: 0.45994994044303894, Test_Loss: 0.4550616443157196\n",
      "Epoch: 9, Train_Loss: 0.4527960419654846, Test_Loss: 0.4534376859664917 *\n",
      "Epoch: 9, Train_Loss: 0.4673042893409729, Test_Loss: 0.45251041650772095 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.4457398056983948, Test_Loss: 0.4396750032901764 *\n",
      "Epoch: 9, Train_Loss: 0.43241456151008606, Test_Loss: 0.5198307037353516\n",
      "Epoch: 9, Train_Loss: 0.47274383902549744, Test_Loss: 0.5054218173027039 *\n",
      "Epoch: 9, Train_Loss: 0.4846918284893036, Test_Loss: 0.509114682674408\n",
      "Epoch: 9, Train_Loss: 0.4611956775188446, Test_Loss: 0.5341918468475342\n",
      "Epoch: 9, Train_Loss: 0.4320387542247772, Test_Loss: 0.4565966725349426 *\n",
      "Epoch: 9, Train_Loss: 0.4318898618221283, Test_Loss: 0.4694507122039795\n",
      "Epoch: 9, Train_Loss: 0.4312734603881836, Test_Loss: 0.4467257559299469 *\n",
      "Epoch: 9, Train_Loss: 0.4313766062259674, Test_Loss: 0.448353111743927\n",
      "Epoch: 9, Train_Loss: 0.4315114915370941, Test_Loss: 0.44906967878341675\n",
      "Epoch: 9, Train_Loss: 0.4320424199104309, Test_Loss: 0.45413872599601746\n",
      "Epoch: 9, Train_Loss: 0.4327162206172943, Test_Loss: 0.4527866542339325 *\n",
      "Epoch: 9, Train_Loss: 0.43165120482444763, Test_Loss: 0.44788143038749695 *\n",
      "Epoch: 9, Train_Loss: 0.43054601550102234, Test_Loss: 0.45248129963874817\n",
      "Epoch: 9, Train_Loss: 0.4315106272697449, Test_Loss: 0.4451257288455963 *\n",
      "Epoch: 9, Train_Loss: 0.4399939775466919, Test_Loss: 0.44592681527137756\n",
      "Epoch: 9, Train_Loss: 0.4460025131702423, Test_Loss: 0.4362560212612152 *\n",
      "Epoch: 9, Train_Loss: 0.44301432371139526, Test_Loss: 0.45510053634643555\n",
      "Epoch: 9, Train_Loss: 0.455358624458313, Test_Loss: 0.4904293119907379\n",
      "Epoch: 9, Train_Loss: 0.43407371640205383, Test_Loss: 0.46202483773231506 *\n",
      "Epoch: 9, Train_Loss: 0.4342814087867737, Test_Loss: 0.9444231986999512\n",
      "Epoch: 9, Train_Loss: 0.42977043986320496, Test_Loss: 0.8865305185317993 *\n",
      "Epoch: 9, Train_Loss: 0.430731862783432, Test_Loss: 0.5728579759597778 *\n",
      "Epoch: 9, Train_Loss: 0.45430755615234375, Test_Loss: 0.45332419872283936 *\n",
      "Epoch: 9, Train_Loss: 0.44258832931518555, Test_Loss: 0.44934597611427307 *\n",
      "Epoch: 9, Train_Loss: 0.43120506405830383, Test_Loss: 0.47021371126174927\n",
      "Epoch: 9, Train_Loss: 0.4292609989643097, Test_Loss: 0.6947394609451294\n",
      "Epoch: 9, Train_Loss: 0.43585073947906494, Test_Loss: 1.3373459577560425\n",
      "Epoch: 9, Train_Loss: 0.49620020389556885, Test_Loss: 1.0127005577087402 *\n",
      "Epoch: 9, Train_Loss: 0.4673021137714386, Test_Loss: 0.5065416097640991 *\n",
      "Epoch: 9, Train_Loss: 0.48573219776153564, Test_Loss: 0.46041247248649597 *\n",
      "Epoch: 9, Train_Loss: 0.4287649691104889, Test_Loss: 0.43408703804016113 *\n",
      "Epoch: 9, Train_Loss: 0.4643329083919525, Test_Loss: 0.43104326725006104 *\n",
      "Epoch: 9, Train_Loss: 0.4724428653717041, Test_Loss: 0.4369948208332062\n",
      "Epoch: 9, Train_Loss: 0.4303250014781952, Test_Loss: 0.4491775333881378\n",
      "Epoch: 9, Train_Loss: 0.4467857778072357, Test_Loss: 0.48510247468948364\n",
      "Epoch: 9, Train_Loss: 0.4596084654331207, Test_Loss: 0.4284665584564209 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.5182737112045288, Test_Loss: 0.48006415367126465\n",
      "Epoch: 9, Train_Loss: 0.5243149399757385, Test_Loss: 0.549860417842865\n",
      "Epoch: 9, Train_Loss: 0.4977701008319855, Test_Loss: 0.8081803321838379\n",
      "Epoch: 9, Train_Loss: 0.45741385221481323, Test_Loss: 0.6589628458023071 *\n",
      "Epoch: 9, Train_Loss: 0.4313640594482422, Test_Loss: 0.4464931786060333 *\n",
      "Epoch: 9, Train_Loss: 0.45263195037841797, Test_Loss: 0.44074350595474243 *\n",
      "Epoch: 9, Train_Loss: 0.42758598923683167, Test_Loss: 0.44041183590888977 *\n",
      "Epoch: 9, Train_Loss: 0.433557391166687, Test_Loss: 0.440521240234375\n",
      "Epoch: 9, Train_Loss: 0.4373129904270172, Test_Loss: 0.4437687397003174\n",
      "Epoch: 9, Train_Loss: 0.44232016801834106, Test_Loss: 1.8032832145690918\n",
      "Epoch: 9, Train_Loss: 0.5215399861335754, Test_Loss: 4.53541898727417\n",
      "Epoch: 9, Train_Loss: 0.4325104057788849, Test_Loss: 0.44181784987449646 *\n",
      "Epoch: 9, Train_Loss: 0.4969334900379181, Test_Loss: 0.4304017722606659 *\n",
      "Epoch: 9, Train_Loss: 0.4310030937194824, Test_Loss: 0.42937126755714417 *\n",
      "Epoch: 9, Train_Loss: 0.4580945372581482, Test_Loss: 0.4328622817993164\n",
      "Epoch: 9, Train_Loss: 0.43737784028053284, Test_Loss: 0.43060439825057983 *\n",
      "Epoch: 9, Train_Loss: 0.7011566758155823, Test_Loss: 0.4309335947036743\n",
      "Epoch: 9, Train_Loss: 0.4852693974971771, Test_Loss: 0.4339781403541565\n",
      "Epoch: 9, Train_Loss: 0.4543832838535309, Test_Loss: 0.4283982515335083 *\n",
      "Epoch: 9, Train_Loss: 0.4334697425365448, Test_Loss: 0.4304928481578827\n",
      "Epoch: 9, Train_Loss: 0.42582711577415466, Test_Loss: 0.4318028688430786\n",
      "Epoch: 9, Train_Loss: 0.4274417459964752, Test_Loss: 0.44308412075042725\n",
      "Epoch: 9, Train_Loss: 0.42555513978004456, Test_Loss: 0.4357440173625946 *\n",
      "Epoch: 9, Train_Loss: 0.43434134125709534, Test_Loss: 0.4313942790031433 *\n",
      "Epoch: 9, Train_Loss: 0.43930283188819885, Test_Loss: 0.4396837651729584\n",
      "Epoch: 9, Train_Loss: 0.4467296302318573, Test_Loss: 0.4277181923389435 *\n",
      "Epoch: 9, Train_Loss: 0.43438348174095154, Test_Loss: 0.42935293912887573\n",
      "Epoch: 9, Train_Loss: 0.43583837151527405, Test_Loss: 0.42585480213165283 *\n",
      "Epoch: 9, Train_Loss: 0.4419703781604767, Test_Loss: 0.43582284450531006\n",
      "Epoch: 9, Train_Loss: 0.42643189430236816, Test_Loss: 0.4265831410884857 *\n",
      "Epoch: 9, Train_Loss: 0.4230329990386963, Test_Loss: 0.4260479807853699 *\n",
      "Epoch: 9, Train_Loss: 0.4326067864894867, Test_Loss: 0.4279474914073944\n",
      "Epoch: 9, Train_Loss: 0.45185375213623047, Test_Loss: 0.43808043003082275\n",
      "Epoch: 9, Train_Loss: 0.4587898552417755, Test_Loss: 0.43288183212280273 *\n",
      "Epoch: 9, Train_Loss: 0.42501378059387207, Test_Loss: 0.42977672815322876 *\n",
      "Epoch: 9, Train_Loss: 0.4601787328720093, Test_Loss: 0.42680779099464417 *\n",
      "Epoch: 9, Train_Loss: 0.4823789894580841, Test_Loss: 0.4290826916694641\n",
      "Epoch: 9, Train_Loss: 0.4773484170436859, Test_Loss: 0.4286259412765503 *\n",
      "Epoch: 9, Train_Loss: 0.4225672781467438, Test_Loss: 0.42552244663238525 *\n",
      "Epoch: 9, Train_Loss: 0.454001247882843, Test_Loss: 0.4698863923549652\n",
      "Epoch: 9, Train_Loss: 0.4277630150318146, Test_Loss: 0.46033307909965515 *\n",
      "Epoch: 9, Train_Loss: 0.4420047998428345, Test_Loss: 4.072412967681885\n",
      "Epoch: 9, Train_Loss: 0.42430397868156433, Test_Loss: 2.2681732177734375 *\n",
      "Epoch: 9, Train_Loss: 0.4463013708591461, Test_Loss: 0.42349913716316223 *\n",
      "Epoch: 9, Train_Loss: 0.48312485218048096, Test_Loss: 0.42812973260879517\n",
      "Epoch: 9, Train_Loss: 2.892869472503662, Test_Loss: 0.47420018911361694\n",
      "Epoch: 9, Train_Loss: 3.3544228076934814, Test_Loss: 0.46801018714904785 *\n",
      "Epoch: 9, Train_Loss: 0.4412861168384552, Test_Loss: 0.44258397817611694 *\n",
      "Epoch: 9, Train_Loss: 0.421398401260376, Test_Loss: 0.49225756525993347\n",
      "Epoch: 9, Train_Loss: 0.5321412682533264, Test_Loss: 0.5146113038063049\n",
      "Epoch: 9, Train_Loss: 0.5545098781585693, Test_Loss: 0.42348676919937134 *\n",
      "Epoch: 9, Train_Loss: 0.44408512115478516, Test_Loss: 0.4547465741634369\n",
      "Epoch: 9, Train_Loss: 0.42150232195854187, Test_Loss: 0.43905019760131836 *\n",
      "Epoch: 9, Train_Loss: 0.46360892057418823, Test_Loss: 0.4346682131290436 *\n",
      "Epoch: 9, Train_Loss: 0.4641227722167969, Test_Loss: 0.424836128950119 *\n",
      "Epoch: 9, Train_Loss: 0.43152615427970886, Test_Loss: 0.5133982300758362\n",
      "Epoch: 9, Train_Loss: 0.47122862935066223, Test_Loss: 0.46962296962738037 *\n",
      "Epoch: 9, Train_Loss: 1.5807549953460693, Test_Loss: 0.5235487818717957\n",
      "Epoch: 9, Train_Loss: 1.8574185371398926, Test_Loss: 0.5055479407310486 *\n",
      "Epoch: 9, Train_Loss: 0.6062590479850769, Test_Loss: 0.45451515913009644 *\n",
      "Epoch: 9, Train_Loss: 0.5004345774650574, Test_Loss: 0.4385804533958435 *\n",
      "Epoch: 9, Train_Loss: 1.9543131589889526, Test_Loss: 0.4308348298072815 *\n",
      "Epoch: 9, Train_Loss: 2.0612905025482178, Test_Loss: 0.43185511231422424\n",
      "Epoch: 9, Train_Loss: 0.4575112760066986, Test_Loss: 0.432504266500473\n",
      "Epoch: 9, Train_Loss: 0.4385295808315277, Test_Loss: 0.4350992739200592\n",
      "Epoch: 9, Train_Loss: 0.6369834542274475, Test_Loss: 0.4330280125141144 *\n",
      "Epoch: 9, Train_Loss: 1.6164624691009521, Test_Loss: 0.42498818039894104 *\n",
      "Epoch: 9, Train_Loss: 1.475588321685791, Test_Loss: 0.42690134048461914\n",
      "Epoch: 9, Train_Loss: 0.4260160028934479, Test_Loss: 0.4198397099971771 *\n",
      "Epoch: 9, Train_Loss: 0.4395767152309418, Test_Loss: 0.4238753616809845\n",
      "Epoch: 9, Train_Loss: 0.4831356406211853, Test_Loss: 0.4360642731189728\n",
      "Epoch: 9, Train_Loss: 1.02767813205719, Test_Loss: 0.42654556035995483 *\n",
      "Epoch: 9, Train_Loss: 0.4272105097770691, Test_Loss: 0.437566339969635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.45756229758262634, Test_Loss: 0.5115302205085754\n",
      "Epoch: 9, Train_Loss: 0.4354225695133209, Test_Loss: 0.7827515006065369\n",
      "Epoch: 9, Train_Loss: 0.4762425720691681, Test_Loss: 0.6930994987487793 *\n",
      "Epoch: 9, Train_Loss: 0.5442172288894653, Test_Loss: 0.5034079551696777 *\n",
      "Epoch: 9, Train_Loss: 0.5871313810348511, Test_Loss: 0.42175015807151794 *\n",
      "Epoch: 9, Train_Loss: 0.6265367865562439, Test_Loss: 0.45553866028785706\n",
      "Epoch: 9, Train_Loss: 0.4790629744529724, Test_Loss: 0.4892435669898987\n",
      "Epoch: 9, Train_Loss: 0.4889232814311981, Test_Loss: 0.6182445287704468\n",
      "Epoch: 9, Train_Loss: 0.5056878328323364, Test_Loss: 0.5978888273239136 *\n",
      "Epoch: 9, Train_Loss: 0.6182574033737183, Test_Loss: 0.8270426988601685\n",
      "Epoch: 9, Train_Loss: 0.6076635122299194, Test_Loss: 0.4553792476654053 *\n",
      "Epoch: 9, Train_Loss: 0.4575514793395996, Test_Loss: 0.4575718641281128\n",
      "Epoch: 9, Train_Loss: 0.6585804224014282, Test_Loss: 0.4261643886566162 *\n",
      "Epoch: 9, Train_Loss: 0.6206234097480774, Test_Loss: 0.418624609708786 *\n",
      "Epoch: 9, Train_Loss: 0.44397851824760437, Test_Loss: 0.4793533980846405\n",
      "Epoch: 9, Train_Loss: 0.4291650950908661, Test_Loss: 0.45288246870040894 *\n",
      "Epoch: 9, Train_Loss: 0.4292021691799164, Test_Loss: 0.4431076645851135 *\n",
      "Epoch: 9, Train_Loss: 0.42089706659317017, Test_Loss: 0.44374707341194153\n",
      "Epoch: 9, Train_Loss: 0.43172571063041687, Test_Loss: 0.6007630228996277\n",
      "Epoch: 9, Train_Loss: 0.43043845891952515, Test_Loss: 0.6486799716949463\n",
      "Epoch: 9, Train_Loss: 0.42965710163116455, Test_Loss: 0.8217810392379761\n",
      "Epoch: 9, Train_Loss: 0.42076802253723145, Test_Loss: 1.0777850151062012\n",
      "Epoch: 9, Train_Loss: 0.430880069732666, Test_Loss: 0.564705491065979 *\n",
      "Epoch: 9, Train_Loss: 0.5469920635223389, Test_Loss: 0.5515738725662231 *\n",
      "Epoch: 9, Train_Loss: 0.7590082883834839, Test_Loss: 0.5402519702911377 *\n",
      "Epoch: 9, Train_Loss: 0.4478238523006439, Test_Loss: 0.5439392328262329\n",
      "Epoch: 9, Train_Loss: 0.45950794219970703, Test_Loss: 0.6570369005203247\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.4904788136482239, Test_Loss: 2.9000344276428223\n",
      "Epoch: 9, Train_Loss: 0.4925910234451294, Test_Loss: 2.7494585514068604 *\n",
      "Epoch: 9, Train_Loss: 0.6004199981689453, Test_Loss: 0.43306395411491394 *\n",
      "Epoch: 9, Train_Loss: 0.43859484791755676, Test_Loss: 0.4295254051685333 *\n",
      "Epoch: 9, Train_Loss: 0.6860277652740479, Test_Loss: 0.4292759895324707 *\n",
      "Epoch: 9, Train_Loss: 0.8308579921722412, Test_Loss: 0.4277709126472473 *\n",
      "Epoch: 9, Train_Loss: 0.6319642066955566, Test_Loss: 0.4566875696182251\n",
      "Epoch: 9, Train_Loss: 0.4385785460472107, Test_Loss: 0.45904403924942017\n",
      "Epoch: 9, Train_Loss: 0.4288608729839325, Test_Loss: 0.43927475810050964 *\n",
      "Epoch: 9, Train_Loss: 0.5065873861312866, Test_Loss: 0.4297178089618683 *\n",
      "Epoch: 9, Train_Loss: 1.0634770393371582, Test_Loss: 0.44141247868537903\n",
      "Epoch: 9, Train_Loss: 0.8627264499664307, Test_Loss: 0.4520208239555359\n",
      "Epoch: 9, Train_Loss: 0.4391736090183258, Test_Loss: 0.4985387325286865\n",
      "Epoch: 9, Train_Loss: 0.4349212646484375, Test_Loss: 0.41925695538520813 *\n",
      "Epoch: 9, Train_Loss: 0.41672638058662415, Test_Loss: 0.43136700987815857\n",
      "Epoch: 9, Train_Loss: 0.626043438911438, Test_Loss: 0.4527512192726135\n",
      "Epoch: 9, Train_Loss: 0.6827202439308167, Test_Loss: 0.41651588678359985 *\n",
      "Epoch: 9, Train_Loss: 0.42029765248298645, Test_Loss: 0.4376490116119385\n",
      "Epoch: 9, Train_Loss: 0.659822404384613, Test_Loss: 0.4465118646621704\n",
      "Epoch: 9, Train_Loss: 0.4367886483669281, Test_Loss: 0.47054556012153625\n",
      "Epoch: 9, Train_Loss: 0.4273843765258789, Test_Loss: 0.4206894338130951 *\n",
      "Epoch: 9, Train_Loss: 0.45634692907333374, Test_Loss: 0.43282628059387207\n",
      "Epoch: 9, Train_Loss: 0.5873535871505737, Test_Loss: 0.4448123872280121\n",
      "Epoch: 9, Train_Loss: 0.4788368344306946, Test_Loss: 0.5035895705223083\n",
      "Epoch: 9, Train_Loss: 0.5202994346618652, Test_Loss: 0.48778051137924194 *\n",
      "Epoch: 9, Train_Loss: 0.4243023097515106, Test_Loss: 0.45674657821655273 *\n",
      "Epoch: 9, Train_Loss: 0.5451135635375977, Test_Loss: 0.42999231815338135 *\n",
      "Epoch: 9, Train_Loss: 0.4513150453567505, Test_Loss: 0.44559746980667114\n",
      "Epoch: 9, Train_Loss: 0.4495716392993927, Test_Loss: 0.43580466508865356 *\n",
      "Epoch: 9, Train_Loss: 0.42926332354545593, Test_Loss: 0.4192543625831604 *\n",
      "Epoch: 9, Train_Loss: 0.41976475715637207, Test_Loss: 0.5365669131278992\n",
      "Epoch: 9, Train_Loss: 0.49709129333496094, Test_Loss: 0.5440226793289185\n",
      "Epoch: 9, Train_Loss: 0.6054838299751282, Test_Loss: 5.430974960327148\n",
      "Epoch: 9, Train_Loss: 0.5789816379547119, Test_Loss: 0.915107786655426 *\n",
      "Epoch: 9, Train_Loss: 0.9055205583572388, Test_Loss: 0.5114343166351318 *\n",
      "Epoch: 9, Train_Loss: 0.7133228778839111, Test_Loss: 0.509443461894989 *\n",
      "Epoch: 9, Train_Loss: 0.5801541209220886, Test_Loss: 0.4296579360961914 *\n",
      "Epoch: 9, Train_Loss: 0.4772919714450836, Test_Loss: 0.4254130721092224 *\n",
      "Epoch: 9, Train_Loss: 0.45582225918769836, Test_Loss: 0.4245067238807678 *\n",
      "Epoch: 9, Train_Loss: 0.4215483069419861, Test_Loss: 0.4580767750740051\n",
      "Epoch: 9, Train_Loss: 0.4160471260547638, Test_Loss: 0.4499397277832031 *\n",
      "Epoch: 9, Train_Loss: 0.5159098505973816, Test_Loss: 0.4232749938964844 *\n",
      "Epoch: 9, Train_Loss: 0.7352123260498047, Test_Loss: 0.4742882549762726\n",
      "Epoch: 9, Train_Loss: 0.7980935573577881, Test_Loss: 0.5864673256874084\n",
      "Epoch: 9, Train_Loss: 1.6192594766616821, Test_Loss: 0.5345370769500732 *\n",
      "Epoch: 9, Train_Loss: 1.4642150402069092, Test_Loss: 0.46928295493125916 *\n",
      "Epoch: 9, Train_Loss: 0.5524649024009705, Test_Loss: 0.4183998703956604 *\n",
      "Epoch: 9, Train_Loss: 0.621490478515625, Test_Loss: 0.48557373881340027\n",
      "Epoch: 9, Train_Loss: 0.41955018043518066, Test_Loss: 0.4478294253349304 *\n",
      "Epoch: 9, Train_Loss: 0.506766676902771, Test_Loss: 0.4888996481895447\n",
      "Epoch: 9, Train_Loss: 0.9817256331443787, Test_Loss: 0.6004254817962646\n",
      "Epoch: 9, Train_Loss: 1.442952036857605, Test_Loss: 0.4251945912837982 *\n",
      "Epoch: 9, Train_Loss: 0.4392314553260803, Test_Loss: 0.42824679613113403\n",
      "Epoch: 9, Train_Loss: 0.4562061131000519, Test_Loss: 0.42527756094932556 *\n",
      "Epoch: 9, Train_Loss: 0.46747466921806335, Test_Loss: 0.43195003271102905\n",
      "Epoch: 9, Train_Loss: 0.6802160739898682, Test_Loss: 0.4257825016975403 *\n",
      "Epoch: 9, Train_Loss: 0.554089367389679, Test_Loss: 0.424947589635849 *\n",
      "Epoch: 9, Train_Loss: 0.6733834743499756, Test_Loss: 0.44484081864356995\n",
      "Epoch: 9, Train_Loss: 0.5855637192726135, Test_Loss: 0.434370756149292 *\n",
      "Epoch: 9, Train_Loss: 0.6357653141021729, Test_Loss: 0.4408227503299713\n",
      "Epoch: 9, Train_Loss: 0.41890749335289, Test_Loss: 0.4614468514919281\n",
      "Epoch: 9, Train_Loss: 0.42784297466278076, Test_Loss: 0.4777635335922241\n",
      "Epoch: 9, Train_Loss: 0.4188256561756134, Test_Loss: 0.4159424901008606 *\n",
      "Epoch: 9, Train_Loss: 0.48460516333580017, Test_Loss: 0.4533326029777527\n",
      "Epoch: 9, Train_Loss: 0.41848456859588623, Test_Loss: 0.5367128849029541\n",
      "Epoch: 9, Train_Loss: 0.45583584904670715, Test_Loss: 0.5853722095489502\n",
      "Epoch: 9, Train_Loss: 16.61548614501953, Test_Loss: 0.5044975280761719 *\n",
      "Epoch: 9, Train_Loss: 0.4163513779640198, Test_Loss: 0.43857941031455994 *\n",
      "Epoch: 9, Train_Loss: 1.918750524520874, Test_Loss: 0.42576971650123596 *\n",
      "Epoch: 9, Train_Loss: 1.4368606805801392, Test_Loss: 0.460708886384964\n",
      "Epoch: 9, Train_Loss: 0.4377190172672272, Test_Loss: 0.5893622040748596\n",
      "Epoch: 9, Train_Loss: 0.5940412282943726, Test_Loss: 0.7102828025817871\n",
      "Epoch: 9, Train_Loss: 3.847369909286499, Test_Loss: 0.5641559958457947 *\n",
      "Epoch: 9, Train_Loss: 6.277427673339844, Test_Loss: 0.9569999575614929\n",
      "Epoch: 9, Train_Loss: 0.4785219430923462, Test_Loss: 0.512374758720398 *\n",
      "Epoch: 9, Train_Loss: 0.6117953062057495, Test_Loss: 0.4671509861946106 *\n",
      "Epoch: 9, Train_Loss: 4.521346092224121, Test_Loss: 0.6502792239189148\n",
      "Epoch: 9, Train_Loss: 0.8757200837135315, Test_Loss: 0.8350951671600342\n",
      "Epoch: 9, Train_Loss: 0.6557581424713135, Test_Loss: 1.0523104667663574\n",
      "Epoch: 9, Train_Loss: 0.42105886340141296, Test_Loss: 1.3966798782348633\n",
      "Epoch: 9, Train_Loss: 0.5024627447128296, Test_Loss: 1.8264681100845337\n",
      "Epoch: 9, Train_Loss: 0.5928841829299927, Test_Loss: 1.673947811126709 *\n",
      "Epoch: 9, Train_Loss: 0.4318043291568756, Test_Loss: 1.2372207641601562 *\n",
      "Epoch: 9, Train_Loss: 0.468203067779541, Test_Loss: 1.4980446100234985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.40684354305267334, Test_Loss: 0.8109279274940491 *\n",
      "Epoch: 9, Train_Loss: 0.40886062383651733, Test_Loss: 0.6558151245117188 *\n",
      "Epoch: 9, Train_Loss: 0.4396325647830963, Test_Loss: 0.7958283424377441\n",
      "Epoch: 9, Train_Loss: 0.4269295334815979, Test_Loss: 0.6991158723831177 *\n",
      "Epoch: 9, Train_Loss: 0.5198401212692261, Test_Loss: 0.6071093082427979 *\n",
      "Epoch: 9, Train_Loss: 0.4653594493865967, Test_Loss: 0.5090819001197815 *\n",
      "Epoch: 9, Train_Loss: 0.44489172101020813, Test_Loss: 0.43933212757110596 *\n",
      "Epoch: 9, Train_Loss: 0.4164144992828369, Test_Loss: 5.332460403442383\n",
      "Epoch: 9, Train_Loss: 0.41835641860961914, Test_Loss: 2.112431764602661 *\n",
      "Epoch: 9, Train_Loss: 0.40896105766296387, Test_Loss: 0.6705209016799927 *\n",
      "Epoch: 9, Train_Loss: 0.4077204763889313, Test_Loss: 0.6898902654647827\n",
      "Epoch: 9, Train_Loss: 0.4063107669353485, Test_Loss: 0.6904953718185425\n",
      "Epoch: 9, Train_Loss: 0.40400245785713196, Test_Loss: 0.4467446804046631 *\n",
      "Epoch: 9, Train_Loss: 0.40397199988365173, Test_Loss: 0.8322453498840332\n",
      "Epoch: 9, Train_Loss: 0.40511974692344666, Test_Loss: 0.8388712406158447\n",
      "Epoch: 9, Train_Loss: 0.40351128578186035, Test_Loss: 0.5764878392219543 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.4037322402000427, Test_Loss: 0.5721009969711304 *\n",
      "Epoch: 9, Train_Loss: 0.40356042981147766, Test_Loss: 0.6070777177810669\n",
      "Epoch: 9, Train_Loss: 0.4119711220264435, Test_Loss: 0.6232936978340149\n",
      "Epoch: 9, Train_Loss: 0.43564295768737793, Test_Loss: 0.7392109036445618\n",
      "Epoch: 9, Train_Loss: 0.4694170355796814, Test_Loss: 0.5378111600875854 *\n",
      "Epoch: 9, Train_Loss: 0.46860402822494507, Test_Loss: 0.67066890001297\n",
      "Epoch: 9, Train_Loss: 0.49413275718688965, Test_Loss: 0.6138654947280884 *\n",
      "Epoch: 9, Train_Loss: 2.219351053237915, Test_Loss: 0.4689346253871918 *\n",
      "Epoch: 9, Train_Loss: 6.79861307144165, Test_Loss: 0.5124557614326477\n",
      "Epoch: 9, Train_Loss: 0.4235607385635376, Test_Loss: 0.5786060690879822\n",
      "Epoch: 9, Train_Loss: 0.5182185173034668, Test_Loss: 0.6262177228927612\n",
      "Epoch: 9, Train_Loss: 0.6103366017341614, Test_Loss: 0.480299711227417 *\n",
      "Epoch: 9, Train_Loss: 0.5493187308311462, Test_Loss: 0.5387606024742126\n",
      "Epoch: 9, Train_Loss: 0.45062094926834106, Test_Loss: 0.6650484800338745\n",
      "Epoch: 9, Train_Loss: 0.5111896395683289, Test_Loss: 0.7023091316223145\n",
      "Epoch: 9, Train_Loss: 0.5650911927223206, Test_Loss: 0.6879384517669678 *\n",
      "Epoch: 9, Train_Loss: 0.6114960312843323, Test_Loss: 0.5663783550262451 *\n",
      "Epoch: 9, Train_Loss: 0.5056256055831909, Test_Loss: 0.49524927139282227 *\n",
      "Epoch: 9, Train_Loss: 0.4598439335823059, Test_Loss: 0.506897509098053\n",
      "Epoch: 9, Train_Loss: 0.4253344237804413, Test_Loss: 0.5109285116195679\n",
      "Epoch: 9, Train_Loss: 0.5281937122344971, Test_Loss: 0.4482748508453369 *\n",
      "Epoch: 9, Train_Loss: 0.5177079439163208, Test_Loss: 0.5663489103317261\n",
      "Epoch: 9, Train_Loss: 0.61842280626297, Test_Loss: 0.5079926252365112 *\n",
      "Epoch: 9, Train_Loss: 0.5216257572174072, Test_Loss: 6.160339832305908\n",
      "Epoch: 9, Train_Loss: 0.5163518190383911, Test_Loss: 0.48622068762779236 *\n",
      "Epoch: 9, Train_Loss: 0.40358924865722656, Test_Loss: 0.40196898579597473 *\n",
      "Epoch: 9, Train_Loss: 0.4170984625816345, Test_Loss: 0.4186045527458191\n",
      "Epoch: 9, Train_Loss: 0.43152689933776855, Test_Loss: 0.4041541516780853 *\n",
      "Epoch: 9, Train_Loss: 0.42819687724113464, Test_Loss: 0.4141100347042084\n",
      "Epoch: 9, Train_Loss: 0.40771594643592834, Test_Loss: 0.4093535840511322 *\n",
      "Epoch: 9, Train_Loss: 0.4001670181751251, Test_Loss: 0.527251124382019\n",
      "Epoch: 9, Train_Loss: 0.41393834352493286, Test_Loss: 0.4610682725906372 *\n",
      "Epoch: 9, Train_Loss: 2.2400636672973633, Test_Loss: 0.400492787361145 *\n",
      "Epoch: 9, Train_Loss: 4.292879104614258, Test_Loss: 0.4449807405471802\n",
      "Epoch: 9, Train_Loss: 0.4006464183330536, Test_Loss: 0.4119999408721924 *\n",
      "Epoch: 9, Train_Loss: 0.40378573536872864, Test_Loss: 0.41114190220832825 *\n",
      "Epoch: 9, Train_Loss: 0.40751686692237854, Test_Loss: 0.4256437420845032\n",
      "Epoch: 9, Train_Loss: 0.4013490378856659, Test_Loss: 0.4305531978607178\n",
      "Epoch: 9, Train_Loss: 0.4048134684562683, Test_Loss: 0.45688387751579285\n",
      "Epoch: 9, Train_Loss: 0.4017443358898163, Test_Loss: 0.522782027721405\n",
      "Epoch: 9, Train_Loss: 0.419902503490448, Test_Loss: 0.45413729548454285 *\n",
      "Epoch: 9, Train_Loss: 0.4460887908935547, Test_Loss: 0.42525529861450195 *\n",
      "Epoch: 9, Train_Loss: 0.42478954792022705, Test_Loss: 0.4024069607257843 *\n",
      "Epoch: 9, Train_Loss: 0.40259402990341187, Test_Loss: 0.40137341618537903 *\n",
      "Epoch: 9, Train_Loss: 0.3986974358558655, Test_Loss: 0.40080535411834717 *\n",
      "Epoch: 9, Train_Loss: 0.39926809072494507, Test_Loss: 0.40083029866218567\n",
      "Epoch: 9, Train_Loss: 0.41327348351478577, Test_Loss: 0.39951205253601074 *\n",
      "Epoch: 9, Train_Loss: 0.39917317032814026, Test_Loss: 0.4005226492881775\n",
      "Epoch: 9, Train_Loss: 0.4002772867679596, Test_Loss: 0.40553951263427734\n",
      "Epoch: 9, Train_Loss: 0.4267769455909729, Test_Loss: 0.4020608067512512 *\n",
      "Epoch: 9, Train_Loss: 0.4314783811569214, Test_Loss: 0.401893675327301 *\n",
      "Epoch: 9, Train_Loss: 0.39969784021377563, Test_Loss: 0.4081193208694458\n",
      "Epoch: 9, Train_Loss: 0.3974144756793976, Test_Loss: 0.4065566658973694 *\n",
      "Epoch: 9, Train_Loss: 0.43174248933792114, Test_Loss: 0.42275893688201904\n",
      "Epoch: 9, Train_Loss: 0.531114935874939, Test_Loss: 0.42075884342193604 *\n",
      "Epoch: 9, Train_Loss: 0.44685739278793335, Test_Loss: 0.5828858613967896\n",
      "Epoch: 9, Train_Loss: 0.44740375876426697, Test_Loss: 0.7410298585891724\n",
      "Epoch: 9, Train_Loss: 0.45179179310798645, Test_Loss: 0.5255001783370972 *\n",
      "Epoch: 9, Train_Loss: 0.4845837354660034, Test_Loss: 0.4319401681423187 *\n",
      "Epoch: 9, Train_Loss: 0.439555823802948, Test_Loss: 0.40290290117263794 *\n",
      "Epoch: 9, Train_Loss: 0.4725293219089508, Test_Loss: 0.4330848455429077\n",
      "Epoch: 9, Train_Loss: 0.48525771498680115, Test_Loss: 0.528626561164856\n",
      "Epoch: 9, Train_Loss: 0.5348002910614014, Test_Loss: 0.9345165491104126\n",
      "Epoch: 9, Train_Loss: 0.43314141035079956, Test_Loss: 1.1500097513198853\n",
      "Epoch: 9, Train_Loss: 0.39762425422668457, Test_Loss: 0.5202131271362305 *\n",
      "Epoch: 9, Train_Loss: 0.39664459228515625, Test_Loss: 0.45407408475875854 *\n",
      "Epoch: 9, Train_Loss: 0.39536410570144653, Test_Loss: 0.4038495719432831 *\n",
      "Epoch: 9, Train_Loss: 0.3950885534286499, Test_Loss: 0.42367541790008545\n",
      "Epoch: 9, Train_Loss: 0.3980591893196106, Test_Loss: 0.41285601258277893 *\n",
      "Epoch: 9, Train_Loss: 2.5351386070251465, Test_Loss: 0.41886165738105774\n",
      "Epoch: 9, Train_Loss: 3.2419397830963135, Test_Loss: 0.43286722898483276\n",
      "Epoch: 9, Train_Loss: 0.39670902490615845, Test_Loss: 0.4299337863922119 *\n",
      "Epoch: 9, Train_Loss: 0.4134831130504608, Test_Loss: 0.40471336245536804 *\n",
      "Epoch: 9, Train_Loss: 0.4047620892524719, Test_Loss: 0.4932331442832947\n",
      "Epoch: 9, Train_Loss: 0.394700825214386, Test_Loss: 0.7778970003128052\n",
      "Epoch: 9, Train_Loss: 0.39510342478752136, Test_Loss: 0.5029182434082031 *\n",
      "Epoch: 9, Train_Loss: 0.3947741389274597, Test_Loss: 0.5574896931648254\n",
      "Epoch: 9, Train_Loss: 0.39406806230545044, Test_Loss: 0.41060352325439453 *\n",
      "Epoch: 9, Train_Loss: 0.39421364665031433, Test_Loss: 0.4110374450683594\n",
      "Epoch: 9, Train_Loss: 0.4041239321231842, Test_Loss: 0.4112258851528168\n",
      "Epoch: 9, Train_Loss: 0.474725604057312, Test_Loss: 0.4114818871021271\n",
      "Epoch: 9, Train_Loss: 0.46269989013671875, Test_Loss: 0.43116477131843567\n",
      "Epoch: 9, Train_Loss: 0.4735661447048187, Test_Loss: 5.513692378997803\n",
      "Epoch: 9, Train_Loss: 0.424413800239563, Test_Loss: 0.7699857950210571 *\n",
      "Epoch: 9, Train_Loss: 0.39735153317451477, Test_Loss: 0.4065393805503845 *\n",
      "Epoch: 9, Train_Loss: 0.5913695693016052, Test_Loss: 0.39665427803993225 *\n",
      "Epoch: 9, Train_Loss: 0.598894476890564, Test_Loss: 0.3976116180419922\n",
      "Epoch: 9, Train_Loss: 0.5764819383621216, Test_Loss: 0.40205585956573486\n",
      "Epoch: 9, Train_Loss: 0.4773935079574585, Test_Loss: 0.39892342686653137 *\n",
      "Epoch: 9, Train_Loss: 0.3941137492656708, Test_Loss: 0.4140646457672119\n",
      "Epoch: 9, Train_Loss: 0.39317893981933594, Test_Loss: 0.39720603823661804 *\n",
      "Epoch: 9, Train_Loss: 0.3954889476299286, Test_Loss: 0.3976758122444153\n",
      "Epoch: 9, Train_Loss: 0.39994919300079346, Test_Loss: 0.40758997201919556\n",
      "Epoch: 9, Train_Loss: 0.411423921585083, Test_Loss: 0.42787984013557434\n",
      "Epoch: 9, Train_Loss: 0.40216949582099915, Test_Loss: 0.3996739685535431 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 0.39247316122055054, Test_Loss: 0.40455499291419983\n",
      "Epoch: 9, Train_Loss: 0.39256560802459717, Test_Loss: 0.40505409240722656\n",
      "Epoch: 9, Train_Loss: 0.4038604199886322, Test_Loss: 0.4018900990486145 *\n",
      "Epoch: 9, Train_Loss: 0.4647067189216614, Test_Loss: 0.3965272307395935 *\n",
      "Epoch: 9, Train_Loss: 0.6018416285514832, Test_Loss: 0.39908796548843384\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 0.5400250554084778, Test_Loss: 0.40947648882865906\n",
      "Epoch: 9, Train_Loss: 0.49738484621047974, Test_Loss: 0.40168359875679016 *\n",
      "Epoch: 9, Train_Loss: 0.5019646286964417, Test_Loss: 0.39639249444007874 *\n",
      "Epoch: 9, Train_Loss: 0.5289030075073242, Test_Loss: 0.39969730377197266\n",
      "Epoch: 9, Train_Loss: 0.44360873103141785, Test_Loss: 0.41494423151016235\n",
      "Epoch: 9, Train_Loss: 0.5263552665710449, Test_Loss: 0.4209345579147339\n",
      "Epoch: 9, Train_Loss: 0.5037333965301514, Test_Loss: 0.41557878255844116 *\n",
      "Epoch: 9, Train_Loss: 0.6638461947441101, Test_Loss: 0.40661391615867615 *\n",
      "Epoch: 9, Train_Loss: 0.4014627933502197, Test_Loss: 0.4013504683971405 *\n",
      "Epoch: 9, Train_Loss: 0.4846268594264984, Test_Loss: 0.401986688375473\n",
      "Epoch: 9, Train_Loss: 3.3374695777893066, Test_Loss: 0.4015922546386719 *\n",
      "Epoch: 9, Train_Loss: 0.531921923160553, Test_Loss: 0.40182459354400635\n",
      "Epoch: 9, Train_Loss: 0.4312731623649597, Test_Loss: 0.4622756242752075\n",
      "Epoch: 9, Train_Loss: 0.43046873807907104, Test_Loss: 1.211655616760254\n",
      "Epoch: 9, Train_Loss: 0.4179595112800598, Test_Loss: 5.040328502655029\n",
      "Epoch: 9, Train_Loss: 0.3974796533584595, Test_Loss: 0.3998042047023773 *\n",
      "Epoch: 9, Train_Loss: 0.3993847072124481, Test_Loss: 0.3914330303668976 *\n",
      "Epoch: 9, Train_Loss: 0.4728214144706726, Test_Loss: 0.42839041352272034\n",
      "Epoch: 9, Train_Loss: 0.5303431153297424, Test_Loss: 0.41091305017471313 *\n",
      "Epoch: 9, Train_Loss: 0.48466742038726807, Test_Loss: 0.42791059613227844\n",
      "Epoch: 9, Train_Loss: 0.4489505887031555, Test_Loss: 0.40172240138053894 *\n",
      "Epoch: 9, Train_Loss: 0.43240487575531006, Test_Loss: 0.5050218105316162\n",
      "Epoch: 9, Train_Loss: 0.4045000374317169, Test_Loss: 0.41349735856056213 *\n",
      "Epoch: 9, Train_Loss: 0.4186186194419861, Test_Loss: 0.39496341347694397 *\n",
      "Epoch: 9, Train_Loss: 0.4013155996799469, Test_Loss: 0.42155951261520386\n",
      "Epoch: 9, Train_Loss: 0.431124746799469, Test_Loss: 0.41269221901893616 *\n",
      "Epoch: 9, Train_Loss: 0.40828999876976013, Test_Loss: 0.39434462785720825 *\n",
      "Epoch: 9, Train_Loss: 0.3892514407634735, Test_Loss: 0.44666826725006104\n",
      "Epoch: 9, Train_Loss: 0.41097211837768555, Test_Loss: 0.4551360011100769\n",
      "Epoch: 9, Train_Loss: 0.4327087998390198, Test_Loss: 0.4439552426338196 *\n",
      "Epoch: 9, Train_Loss: 0.42169883847236633, Test_Loss: 0.4726513922214508\n",
      "Epoch: 9, Train_Loss: 0.3898877799510956, Test_Loss: 0.41729557514190674 *\n",
      "Epoch: 9, Train_Loss: 0.38875338435173035, Test_Loss: 0.44250285625457764\n",
      "Epoch: 9, Train_Loss: 0.3878719210624695, Test_Loss: 0.39405301213264465 *\n",
      "Epoch: 9, Train_Loss: 0.3882487714290619, Test_Loss: 0.3942382335662842\n",
      "Epoch: 9, Train_Loss: 0.3899766206741333, Test_Loss: 0.40464329719543457\n",
      "Epoch: 9, Train_Loss: 0.39128655195236206, Test_Loss: 0.40678244829177856\n",
      "Epoch: 9, Train_Loss: 0.3920544683933258, Test_Loss: 0.40452244877815247 *\n",
      "Epoch: 9, Train_Loss: 0.39003390073776245, Test_Loss: 0.4027179479598999 *\n",
      "Epoch: 9, Train_Loss: 0.38801372051239014, Test_Loss: 0.3964200019836426 *\n",
      "Epoch: 9, Train_Loss: 0.3883489966392517, Test_Loss: 0.3919029235839844 *\n",
      "Epoch: 9, Train_Loss: 0.39979317784309387, Test_Loss: 0.40678587555885315\n",
      "Epoch: 9, Train_Loss: 0.40615469217300415, Test_Loss: 0.3920033276081085 *\n",
      "Epoch: 9, Train_Loss: 0.4140651822090149, Test_Loss: 0.4026738405227661\n",
      "Epoch: 9, Train_Loss: 0.4120880663394928, Test_Loss: 0.45458295941352844\n",
      "Epoch: 9, Train_Loss: 0.3970882296562195, Test_Loss: 0.4054209887981415 *\n",
      "Epoch: 9, Train_Loss: 0.40263330936431885, Test_Loss: 0.748404860496521\n",
      "Epoch: 9, Train_Loss: 0.3871096968650818, Test_Loss: 0.8604834079742432\n",
      "Epoch: 9, Train_Loss: 0.3879173696041107, Test_Loss: 0.5621392726898193 *\n",
      "Epoch: 9, Train_Loss: 0.4068926274776459, Test_Loss: 0.41449153423309326 *\n",
      "Epoch: 9, Train_Loss: 0.4094618260860443, Test_Loss: 0.4110003113746643 *\n",
      "Epoch: 9, Train_Loss: 0.38878631591796875, Test_Loss: 0.4063636362552643 *\n",
      "Epoch: 9, Train_Loss: 0.3916253447532654, Test_Loss: 0.5102023482322693\n",
      "Epoch: 9, Train_Loss: 0.39058345556259155, Test_Loss: 0.9483070373535156\n",
      "Epoch: 10, Train_Loss: 0.4210570752620697, Test_Loss: 1.0091577768325806 *\n",
      "Epoch: 10, Train_Loss: 0.4259145259857178, Test_Loss: 0.43983209133148193 *\n",
      "Epoch: 10, Train_Loss: 0.45186564326286316, Test_Loss: 0.45871832966804504\n",
      "Epoch: 10, Train_Loss: 0.3963637948036194, Test_Loss: 0.38841870427131653 *\n",
      "Epoch: 10, Train_Loss: 0.39806729555130005, Test_Loss: 0.39206305146217346\n",
      "Epoch: 10, Train_Loss: 0.4595825672149658, Test_Loss: 0.39382484555244446\n",
      "Epoch: 10, Train_Loss: 0.38890331983566284, Test_Loss: 0.39645230770111084\n",
      "Epoch: 10, Train_Loss: 0.3985415995121002, Test_Loss: 0.4277275800704956\n",
      "Epoch: 10, Train_Loss: 0.4206097722053528, Test_Loss: 0.40076401829719543 *\n",
      "Epoch: 10, Train_Loss: 0.4315676689147949, Test_Loss: 0.3952016532421112 *\n",
      "Epoch: 10, Train_Loss: 0.5109585523605347, Test_Loss: 0.4960746467113495\n",
      "Epoch: 10, Train_Loss: 0.4465668797492981, Test_Loss: 0.7913786172866821\n",
      "Epoch: 10, Train_Loss: 0.4071846604347229, Test_Loss: 0.6079419851303101 *\n",
      "Epoch: 10, Train_Loss: 0.3908567428588867, Test_Loss: 0.42835527658462524 *\n",
      "Epoch: 10, Train_Loss: 0.4099258482456207, Test_Loss: 0.3993045389652252 *\n",
      "Epoch: 10, Train_Loss: 0.3865036070346832, Test_Loss: 0.3988147974014282 *\n",
      "Epoch: 10, Train_Loss: 0.39036449790000916, Test_Loss: 0.39920660853385925\n",
      "Epoch: 10, Train_Loss: 0.39331233501434326, Test_Loss: 0.39851176738739014 *\n",
      "Epoch: 10, Train_Loss: 0.39989760518074036, Test_Loss: 0.5633686184883118\n",
      "Epoch: 10, Train_Loss: 0.4458782374858856, Test_Loss: 5.675099849700928\n",
      "Epoch: 10, Train_Loss: 0.441900372505188, Test_Loss: 0.4811403155326843 *\n",
      "Epoch: 10, Train_Loss: 0.4227944612503052, Test_Loss: 0.3943222761154175 *\n",
      "Epoch: 10, Train_Loss: 0.4106242060661316, Test_Loss: 0.38753601908683777 *\n",
      "Epoch: 10, Train_Loss: 0.40680527687072754, Test_Loss: 0.38924092054367065\n",
      "Epoch: 10, Train_Loss: 0.4012667238712311, Test_Loss: 0.38949358463287354\n",
      "Epoch: 10, Train_Loss: 0.5427813529968262, Test_Loss: 0.3878079354763031 *\n",
      "Epoch: 10, Train_Loss: 0.5571597814559937, Test_Loss: 0.3955846130847931\n",
      "Epoch: 10, Train_Loss: 0.3861372172832489, Test_Loss: 0.38611096143722534 *\n",
      "Epoch: 10, Train_Loss: 0.4169120490550995, Test_Loss: 0.38501283526420593 *\n",
      "Epoch: 10, Train_Loss: 0.3851628601551056, Test_Loss: 0.39433541893959045\n",
      "Epoch: 10, Train_Loss: 0.3853243887424469, Test_Loss: 0.4129192531108856\n",
      "Epoch: 10, Train_Loss: 0.383019357919693, Test_Loss: 0.39423877000808716 *\n",
      "Epoch: 10, Train_Loss: 0.384716659784317, Test_Loss: 0.3972265124320984\n",
      "Epoch: 10, Train_Loss: 0.3917313516139984, Test_Loss: 0.3996651768684387\n",
      "Epoch: 10, Train_Loss: 0.4009881317615509, Test_Loss: 0.3844524621963501 *\n",
      "Epoch: 10, Train_Loss: 0.3914736807346344, Test_Loss: 0.3847332000732422\n",
      "Epoch: 10, Train_Loss: 0.39756062626838684, Test_Loss: 0.38255807757377625 *\n",
      "Epoch: 10, Train_Loss: 0.39268213510513306, Test_Loss: 0.39797937870025635\n",
      "Epoch: 10, Train_Loss: 0.38214948773384094, Test_Loss: 0.38280394673347473 *\n",
      "Epoch: 10, Train_Loss: 0.3829784691333771, Test_Loss: 0.3852754533290863\n",
      "Epoch: 10, Train_Loss: 0.38273805379867554, Test_Loss: 0.38227322697639465 *\n",
      "Epoch: 10, Train_Loss: 0.4099392294883728, Test_Loss: 0.3930862247943878\n",
      "Epoch: 10, Train_Loss: 0.40818846225738525, Test_Loss: 0.3950711488723755\n",
      "Epoch: 10, Train_Loss: 0.3989734649658203, Test_Loss: 0.39232948422431946 *\n",
      "Epoch: 10, Train_Loss: 0.3957390785217285, Test_Loss: 0.3861493170261383 *\n",
      "Epoch: 10, Train_Loss: 0.4437462091445923, Test_Loss: 0.3854333460330963 *\n",
      "Epoch: 10, Train_Loss: 0.4185399115085602, Test_Loss: 0.38540834188461304 *\n",
      "Epoch: 10, Train_Loss: 0.3920303285121918, Test_Loss: 0.3833940327167511 *\n",
      "Epoch: 10, Train_Loss: 0.4027613401412964, Test_Loss: 0.3907631039619446\n",
      "Epoch: 10, Train_Loss: 0.40047377347946167, Test_Loss: 0.4463680386543274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.38930946588516235, Test_Loss: 2.440291166305542\n",
      "Epoch: 10, Train_Loss: 0.38770177960395813, Test_Loss: 3.855689525604248\n",
      "Epoch: 10, Train_Loss: 0.4077349305152893, Test_Loss: 0.3880726993083954 *\n",
      "Epoch: 10, Train_Loss: 0.43124401569366455, Test_Loss: 0.38019758462905884 *\n",
      "Epoch: 10, Train_Loss: 2.627474784851074, Test_Loss: 0.42037057876586914\n",
      "Epoch: 10, Train_Loss: 3.4360156059265137, Test_Loss: 0.3967232406139374 *\n",
      "Epoch: 10, Train_Loss: 0.4073995053768158, Test_Loss: 0.4151219427585602\n",
      "Epoch: 10, Train_Loss: 0.38702020049095154, Test_Loss: 0.41860416531562805\n",
      "Epoch: 10, Train_Loss: 0.4390851557254791, Test_Loss: 0.5008223652839661\n",
      "Epoch: 10, Train_Loss: 0.5471526384353638, Test_Loss: 0.38560691475868225 *\n",
      "Epoch: 10, Train_Loss: 0.4106496274471283, Test_Loss: 0.3961904048919678\n",
      "Epoch: 10, Train_Loss: 0.3844909965991974, Test_Loss: 0.40555453300476074\n",
      "Epoch: 10, Train_Loss: 0.3923545777797699, Test_Loss: 0.3966571092605591 *\n",
      "Epoch: 10, Train_Loss: 0.44726231694221497, Test_Loss: 0.3876873254776001 *\n",
      "Epoch: 10, Train_Loss: 0.39077338576316833, Test_Loss: 0.435443639755249\n",
      "Epoch: 10, Train_Loss: 0.3907685875892639, Test_Loss: 0.42944881319999695 *\n",
      "Epoch: 10, Train_Loss: 1.1480516195297241, Test_Loss: 0.4529970586299896\n",
      "Epoch: 10, Train_Loss: 1.4444520473480225, Test_Loss: 0.4645318388938904\n",
      "Epoch: 10, Train_Loss: 0.8713570237159729, Test_Loss: 0.39975854754447937 *\n",
      "Epoch: 10, Train_Loss: 0.4743962287902832, Test_Loss: 0.41350674629211426\n",
      "Epoch: 10, Train_Loss: 1.2940781116485596, Test_Loss: 0.38034215569496155 *\n",
      "Epoch: 10, Train_Loss: 2.2106471061706543, Test_Loss: 0.38216185569763184\n",
      "Epoch: 10, Train_Loss: 0.5512935519218445, Test_Loss: 0.3840937614440918\n",
      "Epoch: 10, Train_Loss: 0.3848884701728821, Test_Loss: 0.39315974712371826\n",
      "Epoch: 10, Train_Loss: 0.42544272541999817, Test_Loss: 0.387774795293808 *\n",
      "Epoch: 10, Train_Loss: 1.187978982925415, Test_Loss: 0.38267982006073 *\n",
      "Epoch: 10, Train_Loss: 1.1559467315673828, Test_Loss: 0.3824787139892578 *\n",
      "Epoch: 10, Train_Loss: 0.42688193917274475, Test_Loss: 0.38177111744880676 *\n",
      "Epoch: 10, Train_Loss: 0.38844966888427734, Test_Loss: 0.3815186619758606 *\n",
      "Epoch: 10, Train_Loss: 0.3843628764152527, Test_Loss: 0.4111382067203522\n",
      "Epoch: 10, Train_Loss: 0.7735424041748047, Test_Loss: 0.3793806731700897 *\n",
      "Epoch: 10, Train_Loss: 0.42301690578460693, Test_Loss: 0.38869476318359375\n",
      "Epoch: 10, Train_Loss: 0.4251733720302582, Test_Loss: 0.43365341424942017\n",
      "Epoch: 10, Train_Loss: 0.42279699444770813, Test_Loss: 0.6658609509468079\n",
      "Epoch: 10, Train_Loss: 0.3940209448337555, Test_Loss: 0.5878657698631287 *\n",
      "Epoch: 10, Train_Loss: 0.46376919746398926, Test_Loss: 0.45860645174980164 *\n",
      "Epoch: 10, Train_Loss: 0.5080602169036865, Test_Loss: 0.39526864886283875 *\n",
      "Epoch: 10, Train_Loss: 0.5934402942657471, Test_Loss: 0.43005234003067017\n",
      "Epoch: 10, Train_Loss: 0.4357454180717468, Test_Loss: 0.4268929958343506 *\n",
      "Epoch: 10, Train_Loss: 0.45053595304489136, Test_Loss: 0.45979589223861694\n",
      "Epoch: 10, Train_Loss: 0.4126577079296112, Test_Loss: 0.41554713249206543 *\n",
      "Epoch: 10, Train_Loss: 0.4686330556869507, Test_Loss: 0.6324735879898071\n",
      "Epoch: 10, Train_Loss: 0.49457499384880066, Test_Loss: 0.4470866918563843 *\n",
      "Epoch: 10, Train_Loss: 0.49325042963027954, Test_Loss: 0.44168031215667725 *\n",
      "Epoch: 10, Train_Loss: 0.5319287180900574, Test_Loss: 0.41220641136169434 *\n",
      "Epoch: 10, Train_Loss: 0.4967118799686432, Test_Loss: 0.3833821713924408 *\n",
      "Epoch: 10, Train_Loss: 0.42472678422927856, Test_Loss: 0.3991689383983612\n",
      "Epoch: 10, Train_Loss: 0.4097534120082855, Test_Loss: 0.5507377982139587\n",
      "Epoch: 10, Train_Loss: 0.3892998993396759, Test_Loss: 0.3917520046234131 *\n",
      "Epoch: 10, Train_Loss: 0.3833756446838379, Test_Loss: 0.48702287673950195\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.3820247948169708, Test_Loss: 0.498205304145813\n",
      "Epoch: 10, Train_Loss: 0.3949548304080963, Test_Loss: 0.5758520364761353\n",
      "Epoch: 10, Train_Loss: 0.39545759558677673, Test_Loss: 0.8183287382125854\n",
      "Epoch: 10, Train_Loss: 0.38194623589515686, Test_Loss: 1.32801353931427\n",
      "Epoch: 10, Train_Loss: 0.3883359432220459, Test_Loss: 0.6656590700149536 *\n",
      "Epoch: 10, Train_Loss: 0.5394715070724487, Test_Loss: 0.6399732828140259 *\n",
      "Epoch: 10, Train_Loss: 0.6357460021972656, Test_Loss: 0.6153541207313538 *\n",
      "Epoch: 10, Train_Loss: 0.38121625781059265, Test_Loss: 0.6217946410179138\n",
      "Epoch: 10, Train_Loss: 0.43258509039878845, Test_Loss: 0.7109887599945068\n",
      "Epoch: 10, Train_Loss: 0.41374847292900085, Test_Loss: 1.0387089252471924\n",
      "Epoch: 10, Train_Loss: 0.41676339507102966, Test_Loss: 4.207620143890381\n",
      "Epoch: 10, Train_Loss: 0.5808987021446228, Test_Loss: 0.43891996145248413 *\n",
      "Epoch: 10, Train_Loss: 0.41100507974624634, Test_Loss: 0.42062151432037354 *\n",
      "Epoch: 10, Train_Loss: 0.5140040516853333, Test_Loss: 0.4045253098011017 *\n",
      "Epoch: 10, Train_Loss: 0.6352907419204712, Test_Loss: 0.3889431059360504 *\n",
      "Epoch: 10, Train_Loss: 0.5620837211608887, Test_Loss: 0.4145483672618866\n",
      "Epoch: 10, Train_Loss: 0.3957260847091675, Test_Loss: 0.44907858967781067\n",
      "Epoch: 10, Train_Loss: 0.3858036398887634, Test_Loss: 0.45426398515701294\n",
      "Epoch: 10, Train_Loss: 0.47142648696899414, Test_Loss: 0.3830234110355377 *\n",
      "Epoch: 10, Train_Loss: 0.7585301995277405, Test_Loss: 0.42121830582618713\n",
      "Epoch: 10, Train_Loss: 0.769262433052063, Test_Loss: 0.4225265681743622\n",
      "Epoch: 10, Train_Loss: 0.4074302911758423, Test_Loss: 0.5370458960533142\n",
      "Epoch: 10, Train_Loss: 0.41850560903549194, Test_Loss: 0.39954471588134766 *\n",
      "Epoch: 10, Train_Loss: 0.38142484426498413, Test_Loss: 0.411749929189682\n",
      "Epoch: 10, Train_Loss: 0.4296706020832062, Test_Loss: 0.4237944781780243\n",
      "Epoch: 10, Train_Loss: 0.7016419768333435, Test_Loss: 0.39824774861335754 *\n",
      "Epoch: 10, Train_Loss: 0.38164031505584717, Test_Loss: 0.39262983202934265 *\n",
      "Epoch: 10, Train_Loss: 0.5460872650146484, Test_Loss: 0.4345945119857788\n",
      "Epoch: 10, Train_Loss: 0.4269635081291199, Test_Loss: 0.47945499420166016\n",
      "Epoch: 10, Train_Loss: 0.41287466883659363, Test_Loss: 0.3869830369949341 *\n",
      "Epoch: 10, Train_Loss: 0.3951636254787445, Test_Loss: 0.4105021357536316\n",
      "Epoch: 10, Train_Loss: 0.4903344511985779, Test_Loss: 0.3875139355659485 *\n",
      "Epoch: 10, Train_Loss: 0.4567753076553345, Test_Loss: 0.4873611629009247\n",
      "Epoch: 10, Train_Loss: 0.435417115688324, Test_Loss: 0.48804807662963867\n",
      "Epoch: 10, Train_Loss: 0.4089588224887848, Test_Loss: 0.44457346200942993 *\n",
      "Epoch: 10, Train_Loss: 0.408798485994339, Test_Loss: 0.3989625573158264 *\n",
      "Epoch: 10, Train_Loss: 0.4622485637664795, Test_Loss: 0.41320475935935974\n",
      "Epoch: 10, Train_Loss: 0.4189872145652771, Test_Loss: 0.4235440194606781\n",
      "Epoch: 10, Train_Loss: 0.4121701419353485, Test_Loss: 0.38624969124794006 *\n",
      "Epoch: 10, Train_Loss: 0.3853180706501007, Test_Loss: 0.4522949159145355\n",
      "Epoch: 10, Train_Loss: 0.4296782612800598, Test_Loss: 0.5092470645904541\n",
      "Epoch: 10, Train_Loss: 0.5891985297203064, Test_Loss: 3.6128499507904053\n",
      "Epoch: 10, Train_Loss: 0.6353652477264404, Test_Loss: 2.446962833404541 *\n",
      "Epoch: 10, Train_Loss: 0.8149848580360413, Test_Loss: 0.44056305289268494 *\n",
      "Epoch: 10, Train_Loss: 0.6483446359634399, Test_Loss: 0.4293561577796936 *\n",
      "Epoch: 10, Train_Loss: 0.6045665144920349, Test_Loss: 0.40111014246940613 *\n",
      "Epoch: 10, Train_Loss: 0.4850369095802307, Test_Loss: 0.39020243287086487 *\n",
      "Epoch: 10, Train_Loss: 0.4380204379558563, Test_Loss: 0.39135807752609253\n",
      "Epoch: 10, Train_Loss: 0.38060683012008667, Test_Loss: 0.4368950426578522\n",
      "Epoch: 10, Train_Loss: 0.3825022280216217, Test_Loss: 0.42699551582336426 *\n",
      "Epoch: 10, Train_Loss: 0.4445648789405823, Test_Loss: 0.37634143233299255 *\n",
      "Epoch: 10, Train_Loss: 0.644865870475769, Test_Loss: 0.4195244610309601\n",
      "Epoch: 10, Train_Loss: 0.7716240286827087, Test_Loss: 0.4521239101886749\n",
      "Epoch: 10, Train_Loss: 1.1129494905471802, Test_Loss: 0.48188507556915283\n",
      "Epoch: 10, Train_Loss: 1.4399882555007935, Test_Loss: 0.40817952156066895 *\n",
      "Epoch: 10, Train_Loss: 0.5539441108703613, Test_Loss: 0.4130983054637909\n",
      "Epoch: 10, Train_Loss: 0.6354343295097351, Test_Loss: 0.4426935315132141\n",
      "Epoch: 10, Train_Loss: 0.38118821382522583, Test_Loss: 0.4441731572151184\n",
      "Epoch: 10, Train_Loss: 0.4093136489391327, Test_Loss: 0.40045425295829773 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.7322404384613037, Test_Loss: 0.5443276166915894\n",
      "Epoch: 10, Train_Loss: 1.4831554889678955, Test_Loss: 0.42182812094688416 *\n",
      "Epoch: 10, Train_Loss: 0.4391346573829651, Test_Loss: 0.3988994359970093 *\n",
      "Epoch: 10, Train_Loss: 0.4052496552467346, Test_Loss: 0.40872249007225037\n",
      "Epoch: 10, Train_Loss: 0.4196779727935791, Test_Loss: 0.410053551197052\n",
      "Epoch: 10, Train_Loss: 0.5179191827774048, Test_Loss: 0.39031678438186646 *\n",
      "Epoch: 10, Train_Loss: 0.6677206754684448, Test_Loss: 0.39893096685409546\n",
      "Epoch: 10, Train_Loss: 0.6074848771095276, Test_Loss: 0.4170406758785248\n",
      "Epoch: 10, Train_Loss: 0.5265012979507446, Test_Loss: 0.42234012484550476\n",
      "Epoch: 10, Train_Loss: 0.6643010377883911, Test_Loss: 0.4411905109882355\n",
      "Epoch: 10, Train_Loss: 0.3832673132419586, Test_Loss: 0.41945314407348633 *\n",
      "Epoch: 10, Train_Loss: 0.3898864984512329, Test_Loss: 0.4566090703010559\n",
      "Epoch: 10, Train_Loss: 0.4021008610725403, Test_Loss: 0.3926710784435272 *\n",
      "Epoch: 10, Train_Loss: 0.44969314336776733, Test_Loss: 0.38916730880737305 *\n",
      "Epoch: 10, Train_Loss: 0.3712063729763031, Test_Loss: 0.44780346751213074\n",
      "Epoch: 10, Train_Loss: 0.4081638753414154, Test_Loss: 0.5351706743240356\n",
      "Epoch: 10, Train_Loss: 16.04597282409668, Test_Loss: 0.5066879987716675 *\n",
      "Epoch: 10, Train_Loss: 0.5395172238349915, Test_Loss: 0.4008769989013672 *\n",
      "Epoch: 10, Train_Loss: 1.5269161462783813, Test_Loss: 0.41140928864479065\n",
      "Epoch: 10, Train_Loss: 1.362734317779541, Test_Loss: 0.4477575123310089\n",
      "Epoch: 10, Train_Loss: 0.4060807228088379, Test_Loss: 0.5194522738456726\n",
      "Epoch: 10, Train_Loss: 0.5003741979598999, Test_Loss: 0.5758907794952393\n",
      "Epoch: 10, Train_Loss: 1.939471960067749, Test_Loss: 0.4814985394477844 *\n",
      "Epoch: 10, Train_Loss: 7.317859172821045, Test_Loss: 0.992704451084137\n",
      "Epoch: 10, Train_Loss: 0.6065640449523926, Test_Loss: 0.6148449778556824 *\n",
      "Epoch: 10, Train_Loss: 0.5041030049324036, Test_Loss: 0.5722266435623169 *\n",
      "Epoch: 10, Train_Loss: 4.119872570037842, Test_Loss: 0.5425069332122803 *\n",
      "Epoch: 10, Train_Loss: 0.9263812303543091, Test_Loss: 0.7134568691253662\n",
      "Epoch: 10, Train_Loss: 0.6913809776306152, Test_Loss: 1.0448919534683228\n",
      "Epoch: 10, Train_Loss: 0.39264318346977234, Test_Loss: 0.7207601070404053 *\n",
      "Epoch: 10, Train_Loss: 0.4574722647666931, Test_Loss: 1.4757416248321533\n",
      "Epoch: 10, Train_Loss: 0.461405873298645, Test_Loss: 1.100887417793274 *\n",
      "Epoch: 10, Train_Loss: 0.3895547389984131, Test_Loss: 1.6110799312591553\n",
      "Epoch: 10, Train_Loss: 0.4246675670146942, Test_Loss: 2.142531156539917\n",
      "Epoch: 10, Train_Loss: 0.36745485663414, Test_Loss: 1.0716907978057861 *\n",
      "Epoch: 10, Train_Loss: 0.3681883215904236, Test_Loss: 0.8705781698226929 *\n",
      "Epoch: 10, Train_Loss: 0.4604206085205078, Test_Loss: 1.3129366636276245\n",
      "Epoch: 10, Train_Loss: 0.45503681898117065, Test_Loss: 1.1644716262817383 *\n",
      "Epoch: 10, Train_Loss: 0.47228190302848816, Test_Loss: 0.941741943359375 *\n",
      "Epoch: 10, Train_Loss: 0.4918758273124695, Test_Loss: 0.7341288328170776 *\n",
      "Epoch: 10, Train_Loss: 0.3902907073497772, Test_Loss: 0.42668387293815613 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.38527339696884155, Test_Loss: 2.978215456008911\n",
      "Epoch: 10, Train_Loss: 0.37936848402023315, Test_Loss: 4.448037147521973\n",
      "Epoch: 10, Train_Loss: 0.3693937361240387, Test_Loss: 0.6951687335968018 *\n",
      "Epoch: 10, Train_Loss: 0.37222084403038025, Test_Loss: 0.7687062621116638\n",
      "Epoch: 10, Train_Loss: 0.3682175576686859, Test_Loss: 0.7619675397872925 *\n",
      "Epoch: 10, Train_Loss: 0.365688294172287, Test_Loss: 0.49869978427886963 *\n",
      "Epoch: 10, Train_Loss: 0.36546897888183594, Test_Loss: 0.741338849067688\n",
      "Epoch: 10, Train_Loss: 0.3664671778678894, Test_Loss: 0.8252482414245605\n",
      "Epoch: 10, Train_Loss: 0.36588340997695923, Test_Loss: 0.6426604986190796 *\n",
      "Epoch: 10, Train_Loss: 0.36539551615715027, Test_Loss: 0.47307097911834717 *\n",
      "Epoch: 10, Train_Loss: 0.3652299642562866, Test_Loss: 0.5707268714904785\n",
      "Epoch: 10, Train_Loss: 0.369728684425354, Test_Loss: 0.47422367334365845 *\n",
      "Epoch: 10, Train_Loss: 0.39794671535491943, Test_Loss: 0.761654257774353\n",
      "Epoch: 10, Train_Loss: 0.39673835039138794, Test_Loss: 0.5245764851570129 *\n",
      "Epoch: 10, Train_Loss: 0.44627881050109863, Test_Loss: 0.6687938570976257\n",
      "Epoch: 10, Train_Loss: 0.3990015983581543, Test_Loss: 0.6132326126098633 *\n",
      "Epoch: 10, Train_Loss: 0.5474472641944885, Test_Loss: 0.4312053620815277 *\n",
      "Epoch: 10, Train_Loss: 8.345808982849121, Test_Loss: 0.4513195753097534\n",
      "Epoch: 10, Train_Loss: 0.40944281220436096, Test_Loss: 0.4458150863647461 *\n",
      "Epoch: 10, Train_Loss: 0.41545990109443665, Test_Loss: 0.7021774053573608\n",
      "Epoch: 10, Train_Loss: 0.5186564922332764, Test_Loss: 0.44556617736816406 *\n",
      "Epoch: 10, Train_Loss: 0.5606216788291931, Test_Loss: 0.4773198664188385\n",
      "Epoch: 10, Train_Loss: 0.42945510149002075, Test_Loss: 0.5414698719978333\n",
      "Epoch: 10, Train_Loss: 0.45096611976623535, Test_Loss: 0.7051277160644531\n",
      "Epoch: 10, Train_Loss: 0.4857000708580017, Test_Loss: 0.7433304786682129\n",
      "Epoch: 10, Train_Loss: 0.5462531447410583, Test_Loss: 0.5451945662498474 *\n",
      "Epoch: 10, Train_Loss: 0.47876524925231934, Test_Loss: 0.46221110224723816 *\n",
      "Epoch: 10, Train_Loss: 0.4363357424736023, Test_Loss: 0.489885151386261\n",
      "Epoch: 10, Train_Loss: 0.3678227365016937, Test_Loss: 0.47702425718307495 *\n",
      "Epoch: 10, Train_Loss: 0.5023851990699768, Test_Loss: 0.4180775582790375 *\n",
      "Epoch: 10, Train_Loss: 0.457825243473053, Test_Loss: 0.5642049312591553\n",
      "Epoch: 10, Train_Loss: 0.6522501707077026, Test_Loss: 0.3992418944835663 *\n",
      "Epoch: 10, Train_Loss: 0.4977914094924927, Test_Loss: 4.669163227081299\n",
      "Epoch: 10, Train_Loss: 0.47424858808517456, Test_Loss: 1.73475980758667 *\n",
      "Epoch: 10, Train_Loss: 0.3764488697052002, Test_Loss: 0.36553090810775757 *\n",
      "Epoch: 10, Train_Loss: 0.3981519043445587, Test_Loss: 0.3753994405269623\n",
      "Epoch: 10, Train_Loss: 0.42258477210998535, Test_Loss: 0.37886402010917664\n",
      "Epoch: 10, Train_Loss: 0.38777288794517517, Test_Loss: 0.38269296288490295\n",
      "Epoch: 10, Train_Loss: 0.3652035892009735, Test_Loss: 0.3768615424633026 *\n",
      "Epoch: 10, Train_Loss: 0.3626348376274109, Test_Loss: 0.429485946893692\n",
      "Epoch: 10, Train_Loss: 0.3674805760383606, Test_Loss: 0.44228795170783997\n",
      "Epoch: 10, Train_Loss: 0.9441529512405396, Test_Loss: 0.36582431197166443 *\n",
      "Epoch: 10, Train_Loss: 5.573177814483643, Test_Loss: 0.39804673194885254\n",
      "Epoch: 10, Train_Loss: 0.3648748993873596, Test_Loss: 0.3808768093585968 *\n",
      "Epoch: 10, Train_Loss: 0.36481836438179016, Test_Loss: 0.37518733739852905 *\n",
      "Epoch: 10, Train_Loss: 0.367394357919693, Test_Loss: 0.3669382631778717 *\n",
      "Epoch: 10, Train_Loss: 0.3654400706291199, Test_Loss: 0.40966734290122986\n",
      "Epoch: 10, Train_Loss: 0.36293816566467285, Test_Loss: 0.4039754867553711 *\n",
      "Epoch: 10, Train_Loss: 0.3626055419445038, Test_Loss: 0.4799025058746338\n",
      "Epoch: 10, Train_Loss: 0.3700939416885376, Test_Loss: 0.44091132283210754 *\n",
      "Epoch: 10, Train_Loss: 0.41650888323783875, Test_Loss: 0.3943183422088623 *\n",
      "Epoch: 10, Train_Loss: 0.3782457113265991, Test_Loss: 0.369811475276947 *\n",
      "Epoch: 10, Train_Loss: 0.3724428415298462, Test_Loss: 0.3628927171230316 *\n",
      "Epoch: 10, Train_Loss: 0.36215272545814514, Test_Loss: 0.3643926680088043\n",
      "Epoch: 10, Train_Loss: 0.361393004655838, Test_Loss: 0.3635674715042114 *\n",
      "Epoch: 10, Train_Loss: 0.37675952911376953, Test_Loss: 0.3649258315563202\n",
      "Epoch: 10, Train_Loss: 0.36275067925453186, Test_Loss: 0.3649708330631256\n",
      "Epoch: 10, Train_Loss: 0.36341431736946106, Test_Loss: 0.3645149767398834 *\n",
      "Epoch: 10, Train_Loss: 0.38435232639312744, Test_Loss: 0.3655978739261627\n",
      "Epoch: 10, Train_Loss: 0.392667680978775, Test_Loss: 0.3646409511566162 *\n",
      "Epoch: 10, Train_Loss: 0.37138664722442627, Test_Loss: 0.36733344197273254\n",
      "Epoch: 10, Train_Loss: 0.3603748083114624, Test_Loss: 0.37638431787490845\n",
      "Epoch: 10, Train_Loss: 0.3658660352230072, Test_Loss: 0.3754896819591522 *\n",
      "Epoch: 10, Train_Loss: 0.4378761053085327, Test_Loss: 0.3883860409259796\n",
      "Epoch: 10, Train_Loss: 0.38499873876571655, Test_Loss: 0.48905038833618164\n",
      "Epoch: 10, Train_Loss: 0.40470200777053833, Test_Loss: 0.645782470703125\n",
      "Epoch: 10, Train_Loss: 0.39360013604164124, Test_Loss: 0.5712323188781738 *\n",
      "Epoch: 10, Train_Loss: 0.4465954303741455, Test_Loss: 0.4311378598213196 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.3968644142150879, Test_Loss: 0.36482417583465576 *\n",
      "Epoch: 10, Train_Loss: 0.4223812520503998, Test_Loss: 0.38311150670051575\n",
      "Epoch: 10, Train_Loss: 0.4072960913181305, Test_Loss: 0.4535979628562927\n",
      "Epoch: 10, Train_Loss: 0.5951247811317444, Test_Loss: 0.7524302005767822\n",
      "Epoch: 10, Train_Loss: 0.4095066785812378, Test_Loss: 1.1070160865783691\n",
      "Epoch: 10, Train_Loss: 0.36731988191604614, Test_Loss: 0.724274754524231 *\n",
      "Epoch: 10, Train_Loss: 0.35995468497276306, Test_Loss: 0.419650673866272 *\n",
      "Epoch: 10, Train_Loss: 0.35892656445503235, Test_Loss: 0.3765355944633484 *\n",
      "Epoch: 10, Train_Loss: 0.3588324785232544, Test_Loss: 0.3766213655471802\n",
      "Epoch: 10, Train_Loss: 0.3596131205558777, Test_Loss: 0.3711853623390198 *\n",
      "Epoch: 10, Train_Loss: 0.9228906631469727, Test_Loss: 0.374377578496933\n",
      "Epoch: 10, Train_Loss: 4.6231818199157715, Test_Loss: 0.3810693025588989\n",
      "Epoch: 10, Train_Loss: 0.3683292269706726, Test_Loss: 0.3996824324131012\n",
      "Epoch: 10, Train_Loss: 0.3703286945819855, Test_Loss: 0.36310163140296936 *\n",
      "Epoch: 10, Train_Loss: 0.369578093290329, Test_Loss: 0.45225989818573\n",
      "Epoch: 10, Train_Loss: 0.35893094539642334, Test_Loss: 0.6583821773529053\n",
      "Epoch: 10, Train_Loss: 0.35917845368385315, Test_Loss: 0.5303636193275452 *\n",
      "Epoch: 10, Train_Loss: 0.35918694734573364, Test_Loss: 0.5890000462532043\n",
      "Epoch: 10, Train_Loss: 0.35802125930786133, Test_Loss: 0.37797045707702637 *\n",
      "Epoch: 10, Train_Loss: 0.3584171533584595, Test_Loss: 0.3782666325569153\n",
      "Epoch: 10, Train_Loss: 0.35879090428352356, Test_Loss: 0.3778315484523773 *\n",
      "Epoch: 10, Train_Loss: 0.4274217486381531, Test_Loss: 0.37841638922691345\n",
      "Epoch: 10, Train_Loss: 0.4197448194026947, Test_Loss: 0.39328762888908386\n",
      "Epoch: 10, Train_Loss: 0.4425835609436035, Test_Loss: 3.656325578689575\n",
      "Epoch: 10, Train_Loss: 0.39572951197624207, Test_Loss: 2.4963889122009277 *\n",
      "Epoch: 10, Train_Loss: 0.3607054650783539, Test_Loss: 0.37502720952033997 *\n",
      "Epoch: 10, Train_Loss: 0.52176433801651, Test_Loss: 0.3638668954372406 *\n",
      "Epoch: 10, Train_Loss: 0.5795172452926636, Test_Loss: 0.3667016625404358\n",
      "Epoch: 10, Train_Loss: 0.5750470161437988, Test_Loss: 0.37572181224823\n",
      "Epoch: 10, Train_Loss: 0.4960080683231354, Test_Loss: 0.3630755543708801 *\n",
      "Epoch: 10, Train_Loss: 0.358813613653183, Test_Loss: 0.3728928864002228\n",
      "Epoch: 10, Train_Loss: 0.3571896255016327, Test_Loss: 0.3635527789592743 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.3595689535140991, Test_Loss: 0.36220356822013855 *\n",
      "Epoch: 10, Train_Loss: 0.3692398965358734, Test_Loss: 0.3674377501010895\n",
      "Epoch: 10, Train_Loss: 0.3749234974384308, Test_Loss: 0.3637199103832245 *\n",
      "Epoch: 10, Train_Loss: 0.37139439582824707, Test_Loss: 0.3837229013442993\n",
      "Epoch: 10, Train_Loss: 0.36022496223449707, Test_Loss: 0.3656681180000305 *\n",
      "Epoch: 10, Train_Loss: 0.35626858472824097, Test_Loss: 0.36151501536369324 *\n",
      "Epoch: 10, Train_Loss: 0.36742085218429565, Test_Loss: 0.3706258237361908\n",
      "Epoch: 10, Train_Loss: 0.39247530698776245, Test_Loss: 0.3599916398525238 *\n",
      "Epoch: 10, Train_Loss: 0.541968584060669, Test_Loss: 0.36290714144706726\n",
      "Epoch: 10, Train_Loss: 0.4877189099788666, Test_Loss: 0.36736416816711426\n",
      "Epoch: 10, Train_Loss: 0.4755275249481201, Test_Loss: 0.3742884695529938\n",
      "Epoch: 10, Train_Loss: 0.41230860352516174, Test_Loss: 0.36090338230133057 *\n",
      "Epoch: 10, Train_Loss: 0.48450392484664917, Test_Loss: 0.36191025376319885\n",
      "Epoch: 10, Train_Loss: 0.4439566731452942, Test_Loss: 0.3752080500125885\n",
      "Epoch: 10, Train_Loss: 0.4450579881668091, Test_Loss: 0.38707640767097473\n",
      "Epoch: 10, Train_Loss: 0.4684291481971741, Test_Loss: 0.38680434226989746 *\n",
      "Epoch: 10, Train_Loss: 0.640113353729248, Test_Loss: 0.3741929233074188 *\n",
      "Epoch: 10, Train_Loss: 0.3711320459842682, Test_Loss: 0.3681606948375702 *\n",
      "Epoch: 10, Train_Loss: 0.3603132367134094, Test_Loss: 0.37037861347198486\n",
      "Epoch: 10, Train_Loss: 3.103248119354248, Test_Loss: 0.37322667241096497\n",
      "Epoch: 10, Train_Loss: 0.8072071075439453, Test_Loss: 0.3587993383407593 *\n",
      "Epoch: 10, Train_Loss: 0.39201730489730835, Test_Loss: 0.42710229754447937\n",
      "Epoch: 10, Train_Loss: 0.38537323474884033, Test_Loss: 0.3883083164691925 *\n",
      "Epoch: 10, Train_Loss: 0.3722585439682007, Test_Loss: 5.505880355834961\n",
      "Epoch: 10, Train_Loss: 0.3782903850078583, Test_Loss: 0.5955691337585449 *\n",
      "Epoch: 10, Train_Loss: 0.36562901735305786, Test_Loss: 0.3565438985824585 *\n",
      "Epoch: 10, Train_Loss: 0.4179808497428894, Test_Loss: 0.3831973373889923\n",
      "Epoch: 10, Train_Loss: 0.5080094337463379, Test_Loss: 0.386227011680603\n",
      "Epoch: 10, Train_Loss: 0.4572623372077942, Test_Loss: 0.39841216802597046\n",
      "Epoch: 10, Train_Loss: 0.4205273687839508, Test_Loss: 0.36206623911857605 *\n",
      "Epoch: 10, Train_Loss: 0.40986913442611694, Test_Loss: 0.43312111496925354\n",
      "Epoch: 10, Train_Loss: 0.3727310001850128, Test_Loss: 0.41043367981910706 *\n",
      "Epoch: 10, Train_Loss: 0.3752726912498474, Test_Loss: 0.3574846088886261 *\n",
      "Epoch: 10, Train_Loss: 0.3637390434741974, Test_Loss: 0.38559767603874207\n",
      "Epoch: 10, Train_Loss: 0.41313302516937256, Test_Loss: 0.37962543964385986 *\n",
      "Epoch: 10, Train_Loss: 0.3853086829185486, Test_Loss: 0.36023035645484924 *\n",
      "Epoch: 10, Train_Loss: 0.3558453917503357, Test_Loss: 0.3754907548427582\n",
      "Epoch: 10, Train_Loss: 0.36375755071640015, Test_Loss: 0.4440286159515381\n",
      "Epoch: 10, Train_Loss: 0.3890358805656433, Test_Loss: 0.3901755213737488 *\n",
      "Epoch: 10, Train_Loss: 0.39010825753211975, Test_Loss: 0.45295560359954834\n",
      "Epoch: 10, Train_Loss: 0.3572331368923187, Test_Loss: 0.4007924199104309 *\n",
      "Epoch: 10, Train_Loss: 0.3536316156387329, Test_Loss: 0.4077892303466797\n",
      "Epoch: 10, Train_Loss: 0.35369056463241577, Test_Loss: 0.36687028408050537 *\n",
      "Epoch: 10, Train_Loss: 0.3531215786933899, Test_Loss: 0.3651919662952423 *\n",
      "Epoch: 10, Train_Loss: 0.35416752099990845, Test_Loss: 0.37082087993621826\n",
      "Epoch: 10, Train_Loss: 0.35600677132606506, Test_Loss: 0.37204810976982117\n",
      "Epoch: 10, Train_Loss: 0.35532626509666443, Test_Loss: 0.3734011948108673\n",
      "Epoch: 10, Train_Loss: 0.3556160032749176, Test_Loss: 0.37437689304351807\n",
      "Epoch: 10, Train_Loss: 0.3531021475791931, Test_Loss: 0.36217987537384033 *\n",
      "Epoch: 10, Train_Loss: 0.35358643531799316, Test_Loss: 0.36906155943870544\n",
      "Epoch: 10, Train_Loss: 0.3577587306499481, Test_Loss: 0.37053999304771423\n",
      "Epoch: 10, Train_Loss: 0.37420421838760376, Test_Loss: 0.3606893718242645 *\n",
      "Epoch: 10, Train_Loss: 0.37240442633628845, Test_Loss: 0.36450427770614624\n",
      "Epoch: 10, Train_Loss: 0.3705326318740845, Test_Loss: 0.4061530828475952\n",
      "Epoch: 10, Train_Loss: 0.3747849762439728, Test_Loss: 0.3843657374382019 *\n",
      "Epoch: 10, Train_Loss: 0.369340717792511, Test_Loss: 0.5819311738014221\n",
      "Epoch: 10, Train_Loss: 0.3544878363609314, Test_Loss: 0.8087875247001648\n",
      "Epoch: 10, Train_Loss: 0.354709655046463, Test_Loss: 0.6074305772781372 *\n",
      "Epoch: 10, Train_Loss: 0.3700244128704071, Test_Loss: 0.4325462877750397 *\n",
      "Epoch: 10, Train_Loss: 0.3858170807361603, Test_Loss: 0.37128162384033203 *\n",
      "Epoch: 10, Train_Loss: 0.3536189794540405, Test_Loss: 0.3601055145263672 *\n",
      "Epoch: 10, Train_Loss: 0.35986340045928955, Test_Loss: 0.4393584728240967\n",
      "Epoch: 10, Train_Loss: 0.3558655083179474, Test_Loss: 0.8058805465698242\n",
      "Epoch: 10, Train_Loss: 0.37430340051651, Test_Loss: 1.088649868965149\n",
      "Epoch: 10, Train_Loss: 0.39949366450309753, Test_Loss: 0.5341788530349731 *\n",
      "Epoch: 10, Train_Loss: 0.4228300452232361, Test_Loss: 0.42711466550827026 *\n",
      "Epoch: 10, Train_Loss: 0.374146044254303, Test_Loss: 0.35539722442626953 *\n",
      "Epoch: 10, Train_Loss: 0.3529890179634094, Test_Loss: 0.3576369285583496\n",
      "Epoch: 10, Train_Loss: 0.4270942211151123, Test_Loss: 0.35445258021354675 *\n",
      "Epoch: 10, Train_Loss: 0.35986918210983276, Test_Loss: 0.3652600049972534\n",
      "Epoch: 10, Train_Loss: 0.35815203189849854, Test_Loss: 0.3730933964252472\n",
      "Epoch: 10, Train_Loss: 0.3766874074935913, Test_Loss: 0.38431987166404724\n",
      "Epoch: 10, Train_Loss: 0.3646172881126404, Test_Loss: 0.35590028762817383 *\n",
      "Epoch: 10, Train_Loss: 0.483710914850235, Test_Loss: 0.46619781851768494\n",
      "Epoch: 10, Train_Loss: 0.42146608233451843, Test_Loss: 0.7313216924667358\n",
      "Epoch: 10, Train_Loss: 0.37714844942092896, Test_Loss: 0.45612892508506775 *\n",
      "Epoch: 10, Train_Loss: 0.3605257272720337, Test_Loss: 0.535169243812561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 0.3673567771911621, Test_Loss: 0.36518990993499756 *\n",
      "Epoch: 10, Train_Loss: 0.36065685749053955, Test_Loss: 0.3656391501426697\n",
      "Epoch: 10, Train_Loss: 0.35306915640830994, Test_Loss: 0.36532390117645264 *\n",
      "Epoch: 10, Train_Loss: 0.3624245822429657, Test_Loss: 0.36482957005500793 *\n",
      "Epoch: 10, Train_Loss: 0.3656189441680908, Test_Loss: 0.379414826631546\n",
      "Epoch: 10, Train_Loss: 0.38338279724121094, Test_Loss: 5.036342620849609\n",
      "Epoch: 10, Train_Loss: 0.4430061876773834, Test_Loss: 1.213731050491333 *\n",
      "Epoch: 10, Train_Loss: 0.3608018457889557, Test_Loss: 0.36367249488830566 *\n",
      "Epoch: 10, Train_Loss: 0.4004560112953186, Test_Loss: 0.3533712327480316 *\n",
      "Epoch: 10, Train_Loss: 0.3697490394115448, Test_Loss: 0.35637450218200684\n",
      "Epoch: 10, Train_Loss: 0.37472203373908997, Test_Loss: 0.3611315190792084\n",
      "Epoch: 10, Train_Loss: 0.4456908404827118, Test_Loss: 0.35397928953170776 *\n",
      "Epoch: 10, Train_Loss: 0.5818036794662476, Test_Loss: 0.36166009306907654\n",
      "Epoch: 10, Train_Loss: 0.35850444436073303, Test_Loss: 0.35228797793388367 *\n",
      "Epoch: 10, Train_Loss: 0.38525456190109253, Test_Loss: 0.3521074056625366 *\n",
      "Epoch: 10, Train_Loss: 0.3495503067970276, Test_Loss: 0.3583625555038452\n",
      "Epoch: 10, Train_Loss: 0.3496246337890625, Test_Loss: 0.36855044960975647\n",
      "Epoch: 10, Train_Loss: 0.3491113483905792, Test_Loss: 0.3579680621623993 *\n",
      "Epoch: 10, Train_Loss: 0.3482614755630493, Test_Loss: 0.35956040024757385\n",
      "Epoch: 10, Train_Loss: 0.3600781559944153, Test_Loss: 0.3594944179058075 *\n",
      "Epoch: 10, Train_Loss: 0.3625917136669159, Test_Loss: 0.35588571429252625 *\n",
      "Epoch: 10, Train_Loss: 0.35503438115119934, Test_Loss: 0.34978488087654114 *\n",
      "Epoch: 10, Train_Loss: 0.35636746883392334, Test_Loss: 0.3507913053035736\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 0.3573656380176544, Test_Loss: 0.359377920627594\n",
      "Epoch: 10, Train_Loss: 0.35164880752563477, Test_Loss: 0.3560428023338318 *\n",
      "Epoch: 10, Train_Loss: 0.34897464513778687, Test_Loss: 0.35017910599708557 *\n",
      "Epoch: 10, Train_Loss: 0.3472958505153656, Test_Loss: 0.35046759247779846\n",
      "Epoch: 10, Train_Loss: 0.37282806634902954, Test_Loss: 0.35863256454467773\n",
      "Epoch: 10, Train_Loss: 0.3720737397670746, Test_Loss: 0.36230140924453735\n",
      "Epoch: 10, Train_Loss: 0.37508007884025574, Test_Loss: 0.3660234212875366\n",
      "Epoch: 10, Train_Loss: 0.35174623131752014, Test_Loss: 0.3585374355316162 *\n",
      "Epoch: 10, Train_Loss: 0.40039798617362976, Test_Loss: 0.3523426651954651 *\n",
      "Epoch: 10, Train_Loss: 0.38498571515083313, Test_Loss: 0.3561049997806549\n",
      "Epoch: 10, Train_Loss: 0.3713340759277344, Test_Loss: 0.35703137516975403\n",
      "Epoch: 10, Train_Loss: 0.3590565025806427, Test_Loss: 0.35143205523490906 *\n",
      "Epoch: 10, Train_Loss: 0.3789874315261841, Test_Loss: 0.4177192449569702\n",
      "Epoch: 10, Train_Loss: 0.34971949458122253, Test_Loss: 0.6256106495857239\n",
      "Epoch: 10, Train_Loss: 0.3617445230484009, Test_Loss: 5.526912689208984\n",
      "Epoch: 10, Train_Loss: 0.368228018283844, Test_Loss: 0.3616544008255005 *\n",
      "Epoch: 10, Train_Loss: 0.3809669315814972, Test_Loss: 0.34713995456695557 *\n",
      "Epoch: 10, Train_Loss: 2.2429168224334717, Test_Loss: 0.37936434149742126\n",
      "Epoch: 10, Train_Loss: 3.864128589630127, Test_Loss: 0.37300923466682434 *\n",
      "Epoch: 10, Train_Loss: 0.3595801293849945, Test_Loss: 0.3844180703163147\n",
      "Epoch: 10, Train_Loss: 0.3619043529033661, Test_Loss: 0.3511020839214325 *\n",
      "Epoch: 10, Train_Loss: 0.3694930672645569, Test_Loss: 0.46022769808769226\n",
      "Epoch: 10, Train_Loss: 0.5024666786193848, Test_Loss: 0.38546061515808105 *\n",
      "Epoch: 10, Train_Loss: 0.38177070021629333, Test_Loss: 0.34750598669052124 *\n",
      "Epoch: 10, Train_Loss: 0.35710927844047546, Test_Loss: 0.38755327463150024\n",
      "Epoch: 10, Train_Loss: 0.34596043825149536, Test_Loss: 0.36163729429244995 *\n",
      "Epoch: 10, Train_Loss: 0.4166097044944763, Test_Loss: 0.35463109612464905 *\n",
      "Epoch: 10, Train_Loss: 0.36348557472229004, Test_Loss: 0.38676777482032776\n",
      "Epoch: 10, Train_Loss: 0.35875454545021057, Test_Loss: 0.40250831842422485\n",
      "Epoch: 10, Train_Loss: 0.8544551134109497, Test_Loss: 0.40203428268432617 *\n",
      "Epoch: 10, Train_Loss: 1.332643747329712, Test_Loss: 0.45362740755081177\n",
      "Epoch: 10, Train_Loss: 1.1476832628250122, Test_Loss: 0.3848642408847809 *\n",
      "Epoch: 10, Train_Loss: 0.4473682641983032, Test_Loss: 0.3875756859779358\n",
      "Epoch: 10, Train_Loss: 0.6980952024459839, Test_Loss: 0.34989452362060547 *\n",
      "Epoch: 10, Train_Loss: 2.298787832260132, Test_Loss: 0.34616732597351074 *\n",
      "Epoch: 10, Train_Loss: 0.8665975332260132, Test_Loss: 0.353243350982666\n",
      "Epoch: 10, Train_Loss: 0.35803940892219543, Test_Loss: 0.35514217615127563\n",
      "Epoch: 10, Train_Loss: 0.3566564917564392, Test_Loss: 0.35424134135246277 *\n",
      "Epoch: 10, Train_Loss: 0.9953913688659668, Test_Loss: 0.35207679867744446 *\n",
      "Epoch: 10, Train_Loss: 1.0709456205368042, Test_Loss: 0.34903234243392944 *\n",
      "Epoch: 10, Train_Loss: 0.7305469512939453, Test_Loss: 0.3484363555908203 *\n",
      "Epoch: 10, Train_Loss: 0.3539370596408844, Test_Loss: 0.34697067737579346 *\n",
      "Epoch: 10, Train_Loss: 0.35604411363601685, Test_Loss: 0.37162551283836365\n",
      "Epoch: 10, Train_Loss: 0.659115195274353, Test_Loss: 0.3539697825908661 *\n",
      "Epoch: 10, Train_Loss: 0.4528651237487793, Test_Loss: 0.35396504402160645 *\n",
      "Epoch: 10, Train_Loss: 0.4060212969779968, Test_Loss: 0.38775357604026794\n",
      "Epoch: 10, Train_Loss: 0.41401585936546326, Test_Loss: 0.6000011563301086\n",
      "Epoch: 10, Train_Loss: 0.3672894537448883, Test_Loss: 0.6156709790229797\n",
      "Epoch: 10, Train_Loss: 0.3790016770362854, Test_Loss: 0.47454798221588135 *\n",
      "Epoch: 10, Train_Loss: 0.4951695203781128, Test_Loss: 0.38231486082077026 *\n",
      "Epoch: 10, Train_Loss: 0.4518466591835022, Test_Loss: 0.37800654768943787 *\n",
      "Epoch: 10, Train_Loss: 0.3741481602191925, Test_Loss: 0.3540651202201843 *\n",
      "Epoch: 10, Train_Loss: 0.46399858593940735, Test_Loss: 0.4171169400215149\n",
      "Epoch: 10, Train_Loss: 0.3882124423980713, Test_Loss: 0.43097996711730957\n",
      "Epoch: 11, Train_Loss: 0.4384691119194031, Test_Loss: 0.4592900276184082 *\n",
      "Epoch: 11, Train_Loss: 0.4514148235321045, Test_Loss: 0.47496557235717773\n",
      "Epoch: 11, Train_Loss: 0.5543185472488403, Test_Loss: 0.41700679063796997 *\n",
      "Epoch: 11, Train_Loss: 0.45538976788520813, Test_Loss: 0.37025415897369385 *\n",
      "Epoch: 11, Train_Loss: 0.483770489692688, Test_Loss: 0.3524629473686218 *\n",
      "Epoch: 11, Train_Loss: 0.3887288272380829, Test_Loss: 0.35910847783088684\n",
      "Epoch: 11, Train_Loss: 0.3635227680206299, Test_Loss: 0.47031211853027344\n",
      "Epoch: 11, Train_Loss: 0.34732571244239807, Test_Loss: 0.37619736790657043 *\n",
      "Epoch: 11, Train_Loss: 0.3516830503940582, Test_Loss: 0.41249799728393555\n",
      "Epoch: 11, Train_Loss: 0.3564998507499695, Test_Loss: 0.3810565173625946 *\n",
      "Epoch: 11, Train_Loss: 0.3515465259552002, Test_Loss: 0.5873769521713257\n",
      "Epoch: 11, Train_Loss: 0.359666109085083, Test_Loss: 0.7618421316146851\n",
      "Epoch: 11, Train_Loss: 0.36603593826293945, Test_Loss: 0.8762789964675903\n",
      "Epoch: 11, Train_Loss: 0.359184205532074, Test_Loss: 0.726679801940918 *\n",
      "Epoch: 11, Train_Loss: 0.4278620481491089, Test_Loss: 0.5033335089683533 *\n",
      "Epoch: 11, Train_Loss: 0.5670729875564575, Test_Loss: 0.49826884269714355 *\n",
      "Epoch: 11, Train_Loss: 0.3813115060329437, Test_Loss: 0.49884331226348877\n",
      "Epoch: 11, Train_Loss: 0.3748367428779602, Test_Loss: 0.5273332595825195\n",
      "Epoch: 11, Train_Loss: 0.37610968947410583, Test_Loss: 0.5469549894332886\n",
      "Epoch: 11, Train_Loss: 0.4528316855430603, Test_Loss: 4.968864917755127\n",
      "Epoch: 11, Train_Loss: 0.47587233781814575, Test_Loss: 0.5658224821090698 *\n",
      "Epoch: 11, Train_Loss: 0.3733321726322174, Test_Loss: 0.3887816071510315 *\n",
      "Epoch: 11, Train_Loss: 0.38502463698387146, Test_Loss: 0.36373817920684814 *\n",
      "Epoch: 11, Train_Loss: 0.6148613095283508, Test_Loss: 0.3629932403564453 *\n",
      "Epoch: 11, Train_Loss: 0.5512911677360535, Test_Loss: 0.3602086305618286 *\n",
      "Epoch: 11, Train_Loss: 0.4089581370353699, Test_Loss: 0.41081321239471436\n",
      "Epoch: 11, Train_Loss: 0.3625478744506836, Test_Loss: 0.40854236483573914 *\n",
      "Epoch: 11, Train_Loss: 0.4203113317489624, Test_Loss: 0.35343825817108154 *\n",
      "Epoch: 11, Train_Loss: 0.6795629858970642, Test_Loss: 0.37281334400177\n",
      "Epoch: 11, Train_Loss: 0.7041608691215515, Test_Loss: 0.37714675068855286\n",
      "Epoch: 11, Train_Loss: 0.42190974950790405, Test_Loss: 0.4802658259868622\n",
      "Epoch: 11, Train_Loss: 0.38698554039001465, Test_Loss: 0.38657671213150024 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.3559490442276001, Test_Loss: 0.3826519548892975 *\n",
      "Epoch: 11, Train_Loss: 0.3482919931411743, Test_Loss: 0.4052400588989258\n",
      "Epoch: 11, Train_Loss: 0.6012302041053772, Test_Loss: 0.3537939488887787 *\n",
      "Epoch: 11, Train_Loss: 0.3698682188987732, Test_Loss: 0.34835508465766907 *\n",
      "Epoch: 11, Train_Loss: 0.4236873388290405, Test_Loss: 0.41651153564453125\n",
      "Epoch: 11, Train_Loss: 0.44875240325927734, Test_Loss: 0.4291366934776306\n",
      "Epoch: 11, Train_Loss: 0.3944498896598816, Test_Loss: 0.36720728874206543 *\n",
      "Epoch: 11, Train_Loss: 0.35849323868751526, Test_Loss: 0.3696092665195465\n",
      "Epoch: 11, Train_Loss: 0.3925240635871887, Test_Loss: 0.36976683139801025\n",
      "Epoch: 11, Train_Loss: 0.4331689178943634, Test_Loss: 0.46190541982650757\n",
      "Epoch: 11, Train_Loss: 0.36737772822380066, Test_Loss: 0.437945693731308 *\n",
      "Epoch: 11, Train_Loss: 0.43259865045547485, Test_Loss: 0.4395919144153595\n",
      "Epoch: 11, Train_Loss: 0.3632807433605194, Test_Loss: 0.3897351026535034 *\n",
      "Epoch: 11, Train_Loss: 0.5114877820014954, Test_Loss: 0.3814440667629242 *\n",
      "Epoch: 11, Train_Loss: 0.39935973286628723, Test_Loss: 0.3962792158126831\n",
      "Epoch: 11, Train_Loss: 0.3873186409473419, Test_Loss: 0.36442673206329346 *\n",
      "Epoch: 11, Train_Loss: 0.3550085127353668, Test_Loss: 0.36009255051612854 *\n",
      "Epoch: 11, Train_Loss: 0.3785227835178375, Test_Loss: 0.4800369143486023\n",
      "Epoch: 11, Train_Loss: 0.6421613693237305, Test_Loss: 1.7747092247009277\n",
      "Epoch: 11, Train_Loss: 0.6391712427139282, Test_Loss: 4.363583564758301\n",
      "Epoch: 11, Train_Loss: 0.7339005470275879, Test_Loss: 0.38557666540145874 *\n",
      "Epoch: 11, Train_Loss: 0.6637293100357056, Test_Loss: 0.42659324407577515\n",
      "Epoch: 11, Train_Loss: 0.5708234310150146, Test_Loss: 0.38887298107147217 *\n",
      "Epoch: 11, Train_Loss: 0.4786754846572876, Test_Loss: 0.36144503951072693 *\n",
      "Epoch: 11, Train_Loss: 0.4034983813762665, Test_Loss: 0.3539104461669922 *\n",
      "Epoch: 11, Train_Loss: 0.34592267870903015, Test_Loss: 0.38825711607933044\n",
      "Epoch: 11, Train_Loss: 0.3531574606895447, Test_Loss: 0.41558176279067993\n",
      "Epoch: 11, Train_Loss: 0.3711102306842804, Test_Loss: 0.3540745973587036 *\n",
      "Epoch: 11, Train_Loss: 0.5919324159622192, Test_Loss: 0.36239898204803467\n",
      "Epoch: 11, Train_Loss: 0.6939818859100342, Test_Loss: 0.39100539684295654\n",
      "Epoch: 11, Train_Loss: 0.7736002802848816, Test_Loss: 0.43925151228904724\n",
      "Epoch: 11, Train_Loss: 1.4973160028457642, Test_Loss: 0.37262651324272156 *\n",
      "Epoch: 11, Train_Loss: 0.6354800462722778, Test_Loss: 0.41317787766456604\n",
      "Epoch: 11, Train_Loss: 0.5538631081581116, Test_Loss: 0.3682141602039337 *\n",
      "Epoch: 11, Train_Loss: 0.3602571189403534, Test_Loss: 0.4429847002029419\n",
      "Epoch: 11, Train_Loss: 0.3511960208415985, Test_Loss: 0.35033831000328064 *\n",
      "Epoch: 11, Train_Loss: 0.6079829931259155, Test_Loss: 0.46205833554267883\n",
      "Epoch: 11, Train_Loss: 1.154179573059082, Test_Loss: 0.4227195978164673 *\n",
      "Epoch: 11, Train_Loss: 0.596376895904541, Test_Loss: 0.3764406442642212 *\n",
      "Epoch: 11, Train_Loss: 0.38356441259384155, Test_Loss: 0.42681562900543213\n",
      "Epoch: 11, Train_Loss: 0.4025454521179199, Test_Loss: 0.40404021739959717 *\n",
      "Epoch: 11, Train_Loss: 0.4211076498031616, Test_Loss: 0.38525712490081787 *\n",
      "Epoch: 11, Train_Loss: 0.6330098509788513, Test_Loss: 0.3978206515312195\n",
      "Epoch: 11, Train_Loss: 0.5332481265068054, Test_Loss: 0.4124424457550049\n",
      "Epoch: 11, Train_Loss: 0.418730229139328, Test_Loss: 0.45745545625686646\n",
      "Epoch: 11, Train_Loss: 0.5439539551734924, Test_Loss: 0.4654904007911682\n",
      "Epoch: 11, Train_Loss: 0.3787674903869629, Test_Loss: 0.388078510761261 *\n",
      "Epoch: 11, Train_Loss: 0.3472927510738373, Test_Loss: 0.47067588567733765\n",
      "Epoch: 11, Train_Loss: 0.39683136343955994, Test_Loss: 0.40320998430252075 *\n",
      "Epoch: 11, Train_Loss: 0.3718680441379547, Test_Loss: 0.34770888090133667 *\n",
      "Epoch: 11, Train_Loss: 0.40236350893974304, Test_Loss: 0.44799506664276123\n",
      "Epoch: 11, Train_Loss: 0.39434435963630676, Test_Loss: 0.4842723309993744\n",
      "Epoch: 11, Train_Loss: 9.668648719787598, Test_Loss: 0.44996240735054016 *\n",
      "Epoch: 11, Train_Loss: 6.969283580780029, Test_Loss: 0.36739039421081543 *\n",
      "Epoch: 11, Train_Loss: 1.1419811248779297, Test_Loss: 0.38145941495895386\n",
      "Epoch: 11, Train_Loss: 1.1010841131210327, Test_Loss: 0.3949838876724243\n",
      "Epoch: 11, Train_Loss: 0.5909761786460876, Test_Loss: 0.3966846466064453\n",
      "Epoch: 11, Train_Loss: 0.4062526822090149, Test_Loss: 0.4349626898765564\n",
      "Epoch: 11, Train_Loss: 0.6983399391174316, Test_Loss: 0.44787949323654175\n",
      "Epoch: 11, Train_Loss: 7.6660685539245605, Test_Loss: 0.6282181739807129\n",
      "Epoch: 11, Train_Loss: 1.5571129322052002, Test_Loss: 0.6487098932266235\n",
      "Epoch: 11, Train_Loss: 0.4305077791213989, Test_Loss: 0.6003983020782471 *\n",
      "Epoch: 11, Train_Loss: 2.8081014156341553, Test_Loss: 0.48321330547332764 *\n",
      "Epoch: 11, Train_Loss: 2.267086982727051, Test_Loss: 0.8642827272415161\n",
      "Epoch: 11, Train_Loss: 0.9800944328308105, Test_Loss: 1.1928949356079102\n",
      "Epoch: 11, Train_Loss: 0.3906664550304413, Test_Loss: 1.1087901592254639 *\n",
      "Epoch: 11, Train_Loss: 0.5290685296058655, Test_Loss: 1.8633309602737427\n",
      "Epoch: 11, Train_Loss: 0.43992626667022705, Test_Loss: 1.661906123161316 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.45492023229599, Test_Loss: 1.8937721252441406\n",
      "Epoch: 11, Train_Loss: 0.38259008526802063, Test_Loss: 1.3034552335739136 *\n",
      "Epoch: 11, Train_Loss: 0.34080564975738525, Test_Loss: 1.33399498462677\n",
      "Epoch: 11, Train_Loss: 0.33579716086387634, Test_Loss: 0.6477304697036743 *\n",
      "Epoch: 11, Train_Loss: 0.35773366689682007, Test_Loss: 0.984394907951355\n",
      "Epoch: 11, Train_Loss: 0.36273348331451416, Test_Loss: 0.8449102640151978 *\n",
      "Epoch: 11, Train_Loss: 0.36202698945999146, Test_Loss: 0.7204774618148804 *\n",
      "Epoch: 11, Train_Loss: 0.43482813239097595, Test_Loss: 0.6031566858291626 *\n",
      "Epoch: 11, Train_Loss: 0.4152251183986664, Test_Loss: 0.4747656583786011 *\n",
      "Epoch: 11, Train_Loss: 0.3751284182071686, Test_Loss: 0.948411226272583\n",
      "Epoch: 11, Train_Loss: 0.347335547208786, Test_Loss: 7.012382507324219\n",
      "Epoch: 11, Train_Loss: 0.34130993485450745, Test_Loss: 0.5461147427558899 *\n",
      "Epoch: 11, Train_Loss: 0.3524976670742035, Test_Loss: 0.8001090288162231\n",
      "Epoch: 11, Train_Loss: 0.3368217945098877, Test_Loss: 0.6510710716247559 *\n",
      "Epoch: 11, Train_Loss: 0.33802416920661926, Test_Loss: 0.5183654427528381 *\n",
      "Epoch: 11, Train_Loss: 0.3346550464630127, Test_Loss: 0.4972279667854309 *\n",
      "Epoch: 11, Train_Loss: 0.3357745409011841, Test_Loss: 0.858833909034729\n",
      "Epoch: 11, Train_Loss: 0.3347359597682953, Test_Loss: 0.8254621028900146 *\n",
      "Epoch: 11, Train_Loss: 0.3347722887992859, Test_Loss: 0.5188146829605103 *\n",
      "Epoch: 11, Train_Loss: 0.33462318778038025, Test_Loss: 0.5663495659828186\n",
      "Epoch: 11, Train_Loss: 0.3359921872615814, Test_Loss: 0.5719578266143799\n",
      "Epoch: 11, Train_Loss: 0.35610607266426086, Test_Loss: 0.8253978490829468\n",
      "Epoch: 11, Train_Loss: 0.36581942439079285, Test_Loss: 0.5142198204994202 *\n",
      "Epoch: 11, Train_Loss: 0.3991314768791199, Test_Loss: 0.6456516981124878\n",
      "Epoch: 11, Train_Loss: 0.35907822847366333, Test_Loss: 0.6924121975898743\n",
      "Epoch: 11, Train_Loss: 0.4342755079269409, Test_Loss: 0.39885273575782776 *\n",
      "Epoch: 11, Train_Loss: 8.497054100036621, Test_Loss: 0.37458518147468567 *\n",
      "Epoch: 11, Train_Loss: 0.6979849338531494, Test_Loss: 0.35565483570098877 *\n",
      "Epoch: 11, Train_Loss: 0.3520607352256775, Test_Loss: 0.5632051825523376\n",
      "Epoch: 11, Train_Loss: 0.4073687493801117, Test_Loss: 0.376399964094162 *\n",
      "Epoch: 11, Train_Loss: 0.48331454396247864, Test_Loss: 0.4157460927963257\n",
      "Epoch: 11, Train_Loss: 0.37131837010383606, Test_Loss: 0.383677214384079 *\n",
      "Epoch: 11, Train_Loss: 0.38081231713294983, Test_Loss: 0.590618371963501\n",
      "Epoch: 11, Train_Loss: 0.46386322379112244, Test_Loss: 0.5638765096664429 *\n",
      "Epoch: 11, Train_Loss: 0.48063924908638, Test_Loss: 0.4834272861480713 *\n",
      "Epoch: 11, Train_Loss: 0.4644489884376526, Test_Loss: 0.39670950174331665 *\n",
      "Epoch: 11, Train_Loss: 0.42933225631713867, Test_Loss: 0.3927895724773407 *\n",
      "Epoch: 11, Train_Loss: 0.3385760486125946, Test_Loss: 0.3987971544265747\n",
      "Epoch: 11, Train_Loss: 0.4004649221897125, Test_Loss: 0.3741111755371094 *\n",
      "Epoch: 11, Train_Loss: 0.40844297409057617, Test_Loss: 0.39445555210113525\n",
      "Epoch: 11, Train_Loss: 0.5445525646209717, Test_Loss: 0.4377973675727844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.42363032698631287, Test_Loss: 3.036939859390259\n",
      "Epoch: 11, Train_Loss: 0.41055580973625183, Test_Loss: 3.407933473587036\n",
      "Epoch: 11, Train_Loss: 0.36388614773750305, Test_Loss: 0.34311965107917786 *\n",
      "Epoch: 11, Train_Loss: 0.35315948724746704, Test_Loss: 0.3354097306728363 *\n",
      "Epoch: 11, Train_Loss: 0.39283621311187744, Test_Loss: 0.34964698553085327\n",
      "Epoch: 11, Train_Loss: 0.3599441945552826, Test_Loss: 0.33687958121299744 *\n",
      "Epoch: 11, Train_Loss: 0.3422672152519226, Test_Loss: 0.36180511116981506\n",
      "Epoch: 11, Train_Loss: 0.33428335189819336, Test_Loss: 0.3828107714653015\n",
      "Epoch: 11, Train_Loss: 0.33530712127685547, Test_Loss: 0.44609224796295166\n",
      "Epoch: 11, Train_Loss: 0.41887181997299194, Test_Loss: 0.33630725741386414 *\n",
      "Epoch: 11, Train_Loss: 5.977060794830322, Test_Loss: 0.3572658598423004\n",
      "Epoch: 11, Train_Loss: 0.3567996621131897, Test_Loss: 0.35583651065826416 *\n",
      "Epoch: 11, Train_Loss: 0.3333487808704376, Test_Loss: 0.3479827046394348 *\n",
      "Epoch: 11, Train_Loss: 0.33849653601646423, Test_Loss: 0.3427935838699341 *\n",
      "Epoch: 11, Train_Loss: 0.3373911678791046, Test_Loss: 0.36801058053970337\n",
      "Epoch: 11, Train_Loss: 0.3335796594619751, Test_Loss: 0.36742451786994934 *\n",
      "Epoch: 11, Train_Loss: 0.3345980644226074, Test_Loss: 0.43506619334220886\n",
      "Epoch: 11, Train_Loss: 0.3340034782886505, Test_Loss: 0.462577223777771\n",
      "Epoch: 11, Train_Loss: 0.3737395107746124, Test_Loss: 0.3463062345981598 *\n",
      "Epoch: 11, Train_Loss: 0.34231922030448914, Test_Loss: 0.34380969405174255 *\n",
      "Epoch: 11, Train_Loss: 0.36245906352996826, Test_Loss: 0.3520626723766327\n",
      "Epoch: 11, Train_Loss: 0.33271166682243347, Test_Loss: 0.3470796048641205 *\n",
      "Epoch: 11, Train_Loss: 0.3325574994087219, Test_Loss: 0.34510838985443115 *\n",
      "Epoch: 11, Train_Loss: 0.3459138572216034, Test_Loss: 0.337257444858551 *\n",
      "Epoch: 11, Train_Loss: 0.3347375988960266, Test_Loss: 0.34130948781967163\n",
      "Epoch: 11, Train_Loss: 0.33411112427711487, Test_Loss: 0.34866592288017273\n",
      "Epoch: 11, Train_Loss: 0.3510657548904419, Test_Loss: 0.361098974943161\n",
      "Epoch: 11, Train_Loss: 0.36399713158607483, Test_Loss: 0.35074397921562195 *\n",
      "Epoch: 11, Train_Loss: 0.34892192482948303, Test_Loss: 0.3357810378074646 *\n",
      "Epoch: 11, Train_Loss: 0.33134031295776367, Test_Loss: 0.3554760813713074\n",
      "Epoch: 11, Train_Loss: 0.3310008943080902, Test_Loss: 0.33359262347221375 *\n",
      "Epoch: 11, Train_Loss: 0.40123143792152405, Test_Loss: 0.35165098309516907\n",
      "Epoch: 11, Train_Loss: 0.3686840534210205, Test_Loss: 0.38438451290130615\n",
      "Epoch: 11, Train_Loss: 0.356157124042511, Test_Loss: 0.5305737257003784\n",
      "Epoch: 11, Train_Loss: 0.34476229548454285, Test_Loss: 0.5596081018447876\n",
      "Epoch: 11, Train_Loss: 0.4141075611114502, Test_Loss: 0.40592241287231445 *\n",
      "Epoch: 11, Train_Loss: 0.3788100779056549, Test_Loss: 0.33858418464660645 *\n",
      "Epoch: 11, Train_Loss: 0.38025417923927307, Test_Loss: 0.35574668645858765\n",
      "Epoch: 11, Train_Loss: 0.4090942144393921, Test_Loss: 0.39345693588256836\n",
      "Epoch: 11, Train_Loss: 0.594915509223938, Test_Loss: 0.5528666973114014\n",
      "Epoch: 11, Train_Loss: 0.3813141882419586, Test_Loss: 0.9023762345314026\n",
      "Epoch: 11, Train_Loss: 0.38155147433280945, Test_Loss: 0.8620620965957642 *\n",
      "Epoch: 11, Train_Loss: 0.3337039649486542, Test_Loss: 0.38672423362731934 *\n",
      "Epoch: 11, Train_Loss: 0.330032080411911, Test_Loss: 0.3721000850200653 *\n",
      "Epoch: 11, Train_Loss: 0.329582154750824, Test_Loss: 0.3508254289627075 *\n",
      "Epoch: 11, Train_Loss: 0.329468697309494, Test_Loss: 0.35277289152145386\n",
      "Epoch: 11, Train_Loss: 0.35742688179016113, Test_Loss: 0.34933483600616455 *\n",
      "Epoch: 11, Train_Loss: 5.062589168548584, Test_Loss: 0.34454256296157837 *\n",
      "Epoch: 11, Train_Loss: 0.5316715240478516, Test_Loss: 0.38666272163391113\n",
      "Epoch: 11, Train_Loss: 0.33440232276916504, Test_Loss: 0.3309011161327362 *\n",
      "Epoch: 11, Train_Loss: 0.34171098470687866, Test_Loss: 0.37780749797821045\n",
      "Epoch: 11, Train_Loss: 0.33117228746414185, Test_Loss: 0.45649221539497375\n",
      "Epoch: 11, Train_Loss: 0.3299037218093872, Test_Loss: 0.6727281808853149\n",
      "Epoch: 11, Train_Loss: 0.33007416129112244, Test_Loss: 0.5826209783554077 *\n",
      "Epoch: 11, Train_Loss: 0.32871028780937195, Test_Loss: 0.3551558256149292 *\n",
      "Epoch: 11, Train_Loss: 0.32917290925979614, Test_Loss: 0.3482874035835266 *\n",
      "Epoch: 11, Train_Loss: 0.3293813467025757, Test_Loss: 0.3479951024055481 *\n",
      "Epoch: 11, Train_Loss: 0.3748571276664734, Test_Loss: 0.3486776649951935\n",
      "Epoch: 11, Train_Loss: 0.3898584842681885, Test_Loss: 0.3532429039478302\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.40435343980789185, Test_Loss: 1.5460634231567383\n",
      "Epoch: 11, Train_Loss: 0.3726920187473297, Test_Loss: 4.546233177185059\n",
      "Epoch: 11, Train_Loss: 0.3336528539657593, Test_Loss: 0.34715980291366577 *\n",
      "Epoch: 11, Train_Loss: 0.4388478398323059, Test_Loss: 0.33689358830451965 *\n",
      "Epoch: 11, Train_Loss: 0.5589922070503235, Test_Loss: 0.33713626861572266\n",
      "Epoch: 11, Train_Loss: 0.5580360293388367, Test_Loss: 0.3435972034931183\n",
      "Epoch: 11, Train_Loss: 0.5282766222953796, Test_Loss: 0.33359721302986145 *\n",
      "Epoch: 11, Train_Loss: 0.3300066888332367, Test_Loss: 0.33311036229133606 *\n",
      "Epoch: 11, Train_Loss: 0.3287062644958496, Test_Loss: 0.33366090059280396\n",
      "Epoch: 11, Train_Loss: 0.3289240896701813, Test_Loss: 0.3294607698917389 *\n",
      "Epoch: 11, Train_Loss: 0.33915024995803833, Test_Loss: 0.3307575583457947\n",
      "Epoch: 11, Train_Loss: 0.34399139881134033, Test_Loss: 0.3316394090652466\n",
      "Epoch: 11, Train_Loss: 0.342992901802063, Test_Loss: 0.34357118606567383\n",
      "Epoch: 11, Train_Loss: 0.3359106779098511, Test_Loss: 0.34186607599258423 *\n",
      "Epoch: 11, Train_Loss: 0.32719382643699646, Test_Loss: 0.33723294734954834 *\n",
      "Epoch: 11, Train_Loss: 0.3339015543460846, Test_Loss: 0.3405565023422241\n",
      "Epoch: 11, Train_Loss: 0.3487136662006378, Test_Loss: 0.3305959701538086 *\n",
      "Epoch: 11, Train_Loss: 0.5012532472610474, Test_Loss: 0.331339955329895\n",
      "Epoch: 11, Train_Loss: 0.4890163242816925, Test_Loss: 0.3290625810623169 *\n",
      "Epoch: 11, Train_Loss: 0.4741784334182739, Test_Loss: 0.34876251220703125\n",
      "Epoch: 11, Train_Loss: 0.3681682348251343, Test_Loss: 0.32963526248931885 *\n",
      "Epoch: 11, Train_Loss: 0.453458696603775, Test_Loss: 0.33027487993240356\n",
      "Epoch: 11, Train_Loss: 0.44234204292297363, Test_Loss: 0.33162498474121094\n",
      "Epoch: 11, Train_Loss: 0.3737369775772095, Test_Loss: 0.3570414185523987\n",
      "Epoch: 11, Train_Loss: 0.45799803733825684, Test_Loss: 0.35022008419036865 *\n",
      "Epoch: 11, Train_Loss: 0.5047135949134827, Test_Loss: 0.3436853885650635 *\n",
      "Epoch: 11, Train_Loss: 0.46419471502304077, Test_Loss: 0.3353036940097809 *\n",
      "Epoch: 11, Train_Loss: 0.33121490478515625, Test_Loss: 0.33577263355255127\n",
      "Epoch: 11, Train_Loss: 2.2792911529541016, Test_Loss: 0.3377492427825928\n",
      "Epoch: 11, Train_Loss: 1.5069314241409302, Test_Loss: 0.3317532539367676 *\n",
      "Epoch: 11, Train_Loss: 0.3702099621295929, Test_Loss: 0.3763732612133026\n",
      "Epoch: 11, Train_Loss: 0.3762936294078827, Test_Loss: 0.37070682644844055 *\n",
      "Epoch: 11, Train_Loss: 0.3554944396018982, Test_Loss: 3.827300786972046\n",
      "Epoch: 11, Train_Loss: 0.35972368717193604, Test_Loss: 2.2786219120025635 *\n",
      "Epoch: 11, Train_Loss: 0.3279235363006592, Test_Loss: 0.3287600874900818 *\n",
      "Epoch: 11, Train_Loss: 0.3543839156627655, Test_Loss: 0.3321884572505951\n",
      "Epoch: 11, Train_Loss: 0.45369887351989746, Test_Loss: 0.3695113956928253\n",
      "Epoch: 11, Train_Loss: 0.40655067563056946, Test_Loss: 0.3557218313217163 *\n",
      "Epoch: 11, Train_Loss: 0.3878974914550781, Test_Loss: 0.35258033871650696 *\n",
      "Epoch: 11, Train_Loss: 0.37436217069625854, Test_Loss: 0.38351747393608093\n",
      "Epoch: 11, Train_Loss: 0.34672811627388, Test_Loss: 0.40524783730506897\n",
      "Epoch: 11, Train_Loss: 0.33497023582458496, Test_Loss: 0.3301813304424286 *\n",
      "Epoch: 11, Train_Loss: 0.3394552171230316, Test_Loss: 0.35135364532470703\n",
      "Epoch: 11, Train_Loss: 0.39047378301620483, Test_Loss: 0.3472525179386139 *\n",
      "Epoch: 11, Train_Loss: 0.3682597279548645, Test_Loss: 0.3421739935874939 *\n",
      "Epoch: 11, Train_Loss: 0.34586137533187866, Test_Loss: 0.33071184158325195 *\n",
      "Epoch: 11, Train_Loss: 0.32898586988449097, Test_Loss: 0.3976908326148987\n",
      "Epoch: 11, Train_Loss: 0.3521353006362915, Test_Loss: 0.3786473274230957 *\n",
      "Epoch: 11, Train_Loss: 0.35462692379951477, Test_Loss: 0.4072888195514679\n",
      "Epoch: 11, Train_Loss: 0.33681076765060425, Test_Loss: 0.3933056592941284 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.32615306973457336, Test_Loss: 0.36674821376800537 *\n",
      "Epoch: 11, Train_Loss: 0.3256966471672058, Test_Loss: 0.3440009653568268 *\n",
      "Epoch: 11, Train_Loss: 0.32457998394966125, Test_Loss: 0.33228686451911926 *\n",
      "Epoch: 11, Train_Loss: 0.3270329535007477, Test_Loss: 0.3359995484352112\n",
      "Epoch: 11, Train_Loss: 0.3275686800479889, Test_Loss: 0.33809128403663635\n",
      "Epoch: 11, Train_Loss: 0.32812488079071045, Test_Loss: 0.34242549538612366\n",
      "Epoch: 11, Train_Loss: 0.33069416880607605, Test_Loss: 0.3396399915218353 *\n",
      "Epoch: 11, Train_Loss: 0.32472142577171326, Test_Loss: 0.3317070007324219 *\n",
      "Epoch: 11, Train_Loss: 0.32480576634407043, Test_Loss: 0.3337942659854889\n",
      "Epoch: 11, Train_Loss: 0.3300919234752655, Test_Loss: 0.330487459897995 *\n",
      "Epoch: 11, Train_Loss: 0.34581029415130615, Test_Loss: 0.33574599027633667\n",
      "Epoch: 11, Train_Loss: 0.3446095585823059, Test_Loss: 0.3321467339992523 *\n",
      "Epoch: 11, Train_Loss: 0.34209397435188293, Test_Loss: 0.3558273911476135\n",
      "Epoch: 11, Train_Loss: 0.35192233324050903, Test_Loss: 0.38491207361221313\n",
      "Epoch: 11, Train_Loss: 0.33551713824272156, Test_Loss: 0.42526087164878845\n",
      "Epoch: 11, Train_Loss: 0.3280695676803589, Test_Loss: 0.7397689819335938\n",
      "Epoch: 11, Train_Loss: 0.325639545917511, Test_Loss: 0.6878224611282349 *\n",
      "Epoch: 11, Train_Loss: 0.3318060040473938, Test_Loss: 0.4431498050689697 *\n",
      "Epoch: 11, Train_Loss: 0.35433125495910645, Test_Loss: 0.34204286336898804 *\n",
      "Epoch: 11, Train_Loss: 0.3309415876865387, Test_Loss: 0.340760737657547 *\n",
      "Epoch: 11, Train_Loss: 0.32914653420448303, Test_Loss: 0.3821548819541931\n",
      "Epoch: 11, Train_Loss: 0.3255499601364136, Test_Loss: 0.6667595505714417\n",
      "Epoch: 11, Train_Loss: 0.33462536334991455, Test_Loss: 1.0401781797409058\n",
      "Epoch: 11, Train_Loss: 0.37799134850502014, Test_Loss: 0.7513689994812012 *\n",
      "Epoch: 11, Train_Loss: 0.3860822319984436, Test_Loss: 0.4053868055343628 *\n",
      "Epoch: 11, Train_Loss: 0.35958579182624817, Test_Loss: 0.3388192653656006 *\n",
      "Epoch: 11, Train_Loss: 0.3233754336833954, Test_Loss: 0.33176520466804504 *\n",
      "Epoch: 11, Train_Loss: 0.3845658302307129, Test_Loss: 0.32652735710144043 *\n",
      "Epoch: 11, Train_Loss: 0.3400111198425293, Test_Loss: 0.332949697971344\n",
      "Epoch: 11, Train_Loss: 0.3259704113006592, Test_Loss: 0.3410167992115021\n",
      "Epoch: 11, Train_Loss: 0.34440964460372925, Test_Loss: 0.36677300930023193\n",
      "Epoch: 11, Train_Loss: 0.34896472096443176, Test_Loss: 0.32442259788513184 *\n",
      "Epoch: 11, Train_Loss: 0.43905341625213623, Test_Loss: 0.40073060989379883\n",
      "Epoch: 11, Train_Loss: 0.4052082598209381, Test_Loss: 0.5199599862098694\n",
      "Epoch: 11, Train_Loss: 0.362187922000885, Test_Loss: 0.5984559655189514\n",
      "Epoch: 11, Train_Loss: 0.34199628233909607, Test_Loss: 0.5475468635559082 *\n",
      "Epoch: 11, Train_Loss: 0.32808631658554077, Test_Loss: 0.33851540088653564 *\n",
      "Epoch: 11, Train_Loss: 0.34426745772361755, Test_Loss: 0.3369404077529907 *\n",
      "Epoch: 11, Train_Loss: 0.32366305589675903, Test_Loss: 0.3365322947502136 *\n",
      "Epoch: 11, Train_Loss: 0.3296032249927521, Test_Loss: 0.336628258228302\n",
      "Epoch: 11, Train_Loss: 0.33437150716781616, Test_Loss: 0.34712719917297363\n",
      "Epoch: 11, Train_Loss: 0.34060317277908325, Test_Loss: 2.8667891025543213\n",
      "Epoch: 11, Train_Loss: 0.41916653513908386, Test_Loss: 3.322476863861084\n",
      "Epoch: 11, Train_Loss: 0.32314175367355347, Test_Loss: 0.33545488119125366 *\n",
      "Epoch: 11, Train_Loss: 0.38175344467163086, Test_Loss: 0.3263491988182068 *\n",
      "Epoch: 11, Train_Loss: 0.3356062173843384, Test_Loss: 0.328798770904541\n",
      "Epoch: 11, Train_Loss: 0.3546161949634552, Test_Loss: 0.3328093886375427\n",
      "Epoch: 11, Train_Loss: 0.38478875160217285, Test_Loss: 0.3256959617137909 *\n",
      "Epoch: 11, Train_Loss: 0.5727580189704895, Test_Loss: 0.3312273621559143\n",
      "Epoch: 11, Train_Loss: 0.3365621864795685, Test_Loss: 0.32714900374412537 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.3564872145652771, Test_Loss: 0.32432758808135986 *\n",
      "Epoch: 11, Train_Loss: 0.32177677750587463, Test_Loss: 0.3286495506763458\n",
      "Epoch: 11, Train_Loss: 0.32270514965057373, Test_Loss: 0.32641151547431946 *\n",
      "Epoch: 11, Train_Loss: 0.3220367729663849, Test_Loss: 0.3412015736103058\n",
      "Epoch: 11, Train_Loss: 0.32038456201553345, Test_Loss: 0.33419322967529297 *\n",
      "Epoch: 11, Train_Loss: 0.3312615156173706, Test_Loss: 0.329091876745224 *\n",
      "Epoch: 11, Train_Loss: 0.330078125, Test_Loss: 0.3310166299343109\n",
      "Epoch: 11, Train_Loss: 0.3325730264186859, Test_Loss: 0.3221743702888489 *\n",
      "Epoch: 11, Train_Loss: 0.327157586812973, Test_Loss: 0.3247426450252533\n",
      "Epoch: 11, Train_Loss: 0.32794255018234253, Test_Loss: 0.32864174246788025\n",
      "Epoch: 11, Train_Loss: 0.33042240142822266, Test_Loss: 0.33551329374313354\n",
      "Epoch: 11, Train_Loss: 0.3213236331939697, Test_Loss: 0.322521835565567 *\n",
      "Epoch: 11, Train_Loss: 0.3196961581707001, Test_Loss: 0.32137900590896606 *\n",
      "Epoch: 11, Train_Loss: 0.3387419581413269, Test_Loss: 0.32911989092826843\n",
      "Epoch: 11, Train_Loss: 0.34047144651412964, Test_Loss: 0.34156736731529236\n",
      "Epoch: 11, Train_Loss: 0.352339506149292, Test_Loss: 0.34138309955596924 *\n",
      "Epoch: 11, Train_Loss: 0.31939730048179626, Test_Loss: 0.33463430404663086 *\n",
      "Epoch: 11, Train_Loss: 0.36335715651512146, Test_Loss: 0.3265485167503357 *\n",
      "Epoch: 11, Train_Loss: 0.36302450299263, Test_Loss: 0.3273516893386841\n",
      "Epoch: 11, Train_Loss: 0.351714164018631, Test_Loss: 0.3292355239391327\n",
      "Epoch: 11, Train_Loss: 0.3231872618198395, Test_Loss: 0.32203900814056396 *\n",
      "Epoch: 11, Train_Loss: 0.35725924372673035, Test_Loss: 0.38690900802612305\n",
      "Epoch: 11, Train_Loss: 0.31954899430274963, Test_Loss: 0.35248255729675293 *\n",
      "Epoch: 11, Train_Loss: 0.33703628182411194, Test_Loss: 5.104585647583008\n",
      "Epoch: 11, Train_Loss: 0.32549723982810974, Test_Loss: 1.106537103652954 *\n",
      "Epoch: 11, Train_Loss: 0.34294214844703674, Test_Loss: 0.31997984647750854 *\n",
      "Epoch: 11, Train_Loss: 0.933533787727356, Test_Loss: 0.3365135192871094\n",
      "Epoch: 11, Train_Loss: 4.25475549697876, Test_Loss: 0.3468693792819977\n",
      "Epoch: 11, Train_Loss: 1.2630994319915771, Test_Loss: 0.34963467717170715\n",
      "Epoch: 11, Train_Loss: 0.34499987959861755, Test_Loss: 0.32687708735466003 *\n",
      "Epoch: 11, Train_Loss: 0.32373490929603577, Test_Loss: 0.4041314423084259\n",
      "Epoch: 11, Train_Loss: 0.4621255397796631, Test_Loss: 0.3947731852531433 *\n",
      "Epoch: 11, Train_Loss: 0.40047740936279297, Test_Loss: 0.32013288140296936 *\n",
      "Epoch: 11, Train_Loss: 0.3360542058944702, Test_Loss: 0.36040759086608887\n",
      "Epoch: 11, Train_Loss: 0.31837889552116394, Test_Loss: 0.33160510659217834 *\n",
      "Epoch: 11, Train_Loss: 0.3783055543899536, Test_Loss: 0.3305848240852356 *\n",
      "Epoch: 11, Train_Loss: 0.3429412543773651, Test_Loss: 0.3234751522541046 *\n",
      "Epoch: 11, Train_Loss: 0.3264175057411194, Test_Loss: 0.38233232498168945\n",
      "Epoch: 11, Train_Loss: 0.5563344955444336, Test_Loss: 0.3574548363685608 *\n",
      "Epoch: 11, Train_Loss: 1.3047878742218018, Test_Loss: 0.43410786986351013\n",
      "Epoch: 11, Train_Loss: 1.4320803880691528, Test_Loss: 0.3786970376968384 *\n",
      "Epoch: 11, Train_Loss: 0.4125472903251648, Test_Loss: 0.35749444365501404 *\n",
      "Epoch: 11, Train_Loss: 0.4325789511203766, Test_Loss: 0.3260596990585327 *\n",
      "Epoch: 11, Train_Loss: 2.20951771736145, Test_Loss: 0.3197450637817383 *\n",
      "Epoch: 11, Train_Loss: 1.1759883165359497, Test_Loss: 0.32496318221092224\n",
      "Epoch: 11, Train_Loss: 0.33746960759162903, Test_Loss: 0.32226619124412537 *\n",
      "Epoch: 11, Train_Loss: 0.32396888732910156, Test_Loss: 0.3265953063964844\n",
      "Epoch: 11, Train_Loss: 0.7464512586593628, Test_Loss: 0.32527461647987366 *\n",
      "Epoch: 11, Train_Loss: 1.0723819732666016, Test_Loss: 0.320594847202301 *\n",
      "Epoch: 11, Train_Loss: 0.9879698157310486, Test_Loss: 0.3220145106315613\n",
      "Epoch: 11, Train_Loss: 0.3253670930862427, Test_Loss: 0.3226793706417084\n",
      "Epoch: 11, Train_Loss: 0.3426516354084015, Test_Loss: 0.32701247930526733\n",
      "Epoch: 11, Train_Loss: 0.5146575570106506, Test_Loss: 0.33439958095550537\n",
      "Epoch: 11, Train_Loss: 0.5619023442268372, Test_Loss: 0.3265749514102936 *\n",
      "Epoch: 11, Train_Loss: 0.34242165088653564, Test_Loss: 0.33736586570739746\n",
      "Epoch: 11, Train_Loss: 0.3812946677207947, Test_Loss: 0.48174965381622314\n",
      "Epoch: 11, Train_Loss: 0.35771864652633667, Test_Loss: 0.6034713983535767\n",
      "Epoch: 11, Train_Loss: 0.3351902961730957, Test_Loss: 0.5378512144088745 *\n",
      "Epoch: 11, Train_Loss: 0.4446285367012024, Test_Loss: 0.38604235649108887 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train_Loss: 0.4882773756980896, Test_Loss: 0.3231978118419647 *\n",
      "Epoch: 11, Train_Loss: 0.3701508939266205, Test_Loss: 0.3343680799007416\n",
      "Epoch: 11, Train_Loss: 0.3819596469402313, Test_Loss: 0.4037330746650696\n",
      "Epoch: 11, Train_Loss: 0.3898021876811981, Test_Loss: 0.52599036693573\n",
      "Epoch: 11, Train_Loss: 0.38611724972724915, Test_Loss: 0.3550100028514862 *\n",
      "Epoch: 11, Train_Loss: 0.475839227437973, Test_Loss: 0.6326472759246826\n",
      "Epoch: 11, Train_Loss: 0.5669280886650085, Test_Loss: 0.3711186349391937 *\n",
      "Epoch: 11, Train_Loss: 0.36798128485679626, Test_Loss: 0.3437349498271942 *\n",
      "Epoch: 11, Train_Loss: 0.4954235255718231, Test_Loss: 0.33708229660987854 *\n",
      "Epoch: 11, Train_Loss: 0.3907146155834198, Test_Loss: 0.32766038179397583 *\n",
      "Epoch: 11, Train_Loss: 0.3507412075996399, Test_Loss: 0.42851966619491577\n",
      "Epoch: 11, Train_Loss: 0.3240859806537628, Test_Loss: 0.40547046065330505 *\n",
      "Epoch: 11, Train_Loss: 0.3244546949863434, Test_Loss: 0.3540210425853729 *\n",
      "Epoch: 11, Train_Loss: 0.33225834369659424, Test_Loss: 0.35896819829940796\n",
      "Epoch: 11, Train_Loss: 0.32063028216362, Test_Loss: 0.6988510489463806\n",
      "Epoch: 11, Train_Loss: 0.3244805634021759, Test_Loss: 0.6835909485816956 *\n",
      "Epoch: 11, Train_Loss: 0.336643785238266, Test_Loss: 0.7211688160896301\n",
      "Epoch: 11, Train_Loss: 0.3306209444999695, Test_Loss: 0.9610748291015625\n",
      "Epoch: 11, Train_Loss: 0.3484722673892975, Test_Loss: 0.576176643371582 *\n",
      "Epoch: 11, Train_Loss: 0.4371366798877716, Test_Loss: 0.5928195118904114\n",
      "Epoch: 11, Train_Loss: 0.5524446368217468, Test_Loss: 0.5921862125396729 *\n",
      "Epoch: 11, Train_Loss: 0.34384703636169434, Test_Loss: 0.5866363048553467 *\n",
      "Epoch: 11, Train_Loss: 0.38529470562934875, Test_Loss: 0.6596123576164246\n",
      "Epoch: 11, Train_Loss: 0.39946797490119934, Test_Loss: 3.930363416671753\n",
      "Epoch: 11, Train_Loss: 0.3580327033996582, Test_Loss: 1.7605156898498535 *\n",
      "Epoch: 11, Train_Loss: 0.35905301570892334, Test_Loss: 0.3611549139022827 *\n",
      "Epoch: 11, Train_Loss: 0.3642968237400055, Test_Loss: 0.34516704082489014 *\n",
      "Epoch: 11, Train_Loss: 0.5688231587409973, Test_Loss: 0.34076669812202454 *\n",
      "Epoch: 11, Train_Loss: 0.5192117691040039, Test_Loss: 0.3223305344581604 *\n",
      "Epoch: 11, Train_Loss: 0.41036757826805115, Test_Loss: 0.3731531500816345\n",
      "Epoch: 11, Train_Loss: 0.3397652208805084, Test_Loss: 0.37859994173049927\n",
      "Epoch: 11, Train_Loss: 0.3617396354675293, Test_Loss: 0.3393373489379883 *\n",
      "Epoch: 11, Train_Loss: 0.5602467060089111, Test_Loss: 0.3431251645088196\n",
      "Epoch: 11, Train_Loss: 0.7008538246154785, Test_Loss: 0.3502570390701294\n",
      "Epoch: 11, Train_Loss: 0.5505225658416748, Test_Loss: 0.36380428075790405\n",
      "Epoch: 11, Train_Loss: 0.3806312084197998, Test_Loss: 0.42712464928627014\n",
      "Epoch: 11, Train_Loss: 0.34611666202545166, Test_Loss: 0.33102354407310486 *\n",
      "Epoch: 11, Train_Loss: 0.31912416219711304, Test_Loss: 0.3597690463066101\n",
      "Epoch: 11, Train_Loss: 0.598899781703949, Test_Loss: 0.3502103388309479 *\n",
      "Epoch: 11, Train_Loss: 0.4160695970058441, Test_Loss: 0.32177525758743286 *\n",
      "Epoch: 11, Train_Loss: 0.3347816467285156, Test_Loss: 0.3434911370277405\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 11\n",
      "Epoch: 11, Train_Loss: 0.5111950635910034, Test_Loss: 0.4000684320926666\n",
      "Epoch: 11, Train_Loss: 0.35379907488822937, Test_Loss: 0.3631831407546997 *\n",
      "Epoch: 11, Train_Loss: 0.3240852952003479, Test_Loss: 0.3264160454273224 *\n",
      "Epoch: 11, Train_Loss: 0.3371657729148865, Test_Loss: 0.34041598439216614\n",
      "Epoch: 11, Train_Loss: 0.42525526881217957, Test_Loss: 0.3801339864730835\n",
      "Epoch: 11, Train_Loss: 0.3637320399284363, Test_Loss: 0.40427863597869873\n",
      "Epoch: 11, Train_Loss: 0.4924171566963196, Test_Loss: 0.43637406826019287\n",
      "Epoch: 11, Train_Loss: 0.330155611038208, Test_Loss: 0.3712742030620575 *\n",
      "Epoch: 11, Train_Loss: 0.5597250461578369, Test_Loss: 0.3446652591228485 *\n",
      "Epoch: 11, Train_Loss: 0.37225908041000366, Test_Loss: 0.36381852626800537\n",
      "Epoch: 11, Train_Loss: 0.34355881810188293, Test_Loss: 0.34303373098373413 *\n",
      "Epoch: 11, Train_Loss: 0.32591360807418823, Test_Loss: 0.3194478154182434 *\n",
      "Epoch: 11, Train_Loss: 0.35408174991607666, Test_Loss: 0.4377114176750183\n",
      "Epoch: 11, Train_Loss: 0.5769920349121094, Test_Loss: 0.3990386724472046 *\n",
      "Epoch: 11, Train_Loss: 0.5752512216567993, Test_Loss: 6.101788520812988\n",
      "Epoch: 11, Train_Loss: 0.5788217782974243, Test_Loss: 0.4093500077724457 *\n",
      "Epoch: 11, Train_Loss: 0.775083065032959, Test_Loss: 0.43059828877449036\n",
      "Epoch: 11, Train_Loss: 0.5422958135604858, Test_Loss: 0.43064531683921814\n",
      "Epoch: 11, Train_Loss: 0.4646478295326233, Test_Loss: 0.32479166984558105 *\n",
      "Epoch: 11, Train_Loss: 0.40232178568840027, Test_Loss: 0.32323554158210754 *\n",
      "Epoch: 11, Train_Loss: 0.32786762714385986, Test_Loss: 0.3350377082824707\n",
      "Epoch: 11, Train_Loss: 0.3239942789077759, Test_Loss: 0.3804769217967987\n",
      "Epoch: 11, Train_Loss: 0.3255137801170349, Test_Loss: 0.3539215624332428 *\n",
      "Epoch: 11, Train_Loss: 0.4733717441558838, Test_Loss: 0.3220072090625763 *\n",
      "Epoch: 11, Train_Loss: 0.5943272113800049, Test_Loss: 0.35107770562171936\n",
      "Epoch: 11, Train_Loss: 0.5808122158050537, Test_Loss: 0.4477507472038269\n",
      "Epoch: 11, Train_Loss: 1.5592989921569824, Test_Loss: 0.3841952681541443 *\n",
      "Epoch: 11, Train_Loss: 1.0510940551757812, Test_Loss: 0.3950769007205963\n",
      "Epoch: 11, Train_Loss: 0.4736461043357849, Test_Loss: 0.3204862177371979 *\n",
      "Epoch: 11, Train_Loss: 0.38325509428977966, Test_Loss: 0.41001594066619873\n",
      "Epoch: 11, Train_Loss: 0.322287917137146, Test_Loss: 0.3430977463722229 *\n",
      "Epoch: 11, Train_Loss: 0.48538699746131897, Test_Loss: 0.42891526222229004\n",
      "Epoch: 11, Train_Loss: 0.9229302406311035, Test_Loss: 0.5016350746154785\n",
      "Epoch: 11, Train_Loss: 0.9953485131263733, Test_Loss: 0.3371228277683258 *\n",
      "Epoch: 11, Train_Loss: 0.35460278391838074, Test_Loss: 0.37910914421081543\n",
      "Epoch: 11, Train_Loss: 0.3549814820289612, Test_Loss: 0.3636069595813751 *\n",
      "Epoch: 11, Train_Loss: 0.38192036747932434, Test_Loss: 0.3689567446708679\n",
      "Epoch: 11, Train_Loss: 0.6002065539360046, Test_Loss: 0.35292452573776245 *\n",
      "Epoch: 11, Train_Loss: 0.473499059677124, Test_Loss: 0.3604441285133362\n",
      "Epoch: 11, Train_Loss: 0.386868417263031, Test_Loss: 0.41207194328308105\n",
      "Epoch: 11, Train_Loss: 0.429052472114563, Test_Loss: 0.38502568006515503 *\n",
      "Epoch: 11, Train_Loss: 0.36078470945358276, Test_Loss: 0.3541606366634369 *\n",
      "Epoch: 11, Train_Loss: 0.3190179765224457, Test_Loss: 0.3886648416519165\n",
      "Epoch: 11, Train_Loss: 0.3638092577457428, Test_Loss: 0.376280277967453 *\n",
      "Epoch: 11, Train_Loss: 0.3245861828327179, Test_Loss: 0.3204236328601837 *\n",
      "Epoch: 11, Train_Loss: 0.40981486439704895, Test_Loss: 0.38641664385795593\n",
      "Epoch: 11, Train_Loss: 0.36027228832244873, Test_Loss: 0.4489792585372925\n",
      "Epoch: 11, Train_Loss: 0.5235719680786133, Test_Loss: 0.47014176845550537\n",
      "Epoch: 11, Train_Loss: 16.070995330810547, Test_Loss: 0.3905165493488312 *\n",
      "Epoch: 11, Train_Loss: 0.48120734095573425, Test_Loss: 0.3438504934310913 *\n",
      "Epoch: 11, Train_Loss: 1.160266399383545, Test_Loss: 0.3350820243358612 *\n",
      "Epoch: 11, Train_Loss: 1.0702919960021973, Test_Loss: 0.33899855613708496\n",
      "Epoch: 11, Train_Loss: 0.3434326946735382, Test_Loss: 0.4713183641433716\n",
      "Epoch: 11, Train_Loss: 0.4843463897705078, Test_Loss: 0.5386574268341064\n",
      "Epoch: 12, Train_Loss: 6.1502556800842285, Test_Loss: 0.44495707750320435 *\n",
      "Epoch: 12, Train_Loss: 3.5301103591918945, Test_Loss: 0.6292233467102051\n",
      "Epoch: 12, Train_Loss: 0.3466722071170807, Test_Loss: 0.41911613941192627 *\n",
      "Epoch: 12, Train_Loss: 0.9398386478424072, Test_Loss: 0.342454731464386 *\n",
      "Epoch: 12, Train_Loss: 4.621701240539551, Test_Loss: 0.4343806803226471\n",
      "Epoch: 12, Train_Loss: 0.7420012950897217, Test_Loss: 0.45933297276496887\n",
      "Epoch: 12, Train_Loss: 0.39173856377601624, Test_Loss: 0.5926129221916199\n",
      "Epoch: 12, Train_Loss: 0.4899054169654846, Test_Loss: 0.6616113185882568\n",
      "Epoch: 12, Train_Loss: 0.4745434522628784, Test_Loss: 0.9569935202598572\n",
      "Epoch: 12, Train_Loss: 0.36843547224998474, Test_Loss: 1.0671108961105347\n",
      "Epoch: 12, Train_Loss: 0.31131279468536377, Test_Loss: 0.6828083395957947 *\n",
      "Epoch: 12, Train_Loss: 0.33774495124816895, Test_Loss: 1.1405680179595947\n",
      "Epoch: 12, Train_Loss: 0.3125182092189789, Test_Loss: 0.6863962411880493 *\n",
      "Epoch: 12, Train_Loss: 0.40250635147094727, Test_Loss: 0.8033924102783203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.4729948043823242, Test_Loss: 0.8272170424461365\n",
      "Epoch: 12, Train_Loss: 0.44470757246017456, Test_Loss: 0.6471676826477051 *\n",
      "Epoch: 12, Train_Loss: 0.4185541868209839, Test_Loss: 0.5193321704864502 *\n",
      "Epoch: 12, Train_Loss: 0.4078672528266907, Test_Loss: 0.40712594985961914 *\n",
      "Epoch: 12, Train_Loss: 0.37224942445755005, Test_Loss: 0.34677162766456604 *\n",
      "Epoch: 12, Train_Loss: 0.32288801670074463, Test_Loss: 7.329083442687988\n",
      "Epoch: 12, Train_Loss: 0.32749831676483154, Test_Loss: 1.0209572315216064 *\n",
      "Epoch: 12, Train_Loss: 0.32682666182518005, Test_Loss: 0.5224020481109619 *\n",
      "Epoch: 12, Train_Loss: 0.31701162457466125, Test_Loss: 0.4323670566082001 *\n",
      "Epoch: 12, Train_Loss: 0.3153753876686096, Test_Loss: 0.42808783054351807 *\n",
      "Epoch: 12, Train_Loss: 0.3093402087688446, Test_Loss: 0.337119460105896 *\n",
      "Epoch: 12, Train_Loss: 0.3099486827850342, Test_Loss: 0.5186824798583984\n",
      "Epoch: 12, Train_Loss: 0.3105446398258209, Test_Loss: 0.5291498899459839\n",
      "Epoch: 12, Train_Loss: 0.30869847536087036, Test_Loss: 0.3631698787212372 *\n",
      "Epoch: 12, Train_Loss: 0.31005582213401794, Test_Loss: 0.3756689429283142\n",
      "Epoch: 12, Train_Loss: 0.30895736813545227, Test_Loss: 0.38450247049331665\n",
      "Epoch: 12, Train_Loss: 0.32127800583839417, Test_Loss: 0.4817799925804138\n",
      "Epoch: 12, Train_Loss: 0.34373414516448975, Test_Loss: 0.402457058429718 *\n",
      "Epoch: 12, Train_Loss: 0.4013095200061798, Test_Loss: 0.361394464969635 *\n",
      "Epoch: 12, Train_Loss: 0.3411974012851715, Test_Loss: 0.40309274196624756\n",
      "Epoch: 12, Train_Loss: 0.4138878881931305, Test_Loss: 0.33339160680770874 *\n",
      "Epoch: 12, Train_Loss: 6.113870620727539, Test_Loss: 0.32132890820503235 *\n",
      "Epoch: 12, Train_Loss: 3.2540764808654785, Test_Loss: 0.32491353154182434\n",
      "Epoch: 12, Train_Loss: 0.3317756652832031, Test_Loss: 0.41316717863082886\n",
      "Epoch: 12, Train_Loss: 0.3655833899974823, Test_Loss: 0.36533957719802856 *\n",
      "Epoch: 12, Train_Loss: 0.42084553837776184, Test_Loss: 0.3348620533943176 *\n",
      "Epoch: 12, Train_Loss: 0.36116549372673035, Test_Loss: 0.3660547435283661\n",
      "Epoch: 12, Train_Loss: 0.3452259600162506, Test_Loss: 0.4358311891555786\n",
      "Epoch: 12, Train_Loss: 0.3726334571838379, Test_Loss: 0.4117695093154907 *\n",
      "Epoch: 12, Train_Loss: 0.39843589067459106, Test_Loss: 0.44805198907852173\n",
      "Epoch: 12, Train_Loss: 0.47688865661621094, Test_Loss: 0.3616962134838104 *\n",
      "Epoch: 12, Train_Loss: 0.41796237230300903, Test_Loss: 0.34694963693618774 *\n",
      "Epoch: 12, Train_Loss: 0.3331233561038971, Test_Loss: 0.3495212197303772\n",
      "Epoch: 12, Train_Loss: 0.3451448380947113, Test_Loss: 0.34405583143234253 *\n",
      "Epoch: 12, Train_Loss: 0.3851405382156372, Test_Loss: 0.32781165838241577 *\n",
      "Epoch: 12, Train_Loss: 0.4502295255661011, Test_Loss: 0.42815276980400085\n",
      "Epoch: 12, Train_Loss: 0.3883269429206848, Test_Loss: 1.0859758853912354\n",
      "Epoch: 12, Train_Loss: 0.3628658354282379, Test_Loss: 5.553815841674805\n",
      "Epoch: 12, Train_Loss: 0.34978997707366943, Test_Loss: 0.3467862010002136 *\n",
      "Epoch: 12, Train_Loss: 0.31876352429389954, Test_Loss: 0.30999499559402466 *\n",
      "Epoch: 12, Train_Loss: 0.3816961646080017, Test_Loss: 0.3317851722240448\n",
      "Epoch: 12, Train_Loss: 0.3634589910507202, Test_Loss: 0.3315337598323822 *\n",
      "Epoch: 12, Train_Loss: 0.3151102364063263, Test_Loss: 0.3196048140525818 *\n",
      "Epoch: 12, Train_Loss: 0.30784404277801514, Test_Loss: 0.33124449849128723\n",
      "Epoch: 12, Train_Loss: 0.3083845376968384, Test_Loss: 0.4529089331626892\n",
      "Epoch: 12, Train_Loss: 0.34223246574401855, Test_Loss: 0.34035226702690125 *\n",
      "Epoch: 12, Train_Loss: 5.129370212554932, Test_Loss: 0.3109365999698639 *\n",
      "Epoch: 12, Train_Loss: 1.377781867980957, Test_Loss: 0.3498002588748932\n",
      "Epoch: 12, Train_Loss: 0.3085506856441498, Test_Loss: 0.3183106780052185 *\n",
      "Epoch: 12, Train_Loss: 0.32200589776039124, Test_Loss: 0.32136234641075134\n",
      "Epoch: 12, Train_Loss: 0.3141322731971741, Test_Loss: 0.3595324456691742\n",
      "Epoch: 12, Train_Loss: 0.30857136845588684, Test_Loss: 0.3207736313343048 *\n",
      "Epoch: 12, Train_Loss: 0.3122376501560211, Test_Loss: 0.4087790250778198\n",
      "Epoch: 12, Train_Loss: 0.30869534611701965, Test_Loss: 0.4203443229198456\n",
      "Epoch: 12, Train_Loss: 0.3385545611381531, Test_Loss: 0.34846872091293335 *\n",
      "Epoch: 12, Train_Loss: 0.3188709020614624, Test_Loss: 0.314300537109375 *\n",
      "Epoch: 12, Train_Loss: 0.3435375690460205, Test_Loss: 0.3408223092556\n",
      "Epoch: 12, Train_Loss: 0.31056350469589233, Test_Loss: 0.3565082252025604\n",
      "Epoch: 12, Train_Loss: 0.31359314918518066, Test_Loss: 0.3370298743247986 *\n",
      "Epoch: 12, Train_Loss: 0.3249986171722412, Test_Loss: 0.33520886301994324 *\n",
      "Epoch: 12, Train_Loss: 0.3093321621417999, Test_Loss: 0.33369535207748413 *\n",
      "Epoch: 12, Train_Loss: 0.3079899847507477, Test_Loss: 0.3369235396385193\n",
      "Epoch: 12, Train_Loss: 0.3177964687347412, Test_Loss: 0.36253291368484497\n",
      "Epoch: 12, Train_Loss: 0.3413083553314209, Test_Loss: 0.35541197657585144 *\n",
      "Epoch: 12, Train_Loss: 0.3291788697242737, Test_Loss: 0.3146645128726959 *\n",
      "Epoch: 12, Train_Loss: 0.30725735425949097, Test_Loss: 0.3531196415424347\n",
      "Epoch: 12, Train_Loss: 0.3060925006866455, Test_Loss: 0.31976041197776794 *\n",
      "Epoch: 12, Train_Loss: 0.3588838279247284, Test_Loss: 0.3114660978317261 *\n",
      "Epoch: 12, Train_Loss: 0.3326592743396759, Test_Loss: 0.34856733679771423\n",
      "Epoch: 12, Train_Loss: 0.32433661818504333, Test_Loss: 0.49917858839035034\n",
      "Epoch: 12, Train_Loss: 0.32845088839530945, Test_Loss: 0.5526102781295776\n",
      "Epoch: 12, Train_Loss: 0.37582623958587646, Test_Loss: 0.42828619480133057 *\n",
      "Epoch: 12, Train_Loss: 0.3772107660770416, Test_Loss: 0.33009183406829834 *\n",
      "Epoch: 12, Train_Loss: 0.3407418727874756, Test_Loss: 0.32406356930732727 *\n",
      "Epoch: 12, Train_Loss: 0.3807179033756256, Test_Loss: 0.3293449282646179\n",
      "Epoch: 12, Train_Loss: 0.5141035318374634, Test_Loss: 0.42152777314186096\n",
      "Epoch: 12, Train_Loss: 0.34558066725730896, Test_Loss: 0.7249716520309448\n",
      "Epoch: 12, Train_Loss: 0.38309651613235474, Test_Loss: 0.9353876113891602\n",
      "Epoch: 12, Train_Loss: 0.30628687143325806, Test_Loss: 0.366794615983963 *\n",
      "Epoch: 12, Train_Loss: 0.30924439430236816, Test_Loss: 0.3456764817237854 *\n",
      "Epoch: 12, Train_Loss: 0.3097248375415802, Test_Loss: 0.31659647822380066 *\n",
      "Epoch: 12, Train_Loss: 0.3074795603752136, Test_Loss: 0.3318570852279663\n",
      "Epoch: 12, Train_Loss: 0.30700209736824036, Test_Loss: 0.3282153904438019 *\n",
      "Epoch: 12, Train_Loss: 4.484567642211914, Test_Loss: 0.32887396216392517\n",
      "Epoch: 12, Train_Loss: 1.2167524099349976, Test_Loss: 0.35491642355918884\n",
      "Epoch: 12, Train_Loss: 0.30526798963546753, Test_Loss: 0.3232114613056183 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.31669914722442627, Test_Loss: 0.3165934085845947 *\n",
      "Epoch: 12, Train_Loss: 0.30861032009124756, Test_Loss: 0.40021848678588867\n",
      "Epoch: 12, Train_Loss: 0.3049895763397217, Test_Loss: 0.66905277967453\n",
      "Epoch: 12, Train_Loss: 0.30489829182624817, Test_Loss: 0.5030621290206909 *\n",
      "Epoch: 12, Train_Loss: 0.3045555353164673, Test_Loss: 0.3642614483833313 *\n",
      "Epoch: 12, Train_Loss: 0.3041256368160248, Test_Loss: 0.31899118423461914 *\n",
      "Epoch: 12, Train_Loss: 0.30508846044540405, Test_Loss: 0.31894341111183167 *\n",
      "Epoch: 12, Train_Loss: 0.3300550878047943, Test_Loss: 0.3194613456726074\n",
      "Epoch: 12, Train_Loss: 0.3658122718334198, Test_Loss: 0.3189322352409363 *\n",
      "Epoch: 12, Train_Loss: 0.3771710693836212, Test_Loss: 0.44882065057754517\n",
      "Epoch: 12, Train_Loss: 0.3739868402481079, Test_Loss: 5.846836566925049\n",
      "Epoch: 12, Train_Loss: 0.31268516182899475, Test_Loss: 0.4209446907043457 *\n",
      "Epoch: 12, Train_Loss: 0.34543541073799133, Test_Loss: 0.3166622221469879 *\n",
      "Epoch: 12, Train_Loss: 0.5210028290748596, Test_Loss: 0.3104615807533264 *\n",
      "Epoch: 12, Train_Loss: 0.5053067803382874, Test_Loss: 0.3140645921230316\n",
      "Epoch: 12, Train_Loss: 0.49902695417404175, Test_Loss: 0.3115239441394806 *\n",
      "Epoch: 12, Train_Loss: 0.335681289434433, Test_Loss: 0.30668866634368896 *\n",
      "Epoch: 12, Train_Loss: 0.30368635058403015, Test_Loss: 0.3145430088043213\n",
      "Epoch: 12, Train_Loss: 0.3031126856803894, Test_Loss: 0.30629226565361023 *\n",
      "Epoch: 12, Train_Loss: 0.3082757890224457, Test_Loss: 0.306107759475708 *\n",
      "Epoch: 12, Train_Loss: 0.313144326210022, Test_Loss: 0.30997714400291443\n",
      "Epoch: 12, Train_Loss: 0.31654202938079834, Test_Loss: 0.32521820068359375\n",
      "Epoch: 12, Train_Loss: 0.31317928433418274, Test_Loss: 0.3124953806400299 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.3026731312274933, Test_Loss: 0.31430351734161377\n",
      "Epoch: 12, Train_Loss: 0.3040531277656555, Test_Loss: 0.31726258993148804\n",
      "Epoch: 12, Train_Loss: 0.3199463486671448, Test_Loss: 0.30601778626441956 *\n",
      "Epoch: 12, Train_Loss: 0.41697394847869873, Test_Loss: 0.30786094069480896\n",
      "Epoch: 12, Train_Loss: 0.4804755449295044, Test_Loss: 0.3042892813682556 *\n",
      "Epoch: 12, Train_Loss: 0.4365001320838928, Test_Loss: 0.33516451716423035\n",
      "Epoch: 12, Train_Loss: 0.3760097324848175, Test_Loss: 0.30668675899505615 *\n",
      "Epoch: 12, Train_Loss: 0.4238048493862152, Test_Loss: 0.3118937909603119\n",
      "Epoch: 12, Train_Loss: 0.4425179064273834, Test_Loss: 0.30848583579063416 *\n",
      "Epoch: 12, Train_Loss: 0.31015118956565857, Test_Loss: 0.3427816331386566\n",
      "Epoch: 12, Train_Loss: 0.4592945873737335, Test_Loss: 0.334565669298172 *\n",
      "Epoch: 12, Train_Loss: 0.39284634590148926, Test_Loss: 0.3405206501483917\n",
      "Epoch: 12, Train_Loss: 0.5476583242416382, Test_Loss: 0.3173128068447113 *\n",
      "Epoch: 12, Train_Loss: 0.3072088360786438, Test_Loss: 0.31227385997772217 *\n",
      "Epoch: 12, Train_Loss: 1.2492626905441284, Test_Loss: 0.3200129270553589\n",
      "Epoch: 12, Train_Loss: 2.484191656112671, Test_Loss: 0.3119748830795288 *\n",
      "Epoch: 12, Train_Loss: 0.3438895046710968, Test_Loss: 0.3149139881134033\n",
      "Epoch: 12, Train_Loss: 0.35464659333229065, Test_Loss: 0.3781437575817108\n",
      "Epoch: 12, Train_Loss: 0.3360150456428528, Test_Loss: 2.290837287902832\n",
      "Epoch: 12, Train_Loss: 0.33229270577430725, Test_Loss: 4.000668525695801\n",
      "Epoch: 12, Train_Loss: 0.30255210399627686, Test_Loss: 0.31780993938446045 *\n",
      "Epoch: 12, Train_Loss: 0.3127546012401581, Test_Loss: 0.30310550332069397 *\n",
      "Epoch: 12, Train_Loss: 0.409143328666687, Test_Loss: 0.32482168078422546\n",
      "Epoch: 12, Train_Loss: 0.3895641565322876, Test_Loss: 0.3053071200847626 *\n",
      "Epoch: 12, Train_Loss: 0.36730480194091797, Test_Loss: 0.3275170624256134\n",
      "Epoch: 12, Train_Loss: 0.3412702977657318, Test_Loss: 0.3406459093093872\n",
      "Epoch: 12, Train_Loss: 0.32191017270088196, Test_Loss: 0.43125227093696594\n",
      "Epoch: 12, Train_Loss: 0.30802398920059204, Test_Loss: 0.30858439207077026 *\n",
      "Epoch: 12, Train_Loss: 0.3174836337566376, Test_Loss: 0.3165079355239868\n",
      "Epoch: 12, Train_Loss: 0.33698326349258423, Test_Loss: 0.3289929926395416\n",
      "Epoch: 12, Train_Loss: 0.3343333899974823, Test_Loss: 0.32048290967941284 *\n",
      "Epoch: 12, Train_Loss: 0.31738123297691345, Test_Loss: 0.30918604135513306 *\n",
      "Epoch: 12, Train_Loss: 0.3027530312538147, Test_Loss: 0.34742891788482666\n",
      "Epoch: 12, Train_Loss: 0.33054015040397644, Test_Loss: 0.3349562883377075 *\n",
      "Epoch: 12, Train_Loss: 0.3302205204963684, Test_Loss: 0.3741503059864044\n",
      "Epoch: 12, Train_Loss: 0.3195165693759918, Test_Loss: 0.38491806387901306\n",
      "Epoch: 12, Train_Loss: 0.30317172408103943, Test_Loss: 0.3225901424884796 *\n",
      "Epoch: 12, Train_Loss: 0.30121123790740967, Test_Loss: 0.3324398100376129\n",
      "Epoch: 12, Train_Loss: 0.30077171325683594, Test_Loss: 0.30264201760292053 *\n",
      "Epoch: 12, Train_Loss: 0.3014698326587677, Test_Loss: 0.3024570643901825 *\n",
      "Epoch: 12, Train_Loss: 0.3048253059387207, Test_Loss: 0.3030945658683777\n",
      "Epoch: 12, Train_Loss: 0.3038400411605835, Test_Loss: 0.30454152822494507\n",
      "Epoch: 12, Train_Loss: 0.3076447546482086, Test_Loss: 0.30259090662002563 *\n",
      "Epoch: 12, Train_Loss: 0.30307871103286743, Test_Loss: 0.3025380074977875 *\n",
      "Epoch: 12, Train_Loss: 0.3000296652317047, Test_Loss: 0.3055019974708557\n",
      "Epoch: 12, Train_Loss: 0.30414149165153503, Test_Loss: 0.30740123987197876\n",
      "Epoch: 12, Train_Loss: 0.31756535172462463, Test_Loss: 0.304251104593277 *\n",
      "Epoch: 12, Train_Loss: 0.32286471128463745, Test_Loss: 0.31398364901542664\n",
      "Epoch: 12, Train_Loss: 0.31908249855041504, Test_Loss: 0.30548039078712463 *\n",
      "Epoch: 12, Train_Loss: 0.3322812020778656, Test_Loss: 0.34178876876831055\n",
      "Epoch: 12, Train_Loss: 0.3093416392803192, Test_Loss: 0.31863459944725037 *\n",
      "Epoch: 12, Train_Loss: 0.3111681640148163, Test_Loss: 0.6721411943435669\n",
      "Epoch: 12, Train_Loss: 0.3006833791732788, Test_Loss: 0.7233341932296753\n",
      "Epoch: 12, Train_Loss: 0.301975816488266, Test_Loss: 0.4554781913757324 *\n",
      "Epoch: 12, Train_Loss: 0.3221902847290039, Test_Loss: 0.3238843083381653 *\n",
      "Epoch: 12, Train_Loss: 0.31096258759498596, Test_Loss: 0.32451170682907104\n",
      "Epoch: 12, Train_Loss: 0.3034933805465698, Test_Loss: 0.32788363099098206\n",
      "Epoch: 12, Train_Loss: 0.3016212284564972, Test_Loss: 0.4652409553527832\n",
      "Epoch: 12, Train_Loss: 0.3073217570781708, Test_Loss: 0.795711874961853\n",
      "Epoch: 12, Train_Loss: 0.34070906043052673, Test_Loss: 0.8047071695327759\n",
      "Epoch: 12, Train_Loss: 0.3437381982803345, Test_Loss: 0.355516642332077 *\n",
      "Epoch: 12, Train_Loss: 0.3488271236419678, Test_Loss: 0.3444039225578308 *\n",
      "Epoch: 12, Train_Loss: 0.3005799651145935, Test_Loss: 0.30657413601875305 *\n",
      "Epoch: 12, Train_Loss: 0.34239375591278076, Test_Loss: 0.3104526400566101\n",
      "Epoch: 12, Train_Loss: 0.3300682306289673, Test_Loss: 0.3110924959182739\n",
      "Epoch: 12, Train_Loss: 0.30647680163383484, Test_Loss: 0.30947914719581604 *\n",
      "Epoch: 12, Train_Loss: 0.3144460618495941, Test_Loss: 0.3607216775417328\n",
      "Epoch: 12, Train_Loss: 0.3289460837841034, Test_Loss: 0.30126672983169556 *\n",
      "Epoch: 12, Train_Loss: 0.3990318179130554, Test_Loss: 0.3289448916912079\n",
      "Epoch: 12, Train_Loss: 0.3844940662384033, Test_Loss: 0.4177934527397156\n",
      "Epoch: 12, Train_Loss: 0.34635332226753235, Test_Loss: 0.6271214485168457\n",
      "Epoch: 12, Train_Loss: 0.3266395032405853, Test_Loss: 0.5364932417869568 *\n",
      "Epoch: 12, Train_Loss: 0.3041280508041382, Test_Loss: 0.31577441096305847 *\n",
      "Epoch: 12, Train_Loss: 0.32283303141593933, Test_Loss: 0.3106071352958679 *\n",
      "Epoch: 12, Train_Loss: 0.29929888248443604, Test_Loss: 0.3101222515106201 *\n",
      "Epoch: 12, Train_Loss: 0.3069731593132019, Test_Loss: 0.3103400468826294\n",
      "Epoch: 12, Train_Loss: 0.3060116767883301, Test_Loss: 0.3120191991329193\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.31380876898765564, Test_Loss: 0.9726191759109497\n",
      "Epoch: 12, Train_Loss: 0.38836902379989624, Test_Loss: 5.381364345550537\n",
      "Epoch: 12, Train_Loss: 0.30021384358406067, Test_Loss: 0.33919328451156616 *\n",
      "Epoch: 12, Train_Loss: 0.35593149065971375, Test_Loss: 0.3089471757411957 *\n",
      "Epoch: 12, Train_Loss: 0.3041314482688904, Test_Loss: 0.30726340413093567 *\n",
      "Epoch: 12, Train_Loss: 0.31996268033981323, Test_Loss: 0.3050503134727478 *\n",
      "Epoch: 12, Train_Loss: 0.3147779405117035, Test_Loss: 0.3030969202518463 *\n",
      "Epoch: 12, Train_Loss: 0.5843637585639954, Test_Loss: 0.3040167987346649\n",
      "Epoch: 12, Train_Loss: 0.3384162187576294, Test_Loss: 0.30909061431884766\n",
      "Epoch: 12, Train_Loss: 0.3231433928012848, Test_Loss: 0.29998165369033813 *\n",
      "Epoch: 12, Train_Loss: 0.30176252126693726, Test_Loss: 0.3009870946407318\n",
      "Epoch: 12, Train_Loss: 0.3007217347621918, Test_Loss: 0.30487000942230225\n",
      "Epoch: 12, Train_Loss: 0.2993994951248169, Test_Loss: 0.32429853081703186\n",
      "Epoch: 12, Train_Loss: 0.29769885540008545, Test_Loss: 0.30790528655052185 *\n",
      "Epoch: 12, Train_Loss: 0.30272921919822693, Test_Loss: 0.3051266670227051 *\n",
      "Epoch: 12, Train_Loss: 0.301120787858963, Test_Loss: 0.3108726441860199\n",
      "Epoch: 12, Train_Loss: 0.3132944107055664, Test_Loss: 0.30009159445762634 *\n",
      "Epoch: 12, Train_Loss: 0.30285730957984924, Test_Loss: 0.3017459511756897\n",
      "Epoch: 12, Train_Loss: 0.3020383417606354, Test_Loss: 0.29897356033325195 *\n",
      "Epoch: 12, Train_Loss: 0.30490249395370483, Test_Loss: 0.3376440405845642\n",
      "Epoch: 12, Train_Loss: 0.2992092967033386, Test_Loss: 0.29867929220199585 *\n",
      "Epoch: 12, Train_Loss: 0.2968375086784363, Test_Loss: 0.30538782477378845\n",
      "Epoch: 12, Train_Loss: 0.3112359941005707, Test_Loss: 0.2993268668651581 *\n",
      "Epoch: 12, Train_Loss: 0.3067820966243744, Test_Loss: 0.32729342579841614\n",
      "Epoch: 12, Train_Loss: 0.3182404339313507, Test_Loss: 0.32570114731788635 *\n",
      "Epoch: 12, Train_Loss: 0.2980891466140747, Test_Loss: 0.3227035105228424 *\n",
      "Epoch: 12, Train_Loss: 0.3194314241409302, Test_Loss: 0.30547186732292175 *\n",
      "Epoch: 12, Train_Loss: 0.3414815366268158, Test_Loss: 0.3048020601272583 *\n",
      "Epoch: 12, Train_Loss: 0.32505083084106445, Test_Loss: 0.30844682455062866\n",
      "Epoch: 12, Train_Loss: 0.29685232043266296, Test_Loss: 0.3011454939842224 *\n",
      "Epoch: 12, Train_Loss: 0.3283221423625946, Test_Loss: 0.32690781354904175\n",
      "Epoch: 12, Train_Loss: 0.3009696304798126, Test_Loss: 0.359135240316391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.30997076630592346, Test_Loss: 3.4735448360443115\n",
      "Epoch: 12, Train_Loss: 0.29661574959754944, Test_Loss: 3.0001955032348633 *\n",
      "Epoch: 12, Train_Loss: 0.3179429769515991, Test_Loss: 0.3054160177707672 *\n",
      "Epoch: 12, Train_Loss: 0.3455226421356201, Test_Loss: 0.2998028099536896 *\n",
      "Epoch: 12, Train_Loss: 3.0773887634277344, Test_Loss: 0.31246131658554077\n",
      "Epoch: 12, Train_Loss: 2.8757967948913574, Test_Loss: 0.29925093054771423 *\n",
      "Epoch: 12, Train_Loss: 0.34687119722366333, Test_Loss: 0.31656917929649353\n",
      "Epoch: 12, Train_Loss: 0.3011963367462158, Test_Loss: 0.3686123192310333\n",
      "Epoch: 12, Train_Loss: 0.407004177570343, Test_Loss: 0.42311134934425354\n",
      "Epoch: 12, Train_Loss: 0.4311829209327698, Test_Loss: 0.29739758372306824 *\n",
      "Epoch: 12, Train_Loss: 0.3357491195201874, Test_Loss: 0.32441800832748413\n",
      "Epoch: 12, Train_Loss: 0.29744625091552734, Test_Loss: 0.31736600399017334 *\n",
      "Epoch: 12, Train_Loss: 0.34365275502204895, Test_Loss: 0.3106474280357361 *\n",
      "Epoch: 12, Train_Loss: 0.34034568071365356, Test_Loss: 0.30325430631637573 *\n",
      "Epoch: 12, Train_Loss: 0.30462348461151123, Test_Loss: 0.3323071599006653\n",
      "Epoch: 12, Train_Loss: 0.37823957204818726, Test_Loss: 0.3364593982696533\n",
      "Epoch: 12, Train_Loss: 1.0288596153259277, Test_Loss: 0.3857663571834564\n",
      "Epoch: 12, Train_Loss: 1.3546624183654785, Test_Loss: 0.37082695960998535 *\n",
      "Epoch: 12, Train_Loss: 0.4223567247390747, Test_Loss: 0.32110387086868286 *\n",
      "Epoch: 12, Train_Loss: 0.4024048447608948, Test_Loss: 0.3138786554336548 *\n",
      "Epoch: 12, Train_Loss: 1.6078726053237915, Test_Loss: 0.2963387072086334 *\n",
      "Epoch: 12, Train_Loss: 1.3875675201416016, Test_Loss: 0.30119362473487854\n",
      "Epoch: 12, Train_Loss: 0.3127073645591736, Test_Loss: 0.2989935576915741 *\n",
      "Epoch: 12, Train_Loss: 0.2979099452495575, Test_Loss: 0.29926830530166626\n",
      "Epoch: 12, Train_Loss: 0.5729638338088989, Test_Loss: 0.30159345269203186\n",
      "Epoch: 12, Train_Loss: 1.0079021453857422, Test_Loss: 0.2979605793952942 *\n",
      "Epoch: 12, Train_Loss: 0.9658060073852539, Test_Loss: 0.30654504895210266\n",
      "Epoch: 12, Train_Loss: 0.2992095649242401, Test_Loss: 0.3125722408294678\n",
      "Epoch: 12, Train_Loss: 0.3196796476840973, Test_Loss: 0.30069732666015625 *\n",
      "Epoch: 12, Train_Loss: 0.36540836095809937, Test_Loss: 0.31505435705184937\n",
      "Epoch: 12, Train_Loss: 0.8084259033203125, Test_Loss: 0.2969174385070801 *\n",
      "Epoch: 12, Train_Loss: 0.3096800744533539, Test_Loss: 0.31579530239105225\n",
      "Epoch: 12, Train_Loss: 0.33497703075408936, Test_Loss: 0.3467121720314026\n",
      "Epoch: 12, Train_Loss: 0.3195417523384094, Test_Loss: 0.6125061511993408\n",
      "Epoch: 12, Train_Loss: 0.38484421372413635, Test_Loss: 0.6584819555282593\n",
      "Epoch: 12, Train_Loss: 0.43931058049201965, Test_Loss: 0.4265788793563843 *\n",
      "Epoch: 12, Train_Loss: 0.43889889121055603, Test_Loss: 0.3147262632846832 *\n",
      "Epoch: 12, Train_Loss: 0.3686295747756958, Test_Loss: 0.32280194759368896\n",
      "Epoch: 12, Train_Loss: 0.33963239192962646, Test_Loss: 0.3247538208961487\n",
      "Epoch: 12, Train_Loss: 0.4142985939979553, Test_Loss: 0.4464532136917114\n",
      "Epoch: 12, Train_Loss: 0.39263466000556946, Test_Loss: 0.337427020072937 *\n",
      "Epoch: 12, Train_Loss: 0.5534212589263916, Test_Loss: 0.6571472883224487\n",
      "Epoch: 12, Train_Loss: 0.5170808434486389, Test_Loss: 0.3661966621875763 *\n",
      "Epoch: 12, Train_Loss: 0.3250371217727661, Test_Loss: 0.34159162640571594 *\n",
      "Epoch: 12, Train_Loss: 0.4020650088787079, Test_Loss: 0.32221800088882446 *\n",
      "Epoch: 12, Train_Loss: 0.36694735288619995, Test_Loss: 0.3149316608905792 *\n",
      "Epoch: 12, Train_Loss: 0.35845187306404114, Test_Loss: 0.35395175218582153\n",
      "Epoch: 12, Train_Loss: 0.30594655871391296, Test_Loss: 0.4356158971786499\n",
      "Epoch: 12, Train_Loss: 0.2962508499622345, Test_Loss: 0.3126917779445648 *\n",
      "Epoch: 12, Train_Loss: 0.2968549132347107, Test_Loss: 0.3783683180809021\n",
      "Epoch: 12, Train_Loss: 0.29518458247184753, Test_Loss: 0.5630320310592651\n",
      "Epoch: 12, Train_Loss: 0.2994975447654724, Test_Loss: 0.47738656401634216 *\n",
      "Epoch: 12, Train_Loss: 0.31669551134109497, Test_Loss: 0.7121312022209167\n",
      "Epoch: 12, Train_Loss: 0.30799779295921326, Test_Loss: 1.100297212600708\n",
      "Epoch: 12, Train_Loss: 0.318307101726532, Test_Loss: 0.4902201294898987 *\n",
      "Epoch: 12, Train_Loss: 0.42866265773773193, Test_Loss: 0.5096896886825562\n",
      "Epoch: 12, Train_Loss: 0.5573736429214478, Test_Loss: 0.49494731426239014 *\n",
      "Epoch: 12, Train_Loss: 0.3096494674682617, Test_Loss: 0.5021070241928101\n",
      "Epoch: 12, Train_Loss: 0.34496834874153137, Test_Loss: 0.6036189198493958\n",
      "Epoch: 12, Train_Loss: 0.3617982566356659, Test_Loss: 1.8878573179244995\n",
      "Epoch: 12, Train_Loss: 0.3223223090171814, Test_Loss: 3.623657464981079\n",
      "Epoch: 12, Train_Loss: 0.4824754595756531, Test_Loss: 0.34750133752822876 *\n",
      "Epoch: 12, Train_Loss: 0.34491610527038574, Test_Loss: 0.31568336486816406 *\n",
      "Epoch: 12, Train_Loss: 0.5470329523086548, Test_Loss: 0.3234584331512451\n",
      "Epoch: 12, Train_Loss: 0.4710357189178467, Test_Loss: 0.3002992570400238 *\n",
      "Epoch: 12, Train_Loss: 0.4002598822116852, Test_Loss: 0.3442896008491516\n",
      "Epoch: 12, Train_Loss: 0.31667694449424744, Test_Loss: 0.35459238290786743\n",
      "Epoch: 12, Train_Loss: 0.31964221596717834, Test_Loss: 0.3330172300338745 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.4897971749305725, Test_Loss: 0.30152368545532227 *\n",
      "Epoch: 12, Train_Loss: 0.6809622645378113, Test_Loss: 0.3255951702594757\n",
      "Epoch: 12, Train_Loss: 0.7159261107444763, Test_Loss: 0.33028876781463623\n",
      "Epoch: 12, Train_Loss: 0.34197837114334106, Test_Loss: 0.4295535087585449\n",
      "Epoch: 12, Train_Loss: 0.3416808545589447, Test_Loss: 0.31556111574172974 *\n",
      "Epoch: 12, Train_Loss: 0.29869431257247925, Test_Loss: 0.3315778970718384\n",
      "Epoch: 12, Train_Loss: 0.5122957825660706, Test_Loss: 0.33716756105422974\n",
      "Epoch: 12, Train_Loss: 0.5367640256881714, Test_Loss: 0.30367961525917053 *\n",
      "Epoch: 12, Train_Loss: 0.3006712794303894, Test_Loss: 0.305738240480423\n",
      "Epoch: 12, Train_Loss: 0.4956014156341553, Test_Loss: 0.3430738151073456\n",
      "Epoch: 12, Train_Loss: 0.30914780497550964, Test_Loss: 0.39598381519317627\n",
      "Epoch: 12, Train_Loss: 0.30875080823898315, Test_Loss: 0.29864323139190674 *\n",
      "Epoch: 12, Train_Loss: 0.3133091330528259, Test_Loss: 0.31952130794525146\n",
      "Epoch: 12, Train_Loss: 0.39887091517448425, Test_Loss: 0.3095841705799103 *\n",
      "Epoch: 12, Train_Loss: 0.35019612312316895, Test_Loss: 0.4113888144493103\n",
      "Epoch: 12, Train_Loss: 0.43111228942871094, Test_Loss: 0.42426246404647827\n",
      "Epoch: 12, Train_Loss: 0.3165278136730194, Test_Loss: 0.3638151288032532 *\n",
      "Epoch: 12, Train_Loss: 0.42775824666023254, Test_Loss: 0.317756712436676 *\n",
      "Epoch: 12, Train_Loss: 0.3275953531265259, Test_Loss: 0.33145174384117126\n",
      "Epoch: 12, Train_Loss: 0.31600770354270935, Test_Loss: 0.3289564847946167 *\n",
      "Epoch: 12, Train_Loss: 0.3043048679828644, Test_Loss: 0.3037842810153961 *\n",
      "Epoch: 12, Train_Loss: 0.316005140542984, Test_Loss: 0.4215637445449829\n",
      "Epoch: 12, Train_Loss: 0.38530224561691284, Test_Loss: 0.4122740924358368 *\n",
      "Epoch: 12, Train_Loss: 0.5033587217330933, Test_Loss: 4.380711078643799\n",
      "Epoch: 12, Train_Loss: 0.46636149287223816, Test_Loss: 1.4865920543670654 *\n",
      "Epoch: 12, Train_Loss: 0.717694878578186, Test_Loss: 0.39793020486831665 *\n",
      "Epoch: 12, Train_Loss: 0.56222003698349, Test_Loss: 0.39864063262939453\n",
      "Epoch: 12, Train_Loss: 0.47832682728767395, Test_Loss: 0.33086860179901123 *\n",
      "Epoch: 12, Train_Loss: 0.3717271089553833, Test_Loss: 0.31338152289390564 *\n",
      "Epoch: 12, Train_Loss: 0.31668415665626526, Test_Loss: 0.312481164932251 *\n",
      "Epoch: 12, Train_Loss: 0.3027013838291168, Test_Loss: 0.3581330478191376\n",
      "Epoch: 12, Train_Loss: 0.30059564113616943, Test_Loss: 0.34119895100593567 *\n",
      "Epoch: 12, Train_Loss: 0.39966195821762085, Test_Loss: 0.2953512370586395 *\n",
      "Epoch: 12, Train_Loss: 0.5985867977142334, Test_Loss: 0.35615667700767517\n",
      "Epoch: 12, Train_Loss: 0.6725478172302246, Test_Loss: 0.39455896615982056\n",
      "Epoch: 12, Train_Loss: 1.4207866191864014, Test_Loss: 0.4436575174331665\n",
      "Epoch: 12, Train_Loss: 1.1909911632537842, Test_Loss: 0.3542085289955139 *\n",
      "Epoch: 12, Train_Loss: 0.4079567790031433, Test_Loss: 0.31239646673202515 *\n",
      "Epoch: 12, Train_Loss: 0.4667162299156189, Test_Loss: 0.3830794095993042\n",
      "Epoch: 12, Train_Loss: 0.2992876172065735, Test_Loss: 0.3586690425872803 *\n",
      "Epoch: 12, Train_Loss: 0.3936571776866913, Test_Loss: 0.3475956916809082 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.7223057746887207, Test_Loss: 0.49954119324684143\n",
      "Epoch: 12, Train_Loss: 1.256077766418457, Test_Loss: 0.36309435963630676 *\n",
      "Epoch: 12, Train_Loss: 0.315262109041214, Test_Loss: 0.34799644351005554 *\n",
      "Epoch: 12, Train_Loss: 0.3271050453186035, Test_Loss: 0.35106393694877625\n",
      "Epoch: 12, Train_Loss: 0.35517019033432007, Test_Loss: 0.3538050949573517\n",
      "Epoch: 12, Train_Loss: 0.547782301902771, Test_Loss: 0.32217133045196533 *\n",
      "Epoch: 12, Train_Loss: 0.4277147650718689, Test_Loss: 0.34032416343688965\n",
      "Epoch: 12, Train_Loss: 0.5066617727279663, Test_Loss: 0.38874074816703796\n",
      "Epoch: 12, Train_Loss: 0.4479827880859375, Test_Loss: 0.36364224553108215 *\n",
      "Epoch: 12, Train_Loss: 0.49238115549087524, Test_Loss: 0.35507169365882874 *\n",
      "Epoch: 12, Train_Loss: 0.29453712701797485, Test_Loss: 0.3416599929332733 *\n",
      "Epoch: 12, Train_Loss: 0.31825217604637146, Test_Loss: 0.36474424600601196\n",
      "Epoch: 12, Train_Loss: 0.29943394660949707, Test_Loss: 0.311298131942749 *\n",
      "Epoch: 12, Train_Loss: 0.366652250289917, Test_Loss: 0.3141821622848511\n",
      "Epoch: 12, Train_Loss: 0.3087387681007385, Test_Loss: 0.4080981910228729\n",
      "Epoch: 12, Train_Loss: 0.35530322790145874, Test_Loss: 0.47583743929862976\n",
      "Epoch: 12, Train_Loss: 16.16761589050293, Test_Loss: 0.4801245927810669\n",
      "Epoch: 12, Train_Loss: 0.364274799823761, Test_Loss: 0.3490229547023773 *\n",
      "Epoch: 12, Train_Loss: 1.3734164237976074, Test_Loss: 0.29382413625717163 *\n",
      "Epoch: 12, Train_Loss: 1.1923484802246094, Test_Loss: 0.3080626130104065\n",
      "Epoch: 12, Train_Loss: 0.30506712198257446, Test_Loss: 0.3822741210460663\n",
      "Epoch: 12, Train_Loss: 0.3862207233905792, Test_Loss: 0.5302932262420654\n",
      "Epoch: 12, Train_Loss: 4.03150749206543, Test_Loss: 0.439078152179718 *\n",
      "Epoch: 12, Train_Loss: 6.00270938873291, Test_Loss: 0.6791103482246399\n",
      "Epoch: 12, Train_Loss: 0.4044572710990906, Test_Loss: 0.35122063755989075 *\n",
      "Epoch: 12, Train_Loss: 0.41013631224632263, Test_Loss: 0.34488266706466675 *\n",
      "Epoch: 12, Train_Loss: 5.6641082763671875, Test_Loss: 0.3823990225791931\n",
      "Epoch: 12, Train_Loss: 0.38667619228363037, Test_Loss: 0.35106173157691956 *\n",
      "Epoch: 12, Train_Loss: 0.36859193444252014, Test_Loss: 0.4320679306983948\n",
      "Epoch: 12, Train_Loss: 0.3495730459690094, Test_Loss: 0.30422648787498474 *\n",
      "Epoch: 12, Train_Loss: 0.4345027804374695, Test_Loss: 0.3552512526512146\n",
      "Epoch: 12, Train_Loss: 0.3611803650856018, Test_Loss: 0.3207031190395355 *\n",
      "Epoch: 12, Train_Loss: 0.29236170649528503, Test_Loss: 0.40472257137298584\n",
      "Epoch: 12, Train_Loss: 0.3015631437301636, Test_Loss: 0.7914398312568665\n",
      "Epoch: 12, Train_Loss: 0.2871469259262085, Test_Loss: 0.4315266013145447 *\n",
      "Epoch: 12, Train_Loss: 0.28969019651412964, Test_Loss: 0.4395154118537903\n",
      "Epoch: 12, Train_Loss: 0.3154450058937073, Test_Loss: 0.3386775255203247 *\n",
      "Epoch: 12, Train_Loss: 0.2964327335357666, Test_Loss: 0.3525696098804474\n",
      "Epoch: 12, Train_Loss: 0.3705229163169861, Test_Loss: 0.36618858575820923\n",
      "Epoch: 12, Train_Loss: 0.3466765582561493, Test_Loss: 0.36605119705200195 *\n",
      "Epoch: 12, Train_Loss: 0.3189249038696289, Test_Loss: 0.31186214089393616 *\n",
      "Epoch: 12, Train_Loss: 0.304190993309021, Test_Loss: 5.337132453918457\n",
      "Epoch: 12, Train_Loss: 0.3253941535949707, Test_Loss: 3.384610652923584 *\n",
      "Epoch: 12, Train_Loss: 0.30120304226875305, Test_Loss: 0.7435141801834106 *\n",
      "Epoch: 12, Train_Loss: 0.3042853772640228, Test_Loss: 0.7748067378997803\n",
      "Epoch: 12, Train_Loss: 0.31606313586235046, Test_Loss: 0.8651776313781738\n",
      "Epoch: 12, Train_Loss: 0.2892139256000519, Test_Loss: 0.5000718832015991 *\n",
      "Epoch: 12, Train_Loss: 0.2872243821620941, Test_Loss: 0.9580533504486084\n",
      "Epoch: 12, Train_Loss: 0.28912001848220825, Test_Loss: 1.126810073852539\n",
      "Epoch: 12, Train_Loss: 0.28740715980529785, Test_Loss: 0.8595582842826843 *\n",
      "Epoch: 12, Train_Loss: 0.2865765690803528, Test_Loss: 0.8449820280075073 *\n",
      "Epoch: 12, Train_Loss: 0.2865726947784424, Test_Loss: 0.9040987491607666\n",
      "Epoch: 12, Train_Loss: 0.29129645228385925, Test_Loss: 0.8318154811859131 *\n",
      "Epoch: 12, Train_Loss: 0.30587252974510193, Test_Loss: 1.1529258489608765\n",
      "Epoch: 12, Train_Loss: 0.3363049626350403, Test_Loss: 0.6792166829109192 *\n",
      "Epoch: 12, Train_Loss: 0.3778011202812195, Test_Loss: 0.7789679765701294\n",
      "Epoch: 12, Train_Loss: 0.4720573425292969, Test_Loss: 0.700369656085968 *\n",
      "Epoch: 12, Train_Loss: 2.2963132858276367, Test_Loss: 0.33976611495018005 *\n",
      "Epoch: 12, Train_Loss: 5.233548641204834, Test_Loss: 0.34998080134391785\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 12\n",
      "Epoch: 12, Train_Loss: 0.3596603274345398, Test_Loss: 0.4151226878166199\n",
      "Epoch: 12, Train_Loss: 0.5297424793243408, Test_Loss: 0.5571575164794922\n",
      "Epoch: 12, Train_Loss: 0.44986218214035034, Test_Loss: 0.4367438554763794 *\n",
      "Epoch: 12, Train_Loss: 0.47987765073776245, Test_Loss: 0.6062296628952026\n",
      "Epoch: 12, Train_Loss: 0.47052037715911865, Test_Loss: 0.43085533380508423 *\n",
      "Epoch: 12, Train_Loss: 0.5614687204360962, Test_Loss: 0.5364891290664673\n",
      "Epoch: 12, Train_Loss: 0.5246912240982056, Test_Loss: 0.5324530601501465 *\n",
      "Epoch: 12, Train_Loss: 0.5326660871505737, Test_Loss: 0.33698779344558716 *\n",
      "Epoch: 12, Train_Loss: 0.42437341809272766, Test_Loss: 0.315082848072052 *\n",
      "Epoch: 12, Train_Loss: 0.33815065026283264, Test_Loss: 0.346004456281662\n",
      "Epoch: 12, Train_Loss: 0.3128299117088318, Test_Loss: 0.3370129466056824 *\n",
      "Epoch: 12, Train_Loss: 0.3838501274585724, Test_Loss: 0.29098254442214966 *\n",
      "Epoch: 12, Train_Loss: 0.40140363574028015, Test_Loss: 0.4525062143802643\n",
      "Epoch: 12, Train_Loss: 0.4680463671684265, Test_Loss: 0.32438212633132935 *\n",
      "Epoch: 12, Train_Loss: 0.3581019341945648, Test_Loss: 5.634927749633789\n",
      "Epoch: 12, Train_Loss: 0.34326475858688354, Test_Loss: 0.6790856719017029 *\n",
      "Epoch: 12, Train_Loss: 0.2881051003932953, Test_Loss: 0.2873290777206421 *\n",
      "Epoch: 12, Train_Loss: 0.3333025872707367, Test_Loss: 0.3212425708770752\n",
      "Epoch: 12, Train_Loss: 0.34992605447769165, Test_Loss: 0.32399511337280273\n",
      "Epoch: 12, Train_Loss: 0.30526596307754517, Test_Loss: 0.3029190003871918 *\n",
      "Epoch: 12, Train_Loss: 0.28619810938835144, Test_Loss: 0.29291898012161255 *\n",
      "Epoch: 12, Train_Loss: 0.28611913323402405, Test_Loss: 0.36268219351768494\n",
      "Epoch: 12, Train_Loss: 0.2920301556587219, Test_Loss: 0.36424520611763\n",
      "Epoch: 12, Train_Loss: 2.6742148399353027, Test_Loss: 0.2885030210018158 *\n",
      "Epoch: 12, Train_Loss: 3.987924098968506, Test_Loss: 0.3298493027687073\n",
      "Epoch: 12, Train_Loss: 0.2926959991455078, Test_Loss: 0.2918485105037689 *\n",
      "Epoch: 12, Train_Loss: 0.286101371049881, Test_Loss: 0.3015497624874115\n",
      "Epoch: 12, Train_Loss: 0.2892332971096039, Test_Loss: 0.3259289264678955\n",
      "Epoch: 12, Train_Loss: 0.28626495599746704, Test_Loss: 0.2922075688838959 *\n",
      "Epoch: 12, Train_Loss: 0.287811279296875, Test_Loss: 0.34407004714012146\n",
      "Epoch: 12, Train_Loss: 0.28553229570388794, Test_Loss: 0.42080605030059814\n",
      "Epoch: 12, Train_Loss: 0.31115972995758057, Test_Loss: 0.3582766652107239 *\n",
      "Epoch: 12, Train_Loss: 0.3188857436180115, Test_Loss: 0.29635196924209595 *\n",
      "Epoch: 12, Train_Loss: 0.30324676632881165, Test_Loss: 0.31547483801841736\n",
      "Epoch: 12, Train_Loss: 0.286044180393219, Test_Loss: 0.33589738607406616\n",
      "Epoch: 12, Train_Loss: 0.28485822677612305, Test_Loss: 0.3273603916168213 *\n",
      "Epoch: 12, Train_Loss: 0.2872632145881653, Test_Loss: 0.3366644084453583\n",
      "Epoch: 12, Train_Loss: 0.29978686571121216, Test_Loss: 0.317584753036499 *\n",
      "Epoch: 12, Train_Loss: 0.28657424449920654, Test_Loss: 0.3170364499092102 *\n",
      "Epoch: 12, Train_Loss: 0.28797265887260437, Test_Loss: 0.3622569441795349\n",
      "Epoch: 12, Train_Loss: 0.3185228407382965, Test_Loss: 0.32440072298049927 *\n",
      "Epoch: 12, Train_Loss: 0.31410127878189087, Test_Loss: 0.2915922701358795 *\n",
      "Epoch: 12, Train_Loss: 0.2852781116962433, Test_Loss: 0.29655882716178894\n",
      "Epoch: 12, Train_Loss: 0.2844735383987427, Test_Loss: 0.29668480157852173\n",
      "Epoch: 12, Train_Loss: 0.3168424069881439, Test_Loss: 0.3004190921783447\n",
      "Epoch: 12, Train_Loss: 0.3523946702480316, Test_Loss: 0.30410414934158325\n",
      "Epoch: 12, Train_Loss: 0.304535835981369, Test_Loss: 0.44185900688171387\n",
      "Epoch: 12, Train_Loss: 0.33456820249557495, Test_Loss: 0.5758895874023438\n",
      "Epoch: 12, Train_Loss: 0.32205531001091003, Test_Loss: 0.5139734745025635 *\n",
      "Epoch: 12, Train_Loss: 0.3454631268978119, Test_Loss: 0.36582010984420776 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train_Loss: 0.3163909316062927, Test_Loss: 0.31114012002944946 *\n",
      "Epoch: 12, Train_Loss: 0.3455026149749756, Test_Loss: 0.2948595881462097 *\n",
      "Epoch: 12, Train_Loss: 0.369638592004776, Test_Loss: 0.3788612186908722\n",
      "Epoch: 12, Train_Loss: 0.430415540933609, Test_Loss: 0.7064957022666931\n",
      "Epoch: 13, Train_Loss: 0.3722913861274719, Test_Loss: 0.9116817712783813 *\n",
      "Epoch: 13, Train_Loss: 0.30269554257392883, Test_Loss: 0.48627257347106934 *\n",
      "Epoch: 13, Train_Loss: 0.28681477904319763, Test_Loss: 0.3538777828216553 *\n",
      "Epoch: 13, Train_Loss: 0.2846260070800781, Test_Loss: 0.3091510832309723 *\n",
      "Epoch: 13, Train_Loss: 0.28547829389572144, Test_Loss: 0.3148278295993805\n",
      "Epoch: 13, Train_Loss: 0.2842666506767273, Test_Loss: 0.2993992865085602 *\n",
      "Epoch: 13, Train_Loss: 2.7522428035736084, Test_Loss: 0.3105289936065674\n",
      "Epoch: 13, Train_Loss: 2.787097930908203, Test_Loss: 0.31015706062316895 *\n",
      "Epoch: 13, Train_Loss: 0.28434938192367554, Test_Loss: 0.32046282291412354\n",
      "Epoch: 13, Train_Loss: 0.29594114422798157, Test_Loss: 0.29104742407798767 *\n",
      "Epoch: 13, Train_Loss: 0.2901093363761902, Test_Loss: 0.3986969292163849\n",
      "Epoch: 13, Train_Loss: 0.2837425172328949, Test_Loss: 0.629467248916626\n",
      "Epoch: 13, Train_Loss: 0.2834475636482239, Test_Loss: 0.37005338072776794 *\n",
      "Epoch: 13, Train_Loss: 0.2840360105037689, Test_Loss: 0.4756077826023102\n",
      "Epoch: 13, Train_Loss: 0.2827754020690918, Test_Loss: 0.29859328269958496 *\n",
      "Epoch: 13, Train_Loss: 0.2831360697746277, Test_Loss: 0.2997918128967285\n",
      "Epoch: 13, Train_Loss: 0.2932986617088318, Test_Loss: 0.2995453178882599 *\n",
      "Epoch: 13, Train_Loss: 0.3571696877479553, Test_Loss: 0.2992645502090454 *\n",
      "Epoch: 13, Train_Loss: 0.3428483009338379, Test_Loss: 0.31577473878860474\n",
      "Epoch: 13, Train_Loss: 0.3662528097629547, Test_Loss: 4.9386091232299805\n",
      "Epoch: 13, Train_Loss: 0.3099888265132904, Test_Loss: 1.2612981796264648 *\n",
      "Epoch: 13, Train_Loss: 0.28971239924430847, Test_Loss: 0.2976700961589813 *\n",
      "Epoch: 13, Train_Loss: 0.5038616061210632, Test_Loss: 0.28913259506225586 *\n",
      "Epoch: 13, Train_Loss: 0.5004608631134033, Test_Loss: 0.29038912057876587\n",
      "Epoch: 13, Train_Loss: 0.4827096462249756, Test_Loss: 0.29882189631462097\n",
      "Epoch: 13, Train_Loss: 0.3686912953853607, Test_Loss: 0.2871341109275818 *\n",
      "Epoch: 13, Train_Loss: 0.28241410851478577, Test_Loss: 0.2870299518108368 *\n",
      "Epoch: 13, Train_Loss: 0.2819882333278656, Test_Loss: 0.2833158075809479 *\n",
      "Epoch: 13, Train_Loss: 0.2865658104419708, Test_Loss: 0.2839202582836151\n",
      "Epoch: 13, Train_Loss: 0.2903907299041748, Test_Loss: 0.2851417362689972\n",
      "Epoch: 13, Train_Loss: 0.3005283772945404, Test_Loss: 0.2879811227321625\n",
      "Epoch: 13, Train_Loss: 0.2959141731262207, Test_Loss: 0.2896263301372528\n",
      "Epoch: 13, Train_Loss: 0.28152111172676086, Test_Loss: 0.30929145216941833\n",
      "Epoch: 13, Train_Loss: 0.2816527187824249, Test_Loss: 0.3012560307979584 *\n",
      "Epoch: 13, Train_Loss: 0.2961803674697876, Test_Loss: 0.28304001688957214 *\n",
      "Epoch: 13, Train_Loss: 0.35680896043777466, Test_Loss: 0.2831308841705322\n",
      "Epoch: 13, Train_Loss: 0.4794851243495941, Test_Loss: 0.28238847851753235 *\n",
      "Epoch: 13, Train_Loss: 0.42582565546035767, Test_Loss: 0.2963799238204956\n",
      "Epoch: 13, Train_Loss: 0.39390337467193604, Test_Loss: 0.28863951563835144 *\n",
      "Epoch: 13, Train_Loss: 0.3715929388999939, Test_Loss: 0.2837032973766327 *\n",
      "Epoch: 13, Train_Loss: 0.40979793667793274, Test_Loss: 0.28471839427948\n",
      "Epoch: 13, Train_Loss: 0.31138405203819275, Test_Loss: 0.2953934669494629\n",
      "Epoch: 13, Train_Loss: 0.4011368751525879, Test_Loss: 0.29783791303634644\n",
      "Epoch: 13, Train_Loss: 0.3567083179950714, Test_Loss: 0.3229699432849884\n",
      "Epoch: 13, Train_Loss: 0.5385733246803284, Test_Loss: 0.28904440999031067 *\n",
      "Epoch: 13, Train_Loss: 0.290893018245697, Test_Loss: 0.2849122881889343 *\n",
      "Epoch: 13, Train_Loss: 0.43332821130752563, Test_Loss: 0.2926376461982727\n",
      "Epoch: 13, Train_Loss: 3.219303607940674, Test_Loss: 0.2934127748012543\n",
      "Epoch: 13, Train_Loss: 0.3780475854873657, Test_Loss: 0.28353095054626465 *\n",
      "Epoch: 13, Train_Loss: 0.3371058702468872, Test_Loss: 0.36131882667541504\n",
      "Epoch: 13, Train_Loss: 0.3361262083053589, Test_Loss: 0.48261237144470215\n",
      "Epoch: 13, Train_Loss: 0.3178079128265381, Test_Loss: 5.546844005584717\n",
      "Epoch: 13, Train_Loss: 0.2855050563812256, Test_Loss: 0.3029170632362366 *\n",
      "Epoch: 13, Train_Loss: 0.28781580924987793, Test_Loss: 0.28165483474731445 *\n",
      "Epoch: 13, Train_Loss: 0.36260420083999634, Test_Loss: 0.30327972769737244\n",
      "Epoch: 13, Train_Loss: 0.3844118118286133, Test_Loss: 0.28522825241088867 *\n",
      "Epoch: 13, Train_Loss: 0.356960654258728, Test_Loss: 0.30730322003364563\n",
      "Epoch: 13, Train_Loss: 0.3272339701652527, Test_Loss: 0.2851411700248718 *\n",
      "Epoch: 13, Train_Loss: 0.3160816431045532, Test_Loss: 0.38487252593040466\n",
      "Epoch: 13, Train_Loss: 0.28633251786231995, Test_Loss: 0.3255162537097931 *\n",
      "Epoch: 13, Train_Loss: 0.29177170991897583, Test_Loss: 0.28127264976501465 *\n",
      "Epoch: 13, Train_Loss: 0.3090393841266632, Test_Loss: 0.3146260976791382\n",
      "Epoch: 13, Train_Loss: 0.34536534547805786, Test_Loss: 0.29499903321266174 *\n",
      "Epoch: 13, Train_Loss: 0.317087322473526, Test_Loss: 0.28759610652923584 *\n",
      "Epoch: 13, Train_Loss: 0.2864988148212433, Test_Loss: 0.3184807002544403\n",
      "Epoch: 13, Train_Loss: 0.294760137796402, Test_Loss: 0.2928951382637024 *\n",
      "Epoch: 13, Train_Loss: 0.30453914403915405, Test_Loss: 0.33806243538856506\n",
      "Epoch: 13, Train_Loss: 0.29860368371009827, Test_Loss: 0.36985087394714355\n",
      "Epoch: 13, Train_Loss: 0.2850097417831421, Test_Loss: 0.31692948937416077 *\n",
      "Epoch: 13, Train_Loss: 0.282382607460022, Test_Loss: 0.3118687570095062 *\n",
      "Epoch: 13, Train_Loss: 0.28104671835899353, Test_Loss: 0.28258270025253296 *\n",
      "Epoch: 13, Train_Loss: 0.28140485286712646, Test_Loss: 0.2832922041416168\n",
      "Epoch: 13, Train_Loss: 0.2816951870918274, Test_Loss: 0.28327471017837524 *\n",
      "Epoch: 13, Train_Loss: 0.28448429703712463, Test_Loss: 0.2834448218345642\n",
      "Epoch: 13, Train_Loss: 0.2862807810306549, Test_Loss: 0.28222230076789856 *\n",
      "Epoch: 13, Train_Loss: 0.28203660249710083, Test_Loss: 0.2825046479701996\n",
      "Epoch: 13, Train_Loss: 0.2802642583847046, Test_Loss: 0.2856254279613495\n",
      "Epoch: 13, Train_Loss: 0.2808637320995331, Test_Loss: 0.282473087310791 *\n",
      "Epoch: 13, Train_Loss: 0.28799542784690857, Test_Loss: 0.2872026562690735\n",
      "Epoch: 13, Train_Loss: 0.29828736186027527, Test_Loss: 0.2869226336479187 *\n",
      "Epoch: 13, Train_Loss: 0.30300652980804443, Test_Loss: 0.2935180366039276\n",
      "Epoch: 13, Train_Loss: 0.3134278357028961, Test_Loss: 0.33442071080207825\n",
      "Epoch: 13, Train_Loss: 0.28729015588760376, Test_Loss: 0.3026854395866394 *\n",
      "Epoch: 13, Train_Loss: 0.2869115173816681, Test_Loss: 0.590064287185669\n",
      "Epoch: 13, Train_Loss: 0.2790190875530243, Test_Loss: 0.8044149875640869\n",
      "Epoch: 13, Train_Loss: 0.27974021434783936, Test_Loss: 0.5200375914573669 *\n",
      "Epoch: 13, Train_Loss: 0.29668840765953064, Test_Loss: 0.3452332615852356 *\n",
      "Epoch: 13, Train_Loss: 0.298294335603714, Test_Loss: 0.3221454322338104 *\n",
      "Epoch: 13, Train_Loss: 0.280168741941452, Test_Loss: 0.28943347930908203 *\n",
      "Epoch: 13, Train_Loss: 0.28082275390625, Test_Loss: 0.37531590461730957\n",
      "Epoch: 13, Train_Loss: 0.2817698121070862, Test_Loss: 0.7922756671905518\n",
      "Epoch: 13, Train_Loss: 0.3221677541732788, Test_Loss: 1.0392553806304932\n",
      "Epoch: 13, Train_Loss: 0.3193219304084778, Test_Loss: 0.3583720624446869 *\n",
      "Epoch: 13, Train_Loss: 0.3472568094730377, Test_Loss: 0.37050947546958923\n",
      "Epoch: 13, Train_Loss: 0.28638672828674316, Test_Loss: 0.28091850876808167 *\n",
      "Epoch: 13, Train_Loss: 0.2924245595932007, Test_Loss: 0.28491610288619995\n",
      "Epoch: 13, Train_Loss: 0.3414391577243805, Test_Loss: 0.2862396538257599\n",
      "Epoch: 13, Train_Loss: 0.2828056216239929, Test_Loss: 0.2899706959724426\n",
      "Epoch: 13, Train_Loss: 0.28969746828079224, Test_Loss: 0.3073844015598297\n",
      "Epoch: 13, Train_Loss: 0.30062010884284973, Test_Loss: 0.30349281430244446 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.3303411900997162, Test_Loss: 0.28335458040237427 *\n",
      "Epoch: 13, Train_Loss: 0.3855019807815552, Test_Loss: 0.39199745655059814\n",
      "Epoch: 13, Train_Loss: 0.334163635969162, Test_Loss: 0.6552921533584595\n",
      "Epoch: 13, Train_Loss: 0.3028797507286072, Test_Loss: 0.3972744643688202 *\n",
      "Epoch: 13, Train_Loss: 0.2827266752719879, Test_Loss: 0.4123798906803131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.30190178751945496, Test_Loss: 0.29163822531700134 *\n",
      "Epoch: 13, Train_Loss: 0.27873051166534424, Test_Loss: 0.2916860580444336\n",
      "Epoch: 13, Train_Loss: 0.2828516662120819, Test_Loss: 0.2915405035018921 *\n",
      "Epoch: 13, Train_Loss: 0.2884197533130646, Test_Loss: 0.29135045409202576 *\n",
      "Epoch: 13, Train_Loss: 0.29983583092689514, Test_Loss: 0.32164570689201355\n",
      "Epoch: 13, Train_Loss: 0.34761038422584534, Test_Loss: 5.692617893218994\n",
      "Epoch: 13, Train_Loss: 0.3256697654724121, Test_Loss: 0.5054785013198853 *\n",
      "Epoch: 13, Train_Loss: 0.3248023986816406, Test_Loss: 0.2903470993041992 *\n",
      "Epoch: 13, Train_Loss: 0.29890528321266174, Test_Loss: 0.2822040319442749 *\n",
      "Epoch: 13, Train_Loss: 0.30428391695022583, Test_Loss: 0.2845221161842346\n",
      "Epoch: 13, Train_Loss: 0.28962403535842896, Test_Loss: 0.2878789007663727\n",
      "Epoch: 13, Train_Loss: 0.45596110820770264, Test_Loss: 0.28079816699028015 *\n",
      "Epoch: 13, Train_Loss: 0.43161192536354065, Test_Loss: 0.2816844880580902\n",
      "Epoch: 13, Train_Loss: 0.2804130017757416, Test_Loss: 0.2785840332508087 *\n",
      "Epoch: 13, Train_Loss: 0.3084438741207123, Test_Loss: 0.2780572474002838 *\n",
      "Epoch: 13, Train_Loss: 0.27678579092025757, Test_Loss: 0.27974197268486023\n",
      "Epoch: 13, Train_Loss: 0.2772083878517151, Test_Loss: 0.28857696056365967\n",
      "Epoch: 13, Train_Loss: 0.2788797914981842, Test_Loss: 0.28653550148010254 *\n",
      "Epoch: 13, Train_Loss: 0.27833297848701477, Test_Loss: 0.29933348298072815\n",
      "Epoch: 13, Train_Loss: 0.2792174518108368, Test_Loss: 0.29508283734321594 *\n",
      "Epoch: 13, Train_Loss: 0.2933136522769928, Test_Loss: 0.2770080268383026 *\n",
      "Epoch: 13, Train_Loss: 0.2810164988040924, Test_Loss: 0.27703848481178284\n",
      "Epoch: 13, Train_Loss: 0.2864023745059967, Test_Loss: 0.2763559818267822 *\n",
      "Epoch: 13, Train_Loss: 0.28408339619636536, Test_Loss: 0.29204559326171875\n",
      "Epoch: 13, Train_Loss: 0.27724888920783997, Test_Loss: 0.2787149250507355 *\n",
      "Epoch: 13, Train_Loss: 0.27856773138046265, Test_Loss: 0.2793552279472351\n",
      "Epoch: 13, Train_Loss: 0.276320219039917, Test_Loss: 0.2780078053474426 *\n",
      "Epoch: 13, Train_Loss: 0.30756044387817383, Test_Loss: 0.28701603412628174\n",
      "Epoch: 13, Train_Loss: 0.2991493344306946, Test_Loss: 0.28476572036743164 *\n",
      "Epoch: 13, Train_Loss: 0.29102325439453125, Test_Loss: 0.29688841104507446\n",
      "Epoch: 13, Train_Loss: 0.2877195477485657, Test_Loss: 0.27979007363319397 *\n",
      "Epoch: 13, Train_Loss: 0.3100273907184601, Test_Loss: 0.278358519077301 *\n",
      "Epoch: 13, Train_Loss: 0.3024003505706787, Test_Loss: 0.2824133038520813\n",
      "Epoch: 13, Train_Loss: 0.2847045361995697, Test_Loss: 0.27887099981307983 *\n",
      "Epoch: 13, Train_Loss: 0.30145031213760376, Test_Loss: 0.2797119617462158\n",
      "Epoch: 13, Train_Loss: 0.29552266001701355, Test_Loss: 0.3491535782814026\n",
      "Epoch: 13, Train_Loss: 0.282918781042099, Test_Loss: 1.5397024154663086\n",
      "Epoch: 13, Train_Loss: 0.27979549765586853, Test_Loss: 4.597925186157227\n",
      "Epoch: 13, Train_Loss: 0.3063347339630127, Test_Loss: 0.29306140542030334 *\n",
      "Epoch: 13, Train_Loss: 0.32900404930114746, Test_Loss: 0.275574266910553 *\n",
      "Epoch: 13, Train_Loss: 2.603813409805298, Test_Loss: 0.2979288399219513\n",
      "Epoch: 13, Train_Loss: 3.2112021446228027, Test_Loss: 0.2809728682041168 *\n",
      "Epoch: 13, Train_Loss: 0.29588961601257324, Test_Loss: 0.3008972108364105\n",
      "Epoch: 13, Train_Loss: 0.27960970997810364, Test_Loss: 0.2992449700832367 *\n",
      "Epoch: 13, Train_Loss: 0.33349859714508057, Test_Loss: 0.40535885095596313\n",
      "Epoch: 13, Train_Loss: 0.423850417137146, Test_Loss: 0.29601553082466125 *\n",
      "Epoch: 13, Train_Loss: 0.306284636259079, Test_Loss: 0.28415822982788086 *\n",
      "Epoch: 13, Train_Loss: 0.281498521566391, Test_Loss: 0.3126819133758545\n",
      "Epoch: 13, Train_Loss: 0.29214853048324585, Test_Loss: 0.2865557074546814 *\n",
      "Epoch: 13, Train_Loss: 0.3464128077030182, Test_Loss: 0.2860116958618164 *\n",
      "Epoch: 13, Train_Loss: 0.28430265188217163, Test_Loss: 0.3209086060523987\n",
      "Epoch: 13, Train_Loss: 0.28576526045799255, Test_Loss: 0.29430896043777466 *\n",
      "Epoch: 13, Train_Loss: 0.9231157302856445, Test_Loss: 0.3587155342102051\n",
      "Epoch: 13, Train_Loss: 1.0450981855392456, Test_Loss: 0.36206790804862976\n",
      "Epoch: 13, Train_Loss: 0.6851829290390015, Test_Loss: 0.30190742015838623 *\n",
      "Epoch: 13, Train_Loss: 0.38333532214164734, Test_Loss: 0.29845690727233887 *\n",
      "Epoch: 13, Train_Loss: 1.1358731985092163, Test_Loss: 0.2801807224750519 *\n",
      "Epoch: 13, Train_Loss: 1.6281383037567139, Test_Loss: 0.2927784323692322\n",
      "Epoch: 13, Train_Loss: 0.40789830684661865, Test_Loss: 0.28657642006874084 *\n",
      "Epoch: 13, Train_Loss: 0.28072023391723633, Test_Loss: 0.28443843126296997 *\n",
      "Epoch: 13, Train_Loss: 0.3390248715877533, Test_Loss: 0.2862975299358368\n",
      "Epoch: 13, Train_Loss: 0.9955651760101318, Test_Loss: 0.2908857464790344\n",
      "Epoch: 13, Train_Loss: 0.827607274055481, Test_Loss: 0.31284835934638977\n",
      "Epoch: 13, Train_Loss: 0.3042822480201721, Test_Loss: 0.3198058009147644\n",
      "Epoch: 13, Train_Loss: 0.30264174938201904, Test_Loss: 0.280244916677475 *\n",
      "Epoch: 13, Train_Loss: 0.2889821231365204, Test_Loss: 0.3080350160598755\n",
      "Epoch: 13, Train_Loss: 0.7274812459945679, Test_Loss: 0.277845561504364 *\n",
      "Epoch: 13, Train_Loss: 0.33635035157203674, Test_Loss: 0.29567283391952515\n",
      "Epoch: 13, Train_Loss: 0.3203452527523041, Test_Loss: 0.31222933530807495\n",
      "Epoch: 13, Train_Loss: 0.2814333438873291, Test_Loss: 0.519539475440979\n",
      "Epoch: 13, Train_Loss: 0.39790552854537964, Test_Loss: 0.6096763014793396\n",
      "Epoch: 13, Train_Loss: 0.41878432035446167, Test_Loss: 0.4398820400238037 *\n",
      "Epoch: 13, Train_Loss: 0.39660176634788513, Test_Loss: 0.3014868497848511 *\n",
      "Epoch: 13, Train_Loss: 0.3435925841331482, Test_Loss: 0.29999274015426636 *\n",
      "Epoch: 13, Train_Loss: 0.2994743287563324, Test_Loss: 0.29610008001327515 *\n",
      "Epoch: 13, Train_Loss: 0.3941608965396881, Test_Loss: 0.3417292833328247\n",
      "Epoch: 13, Train_Loss: 0.39246928691864014, Test_Loss: 0.3223845660686493 *\n",
      "Epoch: 13, Train_Loss: 0.47421273589134216, Test_Loss: 0.48062315583229065\n",
      "Epoch: 13, Train_Loss: 0.47432810068130493, Test_Loss: 0.33489078283309937 *\n",
      "Epoch: 13, Train_Loss: 0.32384368777275085, Test_Loss: 0.330489844083786 *\n",
      "Epoch: 13, Train_Loss: 0.3541686534881592, Test_Loss: 0.33454012870788574\n",
      "Epoch: 13, Train_Loss: 0.3491929769515991, Test_Loss: 0.3299022316932678 *\n",
      "Epoch: 13, Train_Loss: 0.3328370153903961, Test_Loss: 0.38565996289253235\n",
      "Epoch: 13, Train_Loss: 0.3059937357902527, Test_Loss: 0.5433328151702881\n",
      "Epoch: 13, Train_Loss: 0.27695682644844055, Test_Loss: 0.3117097318172455 *\n",
      "Epoch: 13, Train_Loss: 0.28383171558380127, Test_Loss: 0.43282565474510193\n",
      "Epoch: 13, Train_Loss: 0.28088828921318054, Test_Loss: 0.3725404143333435 *\n",
      "Epoch: 13, Train_Loss: 0.28018951416015625, Test_Loss: 0.5365064144134521\n",
      "Epoch: 13, Train_Loss: 0.3072107434272766, Test_Loss: 0.6062952280044556\n",
      "Epoch: 13, Train_Loss: 0.2961861789226532, Test_Loss: 0.9790761470794678\n",
      "Epoch: 13, Train_Loss: 0.2989460229873657, Test_Loss: 0.41714221239089966 *\n",
      "Epoch: 13, Train_Loss: 0.5371252298355103, Test_Loss: 0.3737863004207611 *\n",
      "Epoch: 13, Train_Loss: 0.41044846177101135, Test_Loss: 0.36413004994392395 *\n",
      "Epoch: 13, Train_Loss: 0.28256526589393616, Test_Loss: 0.371027410030365\n",
      "Epoch: 13, Train_Loss: 0.3127603828907013, Test_Loss: 0.41816335916519165\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.3426646888256073, Test_Loss: 0.6158190369606018\n",
      "Epoch: 13, Train_Loss: 0.3224526345729828, Test_Loss: 5.753833293914795\n",
      "Epoch: 13, Train_Loss: 0.5742229223251343, Test_Loss: 0.41842690110206604 *\n",
      "Epoch: 13, Train_Loss: 0.3261692225933075, Test_Loss: 0.3319914638996124 *\n",
      "Epoch: 13, Train_Loss: 0.431388795375824, Test_Loss: 0.31397366523742676 *\n",
      "Epoch: 13, Train_Loss: 0.3722449839115143, Test_Loss: 0.28117066621780396 *\n",
      "Epoch: 13, Train_Loss: 0.35591110587120056, Test_Loss: 0.29698267579078674\n",
      "Epoch: 13, Train_Loss: 0.29805561900138855, Test_Loss: 0.33832839131355286\n",
      "Epoch: 13, Train_Loss: 0.2925763726234436, Test_Loss: 0.33403754234313965 *\n",
      "Epoch: 13, Train_Loss: 0.3372194170951843, Test_Loss: 0.27792003750801086 *\n",
      "Epoch: 13, Train_Loss: 0.6544443368911743, Test_Loss: 0.29304540157318115\n",
      "Epoch: 13, Train_Loss: 0.6873459815979004, Test_Loss: 0.2983827292919159\n",
      "Epoch: 13, Train_Loss: 0.2972700595855713, Test_Loss: 0.4070091247558594\n",
      "Epoch: 13, Train_Loss: 0.3368365466594696, Test_Loss: 0.2997637391090393 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.28790801763534546, Test_Loss: 0.2963705062866211 *\n",
      "Epoch: 13, Train_Loss: 0.35275185108184814, Test_Loss: 0.3044006824493408\n",
      "Epoch: 13, Train_Loss: 0.5544450879096985, Test_Loss: 0.2788110375404358 *\n",
      "Epoch: 13, Train_Loss: 0.28305336833000183, Test_Loss: 0.27975863218307495\n",
      "Epoch: 13, Train_Loss: 0.4510767459869385, Test_Loss: 0.32459041476249695\n",
      "Epoch: 13, Train_Loss: 0.303577721118927, Test_Loss: 0.3635919988155365\n",
      "Epoch: 13, Train_Loss: 0.297227144241333, Test_Loss: 0.280172199010849 *\n",
      "Epoch: 13, Train_Loss: 0.30322349071502686, Test_Loss: 0.3017354905605316\n",
      "Epoch: 13, Train_Loss: 0.37708213925361633, Test_Loss: 0.2774302661418915 *\n",
      "Epoch: 13, Train_Loss: 0.34912464022636414, Test_Loss: 0.37042349576950073\n",
      "Epoch: 13, Train_Loss: 0.3296390175819397, Test_Loss: 0.35223865509033203 *\n",
      "Epoch: 13, Train_Loss: 0.2892143130302429, Test_Loss: 0.3554125726222992\n",
      "Epoch: 13, Train_Loss: 0.32068800926208496, Test_Loss: 0.2902407646179199 *\n",
      "Epoch: 13, Train_Loss: 0.3494011461734772, Test_Loss: 0.3070191442966461\n",
      "Epoch: 13, Train_Loss: 0.2968153655529022, Test_Loss: 0.32524386048316956\n",
      "Epoch: 13, Train_Loss: 0.29473215341567993, Test_Loss: 0.29372990131378174 *\n",
      "Epoch: 13, Train_Loss: 0.2849673628807068, Test_Loss: 0.2943495810031891\n",
      "Epoch: 13, Train_Loss: 0.3254009187221527, Test_Loss: 0.4221411347389221\n",
      "Epoch: 13, Train_Loss: 0.48755431175231934, Test_Loss: 2.7927494049072266\n",
      "Epoch: 13, Train_Loss: 0.5274407267570496, Test_Loss: 3.1061768531799316\n",
      "Epoch: 13, Train_Loss: 0.6821910738945007, Test_Loss: 0.32532641291618347 *\n",
      "Epoch: 13, Train_Loss: 0.5120640993118286, Test_Loss: 0.3417693078517914\n",
      "Epoch: 13, Train_Loss: 0.504352331161499, Test_Loss: 0.2952103018760681 *\n",
      "Epoch: 13, Train_Loss: 0.36200177669525146, Test_Loss: 0.33223843574523926\n",
      "Epoch: 13, Train_Loss: 0.32345184683799744, Test_Loss: 0.2984236478805542 *\n",
      "Epoch: 13, Train_Loss: 0.2842380404472351, Test_Loss: 0.34033891558647156\n",
      "Epoch: 13, Train_Loss: 0.28491872549057007, Test_Loss: 0.3580351173877716\n",
      "Epoch: 13, Train_Loss: 0.35114380717277527, Test_Loss: 0.2770744264125824 *\n",
      "Epoch: 13, Train_Loss: 0.570594310760498, Test_Loss: 0.3197496831417084\n",
      "Epoch: 13, Train_Loss: 0.692412257194519, Test_Loss: 0.318244606256485 *\n",
      "Epoch: 13, Train_Loss: 1.0125763416290283, Test_Loss: 0.38215240836143494\n",
      "Epoch: 13, Train_Loss: 1.2466073036193848, Test_Loss: 0.3114006519317627 *\n",
      "Epoch: 13, Train_Loss: 0.4035918414592743, Test_Loss: 0.3694257140159607\n",
      "Epoch: 13, Train_Loss: 0.503902018070221, Test_Loss: 0.3610769212245941 *\n",
      "Epoch: 13, Train_Loss: 0.2744595408439636, Test_Loss: 0.39664557576179504\n",
      "Epoch: 13, Train_Loss: 0.3147667646408081, Test_Loss: 0.28854504227638245 *\n",
      "Epoch: 13, Train_Loss: 0.5082695484161377, Test_Loss: 0.4108940064907074\n",
      "Epoch: 13, Train_Loss: 1.2913894653320312, Test_Loss: 0.31731560826301575 *\n",
      "Epoch: 13, Train_Loss: 0.3241344392299652, Test_Loss: 0.3680821657180786\n",
      "Epoch: 13, Train_Loss: 0.3072713017463684, Test_Loss: 0.39648139476776123\n",
      "Epoch: 13, Train_Loss: 0.3076220154762268, Test_Loss: 0.4011334180831909\n",
      "Epoch: 13, Train_Loss: 0.41371041536331177, Test_Loss: 0.33667320013046265 *\n",
      "Epoch: 13, Train_Loss: 0.5431789755821228, Test_Loss: 0.3626416325569153\n",
      "Epoch: 13, Train_Loss: 0.55036461353302, Test_Loss: 0.4115923047065735\n",
      "Epoch: 13, Train_Loss: 0.43614661693573, Test_Loss: 0.4211782217025757\n",
      "Epoch: 13, Train_Loss: 0.5687990188598633, Test_Loss: 0.4283875524997711\n",
      "Epoch: 13, Train_Loss: 0.2844042181968689, Test_Loss: 0.3289342522621155 *\n",
      "Epoch: 13, Train_Loss: 0.28529614210128784, Test_Loss: 0.3705938458442688\n",
      "Epoch: 13, Train_Loss: 0.28881457448005676, Test_Loss: 0.30912846326828003 *\n",
      "Epoch: 13, Train_Loss: 0.33897364139556885, Test_Loss: 0.2926107347011566 *\n",
      "Epoch: 13, Train_Loss: 0.27950409054756165, Test_Loss: 0.3104269206523895\n",
      "Epoch: 13, Train_Loss: 0.30985039472579956, Test_Loss: 0.4909293055534363\n",
      "Epoch: 13, Train_Loss: 16.29839515686035, Test_Loss: 0.5427802801132202\n",
      "Epoch: 13, Train_Loss: 0.390498548746109, Test_Loss: 0.36171790957450867 *\n",
      "Epoch: 13, Train_Loss: 1.515136957168579, Test_Loss: 0.28324055671691895 *\n",
      "Epoch: 13, Train_Loss: 1.389143466949463, Test_Loss: 0.2879279851913452\n",
      "Epoch: 13, Train_Loss: 0.3008909523487091, Test_Loss: 0.3055790066719055\n",
      "Epoch: 13, Train_Loss: 0.3509750962257385, Test_Loss: 0.4148250222206116\n",
      "Epoch: 13, Train_Loss: 2.1635844707489014, Test_Loss: 0.4572792947292328\n",
      "Epoch: 13, Train_Loss: 7.762935161590576, Test_Loss: 0.6237443685531616\n",
      "Epoch: 13, Train_Loss: 0.4445006251335144, Test_Loss: 0.343183308839798 *\n",
      "Epoch: 13, Train_Loss: 0.33094730973243713, Test_Loss: 0.32975906133651733 *\n",
      "Epoch: 13, Train_Loss: 5.431082725524902, Test_Loss: 0.36642444133758545\n",
      "Epoch: 13, Train_Loss: 0.49379801750183105, Test_Loss: 0.3747451603412628\n",
      "Epoch: 13, Train_Loss: 0.38918834924697876, Test_Loss: 0.5096895694732666\n",
      "Epoch: 13, Train_Loss: 0.27907446026802063, Test_Loss: 0.2911037504673004 *\n",
      "Epoch: 13, Train_Loss: 0.3117828369140625, Test_Loss: 0.4251744747161865\n",
      "Epoch: 13, Train_Loss: 0.3827097713947296, Test_Loss: 0.2837398052215576 *\n",
      "Epoch: 13, Train_Loss: 0.27648836374282837, Test_Loss: 0.4482889771461487\n",
      "Epoch: 13, Train_Loss: 0.3214767575263977, Test_Loss: 0.8920025825500488\n",
      "Epoch: 13, Train_Loss: 0.2698531150817871, Test_Loss: 0.5409457683563232 *\n",
      "Epoch: 13, Train_Loss: 0.27049726247787476, Test_Loss: 0.5096979141235352 *\n",
      "Epoch: 13, Train_Loss: 0.3022698760032654, Test_Loss: 0.28227582573890686 *\n",
      "Epoch: 13, Train_Loss: 0.33112144470214844, Test_Loss: 0.2736808657646179 *\n",
      "Epoch: 13, Train_Loss: 0.3274744749069214, Test_Loss: 0.27495431900024414\n",
      "Epoch: 13, Train_Loss: 0.3407477140426636, Test_Loss: 0.2732883393764496 *\n",
      "Epoch: 13, Train_Loss: 0.28425362706184387, Test_Loss: 0.28581503033638\n",
      "Epoch: 13, Train_Loss: 0.2785728871822357, Test_Loss: 1.5953139066696167\n",
      "Epoch: 13, Train_Loss: 0.2852036952972412, Test_Loss: 5.411632537841797\n",
      "Epoch: 13, Train_Loss: 0.2707388699054718, Test_Loss: 0.45826271176338196 *\n",
      "Epoch: 13, Train_Loss: 0.2935061454772949, Test_Loss: 0.5036671757698059\n",
      "Epoch: 13, Train_Loss: 0.2712439298629761, Test_Loss: 0.48285800218582153 *\n",
      "Epoch: 13, Train_Loss: 0.26809123158454895, Test_Loss: 0.3132220506668091 *\n",
      "Epoch: 13, Train_Loss: 0.2677113115787506, Test_Loss: 0.46273624897003174\n",
      "Epoch: 13, Train_Loss: 0.26833096146583557, Test_Loss: 0.5817742347717285\n",
      "Epoch: 13, Train_Loss: 0.27003127336502075, Test_Loss: 0.4957711100578308 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.26808908581733704, Test_Loss: 0.3116094470024109 *\n",
      "Epoch: 13, Train_Loss: 0.2678297162055969, Test_Loss: 0.46532315015792847\n",
      "Epoch: 13, Train_Loss: 0.2729516625404358, Test_Loss: 0.33824267983436584 *\n",
      "Epoch: 13, Train_Loss: 0.2913263738155365, Test_Loss: 0.72771817445755\n",
      "Epoch: 13, Train_Loss: 0.2925044298171997, Test_Loss: 0.4176798462867737 *\n",
      "Epoch: 13, Train_Loss: 0.3834143280982971, Test_Loss: 0.47608280181884766\n",
      "Epoch: 13, Train_Loss: 0.3290734887123108, Test_Loss: 0.38552820682525635 *\n",
      "Epoch: 13, Train_Loss: 0.6167320013046265, Test_Loss: 0.30631327629089355 *\n",
      "Epoch: 13, Train_Loss: 6.628515243530273, Test_Loss: 0.3011794984340668 *\n",
      "Epoch: 13, Train_Loss: 0.3251791298389435, Test_Loss: 0.2806967496871948 *\n",
      "Epoch: 13, Train_Loss: 0.42398154735565186, Test_Loss: 0.6237970590591431\n",
      "Epoch: 13, Train_Loss: 0.42703714966773987, Test_Loss: 0.30068033933639526 *\n",
      "Epoch: 13, Train_Loss: 0.5066730976104736, Test_Loss: 0.547793984413147\n",
      "Epoch: 13, Train_Loss: 0.43983131647109985, Test_Loss: 0.3123287558555603 *\n",
      "Epoch: 13, Train_Loss: 0.38478532433509827, Test_Loss: 0.6031923294067383\n",
      "Epoch: 13, Train_Loss: 0.3471415936946869, Test_Loss: 0.5064007043838501 *\n",
      "Epoch: 13, Train_Loss: 0.4161967635154724, Test_Loss: 0.34798768162727356 *\n",
      "Epoch: 13, Train_Loss: 0.375214159488678, Test_Loss: 0.28427597880363464 *\n",
      "Epoch: 13, Train_Loss: 0.3351886570453644, Test_Loss: 0.3087865710258484\n",
      "Epoch: 13, Train_Loss: 0.27209919691085815, Test_Loss: 0.3248523771762848\n",
      "Epoch: 13, Train_Loss: 0.36527150869369507, Test_Loss: 0.2900635600090027 *\n",
      "Epoch: 13, Train_Loss: 0.32676970958709717, Test_Loss: 0.3303791582584381\n",
      "Epoch: 13, Train_Loss: 0.37877267599105835, Test_Loss: 0.34331560134887695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.3148954212665558, Test_Loss: 3.70788311958313\n",
      "Epoch: 13, Train_Loss: 0.33318376541137695, Test_Loss: 2.234757900238037 *\n",
      "Epoch: 13, Train_Loss: 0.2770436704158783, Test_Loss: 0.26918232440948486 *\n",
      "Epoch: 13, Train_Loss: 0.31955215334892273, Test_Loss: 0.27171432971954346\n",
      "Epoch: 13, Train_Loss: 0.40091603994369507, Test_Loss: 0.27827808260917664\n",
      "Epoch: 13, Train_Loss: 0.3280113935470581, Test_Loss: 0.28006139397621155\n",
      "Epoch: 13, Train_Loss: 0.2709578573703766, Test_Loss: 0.29406917095184326\n",
      "Epoch: 13, Train_Loss: 0.2722345292568207, Test_Loss: 0.3158186972141266\n",
      "Epoch: 13, Train_Loss: 0.2718809247016907, Test_Loss: 0.32717400789260864\n",
      "Epoch: 13, Train_Loss: 1.2105815410614014, Test_Loss: 0.26964685320854187 *\n",
      "Epoch: 13, Train_Loss: 5.491921424865723, Test_Loss: 0.29353034496307373\n",
      "Epoch: 13, Train_Loss: 0.2699231505393982, Test_Loss: 0.28609713912010193 *\n",
      "Epoch: 13, Train_Loss: 0.27446645498275757, Test_Loss: 0.2814409136772156 *\n",
      "Epoch: 13, Train_Loss: 0.2762134075164795, Test_Loss: 0.27272677421569824 *\n",
      "Epoch: 13, Train_Loss: 0.27544379234313965, Test_Loss: 0.29095780849456787\n",
      "Epoch: 13, Train_Loss: 0.2722129225730896, Test_Loss: 0.31626904010772705\n",
      "Epoch: 13, Train_Loss: 0.26876357197761536, Test_Loss: 0.36162474751472473\n",
      "Epoch: 13, Train_Loss: 0.27476900815963745, Test_Loss: 0.3578875660896301 *\n",
      "Epoch: 13, Train_Loss: 0.3061862587928772, Test_Loss: 0.2888485789299011 *\n",
      "Epoch: 13, Train_Loss: 0.28524333238601685, Test_Loss: 0.272579550743103 *\n",
      "Epoch: 13, Train_Loss: 0.27263009548187256, Test_Loss: 0.2679980993270874 *\n",
      "Epoch: 13, Train_Loss: 0.2722146213054657, Test_Loss: 0.27195027470588684\n",
      "Epoch: 13, Train_Loss: 0.27062949538230896, Test_Loss: 0.2699209153652191 *\n",
      "Epoch: 13, Train_Loss: 0.28888189792633057, Test_Loss: 0.2683757245540619 *\n",
      "Epoch: 13, Train_Loss: 0.2683490216732025, Test_Loss: 0.2693164050579071\n",
      "Epoch: 13, Train_Loss: 0.268189936876297, Test_Loss: 0.2701456844806671\n",
      "Epoch: 13, Train_Loss: 0.307226300239563, Test_Loss: 0.27461573481559753\n",
      "Epoch: 13, Train_Loss: 0.3280300498008728, Test_Loss: 0.26751506328582764 *\n",
      "Epoch: 13, Train_Loss: 0.2822231948375702, Test_Loss: 0.2716759443283081\n",
      "Epoch: 13, Train_Loss: 0.2683791220188141, Test_Loss: 0.27397191524505615\n",
      "Epoch: 13, Train_Loss: 0.2729910612106323, Test_Loss: 0.2881358861923218\n",
      "Epoch: 13, Train_Loss: 0.3531642258167267, Test_Loss: 0.2992803752422333\n",
      "Epoch: 13, Train_Loss: 0.32422763109207153, Test_Loss: 0.3535299003124237\n",
      "Epoch: 13, Train_Loss: 0.344984233379364, Test_Loss: 0.6638105511665344\n",
      "Epoch: 13, Train_Loss: 0.3092889189720154, Test_Loss: 0.6113038063049316 *\n",
      "Epoch: 13, Train_Loss: 0.31364157795906067, Test_Loss: 0.3585575222969055 *\n",
      "Epoch: 13, Train_Loss: 0.31768786907196045, Test_Loss: 0.2811146378517151 *\n",
      "Epoch: 13, Train_Loss: 0.3358551561832428, Test_Loss: 0.28160911798477173\n",
      "Epoch: 13, Train_Loss: 0.2752322852611542, Test_Loss: 0.3547464609146118\n",
      "Epoch: 13, Train_Loss: 0.3973793387413025, Test_Loss: 0.7560564279556274\n",
      "Epoch: 13, Train_Loss: 0.28946399688720703, Test_Loss: 1.4122341871261597\n",
      "Epoch: 13, Train_Loss: 0.27102214097976685, Test_Loss: 0.8520510196685791 *\n",
      "Epoch: 13, Train_Loss: 0.2689220905303955, Test_Loss: 0.3291366696357727 *\n",
      "Epoch: 13, Train_Loss: 0.2660715878009796, Test_Loss: 0.28294917941093445 *\n",
      "Epoch: 13, Train_Loss: 0.26584532856941223, Test_Loss: 0.27492931485176086 *\n",
      "Epoch: 13, Train_Loss: 0.2675798833370209, Test_Loss: 0.26938462257385254 *\n",
      "Epoch: 13, Train_Loss: 1.0591570138931274, Test_Loss: 0.2753494679927826\n",
      "Epoch: 13, Train_Loss: 4.4916863441467285, Test_Loss: 0.29233497381210327\n",
      "Epoch: 13, Train_Loss: 0.2689323127269745, Test_Loss: 0.3171730637550354\n",
      "Epoch: 13, Train_Loss: 0.279857873916626, Test_Loss: 0.26724568009376526 *\n",
      "Epoch: 13, Train_Loss: 0.2808002829551697, Test_Loss: 0.337517112493515\n",
      "Epoch: 13, Train_Loss: 0.26398083567619324, Test_Loss: 0.4407060146331787\n",
      "Epoch: 13, Train_Loss: 0.26385706663131714, Test_Loss: 0.5320925712585449\n",
      "Epoch: 13, Train_Loss: 0.26462098956108093, Test_Loss: 0.4855627715587616 *\n",
      "Epoch: 13, Train_Loss: 0.2637186348438263, Test_Loss: 0.27890995144844055 *\n",
      "Epoch: 13, Train_Loss: 0.2635536193847656, Test_Loss: 0.2777290940284729 *\n",
      "Epoch: 13, Train_Loss: 0.26473936438560486, Test_Loss: 0.2777031660079956 *\n",
      "Epoch: 13, Train_Loss: 0.35055989027023315, Test_Loss: 0.2781451940536499\n",
      "Epoch: 13, Train_Loss: 0.3439835011959076, Test_Loss: 0.2881426513195038\n",
      "Epoch: 13, Train_Loss: 0.36766281723976135, Test_Loss: 2.658085823059082\n",
      "Epoch: 13, Train_Loss: 0.31843966245651245, Test_Loss: 3.3359458446502686\n",
      "Epoch: 13, Train_Loss: 0.26392215490341187, Test_Loss: 0.27560296654701233 *\n",
      "Epoch: 13, Train_Loss: 0.42352187633514404, Test_Loss: 0.2688532769680023 *\n",
      "Epoch: 13, Train_Loss: 0.4824722707271576, Test_Loss: 0.2687560021877289 *\n",
      "Epoch: 13, Train_Loss: 0.4774743914604187, Test_Loss: 0.2783821225166321\n",
      "Epoch: 13, Train_Loss: 0.4061180353164673, Test_Loss: 0.266916960477829 *\n",
      "Epoch: 13, Train_Loss: 0.26498275995254517, Test_Loss: 0.2692869007587433\n",
      "Epoch: 13, Train_Loss: 0.26315799355506897, Test_Loss: 0.26488566398620605 *\n",
      "Epoch: 13, Train_Loss: 0.2652863562107086, Test_Loss: 0.26508310437202454\n",
      "Epoch: 13, Train_Loss: 0.2733845114707947, Test_Loss: 0.265953928232193\n",
      "Epoch: 13, Train_Loss: 0.27594229578971863, Test_Loss: 0.2637948989868164 *\n",
      "Epoch: 13, Train_Loss: 0.2740497589111328, Test_Loss: 0.271571546792984\n",
      "Epoch: 13, Train_Loss: 0.265128493309021, Test_Loss: 0.2985888719558716\n",
      "Epoch: 13, Train_Loss: 0.26282045245170593, Test_Loss: 0.295364111661911 *\n",
      "Epoch: 13, Train_Loss: 0.27090370655059814, Test_Loss: 0.26571786403656006 *\n",
      "Epoch: 13, Train_Loss: 0.2941858172416687, Test_Loss: 0.2633371651172638 *\n",
      "Epoch: 13, Train_Loss: 0.45737841725349426, Test_Loss: 0.2636735737323761\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 13\n",
      "Epoch: 13, Train_Loss: 0.43790388107299805, Test_Loss: 0.2654670178890228\n",
      "Epoch: 13, Train_Loss: 0.4302459955215454, Test_Loss: 0.26549631357192993\n",
      "Epoch: 13, Train_Loss: 0.3261643648147583, Test_Loss: 0.26457974314689636 *\n",
      "Epoch: 13, Train_Loss: 0.4016050100326538, Test_Loss: 0.2638256549835205 *\n",
      "Epoch: 13, Train_Loss: 0.34208452701568604, Test_Loss: 0.263407438993454 *\n",
      "Epoch: 13, Train_Loss: 0.3645179867744446, Test_Loss: 0.27253779768943787\n",
      "Epoch: 13, Train_Loss: 0.35854873061180115, Test_Loss: 0.26680073142051697 *\n",
      "Epoch: 13, Train_Loss: 0.5394404530525208, Test_Loss: 0.26582029461860657 *\n",
      "Epoch: 13, Train_Loss: 0.27371490001678467, Test_Loss: 0.2631341516971588 *\n",
      "Epoch: 13, Train_Loss: 0.2729647159576416, Test_Loss: 0.26526394486427307\n",
      "Epoch: 13, Train_Loss: 3.056668996810913, Test_Loss: 0.26420435309410095 *\n",
      "Epoch: 13, Train_Loss: 0.64451003074646, Test_Loss: 0.26288437843322754 *\n",
      "Epoch: 13, Train_Loss: 0.30828553438186646, Test_Loss: 0.30765509605407715\n",
      "Epoch: 13, Train_Loss: 0.3270988464355469, Test_Loss: 0.29573360085487366 *\n",
      "Epoch: 13, Train_Loss: 0.318615198135376, Test_Loss: 4.8477373123168945\n",
      "Epoch: 13, Train_Loss: 0.2868967354297638, Test_Loss: 1.1390236616134644 *\n",
      "Epoch: 13, Train_Loss: 0.2649504840373993, Test_Loss: 0.26318758726119995 *\n",
      "Epoch: 13, Train_Loss: 0.3128673732280731, Test_Loss: 0.2796936631202698\n",
      "Epoch: 13, Train_Loss: 0.38075143098831177, Test_Loss: 0.2731866240501404 *\n",
      "Epoch: 13, Train_Loss: 0.35051229596138, Test_Loss: 0.29788678884506226\n",
      "Epoch: 13, Train_Loss: 0.3259580135345459, Test_Loss: 0.27382850646972656 *\n",
      "Epoch: 13, Train_Loss: 0.32884538173675537, Test_Loss: 0.3296782970428467\n",
      "Epoch: 13, Train_Loss: 0.2802852690219879, Test_Loss: 0.3263658881187439 *\n",
      "Epoch: 13, Train_Loss: 0.28451022505760193, Test_Loss: 0.2644704580307007 *\n",
      "Epoch: 13, Train_Loss: 0.26960328221321106, Test_Loss: 0.29358989000320435\n",
      "Epoch: 13, Train_Loss: 0.31756341457366943, Test_Loss: 0.2799636423587799 *\n",
      "Epoch: 13, Train_Loss: 0.2950020134449005, Test_Loss: 0.2728251814842224 *\n",
      "Epoch: 13, Train_Loss: 0.2636399567127228, Test_Loss: 0.2685689926147461 *\n",
      "Epoch: 13, Train_Loss: 0.27132269740104675, Test_Loss: 0.27659115195274353\n",
      "Epoch: 13, Train_Loss: 0.2967810332775116, Test_Loss: 0.30471426248550415\n",
      "Epoch: 13, Train_Loss: 0.29687175154685974, Test_Loss: 0.3550907075405121\n",
      "Epoch: 13, Train_Loss: 0.26430225372314453, Test_Loss: 0.30850496888160706 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train_Loss: 0.2615768313407898, Test_Loss: 0.3019571304321289 *\n",
      "Epoch: 13, Train_Loss: 0.2624341547489166, Test_Loss: 0.26829785108566284 *\n",
      "Epoch: 13, Train_Loss: 0.2613911032676697, Test_Loss: 0.2656526267528534 *\n",
      "Epoch: 13, Train_Loss: 0.2620706558227539, Test_Loss: 0.26854756474494934\n",
      "Epoch: 13, Train_Loss: 0.2627399265766144, Test_Loss: 0.2667233347892761 *\n",
      "Epoch: 13, Train_Loss: 0.2628355324268341, Test_Loss: 0.2692376971244812\n",
      "Epoch: 13, Train_Loss: 0.26252561807632446, Test_Loss: 0.2681940793991089 *\n",
      "Epoch: 13, Train_Loss: 0.2621639370918274, Test_Loss: 0.2635091245174408 *\n",
      "Epoch: 13, Train_Loss: 0.26230382919311523, Test_Loss: 0.266693651676178\n",
      "Epoch: 13, Train_Loss: 0.2618425190448761, Test_Loss: 0.27100032567977905\n",
      "Epoch: 13, Train_Loss: 0.27724912762641907, Test_Loss: 0.269846647977829 *\n",
      "Epoch: 13, Train_Loss: 0.2771693468093872, Test_Loss: 0.27210479974746704\n",
      "Epoch: 13, Train_Loss: 0.2770613431930542, Test_Loss: 0.3080010414123535\n",
      "Epoch: 13, Train_Loss: 0.28007495403289795, Test_Loss: 0.29881611466407776 *\n",
      "Epoch: 13, Train_Loss: 0.27045634388923645, Test_Loss: 0.47958511114120483\n",
      "Epoch: 13, Train_Loss: 0.260416179895401, Test_Loss: 0.809658408164978\n",
      "Epoch: 13, Train_Loss: 0.2607007622718811, Test_Loss: 0.6208169460296631 *\n",
      "Epoch: 13, Train_Loss: 0.2716805934906006, Test_Loss: 0.37767043709754944 *\n",
      "Epoch: 13, Train_Loss: 0.2825358510017395, Test_Loss: 0.31042179465293884 *\n",
      "Epoch: 13, Train_Loss: 0.26070892810821533, Test_Loss: 0.2697984278202057 *\n",
      "Epoch: 13, Train_Loss: 0.2625783383846283, Test_Loss: 0.3347238302230835\n",
      "Epoch: 13, Train_Loss: 0.2608835697174072, Test_Loss: 0.7239323854446411\n",
      "Epoch: 14, Train_Loss: 0.29730865359306335, Test_Loss: 1.004427194595337 *\n",
      "Epoch: 14, Train_Loss: 0.30801713466644287, Test_Loss: 0.5375955104827881 *\n",
      "Epoch: 14, Train_Loss: 0.3200681805610657, Test_Loss: 0.37182921171188354 *\n",
      "Epoch: 14, Train_Loss: 0.27738627791404724, Test_Loss: 0.2654957175254822 *\n",
      "Epoch: 14, Train_Loss: 0.26150113344192505, Test_Loss: 0.26514750719070435 *\n",
      "Epoch: 14, Train_Loss: 0.32211560010910034, Test_Loss: 0.26053670048713684 *\n",
      "Epoch: 14, Train_Loss: 0.26638537645339966, Test_Loss: 0.2749403119087219\n",
      "Epoch: 14, Train_Loss: 0.26829981803894043, Test_Loss: 0.28477755188941956\n",
      "Epoch: 14, Train_Loss: 0.2840920090675354, Test_Loss: 0.29808858036994934\n",
      "Epoch: 14, Train_Loss: 0.27167972922325134, Test_Loss: 0.26443660259246826 *\n",
      "Epoch: 14, Train_Loss: 0.3888517916202545, Test_Loss: 0.3748723268508911\n",
      "Epoch: 14, Train_Loss: 0.32593029737472534, Test_Loss: 0.6084145307540894\n",
      "Epoch: 14, Train_Loss: 0.286263644695282, Test_Loss: 0.36943796277046204 *\n",
      "Epoch: 14, Train_Loss: 0.2740313410758972, Test_Loss: 0.4666828513145447\n",
      "Epoch: 14, Train_Loss: 0.2789958417415619, Test_Loss: 0.2750365138053894 *\n",
      "Epoch: 14, Train_Loss: 0.2668815851211548, Test_Loss: 0.2758251428604126\n",
      "Epoch: 14, Train_Loss: 0.2626243233680725, Test_Loss: 0.27589064836502075\n",
      "Epoch: 14, Train_Loss: 0.27144837379455566, Test_Loss: 0.2760772407054901\n",
      "Epoch: 14, Train_Loss: 0.277333527803421, Test_Loss: 0.2844524383544922\n",
      "Epoch: 14, Train_Loss: 0.2969168722629547, Test_Loss: 4.264219760894775\n",
      "Epoch: 14, Train_Loss: 0.35040587186813354, Test_Loss: 1.9740428924560547 *\n",
      "Epoch: 14, Train_Loss: 0.27521008253097534, Test_Loss: 0.2693394720554352 *\n",
      "Epoch: 14, Train_Loss: 0.3069598376750946, Test_Loss: 0.2639622092247009 *\n",
      "Epoch: 14, Train_Loss: 0.28294703364372253, Test_Loss: 0.26514706015586853\n",
      "Epoch: 14, Train_Loss: 0.27568838000297546, Test_Loss: 0.2754688858985901\n",
      "Epoch: 14, Train_Loss: 0.35420432686805725, Test_Loss: 0.26369941234588623 *\n",
      "Epoch: 14, Train_Loss: 0.4966246783733368, Test_Loss: 0.26278647780418396 *\n",
      "Epoch: 14, Train_Loss: 0.2645430266857147, Test_Loss: 0.2591050863265991 *\n",
      "Epoch: 14, Train_Loss: 0.293014794588089, Test_Loss: 0.2598930299282074\n",
      "Epoch: 14, Train_Loss: 0.2585957944393158, Test_Loss: 0.2606041431427002\n",
      "Epoch: 14, Train_Loss: 0.2585761249065399, Test_Loss: 0.25950175523757935 *\n",
      "Epoch: 14, Train_Loss: 0.2611485719680786, Test_Loss: 0.26911890506744385\n",
      "Epoch: 14, Train_Loss: 0.25963178277015686, Test_Loss: 0.29704782366752625\n",
      "Epoch: 14, Train_Loss: 0.2624770998954773, Test_Loss: 0.2894667685031891 *\n",
      "Epoch: 14, Train_Loss: 0.2751973867416382, Test_Loss: 0.2591821253299713 *\n",
      "Epoch: 14, Train_Loss: 0.2658616006374359, Test_Loss: 0.258739709854126 *\n",
      "Epoch: 14, Train_Loss: 0.2702585756778717, Test_Loss: 0.2590150833129883\n",
      "Epoch: 14, Train_Loss: 0.2695649564266205, Test_Loss: 0.2646777331829071\n",
      "Epoch: 14, Train_Loss: 0.2607901990413666, Test_Loss: 0.2606690526008606 *\n",
      "Epoch: 14, Train_Loss: 0.259480357170105, Test_Loss: 0.2598066031932831 *\n",
      "Epoch: 14, Train_Loss: 0.2590876817703247, Test_Loss: 0.26374149322509766\n",
      "Epoch: 14, Train_Loss: 0.2829294800758362, Test_Loss: 0.2611936032772064 *\n",
      "Epoch: 14, Train_Loss: 0.2715013921260834, Test_Loss: 0.2688594162464142\n",
      "Epoch: 14, Train_Loss: 0.28170058131217957, Test_Loss: 0.2678355574607849 *\n",
      "Epoch: 14, Train_Loss: 0.2631712853908539, Test_Loss: 0.2610833942890167 *\n",
      "Epoch: 14, Train_Loss: 0.3008776605129242, Test_Loss: 0.2602050006389618 *\n",
      "Epoch: 14, Train_Loss: 0.2884312868118286, Test_Loss: 0.2670013904571533\n",
      "Epoch: 14, Train_Loss: 0.27889373898506165, Test_Loss: 0.2625676095485687 *\n",
      "Epoch: 14, Train_Loss: 0.2712879478931427, Test_Loss: 0.25832098722457886 *\n",
      "Epoch: 14, Train_Loss: 0.28747937083244324, Test_Loss: 0.3256984353065491\n",
      "Epoch: 14, Train_Loss: 0.2602591812610626, Test_Loss: 0.31559574604034424 *\n",
      "Epoch: 14, Train_Loss: 0.2685229778289795, Test_Loss: 5.991482257843018\n",
      "Epoch: 14, Train_Loss: 0.2829929292201996, Test_Loss: 0.3697851598262787 *\n",
      "Epoch: 14, Train_Loss: 0.2953099310398102, Test_Loss: 0.2582859992980957 *\n",
      "Epoch: 14, Train_Loss: 2.280534505844116, Test_Loss: 0.2818266749382019\n",
      "Epoch: 14, Train_Loss: 3.5286474227905273, Test_Loss: 0.26048776507377625 *\n",
      "Epoch: 14, Train_Loss: 0.26660144329071045, Test_Loss: 0.2796098291873932\n",
      "Epoch: 14, Train_Loss: 0.2751293480396271, Test_Loss: 0.26372045278549194 *\n",
      "Epoch: 14, Train_Loss: 0.2854248285293579, Test_Loss: 0.3448309302330017\n",
      "Epoch: 14, Train_Loss: 0.4079464077949524, Test_Loss: 0.31198567152023315 *\n",
      "Epoch: 14, Train_Loss: 0.29416030645370483, Test_Loss: 0.25788092613220215 *\n",
      "Epoch: 14, Train_Loss: 0.2704736292362213, Test_Loss: 0.2891409397125244\n",
      "Epoch: 14, Train_Loss: 0.25882068276405334, Test_Loss: 0.27254804968833923 *\n",
      "Epoch: 14, Train_Loss: 0.33494341373443604, Test_Loss: 0.2647531032562256 *\n",
      "Epoch: 14, Train_Loss: 0.2697316110134125, Test_Loss: 0.28623712062835693\n",
      "Epoch: 14, Train_Loss: 0.2725468873977661, Test_Loss: 0.26215821504592896 *\n",
      "Epoch: 14, Train_Loss: 0.7603397369384766, Test_Loss: 0.30583637952804565\n",
      "Epoch: 14, Train_Loss: 0.9459298849105835, Test_Loss: 0.3361433148384094\n",
      "Epoch: 14, Train_Loss: 0.8798553943634033, Test_Loss: 0.29177337884902954 *\n",
      "Epoch: 14, Train_Loss: 0.33705347776412964, Test_Loss: 0.2965458631515503\n",
      "Epoch: 14, Train_Loss: 0.6501389741897583, Test_Loss: 0.26422762870788574 *\n",
      "Epoch: 14, Train_Loss: 1.727866530418396, Test_Loss: 0.2742171585559845\n",
      "Epoch: 14, Train_Loss: 0.6925604343414307, Test_Loss: 0.28546667098999023\n",
      "Epoch: 14, Train_Loss: 0.2741798758506775, Test_Loss: 0.29306989908218384\n",
      "Epoch: 14, Train_Loss: 0.2735070586204529, Test_Loss: 0.2823106646537781 *\n",
      "Epoch: 14, Train_Loss: 0.8242690563201904, Test_Loss: 0.28111788630485535 *\n",
      "Epoch: 14, Train_Loss: 0.6595194339752197, Test_Loss: 0.3360520303249359\n",
      "Epoch: 14, Train_Loss: 0.5587981343269348, Test_Loss: 0.32401448488235474 *\n",
      "Epoch: 14, Train_Loss: 0.26449644565582275, Test_Loss: 0.28559592366218567 *\n",
      "Epoch: 14, Train_Loss: 0.2674747705459595, Test_Loss: 0.28208181262016296 *\n",
      "Epoch: 14, Train_Loss: 0.621160626411438, Test_Loss: 0.2677627205848694 *\n",
      "Epoch: 14, Train_Loss: 0.42608001828193665, Test_Loss: 0.27805429697036743\n",
      "Epoch: 14, Train_Loss: 0.280627965927124, Test_Loss: 0.3056897819042206\n",
      "Epoch: 14, Train_Loss: 0.2892870008945465, Test_Loss: 0.4502694606781006\n",
      "Epoch: 14, Train_Loss: 0.36709296703338623, Test_Loss: 0.45918166637420654\n",
      "Epoch: 14, Train_Loss: 0.36102038621902466, Test_Loss: 0.4579721689224243 *\n",
      "Epoch: 14, Train_Loss: 0.3679407835006714, Test_Loss: 0.3144720792770386 *\n",
      "Epoch: 14, Train_Loss: 0.4001524746417999, Test_Loss: 0.2836381196975708 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.3114190399646759, Test_Loss: 0.31941017508506775\n",
      "Epoch: 14, Train_Loss: 0.3420508801937103, Test_Loss: 0.3880661725997925\n",
      "Epoch: 14, Train_Loss: 0.28320303559303284, Test_Loss: 0.5553766489028931\n",
      "Epoch: 14, Train_Loss: 0.3176787495613098, Test_Loss: 0.2897046208381653 *\n",
      "Epoch: 14, Train_Loss: 0.3079928159713745, Test_Loss: 0.4342906177043915\n",
      "Epoch: 14, Train_Loss: 0.42414402961730957, Test_Loss: 0.3267315626144409 *\n",
      "Epoch: 14, Train_Loss: 0.43179070949554443, Test_Loss: 0.34669196605682373\n",
      "Epoch: 14, Train_Loss: 0.38306713104248047, Test_Loss: 0.30052056908607483 *\n",
      "Epoch: 14, Train_Loss: 0.31001874804496765, Test_Loss: 0.2917640507221222 *\n",
      "Epoch: 14, Train_Loss: 0.2931446433067322, Test_Loss: 0.43711960315704346\n",
      "Epoch: 14, Train_Loss: 0.26414549350738525, Test_Loss: 0.2860676050186157 *\n",
      "Epoch: 14, Train_Loss: 0.2689385414123535, Test_Loss: 0.2917637526988983\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.2801316976547241, Test_Loss: 0.31208184361457825\n",
      "Epoch: 14, Train_Loss: 0.2622368037700653, Test_Loss: 0.4430096745491028\n",
      "Epoch: 14, Train_Loss: 0.28061234951019287, Test_Loss: 0.6700998544692993\n",
      "Epoch: 14, Train_Loss: 0.29761847853660583, Test_Loss: 0.4173318147659302 *\n",
      "Epoch: 14, Train_Loss: 0.27681756019592285, Test_Loss: 0.5019540786743164\n",
      "Epoch: 14, Train_Loss: 0.45976385474205017, Test_Loss: 0.28672245144844055 *\n",
      "Epoch: 14, Train_Loss: 0.3605482578277588, Test_Loss: 0.28868547081947327\n",
      "Epoch: 14, Train_Loss: 0.2895752489566803, Test_Loss: 0.2885451018810272 *\n",
      "Epoch: 14, Train_Loss: 0.29373326897621155, Test_Loss: 0.29464682936668396\n",
      "Epoch: 14, Train_Loss: 0.3020644187927246, Test_Loss: 0.3917650580406189\n",
      "Epoch: 14, Train_Loss: 0.4118191599845886, Test_Loss: 5.744829177856445\n",
      "Epoch: 14, Train_Loss: 0.4606529176235199, Test_Loss: 0.8935645818710327 *\n",
      "Epoch: 14, Train_Loss: 0.3089680075645447, Test_Loss: 0.355594277381897 *\n",
      "Epoch: 14, Train_Loss: 0.37021327018737793, Test_Loss: 0.28662657737731934 *\n",
      "Epoch: 14, Train_Loss: 0.34748291969299316, Test_Loss: 0.29231318831443787\n",
      "Epoch: 14, Train_Loss: 0.32662540674209595, Test_Loss: 0.2653670012950897 *\n",
      "Epoch: 14, Train_Loss: 0.35508203506469727, Test_Loss: 0.31437307596206665\n",
      "Epoch: 14, Train_Loss: 0.278082013130188, Test_Loss: 0.3115401566028595 *\n",
      "Epoch: 14, Train_Loss: 0.2870541214942932, Test_Loss: 0.27049770951271057 *\n",
      "Epoch: 14, Train_Loss: 0.6349259614944458, Test_Loss: 0.27262523770332336\n",
      "Epoch: 14, Train_Loss: 0.6172637939453125, Test_Loss: 0.2716748118400574 *\n",
      "Epoch: 14, Train_Loss: 0.3054671883583069, Test_Loss: 0.32288259267807007\n",
      "Epoch: 14, Train_Loss: 0.29662516713142395, Test_Loss: 0.30919572710990906 *\n",
      "Epoch: 14, Train_Loss: 0.27557802200317383, Test_Loss: 0.2760496735572815 *\n",
      "Epoch: 14, Train_Loss: 0.2669040858745575, Test_Loss: 0.29085829854011536\n",
      "Epoch: 14, Train_Loss: 0.5836089849472046, Test_Loss: 0.27138200402259827 *\n",
      "Epoch: 14, Train_Loss: 0.2771950662136078, Test_Loss: 0.2616942524909973 *\n",
      "Epoch: 14, Train_Loss: 0.33099448680877686, Test_Loss: 0.3556840121746063\n",
      "Epoch: 14, Train_Loss: 0.3399260938167572, Test_Loss: 0.3658091425895691\n",
      "Epoch: 14, Train_Loss: 0.30183297395706177, Test_Loss: 0.2837679386138916 *\n",
      "Epoch: 14, Train_Loss: 0.29445481300354004, Test_Loss: 0.2736254632472992 *\n",
      "Epoch: 14, Train_Loss: 0.33296191692352295, Test_Loss: 0.26817405223846436 *\n",
      "Epoch: 14, Train_Loss: 0.38653120398521423, Test_Loss: 0.3213629424571991\n",
      "Epoch: 14, Train_Loss: 0.2859978675842285, Test_Loss: 0.30066415667533875 *\n",
      "Epoch: 14, Train_Loss: 0.3134804368019104, Test_Loss: 0.34663963317871094\n",
      "Epoch: 14, Train_Loss: 0.2900107502937317, Test_Loss: 0.2829505205154419 *\n",
      "Epoch: 14, Train_Loss: 0.36604005098342896, Test_Loss: 0.2814277112483978 *\n",
      "Epoch: 14, Train_Loss: 0.2988141179084778, Test_Loss: 0.3195651173591614\n",
      "Epoch: 14, Train_Loss: 0.2907795310020447, Test_Loss: 0.2783561944961548 *\n",
      "Epoch: 14, Train_Loss: 0.26942458748817444, Test_Loss: 0.25733235478401184 *\n",
      "Epoch: 14, Train_Loss: 0.28263378143310547, Test_Loss: 0.3886133134365082\n",
      "Epoch: 14, Train_Loss: 0.5744003057479858, Test_Loss: 0.8188060522079468\n",
      "Epoch: 14, Train_Loss: 0.5371965169906616, Test_Loss: 4.857693195343018\n",
      "Epoch: 14, Train_Loss: 0.7070997953414917, Test_Loss: 0.2802858352661133 *\n",
      "Epoch: 14, Train_Loss: 0.5223658680915833, Test_Loss: 0.2818276882171631\n",
      "Epoch: 14, Train_Loss: 0.47127437591552734, Test_Loss: 0.2905392348766327\n",
      "Epoch: 14, Train_Loss: 0.38930970430374146, Test_Loss: 0.2881143391132355 *\n",
      "Epoch: 14, Train_Loss: 0.31928813457489014, Test_Loss: 0.2739158272743225 *\n",
      "Epoch: 14, Train_Loss: 0.26328471302986145, Test_Loss: 0.27134042978286743 *\n",
      "Epoch: 14, Train_Loss: 0.2673783600330353, Test_Loss: 0.36532437801361084\n",
      "Epoch: 14, Train_Loss: 0.2992118299007416, Test_Loss: 0.2844873070716858 *\n",
      "Epoch: 14, Train_Loss: 0.4962432384490967, Test_Loss: 0.25714704394340515 *\n",
      "Epoch: 14, Train_Loss: 0.5742459297180176, Test_Loss: 0.27847597002983093\n",
      "Epoch: 14, Train_Loss: 0.7278993129730225, Test_Loss: 0.29775357246398926\n",
      "Epoch: 14, Train_Loss: 1.2669668197631836, Test_Loss: 0.26033735275268555 *\n",
      "Epoch: 14, Train_Loss: 0.5393972992897034, Test_Loss: 0.33281856775283813\n",
      "Epoch: 14, Train_Loss: 0.47569161653518677, Test_Loss: 0.3088708817958832 *\n",
      "Epoch: 14, Train_Loss: 0.2623269855976105, Test_Loss: 0.3672734498977661\n",
      "Epoch: 14, Train_Loss: 0.2651916742324829, Test_Loss: 0.28391674160957336 *\n",
      "Epoch: 14, Train_Loss: 0.5121331214904785, Test_Loss: 0.3229501247406006\n",
      "Epoch: 14, Train_Loss: 0.962551474571228, Test_Loss: 0.29364126920700073 *\n",
      "Epoch: 14, Train_Loss: 0.40925100445747375, Test_Loss: 0.32548293471336365\n",
      "Epoch: 14, Train_Loss: 0.3157336711883545, Test_Loss: 0.41237330436706543\n",
      "Epoch: 14, Train_Loss: 0.27951890230178833, Test_Loss: 0.35875260829925537 *\n",
      "Epoch: 14, Train_Loss: 0.3371445834636688, Test_Loss: 0.35504862666130066 *\n",
      "Epoch: 14, Train_Loss: 0.5479458570480347, Test_Loss: 0.34986090660095215 *\n",
      "Epoch: 14, Train_Loss: 0.502713143825531, Test_Loss: 0.38344255089759827\n",
      "Epoch: 14, Train_Loss: 0.339880108833313, Test_Loss: 0.4614923596382141\n",
      "Epoch: 14, Train_Loss: 0.47291186451911926, Test_Loss: 0.44352519512176514 *\n",
      "Epoch: 14, Train_Loss: 0.2845644950866699, Test_Loss: 0.3260059356689453 *\n",
      "Epoch: 14, Train_Loss: 0.25622645020484924, Test_Loss: 0.38921135663986206\n",
      "Epoch: 14, Train_Loss: 0.29739072918891907, Test_Loss: 0.3298164904117584 *\n",
      "Epoch: 14, Train_Loss: 0.2881041169166565, Test_Loss: 0.2706620395183563 *\n",
      "Epoch: 14, Train_Loss: 0.3098123371601105, Test_Loss: 0.3139164447784424\n",
      "Epoch: 14, Train_Loss: 0.3039681613445282, Test_Loss: 0.41867008805274963\n",
      "Epoch: 14, Train_Loss: 11.751344680786133, Test_Loss: 0.422341912984848\n",
      "Epoch: 14, Train_Loss: 4.690079689025879, Test_Loss: 0.3585398197174072 *\n",
      "Epoch: 14, Train_Loss: 1.18610417842865, Test_Loss: 0.2800712287425995 *\n",
      "Epoch: 14, Train_Loss: 0.9997971057891846, Test_Loss: 0.270084023475647 *\n",
      "Epoch: 14, Train_Loss: 0.412624329328537, Test_Loss: 0.2831425964832306\n",
      "Epoch: 14, Train_Loss: 0.32208487391471863, Test_Loss: 0.3228233754634857\n",
      "Epoch: 14, Train_Loss: 0.8155828714370728, Test_Loss: 0.36936476826667786\n",
      "Epoch: 14, Train_Loss: 7.770242214202881, Test_Loss: 0.48940759897232056\n",
      "Epoch: 14, Train_Loss: 1.2420953512191772, Test_Loss: 0.3180578351020813 *\n",
      "Epoch: 14, Train_Loss: 0.31644731760025024, Test_Loss: 0.3140856623649597 *\n",
      "Epoch: 14, Train_Loss: 3.524179458618164, Test_Loss: 0.309251070022583 *\n",
      "Epoch: 14, Train_Loss: 2.369666814804077, Test_Loss: 0.35061994194984436\n",
      "Epoch: 14, Train_Loss: 0.571431040763855, Test_Loss: 0.4219287633895874\n",
      "Epoch: 14, Train_Loss: 0.27372172474861145, Test_Loss: 0.4009634256362915 *\n",
      "Epoch: 14, Train_Loss: 0.3301500082015991, Test_Loss: 0.36701759696006775 *\n",
      "Epoch: 14, Train_Loss: 0.3236624300479889, Test_Loss: 0.2892288565635681 *\n",
      "Epoch: 14, Train_Loss: 0.3076305389404297, Test_Loss: 0.41161760687828064\n",
      "Epoch: 14, Train_Loss: 0.26791009306907654, Test_Loss: 0.4936056137084961\n",
      "Epoch: 14, Train_Loss: 0.25180327892303467, Test_Loss: 0.772581934928894\n",
      "Epoch: 14, Train_Loss: 0.2512429654598236, Test_Loss: 0.3843066096305847 *\n",
      "Epoch: 14, Train_Loss: 0.27414077520370483, Test_Loss: 0.3060574233531952 *\n",
      "Epoch: 14, Train_Loss: 0.31714969873428345, Test_Loss: 0.3128606975078583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.27732864022254944, Test_Loss: 0.3185652494430542\n",
      "Epoch: 14, Train_Loss: 0.3531208038330078, Test_Loss: 0.3169359564781189 *\n",
      "Epoch: 14, Train_Loss: 0.3545927107334137, Test_Loss: 0.2968533933162689 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.2895161807537079, Test_Loss: 0.405744731426239\n",
      "Epoch: 14, Train_Loss: 0.27451980113983154, Test_Loss: 7.524106979370117\n",
      "Epoch: 14, Train_Loss: 0.2575751543045044, Test_Loss: 0.6532155275344849 *\n",
      "Epoch: 14, Train_Loss: 0.2858183979988098, Test_Loss: 0.5128324031829834 *\n",
      "Epoch: 14, Train_Loss: 0.25243404507637024, Test_Loss: 0.43828117847442627 *\n",
      "Epoch: 14, Train_Loss: 0.25648969411849976, Test_Loss: 0.3869616389274597 *\n",
      "Epoch: 14, Train_Loss: 0.2505777180194855, Test_Loss: 0.34242770075798035 *\n",
      "Epoch: 14, Train_Loss: 0.2513388395309448, Test_Loss: 0.5889111757278442\n",
      "Epoch: 14, Train_Loss: 0.2521025538444519, Test_Loss: 0.515396237373352 *\n",
      "Epoch: 14, Train_Loss: 0.2509903907775879, Test_Loss: 0.3094351291656494 *\n",
      "Epoch: 14, Train_Loss: 0.2505585253238678, Test_Loss: 0.44288694858551025\n",
      "Epoch: 14, Train_Loss: 0.25166428089141846, Test_Loss: 0.3485146760940552 *\n",
      "Epoch: 14, Train_Loss: 0.2646110951900482, Test_Loss: 0.6664962768554688\n",
      "Epoch: 14, Train_Loss: 0.265371173620224, Test_Loss: 0.4373442232608795 *\n",
      "Epoch: 14, Train_Loss: 0.35979339480400085, Test_Loss: 0.4686373472213745\n",
      "Epoch: 14, Train_Loss: 0.2943061590194702, Test_Loss: 0.42127785086631775 *\n",
      "Epoch: 14, Train_Loss: 0.41534775495529175, Test_Loss: 0.3230082392692566 *\n",
      "Epoch: 14, Train_Loss: 6.985320091247559, Test_Loss: 0.3047701120376587 *\n",
      "Epoch: 14, Train_Loss: 0.4453412592411041, Test_Loss: 0.2928914427757263 *\n",
      "Epoch: 14, Train_Loss: 0.3863183557987213, Test_Loss: 0.44573062658309937\n",
      "Epoch: 14, Train_Loss: 0.3722047507762909, Test_Loss: 0.3706929683685303 *\n",
      "Epoch: 14, Train_Loss: 0.38656890392303467, Test_Loss: 0.5778285264968872\n",
      "Epoch: 14, Train_Loss: 0.3313296139240265, Test_Loss: 0.38453173637390137 *\n",
      "Epoch: 14, Train_Loss: 0.3562968969345093, Test_Loss: 0.5197779536247253\n",
      "Epoch: 14, Train_Loss: 0.3315993547439575, Test_Loss: 0.4128328561782837 *\n",
      "Epoch: 14, Train_Loss: 0.3744632601737976, Test_Loss: 0.3581090569496155 *\n",
      "Epoch: 14, Train_Loss: 0.387386679649353, Test_Loss: 0.28707098960876465 *\n",
      "Epoch: 14, Train_Loss: 0.358525812625885, Test_Loss: 0.30145591497421265\n",
      "Epoch: 14, Train_Loss: 0.2535199224948883, Test_Loss: 0.32260313630104065\n",
      "Epoch: 14, Train_Loss: 0.3272077739238739, Test_Loss: 0.2842656970024109 *\n",
      "Epoch: 14, Train_Loss: 0.32235974073410034, Test_Loss: 0.26158273220062256 *\n",
      "Epoch: 14, Train_Loss: 0.418190062046051, Test_Loss: 0.41857218742370605\n",
      "Epoch: 14, Train_Loss: 0.32720351219177246, Test_Loss: 2.1515579223632812\n",
      "Epoch: 14, Train_Loss: 0.32446449995040894, Test_Loss: 3.894134521484375\n",
      "Epoch: 14, Train_Loss: 0.2810574471950531, Test_Loss: 0.2575628161430359 *\n",
      "Epoch: 14, Train_Loss: 0.2731618881225586, Test_Loss: 0.25081226229667664 *\n",
      "Epoch: 14, Train_Loss: 0.3564606308937073, Test_Loss: 0.27906858921051025\n",
      "Epoch: 14, Train_Loss: 0.29327642917633057, Test_Loss: 0.2874414920806885\n",
      "Epoch: 14, Train_Loss: 0.25146782398223877, Test_Loss: 0.2799336016178131 *\n",
      "Epoch: 14, Train_Loss: 0.24959707260131836, Test_Loss: 0.2806652784347534\n",
      "Epoch: 14, Train_Loss: 0.2501665949821472, Test_Loss: 0.3413146138191223\n",
      "Epoch: 14, Train_Loss: 0.3816138505935669, Test_Loss: 0.25859344005584717 *\n",
      "Epoch: 14, Train_Loss: 6.174342155456543, Test_Loss: 0.2642960846424103\n",
      "Epoch: 14, Train_Loss: 0.2605915069580078, Test_Loss: 0.27600032091140747\n",
      "Epoch: 14, Train_Loss: 0.253915011882782, Test_Loss: 0.26669564843177795 *\n",
      "Epoch: 14, Train_Loss: 0.2589608132839203, Test_Loss: 0.25932037830352783 *\n",
      "Epoch: 14, Train_Loss: 0.25792479515075684, Test_Loss: 0.32408076524734497\n",
      "Epoch: 14, Train_Loss: 0.2550310492515564, Test_Loss: 0.2946540415287018 *\n",
      "Epoch: 14, Train_Loss: 0.2508309483528137, Test_Loss: 0.32750871777534485\n",
      "Epoch: 14, Train_Loss: 0.25262847542762756, Test_Loss: 0.3263455331325531 *\n",
      "Epoch: 14, Train_Loss: 0.29172244668006897, Test_Loss: 0.27241793274879456 *\n",
      "Epoch: 14, Train_Loss: 0.27395910024642944, Test_Loss: 0.269050270318985 *\n",
      "Epoch: 14, Train_Loss: 0.2544333040714264, Test_Loss: 0.26224225759506226 *\n",
      "Epoch: 14, Train_Loss: 0.2527143657207489, Test_Loss: 0.28061801195144653\n",
      "Epoch: 14, Train_Loss: 0.25192832946777344, Test_Loss: 0.27441203594207764 *\n",
      "Epoch: 14, Train_Loss: 0.2690127491950989, Test_Loss: 0.2612469494342804 *\n",
      "Epoch: 14, Train_Loss: 0.2513255476951599, Test_Loss: 0.2643819749355316\n",
      "Epoch: 14, Train_Loss: 0.25211256742477417, Test_Loss: 0.2705298364162445\n",
      "Epoch: 14, Train_Loss: 0.2740171253681183, Test_Loss: 0.2836899757385254\n",
      "Epoch: 14, Train_Loss: 0.2961987853050232, Test_Loss: 0.26299571990966797 *\n",
      "Epoch: 14, Train_Loss: 0.2731327414512634, Test_Loss: 0.25297656655311584 *\n",
      "Epoch: 14, Train_Loss: 0.24949179589748383, Test_Loss: 0.2599579095840454\n",
      "Epoch: 14, Train_Loss: 0.25064224004745483, Test_Loss: 0.26595932245254517\n",
      "Epoch: 14, Train_Loss: 0.32208162546157837, Test_Loss: 0.2913503348827362\n",
      "Epoch: 14, Train_Loss: 0.3025360703468323, Test_Loss: 0.27982065081596375 *\n",
      "Epoch: 14, Train_Loss: 0.3163699507713318, Test_Loss: 0.5852439999580383\n",
      "Epoch: 14, Train_Loss: 0.29387322068214417, Test_Loss: 0.6155710220336914\n",
      "Epoch: 14, Train_Loss: 0.271702378988266, Test_Loss: 0.38942044973373413 *\n",
      "Epoch: 14, Train_Loss: 0.2988503873348236, Test_Loss: 0.26599404215812683 *\n",
      "Epoch: 14, Train_Loss: 0.30275431275367737, Test_Loss: 0.26764941215515137\n",
      "Epoch: 14, Train_Loss: 0.2780205309391022, Test_Loss: 0.28478142619132996\n",
      "Epoch: 14, Train_Loss: 0.3870849609375, Test_Loss: 0.45785170793533325\n",
      "Epoch: 14, Train_Loss: 0.2730363607406616, Test_Loss: 0.8858169913291931\n",
      "Epoch: 14, Train_Loss: 0.2747116982936859, Test_Loss: 0.9247254729270935\n",
      "Epoch: 14, Train_Loss: 0.25012823939323425, Test_Loss: 0.3035275638103485 *\n",
      "Epoch: 14, Train_Loss: 0.24812602996826172, Test_Loss: 0.29756346344947815 *\n",
      "Epoch: 14, Train_Loss: 0.24793021380901337, Test_Loss: 0.25563761591911316 *\n",
      "Epoch: 14, Train_Loss: 0.24805864691734314, Test_Loss: 0.25952839851379395\n",
      "Epoch: 14, Train_Loss: 0.3007102906703949, Test_Loss: 0.26406434178352356\n",
      "Epoch: 14, Train_Loss: 5.021575450897217, Test_Loss: 0.2644190490245819\n",
      "Epoch: 14, Train_Loss: 0.38071009516716003, Test_Loss: 0.3031409978866577\n",
      "Epoch: 14, Train_Loss: 0.25409990549087524, Test_Loss: 0.2516586482524872 *\n",
      "Epoch: 14, Train_Loss: 0.26122933626174927, Test_Loss: 0.27369630336761475\n",
      "Epoch: 14, Train_Loss: 0.2496688812971115, Test_Loss: 0.37708985805511475\n",
      "Epoch: 14, Train_Loss: 0.24811144173145294, Test_Loss: 0.5603175163269043\n",
      "Epoch: 14, Train_Loss: 0.2489471286535263, Test_Loss: 0.5117181539535522 *\n",
      "Epoch: 14, Train_Loss: 0.24764548242092133, Test_Loss: 0.2833045423030853 *\n",
      "Epoch: 14, Train_Loss: 0.2479867786169052, Test_Loss: 0.28503137826919556\n",
      "Epoch: 14, Train_Loss: 0.24842528998851776, Test_Loss: 0.283491313457489 *\n",
      "Epoch: 14, Train_Loss: 0.2969639301300049, Test_Loss: 0.28570857644081116\n",
      "Epoch: 14, Train_Loss: 0.3036232590675354, Test_Loss: 0.28292179107666016 *\n",
      "Epoch: 14, Train_Loss: 0.33184316754341125, Test_Loss: 0.8407666683197021\n",
      "Epoch: 14, Train_Loss: 0.30275997519493103, Test_Loss: 4.8927106857299805\n",
      "Epoch: 14, Train_Loss: 0.24953177571296692, Test_Loss: 0.2855891287326813 *\n",
      "Epoch: 14, Train_Loss: 0.3594520390033722, Test_Loss: 0.25661465525627136 *\n",
      "Epoch: 14, Train_Loss: 0.468746542930603, Test_Loss: 0.256076455116272 *\n",
      "Epoch: 14, Train_Loss: 0.4714896082878113, Test_Loss: 0.26230189204216003\n",
      "Epoch: 14, Train_Loss: 0.44058915972709656, Test_Loss: 0.2541043162345886 *\n",
      "Epoch: 14, Train_Loss: 0.2506856620311737, Test_Loss: 0.2512564957141876 *\n",
      "Epoch: 14, Train_Loss: 0.2472655475139618, Test_Loss: 0.25098857283592224 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.24799944460391998, Test_Loss: 0.2489849328994751 *\n",
      "Epoch: 14, Train_Loss: 0.2535352110862732, Test_Loss: 0.24959337711334229\n",
      "Epoch: 14, Train_Loss: 0.25715172290802, Test_Loss: 0.24863961338996887 *\n",
      "Epoch: 14, Train_Loss: 0.25950005650520325, Test_Loss: 0.25429314374923706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.2522236108779907, Test_Loss: 0.27052316069602966\n",
      "Epoch: 14, Train_Loss: 0.24696899950504303, Test_Loss: 0.27527710795402527\n",
      "Epoch: 14, Train_Loss: 0.25291353464126587, Test_Loss: 0.2602088451385498 *\n",
      "Epoch: 14, Train_Loss: 0.2624654769897461, Test_Loss: 0.24763989448547363 *\n",
      "Epoch: 14, Train_Loss: 0.4082569479942322, Test_Loss: 0.24804554879665375\n",
      "Epoch: 14, Train_Loss: 0.4163990914821625, Test_Loss: 0.254757821559906\n",
      "Epoch: 14, Train_Loss: 0.4373767375946045, Test_Loss: 0.25611281394958496\n",
      "Epoch: 14, Train_Loss: 0.2839357256889343, Test_Loss: 0.24772728979587555 *\n",
      "Epoch: 14, Train_Loss: 0.3904188871383667, Test_Loss: 0.25196266174316406\n",
      "Epoch: 14, Train_Loss: 0.3631350100040436, Test_Loss: 0.2475644052028656 *\n",
      "Epoch: 14, Train_Loss: 0.3069191575050354, Test_Loss: 0.2656703293323517\n",
      "Epoch: 14, Train_Loss: 0.36921462416648865, Test_Loss: 0.2612266540527344 *\n",
      "Epoch: 14, Train_Loss: 0.4543605446815491, Test_Loss: 0.2565097212791443 *\n",
      "Epoch: 14, Train_Loss: 0.3515852689743042, Test_Loss: 0.2480388581752777 *\n",
      "Epoch: 14, Train_Loss: 0.25536999106407166, Test_Loss: 0.24770031869411469 *\n",
      "Epoch: 14, Train_Loss: 2.358940601348877, Test_Loss: 0.25485095381736755\n",
      "Epoch: 14, Train_Loss: 1.2713899612426758, Test_Loss: 0.2506929337978363 *\n",
      "Epoch: 14, Train_Loss: 0.2895078659057617, Test_Loss: 0.2608032524585724\n",
      "Epoch: 14, Train_Loss: 0.30347174406051636, Test_Loss: 0.3150864243507385\n",
      "Epoch: 14, Train_Loss: 0.2994588613510132, Test_Loss: 3.058681011199951\n",
      "Epoch: 14, Train_Loss: 0.28063616156578064, Test_Loss: 2.6284780502319336 *\n",
      "Epoch: 14, Train_Loss: 0.24955181777477264, Test_Loss: 0.24872064590454102 *\n",
      "Epoch: 14, Train_Loss: 0.2794923186302185, Test_Loss: 0.24762557446956635 *\n",
      "Epoch: 14, Train_Loss: 0.36736685037612915, Test_Loss: 0.2639221251010895\n",
      "Epoch: 14, Train_Loss: 0.3223433196544647, Test_Loss: 0.25386786460876465 *\n",
      "Epoch: 14, Train_Loss: 0.2993389666080475, Test_Loss: 0.28384822607040405\n",
      "Epoch: 14, Train_Loss: 0.2945225238800049, Test_Loss: 0.29559847712516785\n",
      "Epoch: 14, Train_Loss: 0.2674838900566101, Test_Loss: 0.3153468668460846\n",
      "Epoch: 14, Train_Loss: 0.2525680661201477, Test_Loss: 0.24978497624397278 *\n",
      "Epoch: 14, Train_Loss: 0.2590377926826477, Test_Loss: 0.2678517997264862\n",
      "Epoch: 14, Train_Loss: 0.31209275126457214, Test_Loss: 0.2675269544124603 *\n",
      "Epoch: 14, Train_Loss: 0.2985146939754486, Test_Loss: 0.26887330412864685\n",
      "Epoch: 14, Train_Loss: 0.26703542470932007, Test_Loss: 0.25264406204223633 *\n",
      "Epoch: 14, Train_Loss: 0.25231215357780457, Test_Loss: 0.29137492179870605\n",
      "Epoch: 14, Train_Loss: 0.2625173032283783, Test_Loss: 0.3013404607772827\n",
      "Epoch: 14, Train_Loss: 0.26524290442466736, Test_Loss: 0.3143322467803955\n",
      "Epoch: 14, Train_Loss: 0.2572031617164612, Test_Loss: 0.2892105281352997 *\n",
      "Epoch: 14, Train_Loss: 0.24893216788768768, Test_Loss: 0.2822189927101135 *\n",
      "Epoch: 14, Train_Loss: 0.25010910630226135, Test_Loss: 0.2676844298839569 *\n",
      "Epoch: 14, Train_Loss: 0.2477034330368042, Test_Loss: 0.24918770790100098 *\n",
      "Epoch: 14, Train_Loss: 0.24989384412765503, Test_Loss: 0.2556556463241577\n",
      "Epoch: 14, Train_Loss: 0.2485397309064865, Test_Loss: 0.25308793783187866 *\n",
      "Epoch: 14, Train_Loss: 0.2501879036426544, Test_Loss: 0.253793329000473\n",
      "Epoch: 14, Train_Loss: 0.25306645035743713, Test_Loss: 0.25278252363204956 *\n",
      "Epoch: 14, Train_Loss: 0.2480705976486206, Test_Loss: 0.24856999516487122 *\n",
      "Epoch: 14, Train_Loss: 0.24769166111946106, Test_Loss: 0.2560768127441406\n",
      "Epoch: 14, Train_Loss: 0.24832287430763245, Test_Loss: 0.24888841807842255 *\n",
      "Epoch: 14, Train_Loss: 0.2637889087200165, Test_Loss: 0.25446516275405884\n",
      "Epoch: 14, Train_Loss: 0.26645946502685547, Test_Loss: 0.25142085552215576 *\n",
      "Epoch: 14, Train_Loss: 0.26519715785980225, Test_Loss: 0.27748820185661316\n",
      "Epoch: 14, Train_Loss: 0.26945826411247253, Test_Loss: 0.2970427870750427\n",
      "Epoch: 14, Train_Loss: 0.2536413073539734, Test_Loss: 0.30100202560424805\n",
      "Epoch: 14, Train_Loss: 0.24902912974357605, Test_Loss: 0.76171875\n",
      "Epoch: 14, Train_Loss: 0.2458302229642868, Test_Loss: 0.7473911643028259 *\n",
      "Epoch: 14, Train_Loss: 0.2535083293914795, Test_Loss: 0.4095991849899292 *\n",
      "Epoch: 14, Train_Loss: 0.26793932914733887, Test_Loss: 0.2860625982284546 *\n",
      "Epoch: 14, Train_Loss: 0.24959157407283783, Test_Loss: 0.2708320617675781 *\n",
      "Epoch: 14, Train_Loss: 0.24819597601890564, Test_Loss: 0.2800830006599426\n",
      "Epoch: 14, Train_Loss: 0.24510905146598816, Test_Loss: 0.504070520401001\n",
      "Epoch: 14, Train_Loss: 0.26129665970802307, Test_Loss: 0.8587735295295715\n",
      "Epoch: 14, Train_Loss: 0.29502856731414795, Test_Loss: 0.7196400165557861 *\n",
      "Epoch: 14, Train_Loss: 0.31335344910621643, Test_Loss: 0.3349774479866028 *\n",
      "Epoch: 14, Train_Loss: 0.27676475048065186, Test_Loss: 0.281283438205719 *\n",
      "Epoch: 14, Train_Loss: 0.24456912279129028, Test_Loss: 0.24978983402252197 *\n",
      "Epoch: 14, Train_Loss: 0.3083893060684204, Test_Loss: 0.24500113725662231 *\n",
      "Epoch: 14, Train_Loss: 0.2602199912071228, Test_Loss: 0.25351250171661377\n",
      "Epoch: 14, Train_Loss: 0.2479475438594818, Test_Loss: 0.2587850093841553\n",
      "Epoch: 14, Train_Loss: 0.26259517669677734, Test_Loss: 0.2882798910140991\n",
      "Epoch: 14, Train_Loss: 0.26661428809165955, Test_Loss: 0.24541109800338745 *\n",
      "Epoch: 14, Train_Loss: 0.3497883677482605, Test_Loss: 0.3098188042640686\n",
      "Epoch: 14, Train_Loss: 0.32084590196609497, Test_Loss: 0.3658027946949005\n",
      "Epoch: 14, Train_Loss: 0.2791973352432251, Test_Loss: 0.5858355760574341\n",
      "Epoch: 14, Train_Loss: 0.26836803555488586, Test_Loss: 0.49640482664108276 *\n",
      "Epoch: 14, Train_Loss: 0.25187593698501587, Test_Loss: 0.27925238013267517 *\n",
      "Epoch: 14, Train_Loss: 0.26260092854499817, Test_Loss: 0.2765755355358124 *\n",
      "Epoch: 14, Train_Loss: 0.2457747757434845, Test_Loss: 0.27485984563827515 *\n",
      "Epoch: 14, Train_Loss: 0.25151142477989197, Test_Loss: 0.276211142539978\n",
      "Epoch: 14, Train_Loss: 0.2615824341773987, Test_Loss: 0.275494247674942 *\n",
      "Epoch: 14, Train_Loss: 0.26626142859458923, Test_Loss: 1.920872688293457\n",
      "Epoch: 14, Train_Loss: 0.3496585488319397, Test_Loss: 4.091378688812256\n",
      "Epoch: 14, Train_Loss: 0.24672414362430573, Test_Loss: 0.25732314586639404 *\n",
      "Epoch: 14, Train_Loss: 0.30531683564186096, Test_Loss: 0.24956342577934265 *\n",
      "Epoch: 14, Train_Loss: 0.2577382028102875, Test_Loss: 0.24960897862911224\n",
      "Epoch: 14, Train_Loss: 0.2717798352241516, Test_Loss: 0.25912508368492126\n",
      "Epoch: 14, Train_Loss: 0.31786128878593445, Test_Loss: 0.2480451613664627 *\n",
      "Epoch: 14, Train_Loss: 0.4966578483581543, Test_Loss: 0.24905633926391602\n",
      "Epoch: 14, Train_Loss: 0.2581847310066223, Test_Loss: 0.24487733840942383 *\n",
      "Epoch: 14, Train_Loss: 0.2764192521572113, Test_Loss: 0.24421539902687073 *\n",
      "Epoch: 14, Train_Loss: 0.24315482378005981, Test_Loss: 0.2452518492937088\n",
      "Epoch: 14, Train_Loss: 0.2432592511177063, Test_Loss: 0.2433445155620575 *\n",
      "Epoch: 14, Train_Loss: 0.24607129395008087, Test_Loss: 0.24986903369426727\n",
      "Epoch: 14, Train_Loss: 0.24536141753196716, Test_Loss: 0.2754312753677368\n",
      "Epoch: 14, Train_Loss: 0.24807648360729218, Test_Loss: 0.2774507999420166\n",
      "Epoch: 14, Train_Loss: 0.2533002197742462, Test_Loss: 0.25095462799072266 *\n",
      "Epoch: 14, Train_Loss: 0.25494956970214844, Test_Loss: 0.24362298846244812 *\n",
      "Epoch: 14, Train_Loss: 0.25264573097229004, Test_Loss: 0.24388942122459412\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 14\n",
      "Epoch: 14, Train_Loss: 0.25392475724220276, Test_Loss: 0.24479377269744873\n",
      "Epoch: 14, Train_Loss: 0.2508121728897095, Test_Loss: 0.24865993857383728\n",
      "Epoch: 14, Train_Loss: 0.24389483034610748, Test_Loss: 0.24417546391487122 *\n",
      "Epoch: 14, Train_Loss: 0.24320083856582642, Test_Loss: 0.24643409252166748\n",
      "Epoch: 14, Train_Loss: 0.2630678713321686, Test_Loss: 0.24347874522209167 *\n",
      "Epoch: 14, Train_Loss: 0.262020468711853, Test_Loss: 0.2501373887062073\n",
      "Epoch: 14, Train_Loss: 0.278062105178833, Test_Loss: 0.24772992730140686 *\n",
      "Epoch: 14, Train_Loss: 0.24278491735458374, Test_Loss: 0.24676333367824554 *\n",
      "Epoch: 14, Train_Loss: 0.27641960978507996, Test_Loss: 0.24384136497974396 *\n",
      "Epoch: 14, Train_Loss: 0.27359580993652344, Test_Loss: 0.24694794416427612\n",
      "Epoch: 14, Train_Loss: 0.2704959511756897, Test_Loss: 0.24829605221748352\n",
      "Epoch: 14, Train_Loss: 0.247674822807312, Test_Loss: 0.24621717631816864 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train_Loss: 0.2801893651485443, Test_Loss: 0.28756293654441833\n",
      "Epoch: 14, Train_Loss: 0.2421645075082779, Test_Loss: 0.2772066593170166 *\n",
      "Epoch: 14, Train_Loss: 0.25531598925590515, Test_Loss: 4.241141319274902\n",
      "Epoch: 14, Train_Loss: 0.251384437084198, Test_Loss: 1.781097650527954 *\n",
      "Epoch: 14, Train_Loss: 0.26663938164711, Test_Loss: 0.24376574158668518 *\n",
      "Epoch: 14, Train_Loss: 1.1256952285766602, Test_Loss: 0.2532395124435425\n",
      "Epoch: 14, Train_Loss: 4.225688934326172, Test_Loss: 0.25728172063827515\n",
      "Epoch: 14, Train_Loss: 0.879000186920166, Test_Loss: 0.2641187608242035\n",
      "Epoch: 14, Train_Loss: 0.26494109630584717, Test_Loss: 0.2556959390640259 *\n",
      "Epoch: 14, Train_Loss: 0.2484055459499359, Test_Loss: 0.31161075830459595\n",
      "Epoch: 14, Train_Loss: 0.36321139335632324, Test_Loss: 0.3168148100376129\n",
      "Epoch: 14, Train_Loss: 0.3225032091140747, Test_Loss: 0.243941068649292 *\n",
      "Epoch: 14, Train_Loss: 0.2619727849960327, Test_Loss: 0.27375584840774536\n",
      "Epoch: 14, Train_Loss: 0.2434084564447403, Test_Loss: 0.2607768476009369 *\n",
      "Epoch: 14, Train_Loss: 0.31172990798950195, Test_Loss: 0.2550162672996521 *\n",
      "Epoch: 14, Train_Loss: 0.2657095789909363, Test_Loss: 0.24667231738567352 *\n",
      "Epoch: 14, Train_Loss: 0.25465285778045654, Test_Loss: 0.25709792971611023\n",
      "Epoch: 14, Train_Loss: 0.5075384378433228, Test_Loss: 0.2869684100151062\n",
      "Epoch: 14, Train_Loss: 0.8821157217025757, Test_Loss: 0.33461061120033264\n",
      "Epoch: 14, Train_Loss: 0.961150586605072, Test_Loss: 0.28710371255874634 *\n",
      "Epoch: 14, Train_Loss: 0.2966269850730896, Test_Loss: 0.2837430238723755 *\n",
      "Epoch: 14, Train_Loss: 0.3801824748516083, Test_Loss: 0.25019097328186035 *\n",
      "Epoch: 14, Train_Loss: 1.9004247188568115, Test_Loss: 0.25065648555755615\n",
      "Epoch: 14, Train_Loss: 0.8913139700889587, Test_Loss: 0.27247220277786255\n",
      "Epoch: 14, Train_Loss: 0.26156020164489746, Test_Loss: 0.27762091159820557\n",
      "Epoch: 14, Train_Loss: 0.25260722637176514, Test_Loss: 0.26272207498550415 *\n",
      "Epoch: 14, Train_Loss: 0.6931159496307373, Test_Loss: 0.27904537320137024\n",
      "Epoch: 14, Train_Loss: 0.6689487099647522, Test_Loss: 0.31828323006629944\n",
      "Epoch: 14, Train_Loss: 0.8364995718002319, Test_Loss: 0.31986483931541443\n",
      "Epoch: 14, Train_Loss: 0.2480149120092392, Test_Loss: 0.29486292600631714 *\n",
      "Epoch: 14, Train_Loss: 0.263884037733078, Test_Loss: 0.2530710995197296 *\n",
      "Epoch: 14, Train_Loss: 0.46445536613464355, Test_Loss: 0.25791242718696594\n",
      "Epoch: 14, Train_Loss: 0.5793755054473877, Test_Loss: 0.25557050108909607 *\n",
      "Epoch: 14, Train_Loss: 0.26211047172546387, Test_Loss: 0.2697887122631073\n",
      "Epoch: 14, Train_Loss: 0.28129786252975464, Test_Loss: 0.3855581283569336\n",
      "Epoch: 14, Train_Loss: 0.3154090642929077, Test_Loss: 0.41333648562431335\n",
      "Epoch: 14, Train_Loss: 0.349639356136322, Test_Loss: 0.5537369251251221\n",
      "Epoch: 14, Train_Loss: 0.36398857831954956, Test_Loss: 0.3472544550895691 *\n",
      "Epoch: 14, Train_Loss: 0.3499179184436798, Test_Loss: 0.2597300708293915 *\n",
      "Epoch: 14, Train_Loss: 0.29016491770744324, Test_Loss: 0.25696077942848206 *\n",
      "Epoch: 14, Train_Loss: 0.3264588713645935, Test_Loss: 0.2920592129230499\n",
      "Epoch: 14, Train_Loss: 0.3499275743961334, Test_Loss: 0.4903162121772766\n",
      "Epoch: 15, Train_Loss: 0.3493976593017578, Test_Loss: 0.25638508796691895 *\n",
      "Epoch: 15, Train_Loss: 0.37737417221069336, Test_Loss: 0.6208939552307129\n",
      "Epoch: 15, Train_Loss: 0.44949325919151306, Test_Loss: 0.3276217579841614 *\n",
      "Epoch: 15, Train_Loss: 0.28165459632873535, Test_Loss: 0.32420438528060913 *\n",
      "Epoch: 15, Train_Loss: 0.3479973077774048, Test_Loss: 0.3075077533721924 *\n",
      "Epoch: 15, Train_Loss: 0.3288784623146057, Test_Loss: 0.30049020051956177 *\n",
      "Epoch: 15, Train_Loss: 0.26540207862854004, Test_Loss: 0.3667285740375519\n",
      "Epoch: 15, Train_Loss: 0.24206921458244324, Test_Loss: 0.2588578164577484 *\n",
      "Epoch: 15, Train_Loss: 0.2455238401889801, Test_Loss: 0.27787622809410095\n",
      "Epoch: 15, Train_Loss: 0.2484813779592514, Test_Loss: 0.25896188616752625 *\n",
      "Epoch: 15, Train_Loss: 0.24451524019241333, Test_Loss: 0.3952818810939789\n",
      "Epoch: 15, Train_Loss: 0.2576519250869751, Test_Loss: 0.7236917614936829\n",
      "Epoch: 15, Train_Loss: 0.29816675186157227, Test_Loss: 0.4267818331718445 *\n",
      "Epoch: 15, Train_Loss: 0.2890152633190155, Test_Loss: 0.5538356304168701\n",
      "Epoch: 15, Train_Loss: 0.3299020230770111, Test_Loss: 0.2681390047073364 *\n",
      "Epoch: 15, Train_Loss: 0.40788835287094116, Test_Loss: 0.26749369502067566 *\n",
      "Epoch: 15, Train_Loss: 0.4657787084579468, Test_Loss: 0.2649631202220917 *\n",
      "Epoch: 15, Train_Loss: 0.25795871019363403, Test_Loss: 0.26661133766174316\n",
      "Epoch: 15, Train_Loss: 0.2912324368953705, Test_Loss: 0.3391198515892029\n",
      "Epoch: 15, Train_Loss: 0.4610215127468109, Test_Loss: 3.6721322536468506\n",
      "Epoch: 15, Train_Loss: 0.32279491424560547, Test_Loss: 2.8931219577789307 *\n",
      "Epoch: 15, Train_Loss: 0.3232618570327759, Test_Loss: 0.346410870552063 *\n",
      "Epoch: 15, Train_Loss: 0.3800317645072937, Test_Loss: 0.2804858088493347 *\n",
      "Epoch: 15, Train_Loss: 0.3883412778377533, Test_Loss: 0.30796441435813904\n",
      "Epoch: 15, Train_Loss: 0.2901146709918976, Test_Loss: 0.2449411302804947 *\n",
      "Epoch: 15, Train_Loss: 0.42026370763778687, Test_Loss: 0.32523608207702637\n",
      "Epoch: 15, Train_Loss: 0.2725770175457001, Test_Loss: 0.34894707798957825\n",
      "Epoch: 15, Train_Loss: 0.2787739932537079, Test_Loss: 0.29861146211624146 *\n",
      "Epoch: 15, Train_Loss: 0.5464365482330322, Test_Loss: 0.2700011730194092 *\n",
      "Epoch: 15, Train_Loss: 0.6774482131004333, Test_Loss: 0.2822493016719818\n",
      "Epoch: 15, Train_Loss: 0.4236654043197632, Test_Loss: 0.27579784393310547 *\n",
      "Epoch: 15, Train_Loss: 0.30980807542800903, Test_Loss: 0.3937729001045227\n",
      "Epoch: 15, Train_Loss: 0.26388585567474365, Test_Loss: 0.267879843711853 *\n",
      "Epoch: 15, Train_Loss: 0.24474108219146729, Test_Loss: 0.2866075336933136\n",
      "Epoch: 15, Train_Loss: 0.5438234210014343, Test_Loss: 0.26774030923843384 *\n",
      "Epoch: 15, Train_Loss: 0.30498549342155457, Test_Loss: 0.2528995871543884 *\n",
      "Epoch: 15, Train_Loss: 0.2661420404911041, Test_Loss: 0.28022170066833496\n",
      "Epoch: 15, Train_Loss: 0.41329142451286316, Test_Loss: 0.34162411093711853\n",
      "Epoch: 15, Train_Loss: 0.29072463512420654, Test_Loss: 0.2931044399738312 *\n",
      "Epoch: 15, Train_Loss: 0.256430059671402, Test_Loss: 0.2467048317193985 *\n",
      "Epoch: 15, Train_Loss: 0.28253328800201416, Test_Loss: 0.25750574469566345\n",
      "Epoch: 15, Train_Loss: 0.36870434880256653, Test_Loss: 0.2603316009044647\n",
      "Epoch: 15, Train_Loss: 0.2912379503250122, Test_Loss: 0.30784285068511963\n",
      "Epoch: 15, Train_Loss: 0.3514719605445862, Test_Loss: 0.33036330342292786\n",
      "Epoch: 15, Train_Loss: 0.2568913996219635, Test_Loss: 0.27005502581596375 *\n",
      "Epoch: 15, Train_Loss: 0.39659351110458374, Test_Loss: 0.25063973665237427 *\n",
      "Epoch: 15, Train_Loss: 0.2944013774394989, Test_Loss: 0.29865753650665283\n",
      "Epoch: 15, Train_Loss: 0.2668178081512451, Test_Loss: 0.273247629404068 *\n",
      "Epoch: 15, Train_Loss: 0.25056135654449463, Test_Loss: 0.2429027259349823 *\n",
      "Epoch: 15, Train_Loss: 0.27892613410949707, Test_Loss: 0.3702952265739441\n",
      "Epoch: 15, Train_Loss: 0.5201343894004822, Test_Loss: 0.3500218093395233 *\n",
      "Epoch: 15, Train_Loss: 0.5030348300933838, Test_Loss: 4.790688514709473\n",
      "Epoch: 15, Train_Loss: 0.5744662284851074, Test_Loss: 0.6105031371116638 *\n",
      "Epoch: 15, Train_Loss: 0.6376983523368835, Test_Loss: 0.30950626730918884 *\n",
      "Epoch: 15, Train_Loss: 0.46632999181747437, Test_Loss: 0.3286151885986328\n",
      "Epoch: 15, Train_Loss: 0.3499770760536194, Test_Loss: 0.2611512839794159 *\n",
      "Epoch: 15, Train_Loss: 0.31366220116615295, Test_Loss: 0.26139703392982483\n",
      "Epoch: 15, Train_Loss: 0.25265583395957947, Test_Loss: 0.24895188212394714 *\n",
      "Epoch: 15, Train_Loss: 0.24677589535713196, Test_Loss: 0.3024281859397888\n",
      "Epoch: 15, Train_Loss: 0.2612432837486267, Test_Loss: 0.2873603403568268 *\n",
      "Epoch: 15, Train_Loss: 0.43294697999954224, Test_Loss: 0.24279199540615082 *\n",
      "Epoch: 15, Train_Loss: 0.44505709409713745, Test_Loss: 0.27577176690101624\n",
      "Epoch: 15, Train_Loss: 0.5201064348220825, Test_Loss: 0.3494967222213745\n",
      "Epoch: 15, Train_Loss: 1.294952154159546, Test_Loss: 0.3156392276287079 *\n",
      "Epoch: 15, Train_Loss: 1.009827733039856, Test_Loss: 0.29648396372795105 *\n",
      "Epoch: 15, Train_Loss: 0.3809630870819092, Test_Loss: 0.2602052092552185 *\n",
      "Epoch: 15, Train_Loss: 0.2964570224285126, Test_Loss: 0.3287481665611267\n",
      "Epoch: 15, Train_Loss: 0.2476230263710022, Test_Loss: 0.2956400215625763 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.4313148260116577, Test_Loss: 0.3023407757282257\n",
      "Epoch: 15, Train_Loss: 0.7593222260475159, Test_Loss: 0.37837761640548706\n",
      "Epoch: 15, Train_Loss: 0.8110092878341675, Test_Loss: 0.2800266742706299 *\n",
      "Epoch: 15, Train_Loss: 0.2949557304382324, Test_Loss: 0.337865948677063\n",
      "Epoch: 15, Train_Loss: 0.26366281509399414, Test_Loss: 0.3140055537223816 *\n",
      "Epoch: 15, Train_Loss: 0.31717026233673096, Test_Loss: 0.33619487285614014\n",
      "Epoch: 15, Train_Loss: 0.5310639142990112, Test_Loss: 0.29752954840660095 *\n",
      "Epoch: 15, Train_Loss: 0.40790945291519165, Test_Loss: 0.3092461824417114\n",
      "Epoch: 15, Train_Loss: 0.3111981749534607, Test_Loss: 0.3945673108100891\n",
      "Epoch: 15, Train_Loss: 0.362471342086792, Test_Loss: 0.35769736766815186 *\n",
      "Epoch: 15, Train_Loss: 0.27285709977149963, Test_Loss: 0.3112441897392273 *\n",
      "Epoch: 15, Train_Loss: 0.24298416078090668, Test_Loss: 0.30068105459213257 *\n",
      "Epoch: 15, Train_Loss: 0.26700252294540405, Test_Loss: 0.3026473820209503\n",
      "Epoch: 15, Train_Loss: 0.24811150133609772, Test_Loss: 0.2502892017364502 *\n",
      "Epoch: 15, Train_Loss: 0.29894325137138367, Test_Loss: 0.26076245307922363\n",
      "Epoch: 15, Train_Loss: 0.2805558145046234, Test_Loss: 0.3930651545524597\n",
      "Epoch: 15, Train_Loss: 0.7740375995635986, Test_Loss: 0.3962991535663605\n",
      "Epoch: 15, Train_Loss: 15.53048324584961, Test_Loss: 0.3985503911972046\n",
      "Epoch: 15, Train_Loss: 0.45070362091064453, Test_Loss: 0.29461342096328735 *\n",
      "Epoch: 15, Train_Loss: 0.9716486930847168, Test_Loss: 0.24727840721607208 *\n",
      "Epoch: 15, Train_Loss: 0.8889856338500977, Test_Loss: 0.252738893032074\n",
      "Epoch: 15, Train_Loss: 0.2786816656589508, Test_Loss: 0.3168841004371643\n",
      "Epoch: 15, Train_Loss: 0.410622775554657, Test_Loss: 0.48775041103363037\n",
      "Epoch: 15, Train_Loss: 6.017574310302734, Test_Loss: 0.3025966286659241 *\n",
      "Epoch: 15, Train_Loss: 2.8912250995635986, Test_Loss: 0.46955734491348267\n",
      "Epoch: 15, Train_Loss: 0.3202465772628784, Test_Loss: 0.29522505402565 *\n",
      "Epoch: 15, Train_Loss: 1.101576328277588, Test_Loss: 0.3176325559616089\n",
      "Epoch: 15, Train_Loss: 4.891523838043213, Test_Loss: 0.4861195683479309\n",
      "Epoch: 15, Train_Loss: 0.5845139026641846, Test_Loss: 0.43494370579719543 *\n",
      "Epoch: 15, Train_Loss: 0.28565579652786255, Test_Loss: 0.5078706741333008\n",
      "Epoch: 15, Train_Loss: 0.2538561224937439, Test_Loss: 0.2997276782989502 *\n",
      "Epoch: 15, Train_Loss: 0.3425012230873108, Test_Loss: 0.3564104437828064\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.34997323155403137, Test_Loss: 0.3374122381210327 *\n",
      "Epoch: 15, Train_Loss: 0.24376235902309418, Test_Loss: 0.49092864990234375\n",
      "Epoch: 15, Train_Loss: 0.2508416473865509, Test_Loss: 0.9887300133705139\n",
      "Epoch: 15, Train_Loss: 0.23720990121364594, Test_Loss: 0.3502568006515503 *\n",
      "Epoch: 15, Train_Loss: 0.2634795904159546, Test_Loss: 0.38280048966407776\n",
      "Epoch: 15, Train_Loss: 0.3307325541973114, Test_Loss: 0.2627611458301544 *\n",
      "Epoch: 15, Train_Loss: 0.24852362275123596, Test_Loss: 0.26377153396606445\n",
      "Epoch: 15, Train_Loss: 0.32734251022338867, Test_Loss: 0.2636736333370209 *\n",
      "Epoch: 15, Train_Loss: 0.32135438919067383, Test_Loss: 0.2576211094856262 *\n",
      "Epoch: 15, Train_Loss: 0.2552007734775543, Test_Loss: 0.24501723051071167 *\n",
      "Epoch: 15, Train_Loss: 0.2486666738986969, Test_Loss: 6.421372413635254\n",
      "Epoch: 15, Train_Loss: 0.24818386137485504, Test_Loss: 1.7248908281326294 *\n",
      "Epoch: 15, Train_Loss: 0.24932420253753662, Test_Loss: 0.404191792011261 *\n",
      "Epoch: 15, Train_Loss: 0.24694973230361938, Test_Loss: 0.3778488039970398 *\n",
      "Epoch: 15, Train_Loss: 0.24311348795890808, Test_Loss: 0.38041943311691284\n",
      "Epoch: 15, Train_Loss: 0.2361757457256317, Test_Loss: 0.26993125677108765 *\n",
      "Epoch: 15, Train_Loss: 0.23665638267993927, Test_Loss: 0.4565909504890442\n",
      "Epoch: 15, Train_Loss: 0.2370760291814804, Test_Loss: 0.5092080235481262\n",
      "Epoch: 15, Train_Loss: 0.23699773848056793, Test_Loss: 0.3889006972312927 *\n",
      "Epoch: 15, Train_Loss: 0.23608586192131042, Test_Loss: 0.4512515664100647\n",
      "Epoch: 15, Train_Loss: 0.23653694987297058, Test_Loss: 0.40639209747314453 *\n",
      "Epoch: 15, Train_Loss: 0.24759496748447418, Test_Loss: 0.5215368866920471\n",
      "Epoch: 15, Train_Loss: 0.2447604387998581, Test_Loss: 0.6413869857788086\n",
      "Epoch: 15, Train_Loss: 0.3175073266029358, Test_Loss: 0.44260793924331665 *\n",
      "Epoch: 15, Train_Loss: 0.2772868275642395, Test_Loss: 0.46473488211631775\n",
      "Epoch: 15, Train_Loss: 0.38109299540519714, Test_Loss: 0.3638468384742737 *\n",
      "Epoch: 15, Train_Loss: 5.9423346519470215, Test_Loss: 0.26402729749679565 *\n",
      "Epoch: 15, Train_Loss: 2.1261820793151855, Test_Loss: 0.29184645414352417\n",
      "Epoch: 15, Train_Loss: 0.3091810345649719, Test_Loss: 0.3282601833343506\n",
      "Epoch: 15, Train_Loss: 0.3252754211425781, Test_Loss: 0.4175054430961609\n",
      "Epoch: 15, Train_Loss: 0.33254045248031616, Test_Loss: 0.4831448793411255\n",
      "Epoch: 15, Train_Loss: 0.3031894266605377, Test_Loss: 0.49076047539711\n",
      "Epoch: 15, Train_Loss: 0.35408639907836914, Test_Loss: 0.4024428129196167 *\n",
      "Epoch: 15, Train_Loss: 0.3570440113544464, Test_Loss: 0.38003045320510864 *\n",
      "Epoch: 15, Train_Loss: 0.3315308094024658, Test_Loss: 0.45228245854377747\n",
      "Epoch: 15, Train_Loss: 0.39086252450942993, Test_Loss: 0.2982350289821625 *\n",
      "Epoch: 15, Train_Loss: 0.3480985760688782, Test_Loss: 0.3117944002151489\n",
      "Epoch: 15, Train_Loss: 0.2572508156299591, Test_Loss: 0.3309253752231598\n",
      "Epoch: 15, Train_Loss: 0.2830195128917694, Test_Loss: 0.3080446422100067 *\n",
      "Epoch: 15, Train_Loss: 0.2901867628097534, Test_Loss: 0.2537548840045929 *\n",
      "Epoch: 15, Train_Loss: 0.4757428765296936, Test_Loss: 0.45178091526031494\n",
      "Epoch: 15, Train_Loss: 0.27325376868247986, Test_Loss: 0.5372092723846436\n",
      "Epoch: 15, Train_Loss: 0.28914469480514526, Test_Loss: 6.796056747436523\n",
      "Epoch: 15, Train_Loss: 0.28361377120018005, Test_Loss: 0.3398268520832062 *\n",
      "Epoch: 15, Train_Loss: 0.24560733139514923, Test_Loss: 0.23787879943847656 *\n",
      "Epoch: 15, Train_Loss: 0.28584372997283936, Test_Loss: 0.28858181834220886\n",
      "Epoch: 15, Train_Loss: 0.2711712121963501, Test_Loss: 0.3778734803199768\n",
      "Epoch: 15, Train_Loss: 0.2429775595664978, Test_Loss: 0.3106893002986908 *\n",
      "Epoch: 15, Train_Loss: 0.23968876898288727, Test_Loss: 0.25205549597740173 *\n",
      "Epoch: 15, Train_Loss: 0.23693838715553284, Test_Loss: 0.3574303388595581\n",
      "Epoch: 15, Train_Loss: 0.2827378213405609, Test_Loss: 0.2802313566207886 *\n",
      "Epoch: 15, Train_Loss: 5.470679759979248, Test_Loss: 0.23731429874897003 *\n",
      "Epoch: 15, Train_Loss: 0.9150470495223999, Test_Loss: 0.264270156621933\n",
      "Epoch: 15, Train_Loss: 0.23997606337070465, Test_Loss: 0.2535000443458557 *\n",
      "Epoch: 15, Train_Loss: 0.2641511559486389, Test_Loss: 0.24126295745372772 *\n",
      "Epoch: 15, Train_Loss: 0.2505077123641968, Test_Loss: 0.3209116458892822\n",
      "Epoch: 15, Train_Loss: 0.24259743094444275, Test_Loss: 0.3569823205471039\n",
      "Epoch: 15, Train_Loss: 0.242786705493927, Test_Loss: 0.3446034789085388 *\n",
      "Epoch: 15, Train_Loss: 0.24121157824993134, Test_Loss: 0.32351016998291016 *\n",
      "Epoch: 15, Train_Loss: 0.2560904324054718, Test_Loss: 0.2742193937301636 *\n",
      "Epoch: 15, Train_Loss: 0.24239608645439148, Test_Loss: 0.2563401162624359 *\n",
      "Epoch: 15, Train_Loss: 0.25757476687431335, Test_Loss: 0.27266210317611694\n",
      "Epoch: 15, Train_Loss: 0.23819687962532043, Test_Loss: 0.3057915270328522\n",
      "Epoch: 15, Train_Loss: 0.23906299471855164, Test_Loss: 0.27404943108558655 *\n",
      "Epoch: 15, Train_Loss: 0.2521858811378479, Test_Loss: 0.26942482590675354 *\n",
      "Epoch: 15, Train_Loss: 0.23975299298763275, Test_Loss: 0.254473477602005 *\n",
      "Epoch: 15, Train_Loss: 0.2371644675731659, Test_Loss: 0.2638506591320038\n",
      "Epoch: 15, Train_Loss: 0.24648788571357727, Test_Loss: 0.30480337142944336\n",
      "Epoch: 15, Train_Loss: 0.27228668332099915, Test_Loss: 0.3143613636493683\n",
      "Epoch: 15, Train_Loss: 0.2631451189517975, Test_Loss: 0.2953707277774811 *\n",
      "Epoch: 15, Train_Loss: 0.2353227734565735, Test_Loss: 0.26218467950820923 *\n",
      "Epoch: 15, Train_Loss: 0.23571979999542236, Test_Loss: 0.24271517992019653 *\n",
      "Epoch: 15, Train_Loss: 0.28778260946273804, Test_Loss: 0.25610753893852234\n",
      "Epoch: 15, Train_Loss: 0.26399827003479004, Test_Loss: 0.28028619289398193\n",
      "Epoch: 15, Train_Loss: 0.26135098934173584, Test_Loss: 0.4715045094490051\n",
      "Epoch: 15, Train_Loss: 0.26220566034317017, Test_Loss: 0.5130162239074707\n",
      "Epoch: 15, Train_Loss: 0.2777503728866577, Test_Loss: 0.41026467084884644 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.28831902146339417, Test_Loss: 0.2735462784767151 *\n",
      "Epoch: 15, Train_Loss: 0.26658520102500916, Test_Loss: 0.24890179932117462 *\n",
      "Epoch: 15, Train_Loss: 0.28631025552749634, Test_Loss: 0.25545474886894226\n",
      "Epoch: 15, Train_Loss: 0.3348187208175659, Test_Loss: 0.3535865843296051\n",
      "Epoch: 15, Train_Loss: 0.2710925042629242, Test_Loss: 0.6746554374694824\n",
      "Epoch: 15, Train_Loss: 0.26001831889152527, Test_Loss: 0.7708374857902527\n",
      "Epoch: 15, Train_Loss: 0.23403578996658325, Test_Loss: 0.33769434690475464 *\n",
      "Epoch: 15, Train_Loss: 0.2369481921195984, Test_Loss: 0.29933521151542664 *\n",
      "Epoch: 15, Train_Loss: 0.23781287670135498, Test_Loss: 0.24453014135360718 *\n",
      "Epoch: 15, Train_Loss: 0.23707614839076996, Test_Loss: 0.250555157661438\n",
      "Epoch: 15, Train_Loss: 0.23649980127811432, Test_Loss: 0.24533860385417938 *\n",
      "Epoch: 15, Train_Loss: 4.411604881286621, Test_Loss: 0.2542163133621216\n",
      "Epoch: 15, Train_Loss: 0.971189558506012, Test_Loss: 0.26093533635139465\n",
      "Epoch: 15, Train_Loss: 0.23520617187023163, Test_Loss: 0.26021766662597656 *\n",
      "Epoch: 15, Train_Loss: 0.2457219809293747, Test_Loss: 0.23920197784900665 *\n",
      "Epoch: 15, Train_Loss: 0.23800039291381836, Test_Loss: 0.35734859108924866\n",
      "Epoch: 15, Train_Loss: 0.23455171287059784, Test_Loss: 0.6247298717498779\n",
      "Epoch: 15, Train_Loss: 0.23531220853328705, Test_Loss: 0.3575803339481354 *\n",
      "Epoch: 15, Train_Loss: 0.23465310037136078, Test_Loss: 0.38469067215919495\n",
      "Epoch: 15, Train_Loss: 0.2346242368221283, Test_Loss: 0.25005707144737244 *\n",
      "Epoch: 15, Train_Loss: 0.2362774908542633, Test_Loss: 0.25029027462005615\n",
      "Epoch: 15, Train_Loss: 0.2594029903411865, Test_Loss: 0.25057828426361084\n",
      "Epoch: 15, Train_Loss: 0.28834831714630127, Test_Loss: 0.2532348036766052\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.3008125424385071, Test_Loss: 0.27785149216651917\n",
      "Epoch: 15, Train_Loss: 0.2959938645362854, Test_Loss: 5.918152332305908\n",
      "Epoch: 15, Train_Loss: 0.24024826288223267, Test_Loss: 0.5097690224647522 *\n",
      "Epoch: 15, Train_Loss: 0.2871944010257721, Test_Loss: 0.24935133755207062 *\n",
      "Epoch: 15, Train_Loss: 0.47541940212249756, Test_Loss: 0.242527037858963 *\n",
      "Epoch: 15, Train_Loss: 0.46367841958999634, Test_Loss: 0.24502567946910858\n",
      "Epoch: 15, Train_Loss: 0.4672049880027771, Test_Loss: 0.24849778413772583\n",
      "Epoch: 15, Train_Loss: 0.2571222484111786, Test_Loss: 0.2384379804134369 *\n",
      "Epoch: 15, Train_Loss: 0.2341996133327484, Test_Loss: 0.23582038283348083 *\n",
      "Epoch: 15, Train_Loss: 0.2334766834974289, Test_Loss: 0.2347085028886795 *\n",
      "Epoch: 15, Train_Loss: 0.23972110450267792, Test_Loss: 0.23434829711914062 *\n",
      "Epoch: 15, Train_Loss: 0.24406202137470245, Test_Loss: 0.23572976887226105\n",
      "Epoch: 15, Train_Loss: 0.2455112636089325, Test_Loss: 0.23967710137367249\n",
      "Epoch: 15, Train_Loss: 0.24307480454444885, Test_Loss: 0.24705512821674347\n",
      "Epoch: 15, Train_Loss: 0.233072891831398, Test_Loss: 0.2665739059448242\n",
      "Epoch: 15, Train_Loss: 0.23527537286281586, Test_Loss: 0.2533707320690155 *\n",
      "Epoch: 15, Train_Loss: 0.24659259617328644, Test_Loss: 0.23506078124046326 *\n",
      "Epoch: 15, Train_Loss: 0.3554270267486572, Test_Loss: 0.23373521864414215 *\n",
      "Epoch: 15, Train_Loss: 0.40765702724456787, Test_Loss: 0.23433054983615875\n",
      "Epoch: 15, Train_Loss: 0.3983394503593445, Test_Loss: 0.2464195042848587\n",
      "Epoch: 15, Train_Loss: 0.3014848232269287, Test_Loss: 0.23425224423408508 *\n",
      "Epoch: 15, Train_Loss: 0.3667011559009552, Test_Loss: 0.2384806126356125\n",
      "Epoch: 15, Train_Loss: 0.37130364775657654, Test_Loss: 0.23682373762130737 *\n",
      "Epoch: 15, Train_Loss: 0.24324311316013336, Test_Loss: 0.2439538985490799\n",
      "Epoch: 15, Train_Loss: 0.3835773169994354, Test_Loss: 0.24242691695690155 *\n",
      "Epoch: 15, Train_Loss: 0.3208354413509369, Test_Loss: 0.24903537333011627\n",
      "Epoch: 15, Train_Loss: 0.48488515615463257, Test_Loss: 0.23508767783641815 *\n",
      "Epoch: 15, Train_Loss: 0.24551162123680115, Test_Loss: 0.23672832548618317\n",
      "Epoch: 15, Train_Loss: 1.3114573955535889, Test_Loss: 0.2446114867925644\n",
      "Epoch: 15, Train_Loss: 2.2585408687591553, Test_Loss: 0.24038492143154144 *\n",
      "Epoch: 15, Train_Loss: 0.27594441175460815, Test_Loss: 0.237826868891716 *\n",
      "Epoch: 15, Train_Loss: 0.29448437690734863, Test_Loss: 0.31804901361465454\n",
      "Epoch: 15, Train_Loss: 0.2831135392189026, Test_Loss: 1.3582916259765625\n",
      "Epoch: 15, Train_Loss: 0.2740860879421234, Test_Loss: 4.791213512420654\n",
      "Epoch: 15, Train_Loss: 0.23351424932479858, Test_Loss: 0.2489357441663742 *\n",
      "Epoch: 15, Train_Loss: 0.24516499042510986, Test_Loss: 0.23357678949832916 *\n",
      "Epoch: 15, Train_Loss: 0.34437912702560425, Test_Loss: 0.2612344026565552\n",
      "Epoch: 15, Train_Loss: 0.3162715435028076, Test_Loss: 0.26377102732658386\n",
      "Epoch: 15, Train_Loss: 0.2955871820449829, Test_Loss: 0.25700241327285767 *\n",
      "Epoch: 15, Train_Loss: 0.2777021825313568, Test_Loss: 0.2519814670085907 *\n",
      "Epoch: 15, Train_Loss: 0.26082026958465576, Test_Loss: 0.3498801589012146\n",
      "Epoch: 15, Train_Loss: 0.23960494995117188, Test_Loss: 0.2515690326690674 *\n",
      "Epoch: 15, Train_Loss: 0.24840959906578064, Test_Loss: 0.24015773832798004 *\n",
      "Epoch: 15, Train_Loss: 0.27380943298339844, Test_Loss: 0.2598191499710083\n",
      "Epoch: 15, Train_Loss: 0.2778123617172241, Test_Loss: 0.25585418939590454 *\n",
      "Epoch: 15, Train_Loss: 0.2532126307487488, Test_Loss: 0.23716841638088226 *\n",
      "Epoch: 15, Train_Loss: 0.2366182804107666, Test_Loss: 0.306824266910553\n",
      "Epoch: 15, Train_Loss: 0.25348198413848877, Test_Loss: 0.2900623381137848 *\n",
      "Epoch: 15, Train_Loss: 0.2558310329914093, Test_Loss: 0.3038676381111145\n",
      "Epoch: 15, Train_Loss: 0.24721765518188477, Test_Loss: 0.2949139475822449 *\n",
      "Epoch: 15, Train_Loss: 0.23535937070846558, Test_Loss: 0.2609786093235016 *\n",
      "Epoch: 15, Train_Loss: 0.23388035595417023, Test_Loss: 0.27422523498535156\n",
      "Epoch: 15, Train_Loss: 0.23233112692832947, Test_Loss: 0.23549652099609375 *\n",
      "Epoch: 15, Train_Loss: 0.2334589660167694, Test_Loss: 0.23608794808387756\n",
      "Epoch: 15, Train_Loss: 0.23687580227851868, Test_Loss: 0.2401246577501297\n",
      "Epoch: 15, Train_Loss: 0.23551179468631744, Test_Loss: 0.2451779693365097\n",
      "Epoch: 15, Train_Loss: 0.23749490082263947, Test_Loss: 0.24235406517982483 *\n",
      "Epoch: 15, Train_Loss: 0.23237209022045135, Test_Loss: 0.23939664661884308 *\n",
      "Epoch: 15, Train_Loss: 0.23217123746871948, Test_Loss: 0.23869511485099792 *\n",
      "Epoch: 15, Train_Loss: 0.23331567645072937, Test_Loss: 0.2657470405101776\n",
      "Epoch: 15, Train_Loss: 0.24100647866725922, Test_Loss: 0.24122175574302673 *\n",
      "Epoch: 15, Train_Loss: 0.2516721189022064, Test_Loss: 0.24501347541809082\n",
      "Epoch: 15, Train_Loss: 0.24699753522872925, Test_Loss: 0.24407202005386353 *\n",
      "Epoch: 15, Train_Loss: 0.2582009732723236, Test_Loss: 0.27333033084869385\n",
      "Epoch: 15, Train_Loss: 0.23536865413188934, Test_Loss: 0.25900721549987793 *\n",
      "Epoch: 15, Train_Loss: 0.233928844332695, Test_Loss: 0.6326900720596313\n",
      "Epoch: 15, Train_Loss: 0.23141612112522125, Test_Loss: 0.7347352504730225\n",
      "Epoch: 15, Train_Loss: 0.23253008723258972, Test_Loss: 0.43683063983917236 *\n",
      "Epoch: 15, Train_Loss: 0.25177913904190063, Test_Loss: 0.26772090792655945 *\n",
      "Epoch: 15, Train_Loss: 0.23857930302619934, Test_Loss: 0.2672925293445587 *\n",
      "Epoch: 15, Train_Loss: 0.23385007679462433, Test_Loss: 0.244052916765213 *\n",
      "Epoch: 15, Train_Loss: 0.23155193030834198, Test_Loss: 0.35019993782043457\n",
      "Epoch: 15, Train_Loss: 0.23932026326656342, Test_Loss: 0.6686844229698181\n",
      "Epoch: 15, Train_Loss: 0.280491441488266, Test_Loss: 0.7282109260559082\n",
      "Epoch: 15, Train_Loss: 0.28517720103263855, Test_Loss: 0.28775927424430847 *\n",
      "Epoch: 15, Train_Loss: 0.2840166389942169, Test_Loss: 0.3115154504776001\n",
      "Epoch: 15, Train_Loss: 0.23218385875225067, Test_Loss: 0.23339195549488068 *\n",
      "Epoch: 15, Train_Loss: 0.2789701521396637, Test_Loss: 0.23406317830085754\n",
      "Epoch: 15, Train_Loss: 0.25793638825416565, Test_Loss: 0.23914353549480438\n",
      "Epoch: 15, Train_Loss: 0.23386268317699432, Test_Loss: 0.24337223172187805\n",
      "Epoch: 15, Train_Loss: 0.24678242206573486, Test_Loss: 0.26603418588638306\n",
      "Epoch: 15, Train_Loss: 0.2601122558116913, Test_Loss: 0.24146655201911926 *\n",
      "Epoch: 15, Train_Loss: 0.3215181231498718, Test_Loss: 0.24363312125205994\n",
      "Epoch: 15, Train_Loss: 0.3110845386981964, Test_Loss: 0.348544716835022\n",
      "Epoch: 15, Train_Loss: 0.27420416474342346, Test_Loss: 0.6215620636940002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.27712884545326233, Test_Loss: 0.4823644161224365 *\n",
      "Epoch: 15, Train_Loss: 0.2371363490819931, Test_Loss: 0.26149502396583557 *\n",
      "Epoch: 15, Train_Loss: 0.24919001758098602, Test_Loss: 0.24691611528396606 *\n",
      "Epoch: 15, Train_Loss: 0.23118679225444794, Test_Loss: 0.24613121151924133 *\n",
      "Epoch: 15, Train_Loss: 0.23663291335105896, Test_Loss: 0.2469238042831421\n",
      "Epoch: 15, Train_Loss: 0.24286918342113495, Test_Loss: 0.24924874305725098\n",
      "Epoch: 15, Train_Loss: 0.2512081265449524, Test_Loss: 0.49847009778022766\n",
      "Epoch: 15, Train_Loss: 0.32876071333885193, Test_Loss: 6.094149112701416\n",
      "Epoch: 15, Train_Loss: 0.23075303435325623, Test_Loss: 0.3188563585281372 *\n",
      "Epoch: 15, Train_Loss: 0.2940790057182312, Test_Loss: 0.24683399498462677 *\n",
      "Epoch: 15, Train_Loss: 0.23580609261989594, Test_Loss: 0.2375062108039856 *\n",
      "Epoch: 15, Train_Loss: 0.2602013349533081, Test_Loss: 0.24163571000099182\n",
      "Epoch: 15, Train_Loss: 0.24969933927059174, Test_Loss: 0.23832175135612488 *\n",
      "Epoch: 15, Train_Loss: 0.5239717364311218, Test_Loss: 0.23519298434257507 *\n",
      "Epoch: 15, Train_Loss: 0.26605701446533203, Test_Loss: 0.2322998195886612 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.26206597685813904, Test_Loss: 0.23153631389141083 *\n",
      "Epoch: 15, Train_Loss: 0.23124250769615173, Test_Loss: 0.23102304339408875 *\n",
      "Epoch: 15, Train_Loss: 0.2299332320690155, Test_Loss: 0.2310490906238556\n",
      "Epoch: 15, Train_Loss: 0.23344449698925018, Test_Loss: 0.23718349635601044\n",
      "Epoch: 15, Train_Loss: 0.23572596907615662, Test_Loss: 0.2524474263191223\n",
      "Epoch: 15, Train_Loss: 0.2350088506937027, Test_Loss: 0.26166394352912903\n",
      "Epoch: 15, Train_Loss: 0.23399443924427032, Test_Loss: 0.25197240710258484 *\n",
      "Epoch: 15, Train_Loss: 0.24572253227233887, Test_Loss: 0.232399582862854 *\n",
      "Epoch: 15, Train_Loss: 0.24090376496315002, Test_Loss: 0.23052412271499634 *\n",
      "Epoch: 15, Train_Loss: 0.23852324485778809, Test_Loss: 0.2305326610803604\n",
      "Epoch: 15, Train_Loss: 0.23641134798526764, Test_Loss: 0.2508252263069153\n",
      "Epoch: 15, Train_Loss: 0.23212577402591705, Test_Loss: 0.2298390418291092 *\n",
      "Epoch: 15, Train_Loss: 0.22984176874160767, Test_Loss: 0.24007117748260498\n",
      "Epoch: 15, Train_Loss: 0.24362540245056152, Test_Loss: 0.23274675011634827 *\n",
      "Epoch: 15, Train_Loss: 0.250089555978775, Test_Loss: 0.239594504237175\n",
      "Epoch: 15, Train_Loss: 0.262195348739624, Test_Loss: 0.2392605096101761 *\n",
      "Epoch: 15, Train_Loss: 0.2316620945930481, Test_Loss: 0.23981498181819916\n",
      "Epoch: 15, Train_Loss: 0.24665634334087372, Test_Loss: 0.23090054094791412 *\n",
      "Epoch: 15, Train_Loss: 0.2537083327770233, Test_Loss: 0.234205424785614\n",
      "Epoch: 15, Train_Loss: 0.2561017870903015, Test_Loss: 0.2390095293521881\n",
      "Epoch: 15, Train_Loss: 0.22996293008327484, Test_Loss: 0.2340601235628128 *\n",
      "Epoch: 15, Train_Loss: 0.2682373821735382, Test_Loss: 0.2382386475801468\n",
      "Epoch: 15, Train_Loss: 0.23301738500595093, Test_Loss: 0.31830328702926636\n",
      "Epoch: 15, Train_Loss: 0.24000775814056396, Test_Loss: 2.6518847942352295\n",
      "Epoch: 15, Train_Loss: 0.2296222299337387, Test_Loss: 3.676118850708008\n",
      "Epoch: 15, Train_Loss: 0.25432005524635315, Test_Loss: 0.24418550729751587 *\n",
      "Epoch: 15, Train_Loss: 0.29989975690841675, Test_Loss: 0.22957737743854523 *\n",
      "Epoch: 15, Train_Loss: 3.3978607654571533, Test_Loss: 0.24886149168014526\n",
      "Epoch: 15, Train_Loss: 2.599310874938965, Test_Loss: 0.2719378173351288\n",
      "Epoch: 15, Train_Loss: 0.2528402805328369, Test_Loss: 0.254651814699173 *\n",
      "Epoch: 15, Train_Loss: 0.22971375286579132, Test_Loss: 0.27917733788490295\n",
      "Epoch: 15, Train_Loss: 0.3203561007976532, Test_Loss: 0.34518566727638245\n",
      "Epoch: 15, Train_Loss: 0.3419361710548401, Test_Loss: 0.23347517848014832 *\n",
      "Epoch: 15, Train_Loss: 0.25624921917915344, Test_Loss: 0.248453289270401\n",
      "Epoch: 15, Train_Loss: 0.2305191159248352, Test_Loss: 0.2507017254829407\n",
      "Epoch: 15, Train_Loss: 0.27897417545318604, Test_Loss: 0.2480679303407669 *\n",
      "Epoch: 15, Train_Loss: 0.2691471576690674, Test_Loss: 0.23626671731472015 *\n",
      "Epoch: 15, Train_Loss: 0.2383863925933838, Test_Loss: 0.3092905282974243\n",
      "Epoch: 15, Train_Loss: 0.3375358581542969, Test_Loss: 0.3085024058818817 *\n",
      "Epoch: 15, Train_Loss: 0.8444010019302368, Test_Loss: 0.31402817368507385\n",
      "Epoch: 15, Train_Loss: 1.0936447381973267, Test_Loss: 0.29119113087654114 *\n",
      "Epoch: 15, Train_Loss: 0.31536081433296204, Test_Loss: 0.2535416781902313 *\n",
      "Epoch: 15, Train_Loss: 0.34052667021751404, Test_Loss: 0.25473275780677795\n",
      "Epoch: 15, Train_Loss: 1.4540932178497314, Test_Loss: 0.24421924352645874 *\n",
      "Epoch: 15, Train_Loss: 0.8826462030410767, Test_Loss: 0.26920926570892334\n",
      "Epoch: 15, Train_Loss: 0.2479487657546997, Test_Loss: 0.27431926131248474\n",
      "Epoch: 15, Train_Loss: 0.23490427434444427, Test_Loss: 0.247496098279953 *\n",
      "Epoch: 15, Train_Loss: 0.566758930683136, Test_Loss: 0.2619702219963074\n",
      "Epoch: 15, Train_Loss: 0.7572387456893921, Test_Loss: 0.2883712649345398\n",
      "Epoch: 15, Train_Loss: 0.8181482553482056, Test_Loss: 0.31641319394111633\n",
      "Epoch: 15, Train_Loss: 0.23191240429878235, Test_Loss: 0.3541785776615143\n",
      "Epoch: 15, Train_Loss: 0.25306230783462524, Test_Loss: 0.24695639312267303 *\n",
      "Epoch: 15, Train_Loss: 0.32132792472839355, Test_Loss: 0.2553454041481018\n",
      "Epoch: 15, Train_Loss: 0.6554718017578125, Test_Loss: 0.23236830532550812 *\n",
      "Epoch: 15, Train_Loss: 0.2447035312652588, Test_Loss: 0.24904149770736694\n",
      "Epoch: 15, Train_Loss: 0.2732846438884735, Test_Loss: 0.29289907217025757\n",
      "Epoch: 15, Train_Loss: 0.2600737512111664, Test_Loss: 0.48789840936660767\n",
      "Epoch: 15, Train_Loss: 0.3226342797279358, Test_Loss: 0.5447836518287659\n",
      "Epoch: 15, Train_Loss: 0.38843169808387756, Test_Loss: 0.3739233613014221 *\n",
      "Epoch: 15, Train_Loss: 0.31156906485557556, Test_Loss: 0.25078433752059937 *\n",
      "Epoch: 15, Train_Loss: 0.26184678077697754, Test_Loss: 0.2557232677936554\n",
      "Epoch: 15, Train_Loss: 0.30848264694213867, Test_Loss: 0.2455398142337799 *\n",
      "Epoch: 15, Train_Loss: 0.3227400779724121, Test_Loss: 0.32519495487213135\n",
      "Epoch: 15, Train_Loss: 0.2708258032798767, Test_Loss: 0.2567470073699951 *\n",
      "Epoch: 15, Train_Loss: 0.3228759765625, Test_Loss: 0.5216018557548523\n",
      "Epoch: 15, Train_Loss: 0.4002755284309387, Test_Loss: 0.31944507360458374 *\n",
      "Epoch: 15, Train_Loss: 0.2613343596458435, Test_Loss: 0.30484479665756226 *\n",
      "Epoch: 15, Train_Loss: 0.3127526044845581, Test_Loss: 0.28494253754615784 *\n",
      "Epoch: 15, Train_Loss: 0.3215402364730835, Test_Loss: 0.2867997884750366\n",
      "Epoch: 15, Train_Loss: 0.2804747223854065, Test_Loss: 0.3258828818798065\n",
      "Epoch: 15, Train_Loss: 0.24095623195171356, Test_Loss: 0.2796406149864197 *\n",
      "Epoch: 15, Train_Loss: 0.23117247223854065, Test_Loss: 0.2597254812717438 *\n",
      "Epoch: 15, Train_Loss: 0.23623962700366974, Test_Loss: 0.25741201639175415 *\n",
      "Epoch: 15, Train_Loss: 0.23191489279270172, Test_Loss: 0.34709087014198303\n",
      "Epoch: 15, Train_Loss: 0.2353755086660385, Test_Loss: 0.5929791927337646\n",
      "Epoch: 15, Train_Loss: 0.26694348454475403, Test_Loss: 0.4416466951370239 *\n",
      "Epoch: 15, Train_Loss: 0.2591215968132019, Test_Loss: 0.6785790920257568\n",
      "Epoch: 15, Train_Loss: 0.2764814496040344, Test_Loss: 0.27422475814819336 *\n",
      "Epoch: 15, Train_Loss: 0.4670954942703247, Test_Loss: 0.2657483220100403 *\n",
      "Epoch: 15, Train_Loss: 0.507112979888916, Test_Loss: 0.2622204124927521 *\n",
      "Epoch: 15, Train_Loss: 0.24481773376464844, Test_Loss: 0.266538143157959\n",
      "Epoch: 15, Train_Loss: 0.27101221680641174, Test_Loss: 0.3665163516998291\n",
      "Epoch: 15, Train_Loss: 0.3584778308868408, Test_Loss: 1.2809878587722778\n",
      "Epoch: 15, Train_Loss: 0.2625311613082886, Test_Loss: 5.342060565948486\n",
      "Epoch: 15, Train_Loss: 0.45903876423835754, Test_Loss: 0.30701977014541626 *\n",
      "Epoch: 15, Train_Loss: 0.36409175395965576, Test_Loss: 0.29871904850006104 *\n",
      "Epoch: 15, Train_Loss: 0.4772151708602905, Test_Loss: 0.28624966740608215 *\n",
      "Epoch: 15, Train_Loss: 0.32652831077575684, Test_Loss: 0.2377006560564041 *\n",
      "Epoch: 15, Train_Loss: 0.35634660720825195, Test_Loss: 0.27061498165130615\n",
      "Epoch: 15, Train_Loss: 0.25942784547805786, Test_Loss: 0.31780534982681274\n",
      "Epoch: 15, Train_Loss: 0.2537025213241577, Test_Loss: 0.3006722331047058 *\n",
      "Epoch: 15, Train_Loss: 0.39252743124961853, Test_Loss: 0.23296770453453064 *\n",
      "Epoch: 15, Train_Loss: 0.8181748390197754, Test_Loss: 0.24690072238445282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train_Loss: 0.601511538028717, Test_Loss: 0.26055166125297546\n",
      "Epoch: 15, Train_Loss: 0.2823641896247864, Test_Loss: 0.3818390369415283\n",
      "Epoch: 15, Train_Loss: 0.2568862736225128, Test_Loss: 0.26069435477256775 *\n",
      "Epoch: 15, Train_Loss: 0.23313359916210175, Test_Loss: 0.2584454417228699 *\n",
      "Epoch: 15, Train_Loss: 0.49043965339660645, Test_Loss: 0.2609652280807495\n",
      "Epoch: 15, Train_Loss: 0.46046388149261475, Test_Loss: 0.2423601597547531 *\n",
      "Epoch: 15, Train_Loss: 0.23299811780452728, Test_Loss: 0.24432729184627533\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 15\n",
      "Epoch: 15, Train_Loss: 0.4089930057525635, Test_Loss: 0.3055560290813446\n",
      "Epoch: 15, Train_Loss: 0.2466837614774704, Test_Loss: 0.328487753868103\n",
      "Epoch: 15, Train_Loss: 0.252110093832016, Test_Loss: 0.23504355549812317 *\n",
      "Epoch: 15, Train_Loss: 0.2696205675601959, Test_Loss: 0.2500050365924835\n",
      "Epoch: 15, Train_Loss: 0.33749714493751526, Test_Loss: 0.2272801399230957 *\n",
      "Epoch: 15, Train_Loss: 0.29016798734664917, Test_Loss: 0.3199896514415741\n",
      "Epoch: 15, Train_Loss: 0.32092374563217163, Test_Loss: 0.3131846487522125 *\n",
      "Epoch: 15, Train_Loss: 0.2353517711162567, Test_Loss: 0.27581021189689636 *\n",
      "Epoch: 15, Train_Loss: 0.3249558210372925, Test_Loss: 0.23685003817081451 *\n",
      "Epoch: 15, Train_Loss: 0.26309630274772644, Test_Loss: 0.25708943605422974\n",
      "Epoch: 15, Train_Loss: 0.25937724113464355, Test_Loss: 0.26047593355178833\n",
      "Epoch: 15, Train_Loss: 0.2379285991191864, Test_Loss: 0.24303732812404633 *\n",
      "Epoch: 15, Train_Loss: 0.24980689585208893, Test_Loss: 0.2895109951496124\n",
      "Epoch: 15, Train_Loss: 0.3426057696342468, Test_Loss: 0.3633766770362854\n",
      "Epoch: 15, Train_Loss: 0.45915281772613525, Test_Loss: 3.420489549636841\n",
      "Epoch: 15, Train_Loss: 0.4244506061077118, Test_Loss: 2.0781729221343994 *\n",
      "Epoch: 15, Train_Loss: 0.6013003587722778, Test_Loss: 0.2731410264968872 *\n",
      "Epoch: 15, Train_Loss: 0.5025299787521362, Test_Loss: 0.2683663070201874 *\n",
      "Epoch: 15, Train_Loss: 0.4135679602622986, Test_Loss: 0.259398490190506 *\n",
      "Epoch: 15, Train_Loss: 0.2923182547092438, Test_Loss: 0.2767477035522461\n",
      "Epoch: 15, Train_Loss: 0.24327394366264343, Test_Loss: 0.24889612197875977 *\n",
      "Epoch: 15, Train_Loss: 0.23423880338668823, Test_Loss: 0.29582270979881287\n",
      "Epoch: 15, Train_Loss: 0.2403055876493454, Test_Loss: 0.2746342420578003 *\n",
      "Epoch: 15, Train_Loss: 0.3620380163192749, Test_Loss: 0.22895151376724243 *\n",
      "Epoch: 15, Train_Loss: 0.49860328435897827, Test_Loss: 0.2822839617729187\n",
      "Epoch: 15, Train_Loss: 0.5840232968330383, Test_Loss: 0.30716651678085327\n",
      "Epoch: 15, Train_Loss: 1.2013698816299438, Test_Loss: 0.35415154695510864\n",
      "Epoch: 15, Train_Loss: 1.1208940744400024, Test_Loss: 0.27162498235702515 *\n",
      "Epoch: 15, Train_Loss: 0.3425406515598297, Test_Loss: 0.31658193469047546\n",
      "Epoch: 15, Train_Loss: 0.3711013197898865, Test_Loss: 0.338302344083786\n",
      "Epoch: 15, Train_Loss: 0.236744225025177, Test_Loss: 0.31316664814949036 *\n",
      "Epoch: 15, Train_Loss: 0.35468292236328125, Test_Loss: 0.25311967730522156 *\n",
      "Epoch: 15, Train_Loss: 0.5347350835800171, Test_Loss: 0.3735346496105194\n",
      "Epoch: 15, Train_Loss: 1.0299935340881348, Test_Loss: 0.28826531767845154 *\n",
      "Epoch: 15, Train_Loss: 0.2571762204170227, Test_Loss: 0.3443944454193115\n",
      "Epoch: 15, Train_Loss: 0.2595873475074768, Test_Loss: 0.3454517126083374\n",
      "Epoch: 15, Train_Loss: 0.292435884475708, Test_Loss: 0.3402714133262634 *\n",
      "Epoch: 15, Train_Loss: 0.5015981197357178, Test_Loss: 0.29250335693359375 *\n",
      "Epoch: 15, Train_Loss: 0.36550551652908325, Test_Loss: 0.31248366832733154\n",
      "Epoch: 15, Train_Loss: 0.43784192204475403, Test_Loss: 0.3923339247703552\n",
      "Epoch: 15, Train_Loss: 0.3763698637485504, Test_Loss: 0.3777853846549988 *\n",
      "Epoch: 15, Train_Loss: 0.3988582193851471, Test_Loss: 0.3770216703414917 *\n",
      "Epoch: 15, Train_Loss: 0.23131147027015686, Test_Loss: 0.28585830330848694 *\n",
      "Epoch: 15, Train_Loss: 0.2496534138917923, Test_Loss: 0.30011671781539917\n",
      "Epoch: 15, Train_Loss: 0.23211541771888733, Test_Loss: 0.25141674280166626 *\n",
      "Epoch: 15, Train_Loss: 0.28101253509521484, Test_Loss: 0.2528735101222992\n",
      "Epoch: 15, Train_Loss: 0.24598820507526398, Test_Loss: 0.30917805433273315\n",
      "Epoch: 15, Train_Loss: 0.29353225231170654, Test_Loss: 0.4049449861049652\n",
      "Epoch: 15, Train_Loss: 15.81615924835205, Test_Loss: 0.47059792280197144\n",
      "Epoch: 15, Train_Loss: 0.31278908252716064, Test_Loss: 0.2974080443382263 *\n",
      "Epoch: 15, Train_Loss: 1.344384789466858, Test_Loss: 0.23295649886131287 *\n",
      "Epoch: 15, Train_Loss: 1.083072543144226, Test_Loss: 0.2456967830657959\n",
      "Epoch: 15, Train_Loss: 0.25397759675979614, Test_Loss: 0.2908409535884857\n",
      "Epoch: 15, Train_Loss: 0.38141006231307983, Test_Loss: 0.4506272077560425\n",
      "Epoch: 16, Train_Loss: 3.625748634338379, Test_Loss: 0.2991366982460022 *\n",
      "Epoch: 16, Train_Loss: 4.856492042541504, Test_Loss: 0.6129162311553955\n",
      "Epoch: 16, Train_Loss: 0.31353211402893066, Test_Loss: 0.30772989988327026 *\n",
      "Epoch: 16, Train_Loss: 0.4134121537208557, Test_Loss: 0.3827044367790222\n",
      "Epoch: 16, Train_Loss: 5.27767276763916, Test_Loss: 0.5095429420471191\n",
      "Epoch: 16, Train_Loss: 0.4174245595932007, Test_Loss: 0.5857545733451843\n",
      "Epoch: 16, Train_Loss: 0.29589131474494934, Test_Loss: 0.7705568671226501\n",
      "Epoch: 16, Train_Loss: 0.26988863945007324, Test_Loss: 0.2829093337059021 *\n",
      "Epoch: 16, Train_Loss: 0.27273204922676086, Test_Loss: 0.48200327157974243\n",
      "Epoch: 16, Train_Loss: 0.42012131214141846, Test_Loss: 0.25639766454696655 *\n",
      "Epoch: 16, Train_Loss: 0.22584763169288635, Test_Loss: 0.5985909700393677\n",
      "Epoch: 16, Train_Loss: 0.25920748710632324, Test_Loss: 1.6312792301177979\n",
      "Epoch: 16, Train_Loss: 0.22471393644809723, Test_Loss: 0.7746241688728333 *\n",
      "Epoch: 16, Train_Loss: 0.2340286821126938, Test_Loss: 0.40267354249954224 *\n",
      "Epoch: 16, Train_Loss: 0.3104903995990753, Test_Loss: 0.22865457832813263 *\n",
      "Epoch: 16, Train_Loss: 0.23870056867599487, Test_Loss: 0.22560818493366241 *\n",
      "Epoch: 16, Train_Loss: 0.2774990499019623, Test_Loss: 0.22586779296398163\n",
      "Epoch: 16, Train_Loss: 0.2595210671424866, Test_Loss: 0.2259092777967453\n",
      "Epoch: 16, Train_Loss: 0.2521989941596985, Test_Loss: 0.23284149169921875\n",
      "Epoch: 16, Train_Loss: 0.23325201869010925, Test_Loss: 3.2822353839874268\n",
      "Epoch: 16, Train_Loss: 0.23239998519420624, Test_Loss: 4.313003063201904\n",
      "Epoch: 16, Train_Loss: 0.2262103110551834, Test_Loss: 0.3264259994029999 *\n",
      "Epoch: 16, Train_Loss: 0.225352942943573, Test_Loss: 0.3218814432621002 *\n",
      "Epoch: 16, Train_Loss: 0.22587254643440247, Test_Loss: 0.3713025450706482\n",
      "Epoch: 16, Train_Loss: 0.22405430674552917, Test_Loss: 0.23937305808067322 *\n",
      "Epoch: 16, Train_Loss: 0.22428984940052032, Test_Loss: 0.3848286271095276\n",
      "Epoch: 16, Train_Loss: 0.22591042518615723, Test_Loss: 0.39466267824172974\n",
      "Epoch: 16, Train_Loss: 0.22395750880241394, Test_Loss: 0.42782193422317505\n",
      "Epoch: 16, Train_Loss: 0.22405488789081573, Test_Loss: 0.41467341780662537 *\n",
      "Epoch: 16, Train_Loss: 0.2253083884716034, Test_Loss: 0.5512640476226807\n",
      "Epoch: 16, Train_Loss: 0.22884634137153625, Test_Loss: 0.40474560856819153 *\n",
      "Epoch: 16, Train_Loss: 0.2347932606935501, Test_Loss: 0.6007790565490723\n",
      "Epoch: 16, Train_Loss: 0.24815426766872406, Test_Loss: 0.3661624789237976 *\n",
      "Epoch: 16, Train_Loss: 0.261944979429245, Test_Loss: 0.39053720235824585\n",
      "Epoch: 16, Train_Loss: 0.2909545302391052, Test_Loss: 0.3492577075958252 *\n",
      "Epoch: 16, Train_Loss: 2.6164700984954834, Test_Loss: 0.23226672410964966 *\n",
      "Epoch: 16, Train_Loss: 5.1289238929748535, Test_Loss: 0.25805550813674927\n",
      "Epoch: 16, Train_Loss: 0.2848854959011078, Test_Loss: 0.23697739839553833 *\n",
      "Epoch: 16, Train_Loss: 0.3623904883861542, Test_Loss: 0.4115344285964966\n",
      "Epoch: 16, Train_Loss: 0.358026385307312, Test_Loss: 0.40725716948509216 *\n",
      "Epoch: 16, Train_Loss: 0.3261075019836426, Test_Loss: 0.6811891794204712\n",
      "Epoch: 16, Train_Loss: 0.3107004165649414, Test_Loss: 0.2903035283088684 *\n",
      "Epoch: 16, Train_Loss: 0.3542111814022064, Test_Loss: 0.4323347508907318\n",
      "Epoch: 16, Train_Loss: 0.31352120637893677, Test_Loss: 0.3090812861919403 *\n",
      "Epoch: 16, Train_Loss: 0.40530240535736084, Test_Loss: 0.2798803448677063 *\n",
      "Epoch: 16, Train_Loss: 0.2947622835636139, Test_Loss: 0.24831990897655487 *\n",
      "Epoch: 16, Train_Loss: 0.25703439116477966, Test_Loss: 0.29808443784713745\n",
      "Epoch: 16, Train_Loss: 0.2468162178993225, Test_Loss: 0.26073774695396423 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.2617332935333252, Test_Loss: 0.2310306280851364 *\n",
      "Epoch: 16, Train_Loss: 0.3870342969894409, Test_Loss: 0.38127589225769043\n",
      "Epoch: 16, Train_Loss: 0.38469362258911133, Test_Loss: 0.2794422209262848 *\n",
      "Epoch: 16, Train_Loss: 0.24791142344474792, Test_Loss: 5.369356155395508\n",
      "Epoch: 16, Train_Loss: 0.286810964345932, Test_Loss: 1.414206624031067 *\n",
      "Epoch: 16, Train_Loss: 0.22614604234695435, Test_Loss: 0.22960060834884644 *\n",
      "Epoch: 16, Train_Loss: 0.25305718183517456, Test_Loss: 0.24572162330150604\n",
      "Epoch: 16, Train_Loss: 0.2781922221183777, Test_Loss: 0.33150196075439453\n",
      "Epoch: 16, Train_Loss: 0.25659894943237305, Test_Loss: 0.30493906140327454 *\n",
      "Epoch: 16, Train_Loss: 0.22602224349975586, Test_Loss: 0.2359503209590912 *\n",
      "Epoch: 16, Train_Loss: 0.2232898324728012, Test_Loss: 0.30403459072113037\n",
      "Epoch: 16, Train_Loss: 0.2307024896144867, Test_Loss: 0.2983207404613495 *\n",
      "Epoch: 16, Train_Loss: 3.0106022357940674, Test_Loss: 0.225164532661438 *\n",
      "Epoch: 16, Train_Loss: 3.336869239807129, Test_Loss: 0.25206685066223145\n",
      "Epoch: 16, Train_Loss: 0.2242950052022934, Test_Loss: 0.24018843472003937 *\n",
      "Epoch: 16, Train_Loss: 0.24029503762722015, Test_Loss: 0.2347443848848343 *\n",
      "Epoch: 16, Train_Loss: 0.24218067526817322, Test_Loss: 0.2534269690513611\n",
      "Epoch: 16, Train_Loss: 0.23195035755634308, Test_Loss: 0.4709319472312927\n",
      "Epoch: 16, Train_Loss: 0.23762933909893036, Test_Loss: 0.347276896238327 *\n",
      "Epoch: 16, Train_Loss: 0.2285531610250473, Test_Loss: 0.3314259946346283 *\n",
      "Epoch: 16, Train_Loss: 0.25293344259262085, Test_Loss: 0.26367950439453125 *\n",
      "Epoch: 16, Train_Loss: 0.2404072880744934, Test_Loss: 0.25050443410873413 *\n",
      "Epoch: 16, Train_Loss: 0.27350935339927673, Test_Loss: 0.24552714824676514 *\n",
      "Epoch: 16, Train_Loss: 0.22888466715812683, Test_Loss: 0.42985230684280396\n",
      "Epoch: 16, Train_Loss: 0.2258816808462143, Test_Loss: 0.34812766313552856 *\n",
      "Epoch: 16, Train_Loss: 0.23267117142677307, Test_Loss: 0.3968845009803772\n",
      "Epoch: 16, Train_Loss: 0.2357294112443924, Test_Loss: 0.27719634771347046 *\n",
      "Epoch: 16, Train_Loss: 0.22387191653251648, Test_Loss: 0.3040721118450165\n",
      "Epoch: 16, Train_Loss: 0.22575066983699799, Test_Loss: 0.3784582018852234\n",
      "Epoch: 16, Train_Loss: 0.26065507531166077, Test_Loss: 0.296528160572052 *\n",
      "Epoch: 16, Train_Loss: 0.2648869454860687, Test_Loss: 0.2892898619174957 *\n",
      "Epoch: 16, Train_Loss: 0.22353334724903107, Test_Loss: 0.23097087442874908 *\n",
      "Epoch: 16, Train_Loss: 0.22463230788707733, Test_Loss: 0.23360219597816467\n",
      "Epoch: 16, Train_Loss: 0.25342875719070435, Test_Loss: 0.24809539318084717\n",
      "Epoch: 16, Train_Loss: 0.29412713646888733, Test_Loss: 0.2502657473087311\n",
      "Epoch: 16, Train_Loss: 0.2540615499019623, Test_Loss: 0.3961155116558075\n",
      "Epoch: 16, Train_Loss: 0.2547341287136078, Test_Loss: 0.47156602144241333\n",
      "Epoch: 16, Train_Loss: 0.26098090410232544, Test_Loss: 0.4604733884334564 *\n",
      "Epoch: 16, Train_Loss: 0.2745123505592346, Test_Loss: 0.2915610074996948 *\n",
      "Epoch: 16, Train_Loss: 0.2541004419326782, Test_Loss: 0.22972626984119415 *\n",
      "Epoch: 16, Train_Loss: 0.2855214476585388, Test_Loss: 0.23737192153930664\n",
      "Epoch: 16, Train_Loss: 0.2813647389411926, Test_Loss: 0.3291890025138855\n",
      "Epoch: 16, Train_Loss: 0.31272655725479126, Test_Loss: 0.6539076566696167\n",
      "Epoch: 16, Train_Loss: 0.24332879483699799, Test_Loss: 0.5740087032318115 *\n",
      "Epoch: 16, Train_Loss: 0.22349952161312103, Test_Loss: 0.5335803627967834 *\n",
      "Epoch: 16, Train_Loss: 0.2223416119813919, Test_Loss: 0.2919972538948059 *\n",
      "Epoch: 16, Train_Loss: 0.22160735726356506, Test_Loss: 0.22973284125328064 *\n",
      "Epoch: 16, Train_Loss: 0.22176577150821686, Test_Loss: 0.2358342856168747\n",
      "Epoch: 16, Train_Loss: 0.22331643104553223, Test_Loss: 0.22747117280960083 *\n",
      "Epoch: 16, Train_Loss: 2.9084959030151367, Test_Loss: 0.2363535314798355\n",
      "Epoch: 16, Train_Loss: 2.3723011016845703, Test_Loss: 0.24428050220012665\n",
      "Epoch: 16, Train_Loss: 0.22295673191547394, Test_Loss: 0.25953584909439087\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.2333858162164688, Test_Loss: 0.22618120908737183 *\n",
      "Epoch: 16, Train_Loss: 0.22888748347759247, Test_Loss: 0.3340771496295929\n",
      "Epoch: 16, Train_Loss: 0.2220306545495987, Test_Loss: 0.5516836643218994\n",
      "Epoch: 16, Train_Loss: 0.22203849256038666, Test_Loss: 0.3518073558807373 *\n",
      "Epoch: 16, Train_Loss: 0.2233094424009323, Test_Loss: 0.44271665811538696\n",
      "Epoch: 16, Train_Loss: 0.2213980257511139, Test_Loss: 0.23739513754844666 *\n",
      "Epoch: 16, Train_Loss: 0.22248336672782898, Test_Loss: 0.23793324828147888\n",
      "Epoch: 16, Train_Loss: 0.2342546433210373, Test_Loss: 0.2378547340631485 *\n",
      "Epoch: 16, Train_Loss: 0.2860568165779114, Test_Loss: 0.23883351683616638\n",
      "Epoch: 16, Train_Loss: 0.2802683711051941, Test_Loss: 0.2582526206970215\n",
      "Epoch: 16, Train_Loss: 0.2988238036632538, Test_Loss: 3.9136903285980225\n",
      "Epoch: 16, Train_Loss: 0.24643127620220184, Test_Loss: 2.0182363986968994 *\n",
      "Epoch: 16, Train_Loss: 0.23212173581123352, Test_Loss: 0.2336839735507965 *\n",
      "Epoch: 16, Train_Loss: 0.45081645250320435, Test_Loss: 0.22868134081363678 *\n",
      "Epoch: 16, Train_Loss: 0.4445880651473999, Test_Loss: 0.2283007949590683 *\n",
      "Epoch: 16, Train_Loss: 0.4297402799129486, Test_Loss: 0.23897859454154968\n",
      "Epoch: 16, Train_Loss: 0.30352672934532166, Test_Loss: 0.22536875307559967 *\n",
      "Epoch: 16, Train_Loss: 0.2218400537967682, Test_Loss: 0.22767378389835358\n",
      "Epoch: 16, Train_Loss: 0.2211533635854721, Test_Loss: 0.22224143147468567 *\n",
      "Epoch: 16, Train_Loss: 0.2261405736207962, Test_Loss: 0.22336649894714355\n",
      "Epoch: 16, Train_Loss: 0.22988522052764893, Test_Loss: 0.22389638423919678\n",
      "Epoch: 16, Train_Loss: 0.23739294707775116, Test_Loss: 0.22394631803035736\n",
      "Epoch: 16, Train_Loss: 0.2334943562746048, Test_Loss: 0.23595699667930603\n",
      "Epoch: 16, Train_Loss: 0.22066862881183624, Test_Loss: 0.2488522231578827\n",
      "Epoch: 16, Train_Loss: 0.22115258872509003, Test_Loss: 0.2466599941253662 *\n",
      "Epoch: 16, Train_Loss: 0.23393836617469788, Test_Loss: 0.2225753217935562 *\n",
      "Epoch: 16, Train_Loss: 0.29818201065063477, Test_Loss: 0.2218376249074936 *\n",
      "Epoch: 16, Train_Loss: 0.4144643545150757, Test_Loss: 0.22201761603355408\n",
      "Epoch: 16, Train_Loss: 0.3676379323005676, Test_Loss: 0.2348303198814392\n",
      "Epoch: 16, Train_Loss: 0.3230360448360443, Test_Loss: 0.22668105363845825 *\n",
      "Epoch: 16, Train_Loss: 0.3252144753932953, Test_Loss: 0.22271424531936646 *\n",
      "Epoch: 16, Train_Loss: 0.3617735803127289, Test_Loss: 0.22709757089614868\n",
      "Epoch: 16, Train_Loss: 0.24486806988716125, Test_Loss: 0.239609494805336\n",
      "Epoch: 16, Train_Loss: 0.3473556339740753, Test_Loss: 0.24828024208545685\n",
      "Epoch: 16, Train_Loss: 0.2970595359802246, Test_Loss: 0.2509520947933197\n",
      "Epoch: 16, Train_Loss: 0.4833828806877136, Test_Loss: 0.22450563311576843 *\n",
      "Epoch: 16, Train_Loss: 0.22988660633563995, Test_Loss: 0.22244390845298767 *\n",
      "Epoch: 16, Train_Loss: 0.47114941477775574, Test_Loss: 0.2236982136964798\n",
      "Epoch: 16, Train_Loss: 3.031947612762451, Test_Loss: 0.22353805601596832 *\n",
      "Epoch: 16, Train_Loss: 0.29720765352249146, Test_Loss: 0.22207656502723694 *\n",
      "Epoch: 16, Train_Loss: 0.2753399908542633, Test_Loss: 0.2955007553100586\n",
      "Epoch: 16, Train_Loss: 0.26308074593544006, Test_Loss: 0.25839805603027344 *\n",
      "Epoch: 16, Train_Loss: 0.24238041043281555, Test_Loss: 5.916921615600586\n",
      "Epoch: 16, Train_Loss: 0.22387224435806274, Test_Loss: 0.35952553153038025 *\n",
      "Epoch: 16, Train_Loss: 0.22867146134376526, Test_Loss: 0.22445574402809143 *\n",
      "Epoch: 16, Train_Loss: 0.3010963499546051, Test_Loss: 0.2462664395570755\n",
      "Epoch: 16, Train_Loss: 0.32325243949890137, Test_Loss: 0.24575591087341309 *\n",
      "Epoch: 16, Train_Loss: 0.2921867370605469, Test_Loss: 0.24270863831043243 *\n",
      "Epoch: 16, Train_Loss: 0.26709407567977905, Test_Loss: 0.22605367004871368 *\n",
      "Epoch: 16, Train_Loss: 0.2590835690498352, Test_Loss: 0.30350133776664734\n",
      "Epoch: 16, Train_Loss: 0.22809608280658722, Test_Loss: 0.2718071937561035 *\n",
      "Epoch: 16, Train_Loss: 0.24308688938617706, Test_Loss: 0.22243686020374298 *\n",
      "Epoch: 16, Train_Loss: 0.23791153728961945, Test_Loss: 0.2402936816215515\n",
      "Epoch: 16, Train_Loss: 0.26971665024757385, Test_Loss: 0.24760721623897552\n",
      "Epoch: 16, Train_Loss: 0.24306072294712067, Test_Loss: 0.22359372675418854 *\n",
      "Epoch: 16, Train_Loss: 0.2228223979473114, Test_Loss: 0.25581032037734985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.23761321604251862, Test_Loss: 0.2695567309856415\n",
      "Epoch: 16, Train_Loss: 0.24562881886959076, Test_Loss: 0.26463747024536133 *\n",
      "Epoch: 16, Train_Loss: 0.23845671117305756, Test_Loss: 0.2872202396392822\n",
      "Epoch: 16, Train_Loss: 0.22341479361057281, Test_Loss: 0.25282177329063416 *\n",
      "Epoch: 16, Train_Loss: 0.2215028554201126, Test_Loss: 0.2716311812400818\n",
      "Epoch: 16, Train_Loss: 0.2203761488199234, Test_Loss: 0.22912956774234772 *\n",
      "Epoch: 16, Train_Loss: 0.2205500602722168, Test_Loss: 0.22903312742710114 *\n",
      "Epoch: 16, Train_Loss: 0.22117602825164795, Test_Loss: 0.24211186170578003\n",
      "Epoch: 16, Train_Loss: 0.2233978509902954, Test_Loss: 0.2431086152791977\n",
      "Epoch: 16, Train_Loss: 0.223646879196167, Test_Loss: 0.23465068638324738 *\n",
      "Epoch: 16, Train_Loss: 0.22070930898189545, Test_Loss: 0.2306337058544159 *\n",
      "Epoch: 16, Train_Loss: 0.21951475739479065, Test_Loss: 0.22728899121284485 *\n",
      "Epoch: 16, Train_Loss: 0.22045312821865082, Test_Loss: 0.22916804254055023\n",
      "Epoch: 16, Train_Loss: 0.22549253702163696, Test_Loss: 0.2303813397884369\n",
      "Epoch: 16, Train_Loss: 0.2369171380996704, Test_Loss: 0.2265351414680481 *\n",
      "Epoch: 16, Train_Loss: 0.23809954524040222, Test_Loss: 0.2327013462781906\n",
      "Epoch: 16, Train_Loss: 0.24577562510967255, Test_Loss: 0.2759050130844116\n",
      "Epoch: 16, Train_Loss: 0.22603820264339447, Test_Loss: 0.24838069081306458 *\n",
      "Epoch: 16, Train_Loss: 0.22844435274600983, Test_Loss: 0.4905892610549927\n",
      "Epoch: 16, Train_Loss: 0.21950243413448334, Test_Loss: 0.7463246583938599\n",
      "Epoch: 16, Train_Loss: 0.21962609887123108, Test_Loss: 0.4824022054672241 *\n",
      "Epoch: 16, Train_Loss: 0.2358088195323944, Test_Loss: 0.3022403120994568 *\n",
      "Epoch: 16, Train_Loss: 0.2347518652677536, Test_Loss: 0.2556430399417877 *\n",
      "Epoch: 16, Train_Loss: 0.22064538300037384, Test_Loss: 0.22603082656860352 *\n",
      "Epoch: 16, Train_Loss: 0.22126995027065277, Test_Loss: 0.30414703488349915\n",
      "Epoch: 16, Train_Loss: 0.22293494641780853, Test_Loss: 0.5537849068641663\n",
      "Epoch: 16, Train_Loss: 0.2640010714530945, Test_Loss: 0.44948136806488037 *\n",
      "Epoch: 16, Train_Loss: 0.2553757131099701, Test_Loss: 0.3541181683540344 *\n",
      "Epoch: 16, Train_Loss: 0.2727150022983551, Test_Loss: 0.32437536120414734 *\n",
      "Epoch: 16, Train_Loss: 0.22433364391326904, Test_Loss: 0.22269627451896667 *\n",
      "Epoch: 16, Train_Loss: 0.23597542941570282, Test_Loss: 0.22390110790729523\n",
      "Epoch: 16, Train_Loss: 0.27201902866363525, Test_Loss: 0.22382591664791107 *\n",
      "Epoch: 16, Train_Loss: 0.222517192363739, Test_Loss: 0.23328562080860138\n",
      "Epoch: 16, Train_Loss: 0.2270859032869339, Test_Loss: 0.23438230156898499\n",
      "Epoch: 16, Train_Loss: 0.23541319370269775, Test_Loss: 0.24617356061935425\n",
      "Epoch: 16, Train_Loss: 0.2822563648223877, Test_Loss: 0.22332102060317993 *\n",
      "Epoch: 16, Train_Loss: 0.3035067319869995, Test_Loss: 0.3530530333518982\n",
      "Epoch: 16, Train_Loss: 0.2602739930152893, Test_Loss: 0.6111961603164673\n",
      "Epoch: 16, Train_Loss: 0.25119447708129883, Test_Loss: 0.3251227140426636 *\n",
      "Epoch: 16, Train_Loss: 0.22482962906360626, Test_Loss: 0.4185965061187744\n",
      "Epoch: 16, Train_Loss: 0.24228060245513916, Test_Loss: 0.23658016324043274 *\n",
      "Epoch: 16, Train_Loss: 0.21978861093521118, Test_Loss: 0.23697492480278015\n",
      "Epoch: 16, Train_Loss: 0.2235228717327118, Test_Loss: 0.23684516549110413 *\n",
      "Epoch: 16, Train_Loss: 0.2269216775894165, Test_Loss: 0.23846453428268433\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.24233567714691162, Test_Loss: 0.25469911098480225\n",
      "Epoch: 16, Train_Loss: 0.29099664092063904, Test_Loss: 5.254445552825928\n",
      "Epoch: 16, Train_Loss: 0.2637312412261963, Test_Loss: 0.8625319004058838 *\n",
      "Epoch: 16, Train_Loss: 0.26352256536483765, Test_Loss: 0.2292415201663971 *\n",
      "Epoch: 16, Train_Loss: 0.23516888916492462, Test_Loss: 0.22348125278949738 *\n",
      "Epoch: 16, Train_Loss: 0.24621395766735077, Test_Loss: 0.22394472360610962\n",
      "Epoch: 16, Train_Loss: 0.23275631666183472, Test_Loss: 0.2302093803882599\n",
      "Epoch: 16, Train_Loss: 0.42247384786605835, Test_Loss: 0.22383710741996765 *\n",
      "Epoch: 16, Train_Loss: 0.35373079776763916, Test_Loss: 0.22144220769405365 *\n",
      "Epoch: 16, Train_Loss: 0.22474443912506104, Test_Loss: 0.21915699541568756 *\n",
      "Epoch: 16, Train_Loss: 0.2493274211883545, Test_Loss: 0.2200397104024887\n",
      "Epoch: 16, Train_Loss: 0.2185872495174408, Test_Loss: 0.22048552334308624\n",
      "Epoch: 16, Train_Loss: 0.22176295518875122, Test_Loss: 0.2312072068452835\n",
      "Epoch: 16, Train_Loss: 0.2290203720331192, Test_Loss: 0.22628098726272583 *\n",
      "Epoch: 16, Train_Loss: 0.22332893311977386, Test_Loss: 0.2500612735748291\n",
      "Epoch: 16, Train_Loss: 0.2199680060148239, Test_Loss: 0.2453426867723465 *\n",
      "Epoch: 16, Train_Loss: 0.23797018826007843, Test_Loss: 0.2195737212896347 *\n",
      "Epoch: 16, Train_Loss: 0.222626194357872, Test_Loss: 0.21845224499702454 *\n",
      "Epoch: 16, Train_Loss: 0.22703756392002106, Test_Loss: 0.21861325204372406\n",
      "Epoch: 16, Train_Loss: 0.22445182502269745, Test_Loss: 0.22291199862957\n",
      "Epoch: 16, Train_Loss: 0.21942377090454102, Test_Loss: 0.21915878355503082 *\n",
      "Epoch: 16, Train_Loss: 0.21978402137756348, Test_Loss: 0.22203010320663452\n",
      "Epoch: 16, Train_Loss: 0.21946114301681519, Test_Loss: 0.2204604148864746 *\n",
      "Epoch: 16, Train_Loss: 0.24508999288082123, Test_Loss: 0.22495655715465546\n",
      "Epoch: 16, Train_Loss: 0.24214480817317963, Test_Loss: 0.22161678969860077 *\n",
      "Epoch: 16, Train_Loss: 0.22906464338302612, Test_Loss: 0.2244698405265808\n",
      "Epoch: 16, Train_Loss: 0.22956785559654236, Test_Loss: 0.21949078142642975 *\n",
      "Epoch: 16, Train_Loss: 0.24557504057884216, Test_Loss: 0.2195379137992859\n",
      "Epoch: 16, Train_Loss: 0.23727378249168396, Test_Loss: 0.21936973929405212 *\n",
      "Epoch: 16, Train_Loss: 0.22431312501430511, Test_Loss: 0.21932537853717804 *\n",
      "Epoch: 16, Train_Loss: 0.24208031594753265, Test_Loss: 0.22115279734134674\n",
      "Epoch: 16, Train_Loss: 0.2344420999288559, Test_Loss: 0.2854229211807251\n",
      "Epoch: 16, Train_Loss: 0.22591744363307953, Test_Loss: 0.7067596912384033\n",
      "Epoch: 16, Train_Loss: 0.22069957852363586, Test_Loss: 5.480801582336426\n",
      "Epoch: 16, Train_Loss: 0.2441801130771637, Test_Loss: 0.23332016170024872 *\n",
      "Epoch: 16, Train_Loss: 0.27682602405548096, Test_Loss: 0.21873323619365692 *\n",
      "Epoch: 16, Train_Loss: 2.530233144760132, Test_Loss: 0.2468406856060028\n",
      "Epoch: 16, Train_Loss: 2.999739646911621, Test_Loss: 0.23001718521118164 *\n",
      "Epoch: 16, Train_Loss: 0.2419876754283905, Test_Loss: 0.2392314225435257\n",
      "Epoch: 16, Train_Loss: 0.22029785811901093, Test_Loss: 0.22560477256774902 *\n",
      "Epoch: 16, Train_Loss: 0.27225440740585327, Test_Loss: 0.32993191480636597\n",
      "Epoch: 16, Train_Loss: 0.34558773040771484, Test_Loss: 0.2517393231391907 *\n",
      "Epoch: 16, Train_Loss: 0.24735884368419647, Test_Loss: 0.21926692128181458 *\n",
      "Epoch: 16, Train_Loss: 0.22282364964485168, Test_Loss: 0.24322649836540222\n",
      "Epoch: 16, Train_Loss: 0.23659393191337585, Test_Loss: 0.23692169785499573 *\n",
      "Epoch: 16, Train_Loss: 0.28087252378463745, Test_Loss: 0.22106687724590302 *\n",
      "Epoch: 16, Train_Loss: 0.22670988738536835, Test_Loss: 0.30886465311050415\n",
      "Epoch: 16, Train_Loss: 0.22711586952209473, Test_Loss: 0.3130296766757965\n",
      "Epoch: 16, Train_Loss: 0.6363856792449951, Test_Loss: 0.28477632999420166 *\n",
      "Epoch: 16, Train_Loss: 0.6600090265274048, Test_Loss: 0.2623097896575928 *\n",
      "Epoch: 16, Train_Loss: 0.5038338899612427, Test_Loss: 0.25742462277412415 *\n",
      "Epoch: 16, Train_Loss: 0.3008224666118622, Test_Loss: 0.2580367624759674\n",
      "Epoch: 16, Train_Loss: 1.0075162649154663, Test_Loss: 0.2355097085237503 *\n",
      "Epoch: 16, Train_Loss: 0.8809129595756531, Test_Loss: 0.377807080745697\n",
      "Epoch: 16, Train_Loss: 0.3208101987838745, Test_Loss: 0.34729355573654175 *\n",
      "Epoch: 16, Train_Loss: 0.2186458855867386, Test_Loss: 0.3392382264137268 *\n",
      "Epoch: 16, Train_Loss: 0.3581998646259308, Test_Loss: 0.3056727647781372 *\n",
      "Epoch: 16, Train_Loss: 0.5072435736656189, Test_Loss: 0.3693339228630066\n",
      "Epoch: 16, Train_Loss: 0.5310631394386292, Test_Loss: 0.47371041774749756\n",
      "Epoch: 16, Train_Loss: 0.22752174735069275, Test_Loss: 0.5149617195129395\n",
      "Epoch: 16, Train_Loss: 0.23395994305610657, Test_Loss: 0.36189091205596924 *\n",
      "Epoch: 16, Train_Loss: 0.25157803297042847, Test_Loss: 0.30195367336273193 *\n",
      "Epoch: 16, Train_Loss: 0.42782241106033325, Test_Loss: 0.24008962512016296 *\n",
      "Epoch: 16, Train_Loss: 0.254019558429718, Test_Loss: 0.23315976560115814 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.27828752994537354, Test_Loss: 0.32452768087387085\n",
      "Epoch: 16, Train_Loss: 0.24331361055374146, Test_Loss: 0.44980937242507935\n",
      "Epoch: 16, Train_Loss: 0.2691977024078369, Test_Loss: 0.35560962557792664 *\n",
      "Epoch: 16, Train_Loss: 0.3155990242958069, Test_Loss: 0.376024067401886\n",
      "Epoch: 16, Train_Loss: 0.34156569838523865, Test_Loss: 0.2625539004802704 *\n",
      "Epoch: 16, Train_Loss: 0.27784502506256104, Test_Loss: 0.243315190076828 *\n",
      "Epoch: 16, Train_Loss: 0.3266056180000305, Test_Loss: 0.23913614451885223 *\n",
      "Epoch: 16, Train_Loss: 0.30418020486831665, Test_Loss: 0.2575870752334595\n",
      "Epoch: 16, Train_Loss: 0.2373497486114502, Test_Loss: 0.3796689808368683\n",
      "Epoch: 16, Train_Loss: 0.24146625399589539, Test_Loss: 0.3406805992126465 *\n",
      "Epoch: 16, Train_Loss: 0.29149505496025085, Test_Loss: 0.3106941282749176 *\n",
      "Epoch: 16, Train_Loss: 0.2636391520500183, Test_Loss: 0.31418123841285706\n",
      "Epoch: 16, Train_Loss: 0.29957568645477295, Test_Loss: 0.2594313621520996 *\n",
      "Epoch: 16, Train_Loss: 0.3292197287082672, Test_Loss: 0.2746536135673523\n",
      "Epoch: 16, Train_Loss: 0.24857577681541443, Test_Loss: 0.27040407061576843 *\n",
      "Epoch: 16, Train_Loss: 0.23799294233322144, Test_Loss: 0.38355398178100586\n",
      "Epoch: 16, Train_Loss: 0.21962043642997742, Test_Loss: 0.23835447430610657 *\n",
      "Epoch: 16, Train_Loss: 0.22062407433986664, Test_Loss: 0.25731706619262695\n",
      "Epoch: 16, Train_Loss: 0.22028768062591553, Test_Loss: 0.24893420934677124 *\n",
      "Epoch: 16, Train_Loss: 0.2256438285112381, Test_Loss: 0.4776172637939453\n",
      "Epoch: 16, Train_Loss: 0.2410530298948288, Test_Loss: 0.6498326063156128\n",
      "Epoch: 16, Train_Loss: 0.23795410990715027, Test_Loss: 0.5354164838790894 *\n",
      "Epoch: 16, Train_Loss: 0.24009296298027039, Test_Loss: 0.3874920904636383 *\n",
      "Epoch: 16, Train_Loss: 0.4998680353164673, Test_Loss: 0.27702099084854126 *\n",
      "Epoch: 16, Train_Loss: 0.4213639199733734, Test_Loss: 0.27668488025665283 *\n",
      "Epoch: 16, Train_Loss: 0.2230169177055359, Test_Loss: 0.2840941250324249\n",
      "Epoch: 16, Train_Loss: 0.2545609474182129, Test_Loss: 0.3799738585948944\n",
      "Epoch: 16, Train_Loss: 0.3251168131828308, Test_Loss: 0.4142368733882904\n",
      "Epoch: 16, Train_Loss: 0.2500170171260834, Test_Loss: 5.972503185272217\n",
      "Epoch: 16, Train_Loss: 0.46702444553375244, Test_Loss: 0.4375062882900238 *\n",
      "Epoch: 16, Train_Loss: 0.2820970118045807, Test_Loss: 0.2766762673854828 *\n",
      "Epoch: 16, Train_Loss: 0.45117560029029846, Test_Loss: 0.23801493644714355 *\n",
      "Epoch: 16, Train_Loss: 0.31838828325271606, Test_Loss: 0.2315187305212021 *\n",
      "Epoch: 16, Train_Loss: 0.3167571425437927, Test_Loss: 0.2270020991563797 *\n",
      "Epoch: 16, Train_Loss: 0.2447793185710907, Test_Loss: 0.2525971233844757\n",
      "Epoch: 16, Train_Loss: 0.2344558984041214, Test_Loss: 0.23012106120586395 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.3296756446361542, Test_Loss: 0.219307079911232 *\n",
      "Epoch: 16, Train_Loss: 0.6832402944564819, Test_Loss: 0.2188919633626938 *\n",
      "Epoch: 16, Train_Loss: 0.6137439012527466, Test_Loss: 0.21766872704029083 *\n",
      "Epoch: 16, Train_Loss: 0.24509954452514648, Test_Loss: 0.262325257062912\n",
      "Epoch: 16, Train_Loss: 0.24529439210891724, Test_Loss: 0.22527824342250824 *\n",
      "Epoch: 16, Train_Loss: 0.222739115357399, Test_Loss: 0.2283906489610672\n",
      "Epoch: 16, Train_Loss: 0.33248403668403625, Test_Loss: 0.2443963885307312\n",
      "Epoch: 16, Train_Loss: 0.554775059223175, Test_Loss: 0.23285412788391113 *\n",
      "Epoch: 16, Train_Loss: 0.21966210007667542, Test_Loss: 0.227890744805336 *\n",
      "Epoch: 16, Train_Loss: 0.3604874014854431, Test_Loss: 0.3002009689807892\n",
      "Epoch: 16, Train_Loss: 0.249009370803833, Test_Loss: 0.2788621783256531 *\n",
      "Epoch: 16, Train_Loss: 0.24928420782089233, Test_Loss: 0.22595542669296265 *\n",
      "Epoch: 16, Train_Loss: 0.28985029458999634, Test_Loss: 0.23517023026943207\n",
      "Epoch: 16, Train_Loss: 0.3570914566516876, Test_Loss: 0.22411972284317017 *\n",
      "Epoch: 16, Train_Loss: 0.3153444528579712, Test_Loss: 0.3297225832939148\n",
      "Epoch: 16, Train_Loss: 0.26018714904785156, Test_Loss: 0.2733340859413147 *\n",
      "Epoch: 16, Train_Loss: 0.2317851185798645, Test_Loss: 0.3011922538280487\n",
      "Epoch: 16, Train_Loss: 0.2646283209323883, Test_Loss: 0.2267594039440155 *\n",
      "Epoch: 16, Train_Loss: 0.28184571862220764, Test_Loss: 0.23295481503009796\n",
      "Epoch: 16, Train_Loss: 0.24473926424980164, Test_Loss: 0.26984381675720215\n",
      "Epoch: 16, Train_Loss: 0.2296236902475357, Test_Loss: 0.2346223145723343 *\n",
      "Epoch: 16, Train_Loss: 0.23755432665348053, Test_Loss: 0.222207710146904 *\n",
      "Epoch: 16, Train_Loss: 0.2682435214519501, Test_Loss: 0.4030020236968994\n",
      "Epoch: 16, Train_Loss: 0.44043776392936707, Test_Loss: 1.9831544160842896\n",
      "Epoch: 16, Train_Loss: 0.43642061948776245, Test_Loss: 3.702321767807007\n",
      "Epoch: 16, Train_Loss: 0.5672417879104614, Test_Loss: 0.24771519005298615 *\n",
      "Epoch: 16, Train_Loss: 0.3839784860610962, Test_Loss: 0.22105683386325836 *\n",
      "Epoch: 16, Train_Loss: 0.42567670345306396, Test_Loss: 0.2833958864212036\n",
      "Epoch: 16, Train_Loss: 0.31760871410369873, Test_Loss: 0.42109012603759766\n",
      "Epoch: 16, Train_Loss: 0.26320478320121765, Test_Loss: 0.26461195945739746 *\n",
      "Epoch: 16, Train_Loss: 0.22500775754451752, Test_Loss: 0.2606159746646881 *\n",
      "Epoch: 16, Train_Loss: 0.23021404445171356, Test_Loss: 0.3104878067970276\n",
      "Epoch: 16, Train_Loss: 0.28544217348098755, Test_Loss: 0.22529388964176178 *\n",
      "Epoch: 16, Train_Loss: 0.5219752788543701, Test_Loss: 0.23559853434562683\n",
      "Epoch: 16, Train_Loss: 0.6841586232185364, Test_Loss: 0.2484978437423706\n",
      "Epoch: 16, Train_Loss: 0.910186767578125, Test_Loss: 0.2591191530227661\n",
      "Epoch: 16, Train_Loss: 1.1277177333831787, Test_Loss: 0.22534215450286865 *\n",
      "Epoch: 16, Train_Loss: 0.3863106966018677, Test_Loss: 0.4589555859565735\n",
      "Epoch: 16, Train_Loss: 0.4543658196926117, Test_Loss: 0.45409703254699707 *\n",
      "Epoch: 16, Train_Loss: 0.21734878420829773, Test_Loss: 0.31990087032318115 *\n",
      "Epoch: 16, Train_Loss: 0.26194822788238525, Test_Loss: 0.23276709020137787 *\n",
      "Epoch: 16, Train_Loss: 0.37172242999076843, Test_Loss: 0.2799972891807556\n",
      "Epoch: 16, Train_Loss: 1.0530449151992798, Test_Loss: 0.2491409182548523 *\n",
      "Epoch: 16, Train_Loss: 0.2587573826313019, Test_Loss: 0.3735520541667938\n",
      "Epoch: 16, Train_Loss: 0.2468617856502533, Test_Loss: 0.5125828981399536\n",
      "Epoch: 16, Train_Loss: 0.24997824430465698, Test_Loss: 0.46267420053482056 *\n",
      "Epoch: 16, Train_Loss: 0.4075477719306946, Test_Loss: 0.35285165905952454 *\n",
      "Epoch: 16, Train_Loss: 0.47254037857055664, Test_Loss: 0.3986397981643677\n",
      "Epoch: 16, Train_Loss: 0.5493806600570679, Test_Loss: 0.4631514549255371\n",
      "Epoch: 16, Train_Loss: 0.401660293340683, Test_Loss: 0.5113571286201477\n",
      "Epoch: 16, Train_Loss: 0.502907931804657, Test_Loss: 0.518207848072052\n",
      "Epoch: 16, Train_Loss: 0.22318784892559052, Test_Loss: 0.3063771724700928 *\n",
      "Epoch: 16, Train_Loss: 0.23067224025726318, Test_Loss: 0.3337394595146179\n",
      "Epoch: 16, Train_Loss: 0.22628705203533173, Test_Loss: 0.272107869386673 *\n",
      "Epoch: 16, Train_Loss: 0.2770763635635376, Test_Loss: 0.2343958616256714 *\n",
      "Epoch: 16, Train_Loss: 0.22407670319080353, Test_Loss: 0.2870021462440491\n",
      "Epoch: 16, Train_Loss: 0.2864081859588623, Test_Loss: 0.4123634696006775\n",
      "Epoch: 16, Train_Loss: 15.543241500854492, Test_Loss: 0.3984649181365967 *\n",
      "Epoch: 16, Train_Loss: 0.32424530386924744, Test_Loss: 0.31242769956588745 *\n",
      "Epoch: 16, Train_Loss: 1.4891818761825562, Test_Loss: 0.232072651386261 *\n",
      "Epoch: 16, Train_Loss: 1.1637567281723022, Test_Loss: 0.23559407889842987\n",
      "Epoch: 16, Train_Loss: 0.24622736871242523, Test_Loss: 0.23972685635089874\n",
      "Epoch: 16, Train_Loss: 0.30955761671066284, Test_Loss: 0.31170737743377686\n",
      "Epoch: 16, Train_Loss: 2.0755531787872314, Test_Loss: 0.33521437644958496\n",
      "Epoch: 16, Train_Loss: 5.799665927886963, Test_Loss: 0.514339029788971\n",
      "Epoch: 16, Train_Loss: 0.3319069743156433, Test_Loss: 0.30719804763793945 *\n",
      "Epoch: 16, Train_Loss: 0.2612353265285492, Test_Loss: 0.39875277876853943\n",
      "Epoch: 16, Train_Loss: 4.405966281890869, Test_Loss: 0.5529541969299316\n",
      "Epoch: 16, Train_Loss: 0.4538061022758484, Test_Loss: 0.6291787624359131\n",
      "Epoch: 16, Train_Loss: 0.3660064935684204, Test_Loss: 1.3028450012207031\n",
      "Epoch: 16, Train_Loss: 0.23104941844940186, Test_Loss: 0.4703861474990845 *\n",
      "Epoch: 16, Train_Loss: 0.23387199640274048, Test_Loss: 0.7767812609672546\n",
      "Epoch: 16, Train_Loss: 0.4186394214630127, Test_Loss: 0.23978646099567413 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train_Loss: 0.21584025025367737, Test_Loss: 1.0270235538482666\n",
      "Epoch: 16, Train_Loss: 0.28493204712867737, Test_Loss: 2.063209295272827\n",
      "Epoch: 16, Train_Loss: 0.21296562254428864, Test_Loss: 2.112323522567749\n",
      "Epoch: 16, Train_Loss: 0.21320684254169464, Test_Loss: 0.43318498134613037 *\n",
      "Epoch: 16, Train_Loss: 0.2409866750240326, Test_Loss: 0.2439952790737152 *\n",
      "Epoch: 16, Train_Loss: 0.27253687381744385, Test_Loss: 0.2506207227706909\n",
      "Epoch: 16, Train_Loss: 0.2804948389530182, Test_Loss: 0.23810496926307678 *\n",
      "Epoch: 16, Train_Loss: 0.23552323877811432, Test_Loss: 0.22310133278369904 *\n",
      "Epoch: 16, Train_Loss: 0.22835540771484375, Test_Loss: 0.22641313076019287\n",
      "Epoch: 16, Train_Loss: 0.21905647218227386, Test_Loss: 0.8034676313400269\n",
      "Epoch: 16, Train_Loss: 0.2193295955657959, Test_Loss: 7.405204772949219\n",
      "Epoch: 16, Train_Loss: 0.21641883254051208, Test_Loss: 0.35930466651916504 *\n",
      "Epoch: 16, Train_Loss: 0.2212982177734375, Test_Loss: 0.6290789246559143\n",
      "Epoch: 16, Train_Loss: 0.21463654935359955, Test_Loss: 0.5799587965011597 *\n",
      "Epoch: 16, Train_Loss: 0.21312960982322693, Test_Loss: 0.31797564029693604 *\n",
      "Epoch: 16, Train_Loss: 0.2127256542444229, Test_Loss: 0.345002144575119\n",
      "Epoch: 16, Train_Loss: 0.21355529129505157, Test_Loss: 0.7966122627258301\n",
      "Epoch: 16, Train_Loss: 0.2142815887928009, Test_Loss: 0.570809543132782 *\n",
      "Epoch: 16, Train_Loss: 0.21279595792293549, Test_Loss: 0.2287450134754181 *\n",
      "Epoch: 16, Train_Loss: 0.21266941726207733, Test_Loss: 0.31231391429901123\n",
      "Epoch: 16, Train_Loss: 0.2185736745595932, Test_Loss: 0.3204549551010132\n",
      "Epoch: 16, Train_Loss: 0.247121199965477, Test_Loss: 1.059173822402954\n",
      "Epoch: 16, Train_Loss: 0.25112733244895935, Test_Loss: 0.6824614405632019 *\n",
      "Epoch: 16, Train_Loss: 0.30324554443359375, Test_Loss: 1.1029942035675049\n",
      "Epoch: 16, Train_Loss: 0.2596910297870636, Test_Loss: 0.606958270072937 *\n",
      "Epoch: 16, Train_Loss: 0.8575314283370972, Test_Loss: 0.22459131479263306 *\n",
      "Epoch: 16, Train_Loss: 6.182068824768066, Test_Loss: 0.24996107816696167\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 16\n",
      "Epoch: 16, Train_Loss: 0.26037508249282837, Test_Loss: 0.23833005130290985 *\n",
      "Epoch: 16, Train_Loss: 0.292648047208786, Test_Loss: 0.7323765754699707\n",
      "Epoch: 16, Train_Loss: 0.32409822940826416, Test_Loss: 0.25052616000175476 *\n",
      "Epoch: 16, Train_Loss: 0.41233181953430176, Test_Loss: 0.5968896150588989\n",
      "Epoch: 16, Train_Loss: 0.35963141918182373, Test_Loss: 0.24900823831558228 *\n",
      "Epoch: 16, Train_Loss: 0.32326000928878784, Test_Loss: 0.6888126134872437\n",
      "Epoch: 16, Train_Loss: 0.30080124735832214, Test_Loss: 0.511505663394928 *\n",
      "Epoch: 16, Train_Loss: 0.3563939034938812, Test_Loss: 0.2962101399898529 *\n",
      "Epoch: 16, Train_Loss: 0.29105880856513977, Test_Loss: 0.26155608892440796 *\n",
      "Epoch: 16, Train_Loss: 0.257537841796875, Test_Loss: 0.3380388617515564\n",
      "Epoch: 16, Train_Loss: 0.225305438041687, Test_Loss: 0.32120880484580994 *\n",
      "Epoch: 16, Train_Loss: 0.3339482247829437, Test_Loss: 0.23362326622009277 *\n",
      "Epoch: 16, Train_Loss: 0.27279219031333923, Test_Loss: 0.24498260021209717\n",
      "Epoch: 16, Train_Loss: 0.4268018305301666, Test_Loss: 0.367184579372406\n",
      "Epoch: 16, Train_Loss: 0.22237202525138855, Test_Loss: 3.2022595405578613\n",
      "Epoch: 16, Train_Loss: 0.2781100571155548, Test_Loss: 3.3120243549346924\n",
      "Epoch: 16, Train_Loss: 0.22292807698249817, Test_Loss: 0.2263023406267166 *\n",
      "Epoch: 16, Train_Loss: 0.2378484457731247, Test_Loss: 0.2148127406835556 *\n",
      "Epoch: 16, Train_Loss: 0.2749122381210327, Test_Loss: 0.25204765796661377\n",
      "Epoch: 16, Train_Loss: 0.24821737408638, Test_Loss: 0.2425907850265503 *\n",
      "Epoch: 16, Train_Loss: 0.21766497194766998, Test_Loss: 0.24493083357810974\n",
      "Epoch: 16, Train_Loss: 0.2137720137834549, Test_Loss: 0.2519623637199402\n",
      "Epoch: 16, Train_Loss: 0.21367235481739044, Test_Loss: 0.24034136533737183 *\n",
      "Epoch: 16, Train_Loss: 1.308550238609314, Test_Loss: 0.22044911980628967 *\n",
      "Epoch: 16, Train_Loss: 5.039976596832275, Test_Loss: 0.23321212828159332\n",
      "Epoch: 16, Train_Loss: 0.21511436998844147, Test_Loss: 0.233656644821167\n",
      "Epoch: 16, Train_Loss: 0.22449859976768494, Test_Loss: 0.23238427937030792 *\n",
      "Epoch: 16, Train_Loss: 0.231362983584404, Test_Loss: 0.22033743560314178 *\n",
      "Epoch: 16, Train_Loss: 0.22841578722000122, Test_Loss: 0.2791416645050049\n",
      "Epoch: 16, Train_Loss: 0.22339589893817902, Test_Loss: 0.28169551491737366\n",
      "Epoch: 16, Train_Loss: 0.22137701511383057, Test_Loss: 0.2874944508075714\n",
      "Epoch: 16, Train_Loss: 0.222942516207695, Test_Loss: 0.3222048878669739\n",
      "Epoch: 16, Train_Loss: 0.23600168526172638, Test_Loss: 0.2267296463251114 *\n",
      "Epoch: 16, Train_Loss: 0.2317333072423935, Test_Loss: 0.22716985642910004\n",
      "Epoch: 16, Train_Loss: 0.22133329510688782, Test_Loss: 0.21493016183376312 *\n",
      "Epoch: 16, Train_Loss: 0.21866600215435028, Test_Loss: 0.21555759012699127\n",
      "Epoch: 16, Train_Loss: 0.21645128726959229, Test_Loss: 0.21588489413261414\n",
      "Epoch: 16, Train_Loss: 0.23434217274188995, Test_Loss: 0.21764212846755981\n",
      "Epoch: 16, Train_Loss: 0.21439002454280853, Test_Loss: 0.21708667278289795 *\n",
      "Epoch: 16, Train_Loss: 0.2146243005990982, Test_Loss: 0.21746306121349335\n",
      "Epoch: 16, Train_Loss: 0.2542722821235657, Test_Loss: 0.21873246133327484\n",
      "Epoch: 16, Train_Loss: 0.2759067416191101, Test_Loss: 0.2145151048898697 *\n",
      "Epoch: 16, Train_Loss: 0.22419695556163788, Test_Loss: 0.21793749928474426\n",
      "Epoch: 16, Train_Loss: 0.21355114877223969, Test_Loss: 0.22010457515716553\n",
      "Epoch: 16, Train_Loss: 0.22234392166137695, Test_Loss: 0.22506961226463318\n",
      "Epoch: 16, Train_Loss: 0.314200758934021, Test_Loss: 0.24911093711853027\n",
      "Epoch: 16, Train_Loss: 0.2688620984554291, Test_Loss: 0.2461739033460617 *\n",
      "Epoch: 16, Train_Loss: 0.25814881920814514, Test_Loss: 0.5881370306015015\n",
      "Epoch: 16, Train_Loss: 0.2496299147605896, Test_Loss: 0.5190340280532837 *\n",
      "Epoch: 16, Train_Loss: 0.2657901346683502, Test_Loss: 0.2812029719352722 *\n",
      "Epoch: 16, Train_Loss: 0.26011839509010315, Test_Loss: 0.21966709196567535 *\n",
      "Epoch: 16, Train_Loss: 0.28517523407936096, Test_Loss: 0.24177804589271545\n",
      "Epoch: 16, Train_Loss: 0.22715270519256592, Test_Loss: 0.31106987595558167\n",
      "Epoch: 16, Train_Loss: 0.3417775332927704, Test_Loss: 0.6063814163208008\n",
      "Epoch: 17, Train_Loss: 0.23201993107795715, Test_Loss: 1.5170778036117554 *\n",
      "Epoch: 17, Train_Loss: 0.2129078209400177, Test_Loss: 0.9318000078201294 *\n",
      "Epoch: 17, Train_Loss: 0.2132434844970703, Test_Loss: 0.2709237337112427 *\n",
      "Epoch: 17, Train_Loss: 0.2120945006608963, Test_Loss: 0.24404846131801605 *\n",
      "Epoch: 17, Train_Loss: 0.21181535720825195, Test_Loss: 0.22017307579517365 *\n",
      "Epoch: 17, Train_Loss: 0.21270602941513062, Test_Loss: 0.22336144745349884\n",
      "Epoch: 17, Train_Loss: 1.2225724458694458, Test_Loss: 0.22106501460075378 *\n",
      "Epoch: 17, Train_Loss: 4.113912582397461, Test_Loss: 0.24105927348136902\n",
      "Epoch: 17, Train_Loss: 0.21454070508480072, Test_Loss: 0.2695790231227875\n",
      "Epoch: 17, Train_Loss: 0.22402788698673248, Test_Loss: 0.21241219341754913 *\n",
      "Epoch: 17, Train_Loss: 0.22295357286930084, Test_Loss: 0.26913973689079285\n",
      "Epoch: 17, Train_Loss: 0.21102850139141083, Test_Loss: 0.3324819505214691\n",
      "Epoch: 17, Train_Loss: 0.21111051738262177, Test_Loss: 0.5939744710922241\n",
      "Epoch: 17, Train_Loss: 0.21197903156280518, Test_Loss: 0.43474581837654114 *\n",
      "Epoch: 17, Train_Loss: 0.21080540120601654, Test_Loss: 0.22939005494117737 *\n",
      "Epoch: 17, Train_Loss: 0.21110080182552338, Test_Loss: 0.22466431558132172 *\n",
      "Epoch: 17, Train_Loss: 0.2135820984840393, Test_Loss: 0.2247401624917984\n",
      "Epoch: 17, Train_Loss: 0.28159385919570923, Test_Loss: 0.22552038729190826\n",
      "Epoch: 17, Train_Loss: 0.27081161737442017, Test_Loss: 0.23041775822639465\n",
      "Epoch: 17, Train_Loss: 0.29392069578170776, Test_Loss: 1.697085976600647\n",
      "Epoch: 17, Train_Loss: 0.24950768053531647, Test_Loss: 4.11797571182251\n",
      "Epoch: 17, Train_Loss: 0.21333180367946625, Test_Loss: 0.2268873006105423 *\n",
      "Epoch: 17, Train_Loss: 0.4074362516403198, Test_Loss: 0.21947740018367767 *\n",
      "Epoch: 17, Train_Loss: 0.4537757635116577, Test_Loss: 0.2202320098876953\n",
      "Epoch: 17, Train_Loss: 0.4450639486312866, Test_Loss: 0.22917383909225464\n",
      "Epoch: 17, Train_Loss: 0.36157339811325073, Test_Loss: 0.21757763624191284 *\n",
      "Epoch: 17, Train_Loss: 0.21237988770008087, Test_Loss: 0.21784546971321106\n",
      "Epoch: 17, Train_Loss: 0.21127614378929138, Test_Loss: 0.21192717552185059 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.2152385413646698, Test_Loss: 0.2117888480424881 *\n",
      "Epoch: 17, Train_Loss: 0.22464001178741455, Test_Loss: 0.21242664754390717\n",
      "Epoch: 17, Train_Loss: 0.22554180026054382, Test_Loss: 0.21091879904270172 *\n",
      "Epoch: 17, Train_Loss: 0.22422127425670624, Test_Loss: 0.21655939519405365\n",
      "Epoch: 17, Train_Loss: 0.21279364824295044, Test_Loss: 0.2466253936290741\n",
      "Epoch: 17, Train_Loss: 0.21032002568244934, Test_Loss: 0.24686861038208008\n",
      "Epoch: 17, Train_Loss: 0.22177648544311523, Test_Loss: 0.21947446465492249 *\n",
      "Epoch: 17, Train_Loss: 0.25003090500831604, Test_Loss: 0.21092037856578827 *\n",
      "Epoch: 17, Train_Loss: 0.4156876802444458, Test_Loss: 0.21129849553108215\n",
      "Epoch: 17, Train_Loss: 0.3893786072731018, Test_Loss: 0.2124357372522354\n",
      "Epoch: 17, Train_Loss: 0.3750235438346863, Test_Loss: 0.2122064232826233 *\n",
      "Epoch: 17, Train_Loss: 0.27570658922195435, Test_Loss: 0.21135511994361877 *\n",
      "Epoch: 17, Train_Loss: 0.33824998140335083, Test_Loss: 0.21118751168251038 *\n",
      "Epoch: 17, Train_Loss: 0.2816523313522339, Test_Loss: 0.21049590408802032 *\n",
      "Epoch: 17, Train_Loss: 0.3161466121673584, Test_Loss: 0.2133074402809143\n",
      "Epoch: 17, Train_Loss: 0.2953975200653076, Test_Loss: 0.21166874468326569 *\n",
      "Epoch: 17, Train_Loss: 0.4702645242214203, Test_Loss: 0.21223576366901398\n",
      "Epoch: 17, Train_Loss: 0.22102037072181702, Test_Loss: 0.21150924265384674 *\n",
      "Epoch: 17, Train_Loss: 0.221197247505188, Test_Loss: 0.21109794080257416 *\n",
      "Epoch: 17, Train_Loss: 3.05708646774292, Test_Loss: 0.21099545061588287 *\n",
      "Epoch: 17, Train_Loss: 0.513532280921936, Test_Loss: 0.21072372794151306 *\n",
      "Epoch: 17, Train_Loss: 0.26235735416412354, Test_Loss: 0.24304616451263428\n",
      "Epoch: 17, Train_Loss: 0.28160610795021057, Test_Loss: 0.24567563831806183\n",
      "Epoch: 17, Train_Loss: 0.262173593044281, Test_Loss: 4.018401145935059\n",
      "Epoch: 17, Train_Loss: 0.23190630972385406, Test_Loss: 1.9665632247924805 *\n",
      "Epoch: 17, Train_Loss: 0.21232128143310547, Test_Loss: 0.2122906893491745 *\n",
      "Epoch: 17, Train_Loss: 0.26109781861305237, Test_Loss: 0.21967732906341553\n",
      "Epoch: 17, Train_Loss: 0.321679025888443, Test_Loss: 0.2693368196487427\n",
      "Epoch: 17, Train_Loss: 0.2899358570575714, Test_Loss: 0.27933716773986816\n",
      "Epoch: 17, Train_Loss: 0.2652543783187866, Test_Loss: 0.23227739334106445 *\n",
      "Epoch: 17, Train_Loss: 0.26458144187927246, Test_Loss: 0.2728034257888794\n",
      "Epoch: 17, Train_Loss: 0.22408904135227203, Test_Loss: 0.2775231897830963\n",
      "Epoch: 17, Train_Loss: 0.2377103567123413, Test_Loss: 0.21502923965454102 *\n",
      "Epoch: 17, Train_Loss: 0.21629773080348969, Test_Loss: 0.23596419394016266\n",
      "Epoch: 17, Train_Loss: 0.2611773908138275, Test_Loss: 0.23073068261146545 *\n",
      "Epoch: 17, Train_Loss: 0.23495665192604065, Test_Loss: 0.22397634387016296 *\n",
      "Epoch: 17, Train_Loss: 0.2106073796749115, Test_Loss: 0.21427160501480103 *\n",
      "Epoch: 17, Train_Loss: 0.22131989896297455, Test_Loss: 0.3094666302204132\n",
      "Epoch: 17, Train_Loss: 0.24677959084510803, Test_Loss: 0.27206510305404663 *\n",
      "Epoch: 17, Train_Loss: 0.24824205040931702, Test_Loss: 0.2765824794769287\n",
      "Epoch: 17, Train_Loss: 0.21158497035503387, Test_Loss: 0.2653425633907318 *\n",
      "Epoch: 17, Train_Loss: 0.20968101918697357, Test_Loss: 0.24876993894577026 *\n",
      "Epoch: 17, Train_Loss: 0.21018069982528687, Test_Loss: 0.2330588847398758 *\n",
      "Epoch: 17, Train_Loss: 0.2093617469072342, Test_Loss: 0.22366324067115784 *\n",
      "Epoch: 17, Train_Loss: 0.2095526158809662, Test_Loss: 0.22793389856815338\n",
      "Epoch: 17, Train_Loss: 0.21112944185733795, Test_Loss: 0.2290046215057373\n",
      "Epoch: 17, Train_Loss: 0.2106335163116455, Test_Loss: 0.23260903358459473\n",
      "Epoch: 17, Train_Loss: 0.2099848836660385, Test_Loss: 0.2310510277748108 *\n",
      "Epoch: 17, Train_Loss: 0.20960760116577148, Test_Loss: 0.2229757010936737 *\n",
      "Epoch: 17, Train_Loss: 0.20982961356639862, Test_Loss: 0.2295529693365097\n",
      "Epoch: 17, Train_Loss: 0.2098906934261322, Test_Loss: 0.22747616469860077 *\n",
      "Epoch: 17, Train_Loss: 0.22276680171489716, Test_Loss: 0.2260771542787552 *\n",
      "Epoch: 17, Train_Loss: 0.22496150434017181, Test_Loss: 0.21854208409786224 *\n",
      "Epoch: 17, Train_Loss: 0.22372417151927948, Test_Loss: 0.2546372711658478\n",
      "Epoch: 17, Train_Loss: 0.22643518447875977, Test_Loss: 0.2658383250236511\n",
      "Epoch: 17, Train_Loss: 0.22092479467391968, Test_Loss: 0.3642224967479706\n",
      "Epoch: 17, Train_Loss: 0.20900706946849823, Test_Loss: 0.7469534277915955\n",
      "Epoch: 17, Train_Loss: 0.20887979865074158, Test_Loss: 0.6156595945358276 *\n",
      "Epoch: 17, Train_Loss: 0.21820227801799774, Test_Loss: 0.3307811915874481 *\n",
      "Epoch: 17, Train_Loss: 0.2266543060541153, Test_Loss: 0.2426486313343048 *\n",
      "Epoch: 17, Train_Loss: 0.20893321931362152, Test_Loss: 0.22500276565551758 *\n",
      "Epoch: 17, Train_Loss: 0.21123667061328888, Test_Loss: 0.2737431526184082\n",
      "Epoch: 17, Train_Loss: 0.2094450443983078, Test_Loss: 0.5801372528076172\n",
      "Epoch: 17, Train_Loss: 0.2545933425426483, Test_Loss: 0.7706608772277832\n",
      "Epoch: 17, Train_Loss: 0.25533178448677063, Test_Loss: 0.574663519859314 *\n",
      "Epoch: 17, Train_Loss: 0.25526729226112366, Test_Loss: 0.3140210211277008 *\n",
      "Epoch: 17, Train_Loss: 0.22243569791316986, Test_Loss: 0.22504250705242157 *\n",
      "Epoch: 17, Train_Loss: 0.2111574411392212, Test_Loss: 0.21485644578933716 *\n",
      "Epoch: 17, Train_Loss: 0.2671223282814026, Test_Loss: 0.20965470373630524 *\n",
      "Epoch: 17, Train_Loss: 0.2136155515909195, Test_Loss: 0.22155337035655975\n",
      "Epoch: 17, Train_Loss: 0.21627014875411987, Test_Loss: 0.22574104368686676\n",
      "Epoch: 17, Train_Loss: 0.23023399710655212, Test_Loss: 0.24376073479652405\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.2214069664478302, Test_Loss: 0.2106943428516388 *\n",
      "Epoch: 17, Train_Loss: 0.34109872579574585, Test_Loss: 0.3096403479576111\n",
      "Epoch: 17, Train_Loss: 0.27296704053878784, Test_Loss: 0.45484355092048645\n",
      "Epoch: 17, Train_Loss: 0.23394788801670074, Test_Loss: 0.45818889141082764\n",
      "Epoch: 17, Train_Loss: 0.21976538002490997, Test_Loss: 0.4325878918170929 *\n",
      "Epoch: 17, Train_Loss: 0.2305627018213272, Test_Loss: 0.22328239679336548 *\n",
      "Epoch: 17, Train_Loss: 0.21443867683410645, Test_Loss: 0.22272753715515137 *\n",
      "Epoch: 17, Train_Loss: 0.21195943653583527, Test_Loss: 0.22226466238498688 *\n",
      "Epoch: 17, Train_Loss: 0.22160035371780396, Test_Loss: 0.22269390523433685\n",
      "Epoch: 17, Train_Loss: 0.22297056019306183, Test_Loss: 0.23681269586086273\n",
      "Epoch: 17, Train_Loss: 0.2495773881673813, Test_Loss: 3.011014461517334\n",
      "Epoch: 17, Train_Loss: 0.29726022481918335, Test_Loss: 2.765937328338623 *\n",
      "Epoch: 17, Train_Loss: 0.2262139767408371, Test_Loss: 0.2202952653169632 *\n",
      "Epoch: 17, Train_Loss: 0.2484041005373001, Test_Loss: 0.21526703238487244 *\n",
      "Epoch: 17, Train_Loss: 0.23162804543972015, Test_Loss: 0.21381638944149017 *\n",
      "Epoch: 17, Train_Loss: 0.22151625156402588, Test_Loss: 0.22528387606143951\n",
      "Epoch: 17, Train_Loss: 0.3058263063430786, Test_Loss: 0.211035817861557 *\n",
      "Epoch: 17, Train_Loss: 0.43693363666534424, Test_Loss: 0.2151162475347519\n",
      "Epoch: 17, Train_Loss: 0.21147966384887695, Test_Loss: 0.20934797823429108 *\n",
      "Epoch: 17, Train_Loss: 0.24156269431114197, Test_Loss: 0.209122434258461 *\n",
      "Epoch: 17, Train_Loss: 0.20825685560703278, Test_Loss: 0.21014322340488434\n",
      "Epoch: 17, Train_Loss: 0.2081567645072937, Test_Loss: 0.21022161841392517\n",
      "Epoch: 17, Train_Loss: 0.2135399729013443, Test_Loss: 0.2228270173072815\n",
      "Epoch: 17, Train_Loss: 0.211289182305336, Test_Loss: 0.23736357688903809\n",
      "Epoch: 17, Train_Loss: 0.20913861691951752, Test_Loss: 0.23987585306167603\n",
      "Epoch: 17, Train_Loss: 0.22401063144207, Test_Loss: 0.20889011025428772 *\n",
      "Epoch: 17, Train_Loss: 0.2122299075126648, Test_Loss: 0.209211528301239\n",
      "Epoch: 17, Train_Loss: 0.2198188751935959, Test_Loss: 0.20823760330677032 *\n",
      "Epoch: 17, Train_Loss: 0.21855831146240234, Test_Loss: 0.2119612693786621\n",
      "Epoch: 17, Train_Loss: 0.209462970495224, Test_Loss: 0.20943701267242432 *\n",
      "Epoch: 17, Train_Loss: 0.20898105204105377, Test_Loss: 0.20835044980049133 *\n",
      "Epoch: 17, Train_Loss: 0.20806467533111572, Test_Loss: 0.20764987170696259 *\n",
      "Epoch: 17, Train_Loss: 0.22963961958885193, Test_Loss: 0.20788876712322235\n",
      "Epoch: 17, Train_Loss: 0.2298842817544937, Test_Loss: 0.20964954793453217\n",
      "Epoch: 17, Train_Loss: 0.23151737451553345, Test_Loss: 0.20885725319385529 *\n",
      "Epoch: 17, Train_Loss: 0.2137044221162796, Test_Loss: 0.2085985243320465 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.24424609541893005, Test_Loss: 0.20774662494659424 *\n",
      "Epoch: 17, Train_Loss: 0.22788876295089722, Test_Loss: 0.20769904553890228 *\n",
      "Epoch: 17, Train_Loss: 0.22763691842556, Test_Loss: 0.20824885368347168\n",
      "Epoch: 17, Train_Loss: 0.2227444052696228, Test_Loss: 0.20763811469078064 *\n",
      "Epoch: 17, Train_Loss: 0.24259622395038605, Test_Loss: 0.24339942634105682\n",
      "Epoch: 17, Train_Loss: 0.20968346297740936, Test_Loss: 0.2391759157180786 *\n",
      "Epoch: 17, Train_Loss: 0.21514949202537537, Test_Loss: 5.3720831871032715\n",
      "Epoch: 17, Train_Loss: 0.2384004294872284, Test_Loss: 0.7370575666427612 *\n",
      "Epoch: 17, Train_Loss: 0.253716379404068, Test_Loss: 0.20857098698616028 *\n",
      "Epoch: 17, Train_Loss: 2.2766740322113037, Test_Loss: 0.23093131184577942\n",
      "Epoch: 17, Train_Loss: 3.5420994758605957, Test_Loss: 0.2589675188064575\n",
      "Epoch: 17, Train_Loss: 0.21769283711910248, Test_Loss: 0.26766499876976013\n",
      "Epoch: 17, Train_Loss: 0.2239357978105545, Test_Loss: 0.21496716141700745 *\n",
      "Epoch: 17, Train_Loss: 0.24625153839588165, Test_Loss: 0.28627175092697144\n",
      "Epoch: 17, Train_Loss: 0.3921217620372772, Test_Loss: 0.26669248938560486 *\n",
      "Epoch: 17, Train_Loss: 0.24660298228263855, Test_Loss: 0.20892110466957092 *\n",
      "Epoch: 17, Train_Loss: 0.21998582780361176, Test_Loss: 0.23721714317798615\n",
      "Epoch: 17, Train_Loss: 0.21106548607349396, Test_Loss: 0.22338303923606873 *\n",
      "Epoch: 17, Train_Loss: 0.290791779756546, Test_Loss: 0.2160671502351761 *\n",
      "Epoch: 17, Train_Loss: 0.21977382898330688, Test_Loss: 0.21549373865127563 *\n",
      "Epoch: 17, Train_Loss: 0.2215871661901474, Test_Loss: 0.2771000862121582\n",
      "Epoch: 17, Train_Loss: 0.7217491865158081, Test_Loss: 0.24489307403564453 *\n",
      "Epoch: 17, Train_Loss: 0.7557739615440369, Test_Loss: 0.2719871401786804\n",
      "Epoch: 17, Train_Loss: 0.7920502424240112, Test_Loss: 0.23868420720100403 *\n",
      "Epoch: 17, Train_Loss: 0.2745765745639801, Test_Loss: 0.25530263781547546\n",
      "Epoch: 17, Train_Loss: 0.7086213827133179, Test_Loss: 0.21753373742103577 *\n",
      "Epoch: 17, Train_Loss: 1.3750050067901611, Test_Loss: 0.27322155237197876\n",
      "Epoch: 17, Train_Loss: 0.6180770993232727, Test_Loss: 0.28438636660575867\n",
      "Epoch: 17, Train_Loss: 0.22481577098369598, Test_Loss: 0.36028340458869934\n",
      "Epoch: 17, Train_Loss: 0.22592395544052124, Test_Loss: 0.2770352065563202 *\n",
      "Epoch: 17, Train_Loss: 0.7121744751930237, Test_Loss: 0.29705703258514404\n",
      "Epoch: 17, Train_Loss: 0.6459283232688904, Test_Loss: 0.4653652310371399\n",
      "Epoch: 17, Train_Loss: 0.441398561000824, Test_Loss: 0.4535897672176361 *\n",
      "Epoch: 17, Train_Loss: 0.2146015763282776, Test_Loss: 0.4164951741695404 *\n",
      "Epoch: 17, Train_Loss: 0.21797698736190796, Test_Loss: 0.25858527421951294 *\n",
      "Epoch: 17, Train_Loss: 0.42196980118751526, Test_Loss: 0.23679572343826294 *\n",
      "Epoch: 17, Train_Loss: 0.34597957134246826, Test_Loss: 0.21748022735118866 *\n",
      "Epoch: 17, Train_Loss: 0.25053858757019043, Test_Loss: 0.2385781705379486\n",
      "Epoch: 17, Train_Loss: 0.24672913551330566, Test_Loss: 0.43182146549224854\n",
      "Epoch: 17, Train_Loss: 0.2607288360595703, Test_Loss: 0.2816464304924011 *\n",
      "Epoch: 17, Train_Loss: 0.2673173248767853, Test_Loss: 0.39347338676452637\n",
      "Epoch: 17, Train_Loss: 0.3174598813056946, Test_Loss: 0.27221325039863586 *\n",
      "Epoch: 17, Train_Loss: 0.3722533583641052, Test_Loss: 0.23944488167762756 *\n",
      "Epoch: 17, Train_Loss: 0.26207780838012695, Test_Loss: 0.21849209070205688 *\n",
      "Epoch: 17, Train_Loss: 0.29442131519317627, Test_Loss: 0.2634466588497162\n",
      "Epoch: 17, Train_Loss: 0.2210518717765808, Test_Loss: 0.511381208896637\n",
      "Epoch: 17, Train_Loss: 0.25443190336227417, Test_Loss: 0.22081784904003143 *\n",
      "Epoch: 17, Train_Loss: 0.24401038885116577, Test_Loss: 0.48178428411483765\n",
      "Epoch: 17, Train_Loss: 0.3704579174518585, Test_Loss: 0.2930448651313782 *\n",
      "Epoch: 17, Train_Loss: 0.25986725091934204, Test_Loss: 0.2735465466976166 *\n",
      "Epoch: 17, Train_Loss: 0.2845735251903534, Test_Loss: 0.27622586488723755\n",
      "Epoch: 17, Train_Loss: 0.24771559238433838, Test_Loss: 0.22770288586616516 *\n",
      "Epoch: 17, Train_Loss: 0.22419250011444092, Test_Loss: 0.38002583384513855\n",
      "Epoch: 17, Train_Loss: 0.20961949229240417, Test_Loss: 0.22616589069366455 *\n",
      "Epoch: 17, Train_Loss: 0.20763123035430908, Test_Loss: 0.23357747495174408\n",
      "Epoch: 17, Train_Loss: 0.21056102216243744, Test_Loss: 0.22875574231147766 *\n",
      "Epoch: 17, Train_Loss: 0.21474483609199524, Test_Loss: 0.4362637400627136\n",
      "Epoch: 17, Train_Loss: 0.2333683967590332, Test_Loss: 0.6375530362129211\n",
      "Epoch: 17, Train_Loss: 0.22457197308540344, Test_Loss: 0.34097516536712646 *\n",
      "Epoch: 17, Train_Loss: 0.22066231071949005, Test_Loss: 0.6243948936462402\n",
      "Epoch: 17, Train_Loss: 0.36115536093711853, Test_Loss: 0.29797741770744324 *\n",
      "Epoch: 17, Train_Loss: 0.32043731212615967, Test_Loss: 0.3014392554759979\n",
      "Epoch: 17, Train_Loss: 0.2306334525346756, Test_Loss: 0.29920265078544617 *\n",
      "Epoch: 17, Train_Loss: 0.23146355152130127, Test_Loss: 0.3060794770717621\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.27602267265319824, Test_Loss: 0.5040064454078674\n",
      "Epoch: 17, Train_Loss: 0.3058874011039734, Test_Loss: 4.188047885894775\n",
      "Epoch: 17, Train_Loss: 0.3898152709007263, Test_Loss: 1.368251085281372 *\n",
      "Epoch: 17, Train_Loss: 0.22901013493537903, Test_Loss: 0.23550572991371155 *\n",
      "Epoch: 17, Train_Loss: 0.2952404022216797, Test_Loss: 0.22426912188529968 *\n",
      "Epoch: 17, Train_Loss: 0.35402387380599976, Test_Loss: 0.21750712394714355 *\n",
      "Epoch: 17, Train_Loss: 0.2755170464515686, Test_Loss: 0.21871685981750488\n",
      "Epoch: 17, Train_Loss: 0.28784915804862976, Test_Loss: 0.24178531765937805\n",
      "Epoch: 17, Train_Loss: 0.23423491418361664, Test_Loss: 0.24120518565177917 *\n",
      "Epoch: 17, Train_Loss: 0.2545406222343445, Test_Loss: 0.21595901250839233 *\n",
      "Epoch: 17, Train_Loss: 0.6034935712814331, Test_Loss: 0.20788788795471191 *\n",
      "Epoch: 17, Train_Loss: 0.6343079209327698, Test_Loss: 0.20986589789390564\n",
      "Epoch: 17, Train_Loss: 0.2339230626821518, Test_Loss: 0.21893373131752014\n",
      "Epoch: 17, Train_Loss: 0.24205151200294495, Test_Loss: 0.25310835242271423\n",
      "Epoch: 17, Train_Loss: 0.21140608191490173, Test_Loss: 0.225491464138031 *\n",
      "Epoch: 17, Train_Loss: 0.22420693933963776, Test_Loss: 0.2377704381942749\n",
      "Epoch: 17, Train_Loss: 0.48345649242401123, Test_Loss: 0.22504912316799164 *\n",
      "Epoch: 17, Train_Loss: 0.2225838154554367, Test_Loss: 0.22170180082321167 *\n",
      "Epoch: 17, Train_Loss: 0.2913109362125397, Test_Loss: 0.268583208322525\n",
      "Epoch: 17, Train_Loss: 0.28517112135887146, Test_Loss: 0.33502185344696045\n",
      "Epoch: 17, Train_Loss: 0.25909945368766785, Test_Loss: 0.23334436118602753 *\n",
      "Epoch: 17, Train_Loss: 0.25996506214141846, Test_Loss: 0.21603639423847198 *\n",
      "Epoch: 17, Train_Loss: 0.30870768427848816, Test_Loss: 0.22678080201148987\n",
      "Epoch: 17, Train_Loss: 0.2832748293876648, Test_Loss: 0.2834053337574005\n",
      "Epoch: 17, Train_Loss: 0.22554253041744232, Test_Loss: 0.27000927925109863 *\n",
      "Epoch: 17, Train_Loss: 0.2419871836900711, Test_Loss: 0.3608540892601013\n",
      "Epoch: 17, Train_Loss: 0.2413879930973053, Test_Loss: 0.2209867537021637 *\n",
      "Epoch: 17, Train_Loss: 0.30504822731018066, Test_Loss: 0.21819666028022766 *\n",
      "Epoch: 17, Train_Loss: 0.2501319646835327, Test_Loss: 0.25200584530830383\n",
      "Epoch: 17, Train_Loss: 0.22705188393592834, Test_Loss: 0.22876624763011932 *\n",
      "Epoch: 17, Train_Loss: 0.23314175009727478, Test_Loss: 0.208570659160614 *\n",
      "Epoch: 17, Train_Loss: 0.23488306999206543, Test_Loss: 0.3923795223236084\n",
      "Epoch: 17, Train_Loss: 0.4810083210468292, Test_Loss: 0.33658063411712646 *\n",
      "Epoch: 17, Train_Loss: 0.48470228910446167, Test_Loss: 6.0178422927856445\n",
      "Epoch: 17, Train_Loss: 0.6412147879600525, Test_Loss: 0.2663121521472931 *\n",
      "Epoch: 17, Train_Loss: 0.3363068401813507, Test_Loss: 0.21358728408813477 *\n",
      "Epoch: 17, Train_Loss: 0.45548781752586365, Test_Loss: 0.25706085562705994\n",
      "Epoch: 17, Train_Loss: 0.34986382722854614, Test_Loss: 0.4056881070137024\n",
      "Epoch: 17, Train_Loss: 0.263554185628891, Test_Loss: 0.30218788981437683 *\n",
      "Epoch: 17, Train_Loss: 0.21362093091011047, Test_Loss: 0.22245903313159943 *\n",
      "Epoch: 17, Train_Loss: 0.21714647114276886, Test_Loss: 0.30060750246047974\n",
      "Epoch: 17, Train_Loss: 0.2554503083229065, Test_Loss: 0.25089311599731445 *\n",
      "Epoch: 17, Train_Loss: 0.4613921344280243, Test_Loss: 0.2063753455877304 *\n",
      "Epoch: 17, Train_Loss: 0.5475645065307617, Test_Loss: 0.22292682528495789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.7134106159210205, Test_Loss: 0.2299828827381134\n",
      "Epoch: 17, Train_Loss: 0.9593734741210938, Test_Loss: 0.2073652148246765 *\n",
      "Epoch: 17, Train_Loss: 0.49515074491500854, Test_Loss: 0.34860384464263916\n",
      "Epoch: 17, Train_Loss: 0.5093227028846741, Test_Loss: 0.5356225371360779\n",
      "Epoch: 17, Train_Loss: 0.2094261795282364, Test_Loss: 0.3571924567222595 *\n",
      "Epoch: 17, Train_Loss: 0.22120292484760284, Test_Loss: 0.24737149477005005 *\n",
      "Epoch: 17, Train_Loss: 0.41458001732826233, Test_Loss: 0.25149452686309814\n",
      "Epoch: 17, Train_Loss: 0.7445193529129028, Test_Loss: 0.23512455821037292 *\n",
      "Epoch: 17, Train_Loss: 0.3253752589225769, Test_Loss: 0.30694618821144104\n",
      "Epoch: 17, Train_Loss: 0.24778424203395844, Test_Loss: 0.5545805096626282\n",
      "Epoch: 17, Train_Loss: 0.23398999869823456, Test_Loss: 0.464730441570282 *\n",
      "Epoch: 17, Train_Loss: 0.32325881719589233, Test_Loss: 0.4539247751235962 *\n",
      "Epoch: 17, Train_Loss: 0.5307865142822266, Test_Loss: 0.40014785528182983 *\n",
      "Epoch: 17, Train_Loss: 0.4879162311553955, Test_Loss: 0.457192599773407\n",
      "Epoch: 17, Train_Loss: 0.3143724203109741, Test_Loss: 0.6444694995880127\n",
      "Epoch: 17, Train_Loss: 0.42869532108306885, Test_Loss: 0.578619658946991 *\n",
      "Epoch: 17, Train_Loss: 0.23081941902637482, Test_Loss: 0.40512704849243164 *\n",
      "Epoch: 17, Train_Loss: 0.20740251243114471, Test_Loss: 0.3490463197231293 *\n",
      "Epoch: 17, Train_Loss: 0.23351620137691498, Test_Loss: 0.3158653974533081 *\n",
      "Epoch: 17, Train_Loss: 0.23528024554252625, Test_Loss: 0.227927103638649 *\n",
      "Epoch: 17, Train_Loss: 0.23527313768863678, Test_Loss: 0.30087828636169434\n",
      "Epoch: 17, Train_Loss: 0.27941977977752686, Test_Loss: 0.3580932021141052\n",
      "Epoch: 17, Train_Loss: 12.608343124389648, Test_Loss: 0.2585603594779968 *\n",
      "Epoch: 17, Train_Loss: 3.016157865524292, Test_Loss: 0.30880463123321533\n",
      "Epoch: 17, Train_Loss: 1.2267643213272095, Test_Loss: 0.24751676619052887 *\n",
      "Epoch: 17, Train_Loss: 0.9329657554626465, Test_Loss: 0.22552019357681274 *\n",
      "Epoch: 17, Train_Loss: 0.30120429396629333, Test_Loss: 0.2226787507534027 *\n",
      "Epoch: 17, Train_Loss: 0.26420557498931885, Test_Loss: 0.25253501534461975\n",
      "Epoch: 17, Train_Loss: 0.9875847101211548, Test_Loss: 0.3921712636947632\n",
      "Epoch: 17, Train_Loss: 6.229696750640869, Test_Loss: 0.3268856406211853 *\n",
      "Epoch: 17, Train_Loss: 0.8602503538131714, Test_Loss: 0.34917718172073364\n",
      "Epoch: 17, Train_Loss: 0.24641965329647064, Test_Loss: 0.35337305068969727\n",
      "Epoch: 17, Train_Loss: 3.4255034923553467, Test_Loss: 0.5115358233451843\n",
      "Epoch: 17, Train_Loss: 1.7452638149261475, Test_Loss: 0.9213190078735352\n",
      "Epoch: 17, Train_Loss: 0.5695779323577881, Test_Loss: 1.0590482950210571\n",
      "Epoch: 17, Train_Loss: 0.22044247388839722, Test_Loss: 0.9484040141105652 *\n",
      "Epoch: 17, Train_Loss: 0.22074522078037262, Test_Loss: 0.4467805027961731 *\n",
      "Epoch: 17, Train_Loss: 0.2576431632041931, Test_Loss: 0.4590356945991516\n",
      "Epoch: 17, Train_Loss: 0.24512161314487457, Test_Loss: 0.8186514377593994\n",
      "Epoch: 17, Train_Loss: 0.20780989527702332, Test_Loss: 1.3908480405807495\n",
      "Epoch: 17, Train_Loss: 0.2034621387720108, Test_Loss: 2.513230323791504\n",
      "Epoch: 17, Train_Loss: 0.20337796211242676, Test_Loss: 0.7690359354019165 *\n",
      "Epoch: 17, Train_Loss: 0.23876050114631653, Test_Loss: 0.32674217224121094 *\n",
      "Epoch: 17, Train_Loss: 0.24140015244483948, Test_Loss: 0.2526334226131439 *\n",
      "Epoch: 17, Train_Loss: 0.2573647201061249, Test_Loss: 0.229091614484787 *\n",
      "Epoch: 17, Train_Loss: 0.2315572202205658, Test_Loss: 0.2120196670293808 *\n",
      "Epoch: 17, Train_Loss: 0.2253802865743637, Test_Loss: 0.21076111495494843 *\n",
      "Epoch: 17, Train_Loss: 0.21408560872077942, Test_Loss: 0.24574406445026398\n",
      "Epoch: 17, Train_Loss: 0.2181628942489624, Test_Loss: 8.26105785369873\n",
      "Epoch: 17, Train_Loss: 0.20723585784435272, Test_Loss: 0.6205504536628723 *\n",
      "Epoch: 17, Train_Loss: 0.22258910536766052, Test_Loss: 0.43830522894859314 *\n",
      "Epoch: 17, Train_Loss: 0.2044396698474884, Test_Loss: 0.29908517003059387 *\n",
      "Epoch: 17, Train_Loss: 0.20521022379398346, Test_Loss: 0.29462844133377075 *\n",
      "Epoch: 17, Train_Loss: 0.20333793759346008, Test_Loss: 0.22148628532886505 *\n",
      "Epoch: 17, Train_Loss: 0.2041560858488083, Test_Loss: 0.45744550228118896\n",
      "Epoch: 17, Train_Loss: 0.2042049765586853, Test_Loss: 0.4457398056983948 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.20364722609519958, Test_Loss: 0.25140616297721863 *\n",
      "Epoch: 17, Train_Loss: 0.20333029329776764, Test_Loss: 0.21919263899326324 *\n",
      "Epoch: 17, Train_Loss: 0.2086276412010193, Test_Loss: 0.2862403988838196\n",
      "Epoch: 17, Train_Loss: 0.2347705066204071, Test_Loss: 0.5897688269615173\n",
      "Epoch: 17, Train_Loss: 0.24026194214820862, Test_Loss: 0.5057957172393799 *\n",
      "Epoch: 17, Train_Loss: 0.3149397075176239, Test_Loss: 0.6244118809700012\n",
      "Epoch: 17, Train_Loss: 0.2211284637451172, Test_Loss: 0.686467170715332\n",
      "Epoch: 17, Train_Loss: 0.33870166540145874, Test_Loss: 0.23933719098567963 *\n",
      "Epoch: 17, Train_Loss: 7.911188125610352, Test_Loss: 0.20995114743709564 *\n",
      "Epoch: 17, Train_Loss: 0.37469154596328735, Test_Loss: 0.21866820752620697\n",
      "Epoch: 17, Train_Loss: 0.21472163498401642, Test_Loss: 0.26555168628692627\n",
      "Epoch: 17, Train_Loss: 0.23424728214740753, Test_Loss: 0.25454771518707275 *\n",
      "Epoch: 17, Train_Loss: 0.27639514207839966, Test_Loss: 0.321512907743454\n",
      "Epoch: 17, Train_Loss: 0.22557437419891357, Test_Loss: 0.2627711892127991 *\n",
      "Epoch: 17, Train_Loss: 0.23264382779598236, Test_Loss: 0.42095786333084106\n",
      "Epoch: 17, Train_Loss: 0.2718234062194824, Test_Loss: 0.3206273317337036 *\n",
      "Epoch: 17, Train_Loss: 0.3509577512741089, Test_Loss: 0.37330901622772217\n",
      "Epoch: 17, Train_Loss: 0.3182857036590576, Test_Loss: 0.279973566532135 *\n",
      "Epoch: 17, Train_Loss: 0.26898473501205444, Test_Loss: 0.27823397517204285 *\n",
      "Epoch: 17, Train_Loss: 0.2047135978937149, Test_Loss: 0.315132200717926\n",
      "Epoch: 17, Train_Loss: 0.2684568762779236, Test_Loss: 0.26163655519485474 *\n",
      "Epoch: 17, Train_Loss: 0.24187009036540985, Test_Loss: 0.24160891771316528 *\n",
      "Epoch: 17, Train_Loss: 0.6988310813903809, Test_Loss: 0.362873375415802\n",
      "Epoch: 17, Train_Loss: 0.37090182304382324, Test_Loss: 1.3482104539871216\n",
      "Epoch: 17, Train_Loss: 0.28877776861190796, Test_Loss: 5.733364582061768\n",
      "Epoch: 17, Train_Loss: 0.23234957456588745, Test_Loss: 0.24282483756542206 *\n",
      "Epoch: 17, Train_Loss: 0.22423428297042847, Test_Loss: 0.20471081137657166 *\n",
      "Epoch: 17, Train_Loss: 0.2542550265789032, Test_Loss: 0.22520489990711212\n",
      "Epoch: 17, Train_Loss: 0.23178038001060486, Test_Loss: 0.20920124650001526 *\n",
      "Epoch: 17, Train_Loss: 0.22782650589942932, Test_Loss: 0.21814273297786713\n",
      "Epoch: 17, Train_Loss: 0.22067739069461823, Test_Loss: 0.2321278601884842\n",
      "Epoch: 17, Train_Loss: 0.22321060299873352, Test_Loss: 0.33990123867988586\n",
      "Epoch: 17, Train_Loss: 0.318145751953125, Test_Loss: 0.2314291000366211 *\n",
      "Epoch: 17, Train_Loss: 5.2629265785217285, Test_Loss: 0.2098149210214615 *\n",
      "Epoch: 17, Train_Loss: 0.21472130715847015, Test_Loss: 0.24787867069244385\n",
      "Epoch: 17, Train_Loss: 0.21773961186408997, Test_Loss: 0.21307845413684845 *\n",
      "Epoch: 17, Train_Loss: 0.22940999269485474, Test_Loss: 0.21766754984855652\n",
      "Epoch: 17, Train_Loss: 0.2113860547542572, Test_Loss: 0.24383889138698578\n",
      "Epoch: 17, Train_Loss: 0.20627301931381226, Test_Loss: 0.23397862911224365 *\n",
      "Epoch: 17, Train_Loss: 0.2085324376821518, Test_Loss: 0.3049408495426178\n",
      "Epoch: 17, Train_Loss: 0.21104100346565247, Test_Loss: 0.33559486269950867\n",
      "Epoch: 17, Train_Loss: 0.21817836165428162, Test_Loss: 0.237770676612854 *\n",
      "Epoch: 17, Train_Loss: 0.21829836070537567, Test_Loss: 0.22442549467086792 *\n",
      "Epoch: 17, Train_Loss: 0.22479160130023956, Test_Loss: 0.20394426584243774 *\n",
      "Epoch: 17, Train_Loss: 0.20843417942523956, Test_Loss: 0.20477667450904846\n",
      "Epoch: 17, Train_Loss: 0.20557770133018494, Test_Loss: 0.20372873544692993 *\n",
      "Epoch: 17, Train_Loss: 0.22204440832138062, Test_Loss: 0.20382876694202423\n",
      "Epoch: 17, Train_Loss: 0.20496922731399536, Test_Loss: 0.20356890559196472 *\n",
      "Epoch: 17, Train_Loss: 0.20883722603321075, Test_Loss: 0.20394527912139893\n",
      "Epoch: 17, Train_Loss: 0.2335173338651657, Test_Loss: 0.2058640867471695\n",
      "Epoch: 17, Train_Loss: 0.2531510889530182, Test_Loss: 0.20527133345603943 *\n",
      "Epoch: 17, Train_Loss: 0.21938301622867584, Test_Loss: 0.20463980734348297 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train_Loss: 0.2029275894165039, Test_Loss: 0.22636912763118744\n",
      "Epoch: 17, Train_Loss: 0.20291438698768616, Test_Loss: 0.20616018772125244 *\n",
      "Epoch: 17, Train_Loss: 0.2672324478626251, Test_Loss: 0.21685181558132172\n",
      "Epoch: 17, Train_Loss: 0.22360533475875854, Test_Loss: 0.2267431914806366\n",
      "Epoch: 17, Train_Loss: 0.2367801070213318, Test_Loss: 0.5243333578109741\n",
      "Epoch: 17, Train_Loss: 0.23546817898750305, Test_Loss: 0.5405575037002563\n",
      "Epoch: 17, Train_Loss: 0.23941238224506378, Test_Loss: 0.31438764929771423 *\n",
      "Epoch: 17, Train_Loss: 0.24803219735622406, Test_Loss: 0.22084972262382507 *\n",
      "Epoch: 17, Train_Loss: 0.25879108905792236, Test_Loss: 0.22365006804466248\n",
      "Epoch: 17, Train_Loss: 0.24134664237499237, Test_Loss: 0.22546601295471191\n",
      "Epoch: 17, Train_Loss: 0.40889453887939453, Test_Loss: 0.3051339089870453\n",
      "Epoch: 17, Train_Loss: 0.2303917407989502, Test_Loss: 0.5100533366203308\n",
      "Epoch: 17, Train_Loss: 0.21427485346794128, Test_Loss: 0.5933666229248047\n",
      "Epoch: 17, Train_Loss: 0.20340025424957275, Test_Loss: 0.2587600648403168 *\n",
      "Epoch: 17, Train_Loss: 0.20194955170154572, Test_Loss: 0.2683936357498169\n",
      "Epoch: 17, Train_Loss: 0.2017533928155899, Test_Loss: 0.20431731641292572 *\n",
      "Epoch: 17, Train_Loss: 0.20197694003582, Test_Loss: 0.20899678766727448\n",
      "Epoch: 17, Train_Loss: 0.28520143032073975, Test_Loss: 0.21049892902374268\n",
      "Epoch: 17, Train_Loss: 4.52841854095459, Test_Loss: 0.21643133461475372\n",
      "Epoch: 17, Train_Loss: 0.2849048376083374, Test_Loss: 0.24819721281528473\n",
      "Epoch: 17, Train_Loss: 0.20680707693099976, Test_Loss: 0.21539361774921417 *\n",
      "Epoch: 17, Train_Loss: 0.20930320024490356, Test_Loss: 0.2128695696592331 *\n",
      "Epoch: 17, Train_Loss: 0.2034696787595749, Test_Loss: 0.33703213930130005\n",
      "Epoch: 17, Train_Loss: 0.20406489074230194, Test_Loss: 0.6386284232139587\n",
      "Epoch: 17, Train_Loss: 0.20382024347782135, Test_Loss: 0.46450871229171753 *\n",
      "Epoch: 17, Train_Loss: 0.20279628038406372, Test_Loss: 0.2835906744003296 *\n",
      "Epoch: 17, Train_Loss: 0.20383955538272858, Test_Loss: 0.25147587060928345 *\n",
      "Epoch: 17, Train_Loss: 0.20408225059509277, Test_Loss: 0.25245678424835205\n",
      "Epoch: 17, Train_Loss: 0.24565845727920532, Test_Loss: 0.25652754306793213\n",
      "Epoch: 17, Train_Loss: 0.23496806621551514, Test_Loss: 0.2580522894859314\n",
      "Epoch: 17, Train_Loss: 0.2665751576423645, Test_Loss: 0.41798460483551025\n",
      "Epoch: 17, Train_Loss: 0.23555132746696472, Test_Loss: 4.8842315673828125\n",
      "Epoch: 17, Train_Loss: 0.20856131613254547, Test_Loss: 0.27463024854660034 *\n",
      "Epoch: 17, Train_Loss: 0.3556879162788391, Test_Loss: 0.21871694922447205 *\n",
      "Epoch: 17, Train_Loss: 0.4544126093387604, Test_Loss: 0.21361692249774933 *\n",
      "Epoch: 17, Train_Loss: 0.44837772846221924, Test_Loss: 0.21964304149150848\n",
      "Epoch: 17, Train_Loss: 0.4253300428390503, Test_Loss: 0.21330076456069946 *\n",
      "Epoch: 17, Train_Loss: 0.20365498960018158, Test_Loss: 0.20905959606170654 *\n",
      "Epoch: 17, Train_Loss: 0.20424139499664307, Test_Loss: 0.20292384922504425 *\n",
      "Epoch: 17, Train_Loss: 0.20482712984085083, Test_Loss: 0.20421640574932098\n",
      "Epoch: 17, Train_Loss: 0.21669164299964905, Test_Loss: 0.20421496033668518 *\n",
      "Epoch: 17, Train_Loss: 0.21760009229183197, Test_Loss: 0.20295119285583496 *\n",
      "Epoch: 17, Train_Loss: 0.21471206843852997, Test_Loss: 0.20959554612636566\n",
      "Epoch: 17, Train_Loss: 0.2071838527917862, Test_Loss: 0.22540146112442017\n",
      "Epoch: 17, Train_Loss: 0.2013135850429535, Test_Loss: 0.2355404943227768\n",
      "Epoch: 17, Train_Loss: 0.20968037843704224, Test_Loss: 0.2223244607448578 *\n",
      "Epoch: 17, Train_Loss: 0.21918466687202454, Test_Loss: 0.203942209482193 *\n",
      "Epoch: 17, Train_Loss: 0.3859894275665283, Test_Loss: 0.20243306457996368 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 17\n",
      "Epoch: 17, Train_Loss: 0.3851509094238281, Test_Loss: 0.20215851068496704 *\n",
      "Epoch: 17, Train_Loss: 0.4007423520088196, Test_Loss: 0.20285747945308685\n",
      "Epoch: 17, Train_Loss: 0.23458148539066315, Test_Loss: 0.2015579342842102 *\n",
      "Epoch: 17, Train_Loss: 0.3312520980834961, Test_Loss: 0.20268070697784424\n",
      "Epoch: 17, Train_Loss: 0.3093511760234833, Test_Loss: 0.20166851580142975 *\n",
      "Epoch: 17, Train_Loss: 0.26662737131118774, Test_Loss: 0.2033548355102539\n",
      "Epoch: 17, Train_Loss: 0.3131232261657715, Test_Loss: 0.20340971648693085\n",
      "Epoch: 17, Train_Loss: 0.4218789339065552, Test_Loss: 0.2031579315662384 *\n",
      "Epoch: 17, Train_Loss: 0.2731611728668213, Test_Loss: 0.2024690955877304 *\n",
      "Epoch: 17, Train_Loss: 0.21168364584445953, Test_Loss: 0.20166413486003876 *\n",
      "Epoch: 17, Train_Loss: 2.451118230819702, Test_Loss: 0.20140685141086578 *\n",
      "Epoch: 17, Train_Loss: 1.0216245651245117, Test_Loss: 0.20160628855228424\n",
      "Epoch: 17, Train_Loss: 0.24918994307518005, Test_Loss: 0.20583684742450714\n",
      "Epoch: 17, Train_Loss: 0.26430708169937134, Test_Loss: 0.2622281014919281\n",
      "Epoch: 17, Train_Loss: 0.25178083777427673, Test_Loss: 2.3276686668395996\n",
      "Epoch: 17, Train_Loss: 0.23977269232273102, Test_Loss: 3.4358198642730713\n",
      "Epoch: 17, Train_Loss: 0.20288805663585663, Test_Loss: 0.20632226765155792 *\n",
      "Epoch: 17, Train_Loss: 0.2279832661151886, Test_Loss: 0.20128858089447021 *\n",
      "Epoch: 17, Train_Loss: 0.3104476034641266, Test_Loss: 0.25917530059814453\n",
      "Epoch: 17, Train_Loss: 0.27219176292419434, Test_Loss: 0.2583339214324951 *\n",
      "Epoch: 17, Train_Loss: 0.2578977942466736, Test_Loss: 0.25419604778289795 *\n",
      "Epoch: 17, Train_Loss: 0.2514858841896057, Test_Loss: 0.23293042182922363 *\n",
      "Epoch: 17, Train_Loss: 0.22732116281986237, Test_Loss: 0.278308629989624\n",
      "Epoch: 17, Train_Loss: 0.2226649671792984, Test_Loss: 0.2077949345111847 *\n",
      "Epoch: 17, Train_Loss: 0.21712328493595123, Test_Loss: 0.21752020716667175\n",
      "Epoch: 17, Train_Loss: 0.237113818526268, Test_Loss: 0.22265636920928955\n",
      "Epoch: 17, Train_Loss: 0.22137045860290527, Test_Loss: 0.22577884793281555\n",
      "Epoch: 17, Train_Loss: 0.20469173789024353, Test_Loss: 0.20854046940803528 *\n",
      "Epoch: 17, Train_Loss: 0.20702317357063293, Test_Loss: 0.28694993257522583\n",
      "Epoch: 17, Train_Loss: 0.2398955225944519, Test_Loss: 0.3072197437286377\n",
      "Epoch: 17, Train_Loss: 0.24394872784614563, Test_Loss: 0.2549327611923218 *\n",
      "Epoch: 17, Train_Loss: 0.20966604351997375, Test_Loss: 0.2577390968799591\n",
      "Epoch: 17, Train_Loss: 0.2006322294473648, Test_Loss: 0.22827006876468658 *\n",
      "Epoch: 17, Train_Loss: 0.2013217955827713, Test_Loss: 0.24855920672416687\n",
      "Epoch: 17, Train_Loss: 0.20054930448532104, Test_Loss: 0.21876849234104156 *\n",
      "Epoch: 17, Train_Loss: 0.2013404667377472, Test_Loss: 0.2191452533006668\n",
      "Epoch: 17, Train_Loss: 0.20127348601818085, Test_Loss: 0.21946308016777039\n",
      "Epoch: 17, Train_Loss: 0.2021588683128357, Test_Loss: 0.22416478395462036\n",
      "Epoch: 17, Train_Loss: 0.2021975964307785, Test_Loss: 0.2207135558128357 *\n",
      "Epoch: 17, Train_Loss: 0.20097382366657257, Test_Loss: 0.21705593168735504 *\n",
      "Epoch: 17, Train_Loss: 0.20055659115314484, Test_Loss: 0.226899191737175\n",
      "Epoch: 17, Train_Loss: 0.2023131549358368, Test_Loss: 0.22130778431892395 *\n",
      "Epoch: 17, Train_Loss: 0.21599334478378296, Test_Loss: 0.22570495307445526\n",
      "Epoch: 17, Train_Loss: 0.22011816501617432, Test_Loss: 0.20345087349414825 *\n",
      "Epoch: 17, Train_Loss: 0.21391722559928894, Test_Loss: 0.22822433710098267\n",
      "Epoch: 17, Train_Loss: 0.22421953082084656, Test_Loss: 0.27348875999450684\n",
      "Epoch: 17, Train_Loss: 0.218572199344635, Test_Loss: 0.22189128398895264 *\n",
      "Epoch: 17, Train_Loss: 0.20609790086746216, Test_Loss: 0.7391409873962402\n",
      "Epoch: 17, Train_Loss: 0.20281440019607544, Test_Loss: 0.6477199196815491 *\n",
      "Epoch: 17, Train_Loss: 0.20995032787322998, Test_Loss: 0.3340783715248108 *\n",
      "Epoch: 17, Train_Loss: 0.22692355513572693, Test_Loss: 0.21632972359657288 *\n",
      "Epoch: 17, Train_Loss: 0.20425410568714142, Test_Loss: 0.22723637521266937\n",
      "Epoch: 17, Train_Loss: 0.20459963381290436, Test_Loss: 0.2170991450548172 *\n",
      "Epoch: 17, Train_Loss: 0.20121051371097565, Test_Loss: 0.32424065470695496\n",
      "Epoch: 18, Train_Loss: 0.2183748483657837, Test_Loss: 0.2709861397743225 *\n",
      "Epoch: 18, Train_Loss: 0.25183969736099243, Test_Loss: 0.5025204420089722\n",
      "Epoch: 18, Train_Loss: 0.2542547881603241, Test_Loss: 0.27496716380119324 *\n",
      "Epoch: 18, Train_Loss: 0.22495144605636597, Test_Loss: 0.25822412967681885 *\n",
      "Epoch: 18, Train_Loss: 0.1998744010925293, Test_Loss: 0.20454472303390503 *\n",
      "Epoch: 18, Train_Loss: 0.2552590072154999, Test_Loss: 0.2019318789243698 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.21622532606124878, Test_Loss: 0.20931412279605865\n",
      "Epoch: 18, Train_Loss: 0.20466819405555725, Test_Loss: 0.2082596719264984 *\n",
      "Epoch: 18, Train_Loss: 0.20372550189495087, Test_Loss: 0.24684825539588928\n",
      "Epoch: 18, Train_Loss: 0.23723351955413818, Test_Loss: 0.20667360723018646 *\n",
      "Epoch: 18, Train_Loss: 0.28820475935935974, Test_Loss: 0.240560844540596\n",
      "Epoch: 18, Train_Loss: 0.250497967004776, Test_Loss: 0.34347766637802124\n",
      "Epoch: 18, Train_Loss: 0.2298893928527832, Test_Loss: 0.5926729440689087\n",
      "Epoch: 18, Train_Loss: 0.21725237369537354, Test_Loss: 0.5357886552810669 *\n",
      "Epoch: 18, Train_Loss: 0.21160435676574707, Test_Loss: 0.3002851605415344 *\n",
      "Epoch: 18, Train_Loss: 0.21343815326690674, Test_Loss: 0.2812614142894745 *\n",
      "Epoch: 18, Train_Loss: 0.20207232236862183, Test_Loss: 0.27696388959884644 *\n",
      "Epoch: 18, Train_Loss: 0.2104908972978592, Test_Loss: 0.2771417498588562\n",
      "Epoch: 18, Train_Loss: 0.2105126827955246, Test_Loss: 0.27984189987182617\n",
      "Epoch: 18, Train_Loss: 0.22590026259422302, Test_Loss: 1.0778567790985107\n",
      "Epoch: 18, Train_Loss: 0.3098500967025757, Test_Loss: 4.323840141296387\n",
      "Epoch: 18, Train_Loss: 0.2024504840373993, Test_Loss: 0.22388581931591034 *\n",
      "Epoch: 18, Train_Loss: 0.2524145841598511, Test_Loss: 0.2068481743335724 *\n",
      "Epoch: 18, Train_Loss: 0.21568210422992706, Test_Loss: 0.20554490387439728 *\n",
      "Epoch: 18, Train_Loss: 0.23416684567928314, Test_Loss: 0.2101849764585495\n",
      "Epoch: 18, Train_Loss: 0.28273335099220276, Test_Loss: 0.20414835214614868 *\n",
      "Epoch: 18, Train_Loss: 0.4391942620277405, Test_Loss: 0.20748865604400635\n",
      "Epoch: 18, Train_Loss: 0.2113509327173233, Test_Loss: 0.20420777797698975 *\n",
      "Epoch: 18, Train_Loss: 0.22926177084445953, Test_Loss: 0.2031368613243103 *\n",
      "Epoch: 18, Train_Loss: 0.199419766664505, Test_Loss: 0.20250724256038666 *\n",
      "Epoch: 18, Train_Loss: 0.19961702823638916, Test_Loss: 0.20760585367679596\n",
      "Epoch: 18, Train_Loss: 0.20674680173397064, Test_Loss: 0.24910959601402283\n",
      "Epoch: 18, Train_Loss: 0.2023637741804123, Test_Loss: 0.21746671199798584 *\n",
      "Epoch: 18, Train_Loss: 0.20419827103614807, Test_Loss: 0.21223726868629456 *\n",
      "Epoch: 18, Train_Loss: 0.21105289459228516, Test_Loss: 0.21939271688461304\n",
      "Epoch: 18, Train_Loss: 0.20841340720653534, Test_Loss: 0.2030295729637146 *\n",
      "Epoch: 18, Train_Loss: 0.20505118370056152, Test_Loss: 0.20007172226905823 *\n",
      "Epoch: 18, Train_Loss: 0.21028189361095428, Test_Loss: 0.20105840265750885\n",
      "Epoch: 18, Train_Loss: 0.20741577446460724, Test_Loss: 0.20156815648078918\n",
      "Epoch: 18, Train_Loss: 0.20028172433376312, Test_Loss: 0.19971005618572235 *\n",
      "Epoch: 18, Train_Loss: 0.1989070177078247, Test_Loss: 0.2014327198266983\n",
      "Epoch: 18, Train_Loss: 0.21682368218898773, Test_Loss: 0.19906960427761078 *\n",
      "Epoch: 18, Train_Loss: 0.21569067239761353, Test_Loss: 0.20209480822086334\n",
      "Epoch: 18, Train_Loss: 0.2243572622537613, Test_Loss: 0.2011370211839676 *\n",
      "Epoch: 18, Train_Loss: 0.2000802755355835, Test_Loss: 0.20092347264289856 *\n",
      "Epoch: 18, Train_Loss: 0.2327895164489746, Test_Loss: 0.20020665228366852 *\n",
      "Epoch: 18, Train_Loss: 0.224439337849617, Test_Loss: 0.19880367815494537 *\n",
      "Epoch: 18, Train_Loss: 0.226706862449646, Test_Loss: 0.19945664703845978\n",
      "Epoch: 18, Train_Loss: 0.205110564827919, Test_Loss: 0.19897356629371643 *\n",
      "Epoch: 18, Train_Loss: 0.23182430863380432, Test_Loss: 0.21860350668430328\n",
      "Epoch: 18, Train_Loss: 0.19872286915779114, Test_Loss: 0.2490769624710083\n",
      "Epoch: 18, Train_Loss: 0.21250633895397186, Test_Loss: 3.3983912467956543\n",
      "Epoch: 18, Train_Loss: 0.21092364192008972, Test_Loss: 2.578409194946289 *\n",
      "Epoch: 18, Train_Loss: 0.22389863431453705, Test_Loss: 0.20296268165111542 *\n",
      "Epoch: 18, Train_Loss: 1.2907878160476685, Test_Loss: 0.20118604600429535 *\n",
      "Epoch: 18, Train_Loss: 4.226762771606445, Test_Loss: 0.2446703016757965\n",
      "Epoch: 18, Train_Loss: 0.5709887146949768, Test_Loss: 0.24049100279808044 *\n",
      "Epoch: 18, Train_Loss: 0.22615715861320496, Test_Loss: 0.22486989200115204 *\n",
      "Epoch: 18, Train_Loss: 0.20932596921920776, Test_Loss: 0.2575511336326599\n",
      "Epoch: 18, Train_Loss: 0.34067994356155396, Test_Loss: 0.2853466868400574\n",
      "Epoch: 18, Train_Loss: 0.26642340421676636, Test_Loss: 0.20088979601860046 *\n",
      "Epoch: 18, Train_Loss: 0.21781331300735474, Test_Loss: 0.2273346334695816\n",
      "Epoch: 18, Train_Loss: 0.1996258795261383, Test_Loss: 0.21831242740154266 *\n",
      "Epoch: 18, Train_Loss: 0.2639850676059723, Test_Loss: 0.21466390788555145 *\n",
      "Epoch: 18, Train_Loss: 0.22191928327083588, Test_Loss: 0.20498836040496826 *\n",
      "Epoch: 18, Train_Loss: 0.2079322338104248, Test_Loss: 0.2746894359588623\n",
      "Epoch: 18, Train_Loss: 0.488038569688797, Test_Loss: 0.25828906893730164 *\n",
      "Epoch: 18, Train_Loss: 0.8123155236244202, Test_Loss: 0.2776612937450409\n",
      "Epoch: 18, Train_Loss: 1.027787446975708, Test_Loss: 0.2537907660007477 *\n",
      "Epoch: 18, Train_Loss: 0.24725250899791718, Test_Loss: 0.2326432466506958 *\n",
      "Epoch: 18, Train_Loss: 0.36626386642456055, Test_Loss: 0.22126346826553345 *\n",
      "Epoch: 18, Train_Loss: 1.7479794025421143, Test_Loss: 0.20140311121940613 *\n",
      "Epoch: 18, Train_Loss: 0.8793744444847107, Test_Loss: 0.20581410825252533\n",
      "Epoch: 18, Train_Loss: 0.21082790195941925, Test_Loss: 0.20881935954093933\n",
      "Epoch: 18, Train_Loss: 0.20677655935287476, Test_Loss: 0.2159360945224762\n",
      "Epoch: 18, Train_Loss: 0.6484431624412537, Test_Loss: 0.21302294731140137 *\n",
      "Epoch: 18, Train_Loss: 0.5067055225372314, Test_Loss: 0.2128005027770996 *\n",
      "Epoch: 18, Train_Loss: 0.7225285172462463, Test_Loss: 0.2613586187362671\n",
      "Epoch: 18, Train_Loss: 0.21508066356182098, Test_Loss: 0.2695954740047455\n",
      "Epoch: 18, Train_Loss: 0.2242863029241562, Test_Loss: 0.23753656446933746 *\n",
      "Epoch: 18, Train_Loss: 0.42979609966278076, Test_Loss: 0.23665189743041992 *\n",
      "Epoch: 18, Train_Loss: 0.41609376668930054, Test_Loss: 0.20696218311786652 *\n",
      "Epoch: 18, Train_Loss: 0.2589265704154968, Test_Loss: 0.23436644673347473\n",
      "Epoch: 18, Train_Loss: 0.2806995213031769, Test_Loss: 0.29939180612564087\n",
      "Epoch: 18, Train_Loss: 0.23932188749313354, Test_Loss: 0.33313608169555664\n",
      "Epoch: 18, Train_Loss: 0.24290134012699127, Test_Loss: 0.4442302882671356\n",
      "Epoch: 18, Train_Loss: 0.3966033160686493, Test_Loss: 0.2875003218650818 *\n",
      "Epoch: 18, Train_Loss: 0.27433690428733826, Test_Loss: 0.22835108637809753 *\n",
      "Epoch: 18, Train_Loss: 0.23985330760478973, Test_Loss: 0.23899249732494354\n",
      "Epoch: 18, Train_Loss: 0.2911655902862549, Test_Loss: 0.22349725663661957 *\n",
      "Epoch: 18, Train_Loss: 0.3213171362876892, Test_Loss: 0.3974399268627167\n",
      "Epoch: 18, Train_Loss: 0.2702946066856384, Test_Loss: 0.2397845983505249 *\n",
      "Epoch: 18, Train_Loss: 0.3008671998977661, Test_Loss: 0.583519697189331\n",
      "Epoch: 18, Train_Loss: 0.40643611550331116, Test_Loss: 0.2901182174682617 *\n",
      "Epoch: 18, Train_Loss: 0.24684421718120575, Test_Loss: 0.2676016688346863 *\n",
      "Epoch: 18, Train_Loss: 0.3192897439002991, Test_Loss: 0.236354798078537 *\n",
      "Epoch: 18, Train_Loss: 0.3192980885505676, Test_Loss: 0.2524584233760834\n",
      "Epoch: 18, Train_Loss: 0.21387845277786255, Test_Loss: 0.2858843207359314\n",
      "Epoch: 18, Train_Loss: 0.19791024923324585, Test_Loss: 0.2094360888004303 *\n",
      "Epoch: 18, Train_Loss: 0.20154042541980743, Test_Loss: 0.2310679703950882\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.203470379114151, Test_Loss: 0.2071770578622818 *\n",
      "Epoch: 18, Train_Loss: 0.203754723072052, Test_Loss: 0.3069402873516083\n",
      "Epoch: 18, Train_Loss: 0.22199366986751556, Test_Loss: 0.43020081520080566\n",
      "Epoch: 18, Train_Loss: 0.2449290156364441, Test_Loss: 0.4470283091068268\n",
      "Epoch: 18, Train_Loss: 0.25030800700187683, Test_Loss: 0.674330472946167\n",
      "Epoch: 18, Train_Loss: 0.27066606283187866, Test_Loss: 0.3754890561103821 *\n",
      "Epoch: 18, Train_Loss: 0.38747891783714294, Test_Loss: 0.37207940220832825 *\n",
      "Epoch: 18, Train_Loss: 0.40013688802719116, Test_Loss: 0.3672026991844177 *\n",
      "Epoch: 18, Train_Loss: 0.21535226702690125, Test_Loss: 0.36853474378585815\n",
      "Epoch: 18, Train_Loss: 0.2654891908168793, Test_Loss: 0.5338574647903442\n",
      "Epoch: 18, Train_Loss: 0.39126455783843994, Test_Loss: 2.2252109050750732\n",
      "Epoch: 18, Train_Loss: 0.31457510590553284, Test_Loss: 3.1836743354797363\n",
      "Epoch: 18, Train_Loss: 0.25608736276626587, Test_Loss: 0.25650790333747864 *\n",
      "Epoch: 18, Train_Loss: 0.2515786290168762, Test_Loss: 0.22106026113033295 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.429996132850647, Test_Loss: 0.22895139455795288\n",
      "Epoch: 18, Train_Loss: 0.3215598464012146, Test_Loss: 0.2095477133989334 *\n",
      "Epoch: 18, Train_Loss: 0.33579015731811523, Test_Loss: 0.279866099357605\n",
      "Epoch: 18, Train_Loss: 0.2286360263824463, Test_Loss: 0.3022371828556061\n",
      "Epoch: 18, Train_Loss: 0.22841434180736542, Test_Loss: 0.24032995104789734 *\n",
      "Epoch: 18, Train_Loss: 0.508938729763031, Test_Loss: 0.20448091626167297 *\n",
      "Epoch: 18, Train_Loss: 0.7215862274169922, Test_Loss: 0.21316932141780853\n",
      "Epoch: 18, Train_Loss: 0.3290315270423889, Test_Loss: 0.22621232271194458\n",
      "Epoch: 18, Train_Loss: 0.24050994217395782, Test_Loss: 0.3385975956916809\n",
      "Epoch: 18, Train_Loss: 0.20367112755775452, Test_Loss: 0.21585536003112793 *\n",
      "Epoch: 18, Train_Loss: 0.20498499274253845, Test_Loss: 0.220650315284729\n",
      "Epoch: 18, Train_Loss: 0.4863952696323395, Test_Loss: 0.2280724048614502\n",
      "Epoch: 18, Train_Loss: 0.25110924243927, Test_Loss: 0.21258339285850525 *\n",
      "Epoch: 18, Train_Loss: 0.2262902557849884, Test_Loss: 0.22879968583583832\n",
      "Epoch: 18, Train_Loss: 0.3540981411933899, Test_Loss: 0.28397059440612793\n",
      "Epoch: 18, Train_Loss: 0.2693502902984619, Test_Loss: 0.2575889825820923 *\n",
      "Epoch: 18, Train_Loss: 0.22396719455718994, Test_Loss: 0.20708337426185608 *\n",
      "Epoch: 18, Train_Loss: 0.265921950340271, Test_Loss: 0.23188689351081848\n",
      "Epoch: 18, Train_Loss: 0.32228589057922363, Test_Loss: 0.20344272255897522 *\n",
      "Epoch: 18, Train_Loss: 0.2495558261871338, Test_Loss: 0.30328983068466187\n",
      "Epoch: 18, Train_Loss: 0.29450395703315735, Test_Loss: 0.26643118262290955 *\n",
      "Epoch: 18, Train_Loss: 0.2112799882888794, Test_Loss: 0.20954161882400513 *\n",
      "Epoch: 18, Train_Loss: 0.32696521282196045, Test_Loss: 0.20441079139709473 *\n",
      "Epoch: 18, Train_Loss: 0.2541915476322174, Test_Loss: 0.22818727791309357\n",
      "Epoch: 18, Train_Loss: 0.22070878744125366, Test_Loss: 0.21870043873786926 *\n",
      "Epoch: 18, Train_Loss: 0.2105407416820526, Test_Loss: 0.20289629697799683 *\n",
      "Epoch: 18, Train_Loss: 0.24247589707374573, Test_Loss: 0.28787845373153687\n",
      "Epoch: 18, Train_Loss: 0.4871840476989746, Test_Loss: 0.3374098539352417\n",
      "Epoch: 18, Train_Loss: 0.45216017961502075, Test_Loss: 4.721377849578857\n",
      "Epoch: 18, Train_Loss: 0.5445905923843384, Test_Loss: 1.185457468032837 *\n",
      "Epoch: 18, Train_Loss: 0.4937725067138672, Test_Loss: 0.21452657878398895 *\n",
      "Epoch: 18, Train_Loss: 0.48121100664138794, Test_Loss: 0.2261640727519989\n",
      "Epoch: 18, Train_Loss: 0.29277512431144714, Test_Loss: 0.24619059264659882\n",
      "Epoch: 18, Train_Loss: 0.2655060887336731, Test_Loss: 0.2363642007112503 *\n",
      "Epoch: 18, Train_Loss: 0.2089190036058426, Test_Loss: 0.2117810845375061 *\n",
      "Epoch: 18, Train_Loss: 0.20601187646389008, Test_Loss: 0.25179508328437805\n",
      "Epoch: 18, Train_Loss: 0.21834439039230347, Test_Loss: 0.24620720744132996 *\n",
      "Epoch: 18, Train_Loss: 0.41645348072052, Test_Loss: 0.2001868486404419 *\n",
      "Epoch: 18, Train_Loss: 0.45321381092071533, Test_Loss: 0.22580140829086304\n",
      "Epoch: 18, Train_Loss: 0.5022995471954346, Test_Loss: 0.24145859479904175\n",
      "Epoch: 18, Train_Loss: 1.0506688356399536, Test_Loss: 0.22185727953910828 *\n",
      "Epoch: 18, Train_Loss: 0.7937318682670593, Test_Loss: 0.2205549031496048 *\n",
      "Epoch: 18, Train_Loss: 0.44203442335128784, Test_Loss: 0.3950943648815155\n",
      "Epoch: 18, Train_Loss: 0.2449703812599182, Test_Loss: 0.3204580545425415 *\n",
      "Epoch: 18, Train_Loss: 0.20384462177753448, Test_Loss: 0.25729191303253174 *\n",
      "Epoch: 18, Train_Loss: 0.4203348755836487, Test_Loss: 0.22544202208518982 *\n",
      "Epoch: 18, Train_Loss: 0.4343520998954773, Test_Loss: 0.25936228036880493\n",
      "Epoch: 18, Train_Loss: 0.6842929124832153, Test_Loss: 0.24353279173374176 *\n",
      "Epoch: 18, Train_Loss: 0.23267413675785065, Test_Loss: 0.4732236862182617\n",
      "Epoch: 18, Train_Loss: 0.2288404405117035, Test_Loss: 0.36088186502456665 *\n",
      "Epoch: 18, Train_Loss: 0.28810060024261475, Test_Loss: 0.44452211260795593\n",
      "Epoch: 18, Train_Loss: 0.5138750672340393, Test_Loss: 0.30425289273262024 *\n",
      "Epoch: 18, Train_Loss: 0.3614879846572876, Test_Loss: 0.3768608868122101\n",
      "Epoch: 18, Train_Loss: 0.2654411494731903, Test_Loss: 0.5657891631126404\n",
      "Epoch: 18, Train_Loss: 0.31783008575439453, Test_Loss: 0.47042030096054077 *\n",
      "Epoch: 18, Train_Loss: 0.2064317911863327, Test_Loss: 0.39895525574684143 *\n",
      "Epoch: 18, Train_Loss: 0.20591524243354797, Test_Loss: 0.2685214579105377 *\n",
      "Epoch: 18, Train_Loss: 0.23977434635162354, Test_Loss: 0.25339069962501526 *\n",
      "Epoch: 18, Train_Loss: 0.20459288358688354, Test_Loss: 0.21013347804546356 *\n",
      "Epoch: 18, Train_Loss: 0.22921273112297058, Test_Loss: 0.22854387760162354\n",
      "Epoch: 18, Train_Loss: 0.24047252535820007, Test_Loss: 0.3510134816169739\n",
      "Epoch: 18, Train_Loss: 1.3933180570602417, Test_Loss: 0.24090325832366943 *\n",
      "Epoch: 18, Train_Loss: 15.049235343933105, Test_Loss: 0.3391505181789398\n",
      "Epoch: 18, Train_Loss: 0.4740152955055237, Test_Loss: 0.2469831109046936 *\n",
      "Epoch: 18, Train_Loss: 0.8936406373977661, Test_Loss: 0.22404435276985168 *\n",
      "Epoch: 18, Train_Loss: 0.7803903818130493, Test_Loss: 0.21062935888767242 *\n",
      "Epoch: 18, Train_Loss: 0.24071815609931946, Test_Loss: 0.2513119876384735\n",
      "Epoch: 18, Train_Loss: 0.41851574182510376, Test_Loss: 0.45257389545440674\n",
      "Epoch: 18, Train_Loss: 5.105825424194336, Test_Loss: 0.26695069670677185 *\n",
      "Epoch: 18, Train_Loss: 2.1205718517303467, Test_Loss: 0.5829885005950928\n",
      "Epoch: 18, Train_Loss: 0.26524582505226135, Test_Loss: 0.3516903519630432 *\n",
      "Epoch: 18, Train_Loss: 1.4429736137390137, Test_Loss: 0.41661959886550903\n",
      "Epoch: 18, Train_Loss: 4.27171516418457, Test_Loss: 0.8672203421592712\n",
      "Epoch: 18, Train_Loss: 0.5704805850982666, Test_Loss: 0.9002748727798462\n",
      "Epoch: 18, Train_Loss: 0.20801153779029846, Test_Loss: 0.9440861344337463\n",
      "Epoch: 18, Train_Loss: 0.19683699309825897, Test_Loss: 0.39012911915779114 *\n",
      "Epoch: 18, Train_Loss: 0.21832920610904694, Test_Loss: 0.5088123083114624\n",
      "Epoch: 18, Train_Loss: 0.24190093576908112, Test_Loss: 0.36646342277526855 *\n",
      "Epoch: 18, Train_Loss: 0.19617855548858643, Test_Loss: 0.7932274341583252\n",
      "Epoch: 18, Train_Loss: 0.1969243437051773, Test_Loss: 2.0635104179382324\n",
      "Epoch: 18, Train_Loss: 0.1956598162651062, Test_Loss: 0.9739831686019897 *\n",
      "Epoch: 18, Train_Loss: 0.20563434064388275, Test_Loss: 0.3536688983440399 *\n",
      "Epoch: 18, Train_Loss: 0.26514679193496704, Test_Loss: 0.22366666793823242 *\n",
      "Epoch: 18, Train_Loss: 0.20131458342075348, Test_Loss: 0.22487108409404755\n",
      "Epoch: 18, Train_Loss: 0.2464992105960846, Test_Loss: 0.22049379348754883 *\n",
      "Epoch: 18, Train_Loss: 0.29901885986328125, Test_Loss: 0.2043466418981552 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.21459276974201202, Test_Loss: 0.20885901153087616\n",
      "Epoch: 18, Train_Loss: 0.2128756046295166, Test_Loss: 5.478909492492676\n",
      "Epoch: 18, Train_Loss: 0.23361387848854065, Test_Loss: 3.193617820739746 *\n",
      "Epoch: 18, Train_Loss: 0.23808543384075165, Test_Loss: 0.567366361618042 *\n",
      "Epoch: 18, Train_Loss: 0.19768528640270233, Test_Loss: 0.35780954360961914 *\n",
      "Epoch: 18, Train_Loss: 0.1970895528793335, Test_Loss: 0.43685656785964966\n",
      "Epoch: 18, Train_Loss: 0.19520792365074158, Test_Loss: 0.21059444546699524 *\n",
      "Epoch: 18, Train_Loss: 0.19560883939266205, Test_Loss: 0.4147525429725647\n",
      "Epoch: 18, Train_Loss: 0.19590461254119873, Test_Loss: 0.8213069438934326\n",
      "Epoch: 18, Train_Loss: 0.1965797394514084, Test_Loss: 0.3733425736427307 *\n",
      "Epoch: 18, Train_Loss: 0.19550222158432007, Test_Loss: 0.25178202986717224 *\n",
      "Epoch: 18, Train_Loss: 0.1957232654094696, Test_Loss: 0.3756176233291626\n",
      "Epoch: 18, Train_Loss: 0.22015812993049622, Test_Loss: 0.4834044277667999\n",
      "Epoch: 18, Train_Loss: 0.2422674596309662, Test_Loss: 0.9284834265708923\n",
      "Epoch: 18, Train_Loss: 0.3174374997615814, Test_Loss: 0.6619749069213867 *\n",
      "Epoch: 18, Train_Loss: 0.2237025499343872, Test_Loss: 0.9744268655776978\n",
      "Epoch: 18, Train_Loss: 0.41843605041503906, Test_Loss: 0.3960976004600525 *\n",
      "Epoch: 18, Train_Loss: 6.641793251037598, Test_Loss: 0.2065054476261139 *\n",
      "Epoch: 18, Train_Loss: 1.8961007595062256, Test_Loss: 0.21285617351531982\n",
      "Epoch: 18, Train_Loss: 0.21341969072818756, Test_Loss: 0.256615549325943\n",
      "Epoch: 18, Train_Loss: 0.23628678917884827, Test_Loss: 0.27876394987106323\n",
      "Epoch: 18, Train_Loss: 0.25756967067718506, Test_Loss: 0.24309004843235016 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.21709664165973663, Test_Loss: 0.2938764691352844\n",
      "Epoch: 18, Train_Loss: 0.2233329862356186, Test_Loss: 0.2650304436683655 *\n",
      "Epoch: 18, Train_Loss: 0.22187693417072296, Test_Loss: 0.3112965524196625\n",
      "Epoch: 18, Train_Loss: 0.2899397611618042, Test_Loss: 0.32682740688323975\n",
      "Epoch: 18, Train_Loss: 0.3078942894935608, Test_Loss: 0.24843688309192657 *\n",
      "Epoch: 18, Train_Loss: 0.2568897306919098, Test_Loss: 0.2371262162923813 *\n",
      "Epoch: 18, Train_Loss: 0.20891135931015015, Test_Loss: 0.2690249979496002\n",
      "Epoch: 18, Train_Loss: 0.2264358550310135, Test_Loss: 0.23570308089256287 *\n",
      "Epoch: 18, Train_Loss: 0.2056802213191986, Test_Loss: 0.2121889740228653 *\n",
      "Epoch: 18, Train_Loss: 0.3947882056236267, Test_Loss: 0.3268018960952759\n",
      "Epoch: 18, Train_Loss: 0.20032724738121033, Test_Loss: 0.2512495815753937 *\n",
      "Epoch: 18, Train_Loss: 0.20951314270496368, Test_Loss: 6.515650272369385\n",
      "Epoch: 18, Train_Loss: 0.2326292246580124, Test_Loss: 0.4267796277999878 *\n",
      "Epoch: 18, Train_Loss: 0.21179193258285522, Test_Loss: 0.20147162675857544 *\n",
      "Epoch: 18, Train_Loss: 0.23299665749073029, Test_Loss: 0.22292734682559967\n",
      "Epoch: 18, Train_Loss: 0.22591230273246765, Test_Loss: 0.20567719638347626 *\n",
      "Epoch: 18, Train_Loss: 0.23447328805923462, Test_Loss: 0.2027125507593155 *\n",
      "Epoch: 18, Train_Loss: 0.2610272765159607, Test_Loss: 0.20742815732955933\n",
      "Epoch: 18, Train_Loss: 0.2080208957195282, Test_Loss: 0.2758638858795166\n",
      "Epoch: 18, Train_Loss: 0.2554340362548828, Test_Loss: 0.24803216755390167 *\n",
      "Epoch: 18, Train_Loss: 4.246257781982422, Test_Loss: 0.1962873637676239 *\n",
      "Epoch: 18, Train_Loss: 0.6806091666221619, Test_Loss: 0.21610087156295776\n",
      "Epoch: 18, Train_Loss: 0.19975389540195465, Test_Loss: 0.22397492825984955\n",
      "Epoch: 18, Train_Loss: 0.2789802551269531, Test_Loss: 0.19986899197101593 *\n",
      "Epoch: 18, Train_Loss: 0.20810353755950928, Test_Loss: 0.21643255650997162\n",
      "Epoch: 18, Train_Loss: 0.19574369490146637, Test_Loss: 0.20349949598312378 *\n",
      "Epoch: 18, Train_Loss: 0.20237019658088684, Test_Loss: 0.2566947340965271\n",
      "Epoch: 18, Train_Loss: 0.20827126502990723, Test_Loss: 0.2973127067089081\n",
      "Epoch: 18, Train_Loss: 0.2230796366930008, Test_Loss: 0.22811269760131836 *\n",
      "Epoch: 18, Train_Loss: 0.20552590489387512, Test_Loss: 0.2393956184387207\n",
      "Epoch: 18, Train_Loss: 0.23012515902519226, Test_Loss: 0.20216946303844452 *\n",
      "Epoch: 18, Train_Loss: 0.19765087962150574, Test_Loss: 0.2217913269996643\n",
      "Epoch: 18, Train_Loss: 0.19487419724464417, Test_Loss: 0.23174501955509186\n",
      "Epoch: 18, Train_Loss: 0.20722739398479462, Test_Loss: 0.2390211522579193\n",
      "Epoch: 18, Train_Loss: 0.20687350630760193, Test_Loss: 0.2263401448726654 *\n",
      "Epoch: 18, Train_Loss: 0.19791485369205475, Test_Loss: 0.21010363101959229 *\n",
      "Epoch: 18, Train_Loss: 0.21433086693286896, Test_Loss: 0.2166660726070404\n",
      "Epoch: 18, Train_Loss: 0.2246115654706955, Test_Loss: 0.20655028522014618 *\n",
      "Epoch: 18, Train_Loss: 0.21086116135120392, Test_Loss: 0.21597005426883698\n",
      "Epoch: 18, Train_Loss: 0.19601808488368988, Test_Loss: 0.23594270646572113\n",
      "Epoch: 18, Train_Loss: 0.19458405673503876, Test_Loss: 0.21888017654418945 *\n",
      "Epoch: 18, Train_Loss: 0.27345895767211914, Test_Loss: 0.200965017080307 *\n",
      "Epoch: 18, Train_Loss: 0.23112429678440094, Test_Loss: 0.23341988027095795\n",
      "Epoch: 18, Train_Loss: 0.24286408722400665, Test_Loss: 0.385154664516449\n",
      "Epoch: 18, Train_Loss: 0.24646392464637756, Test_Loss: 0.42856764793395996\n",
      "Epoch: 18, Train_Loss: 0.2587698698043823, Test_Loss: 0.3671931028366089 *\n",
      "Epoch: 18, Train_Loss: 0.23698961734771729, Test_Loss: 0.24230431020259857 *\n",
      "Epoch: 18, Train_Loss: 0.22174952924251556, Test_Loss: 0.20059432089328766 *\n",
      "Epoch: 18, Train_Loss: 0.24727210402488708, Test_Loss: 0.20852582156658173\n",
      "Epoch: 18, Train_Loss: 0.4810485243797302, Test_Loss: 0.2679039239883423\n",
      "Epoch: 18, Train_Loss: 0.24944378435611725, Test_Loss: 0.432491660118103\n",
      "Epoch: 18, Train_Loss: 0.21412470936775208, Test_Loss: 0.33070528507232666 *\n",
      "Epoch: 18, Train_Loss: 0.1940496265888214, Test_Loss: 0.38065075874328613\n",
      "Epoch: 18, Train_Loss: 0.19426009058952332, Test_Loss: 0.24612179398536682 *\n",
      "Epoch: 18, Train_Loss: 0.19405430555343628, Test_Loss: 0.19920332729816437 *\n",
      "Epoch: 18, Train_Loss: 0.1938657909631729, Test_Loss: 0.2043341100215912\n",
      "Epoch: 18, Train_Loss: 0.195359006524086, Test_Loss: 0.20111876726150513 *\n",
      "Epoch: 18, Train_Loss: 3.978660821914673, Test_Loss: 0.2276322841644287\n",
      "Epoch: 18, Train_Loss: 0.7083424925804138, Test_Loss: 0.2105158120393753 *\n",
      "Epoch: 18, Train_Loss: 0.21681496500968933, Test_Loss: 0.2215782105922699\n",
      "Epoch: 18, Train_Loss: 0.1965387761592865, Test_Loss: 0.1998872607946396 *\n",
      "Epoch: 18, Train_Loss: 0.19950032234191895, Test_Loss: 0.38290977478027344\n",
      "Epoch: 18, Train_Loss: 0.21204034984111786, Test_Loss: 0.6744301319122314\n",
      "Epoch: 18, Train_Loss: 0.20178981125354767, Test_Loss: 0.41521042585372925 *\n",
      "Epoch: 18, Train_Loss: 0.1990591585636139, Test_Loss: 0.7709358930587769\n",
      "Epoch: 18, Train_Loss: 0.2023925930261612, Test_Loss: 0.38757026195526123 *\n",
      "Epoch: 18, Train_Loss: 0.2150537669658661, Test_Loss: 0.3684672713279724 *\n",
      "Epoch: 18, Train_Loss: 0.2130172699689865, Test_Loss: 0.34415483474731445 *\n",
      "Epoch: 18, Train_Loss: 0.2035856395959854, Test_Loss: 0.32086092233657837 *\n",
      "Epoch: 18, Train_Loss: 0.22742612659931183, Test_Loss: 0.3307116627693176\n",
      "Epoch: 18, Train_Loss: 0.23630647361278534, Test_Loss: 3.9403347969055176\n",
      "Epoch: 18, Train_Loss: 0.20003755390644073, Test_Loss: 0.7791904807090759 *\n",
      "Epoch: 18, Train_Loss: 0.26984649896621704, Test_Loss: 0.2220328450202942 *\n",
      "Epoch: 18, Train_Loss: 0.4338606595993042, Test_Loss: 0.19951847195625305 *\n",
      "Epoch: 18, Train_Loss: 0.40827274322509766, Test_Loss: 0.19997747242450714\n",
      "Epoch: 18, Train_Loss: 0.4053531289100647, Test_Loss: 0.20431029796600342\n",
      "Epoch: 18, Train_Loss: 0.2095373272895813, Test_Loss: 0.19818612933158875 *\n",
      "Epoch: 18, Train_Loss: 0.19406983256340027, Test_Loss: 0.19750376045703888 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.19391310214996338, Test_Loss: 0.1956380009651184 *\n",
      "Epoch: 18, Train_Loss: 0.20048554241657257, Test_Loss: 0.19615113735198975\n",
      "Epoch: 18, Train_Loss: 0.1993381828069687, Test_Loss: 0.19866730272769928\n",
      "Epoch: 18, Train_Loss: 0.19725452363491058, Test_Loss: 0.20622922480106354\n",
      "Epoch: 18, Train_Loss: 0.19746148586273193, Test_Loss: 0.2008529007434845 *\n",
      "Epoch: 18, Train_Loss: 0.196559339761734, Test_Loss: 0.21910612285137177\n",
      "Epoch: 18, Train_Loss: 0.19568228721618652, Test_Loss: 0.21010613441467285 *\n",
      "Epoch: 18, Train_Loss: 0.1990862786769867, Test_Loss: 0.19750046730041504 *\n",
      "Epoch: 18, Train_Loss: 0.30381059646606445, Test_Loss: 0.19681280851364136 *\n",
      "Epoch: 18, Train_Loss: 0.3476060926914215, Test_Loss: 0.19929146766662598\n",
      "Epoch: 18, Train_Loss: 0.32779690623283386, Test_Loss: 0.1985594779253006 *\n",
      "Epoch: 18, Train_Loss: 0.253693550825119, Test_Loss: 0.19814595580101013 *\n",
      "Epoch: 18, Train_Loss: 0.36209046840667725, Test_Loss: 0.20278659462928772\n",
      "Epoch: 18, Train_Loss: 0.3668646216392517, Test_Loss: 0.19784441590309143 *\n",
      "Epoch: 18, Train_Loss: 0.2205241322517395, Test_Loss: 0.2004716992378235\n",
      "Epoch: 18, Train_Loss: 0.3827191889286041, Test_Loss: 0.20545481145381927\n",
      "Epoch: 18, Train_Loss: 0.3120580017566681, Test_Loss: 0.2015353888273239 *\n",
      "Epoch: 18, Train_Loss: 0.44402551651000977, Test_Loss: 0.20256958901882172\n",
      "Epoch: 18, Train_Loss: 0.21334891021251678, Test_Loss: 0.19906949996948242 *\n",
      "Epoch: 18, Train_Loss: 1.4694007635116577, Test_Loss: 0.19671228528022766 *\n",
      "Epoch: 18, Train_Loss: 2.1677308082580566, Test_Loss: 0.19606107473373413 *\n",
      "Epoch: 18, Train_Loss: 0.23505237698554993, Test_Loss: 0.19715581834316254\n",
      "Epoch: 18, Train_Loss: 0.2419930398464203, Test_Loss: 0.25423210859298706\n",
      "Epoch: 18, Train_Loss: 0.24270081520080566, Test_Loss: 0.5707881450653076\n",
      "Epoch: 18, Train_Loss: 0.24444323778152466, Test_Loss: 5.402463436126709\n",
      "Epoch: 18, Train_Loss: 0.19401080906391144, Test_Loss: 0.20683668553829193 *\n",
      "Epoch: 18, Train_Loss: 0.20317481458187103, Test_Loss: 0.1941622793674469 *\n",
      "Epoch: 18, Train_Loss: 0.3075326681137085, Test_Loss: 0.2335316687822342\n",
      "Epoch: 18, Train_Loss: 0.2832101881504059, Test_Loss: 0.26596981287002563\n",
      "Epoch: 18, Train_Loss: 0.27710142731666565, Test_Loss: 0.2627400755882263 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.2681247591972351, Test_Loss: 0.19774329662322998 *\n",
      "Epoch: 18, Train_Loss: 0.2543979585170746, Test_Loss: 0.29873740673065186\n",
      "Epoch: 18, Train_Loss: 0.2134094387292862, Test_Loss: 0.22425763309001923 *\n",
      "Epoch: 18, Train_Loss: 0.2139587104320526, Test_Loss: 0.198529914021492 *\n",
      "Epoch: 18, Train_Loss: 0.2184712290763855, Test_Loss: 0.22742848098278046\n",
      "Epoch: 18, Train_Loss: 0.2260180413722992, Test_Loss: 0.21560919284820557 *\n",
      "Epoch: 18, Train_Loss: 0.2035887986421585, Test_Loss: 0.19922678172588348 *\n",
      "Epoch: 18, Train_Loss: 0.19307488203048706, Test_Loss: 0.2549743056297302\n",
      "Epoch: 18, Train_Loss: 0.23031988739967346, Test_Loss: 0.346660852432251\n",
      "Epoch: 18, Train_Loss: 0.2358575314283371, Test_Loss: 0.23785096406936646 *\n",
      "Epoch: 18, Train_Loss: 0.21265196800231934, Test_Loss: 0.27728208899497986\n",
      "Epoch: 18, Train_Loss: 0.19384212791919708, Test_Loss: 0.22819465398788452 *\n",
      "Epoch: 18, Train_Loss: 0.1936005800962448, Test_Loss: 0.24824784696102142\n",
      "Epoch: 18, Train_Loss: 0.19293519854545593, Test_Loss: 0.22155262529850006 *\n",
      "Epoch: 18, Train_Loss: 0.19417668879032135, Test_Loss: 0.21596643328666687 *\n",
      "Epoch: 18, Train_Loss: 0.19308508932590485, Test_Loss: 0.21815592050552368\n",
      "Epoch: 18, Train_Loss: 0.19584594666957855, Test_Loss: 0.21771368384361267 *\n",
      "Epoch: 18, Train_Loss: 0.19516676664352417, Test_Loss: 0.2168707400560379 *\n",
      "Epoch: 18, Train_Loss: 0.19394376873970032, Test_Loss: 0.2177942842245102\n",
      "Epoch: 18, Train_Loss: 0.1934126764535904, Test_Loss: 0.22607284784317017\n",
      "Epoch: 18, Train_Loss: 0.19466397166252136, Test_Loss: 0.22433403134346008 *\n",
      "Epoch: 18, Train_Loss: 0.20498821139335632, Test_Loss: 0.2357717603445053\n",
      "Epoch: 18, Train_Loss: 0.21373257040977478, Test_Loss: 0.19931267201900482 *\n",
      "Epoch: 18, Train_Loss: 0.20728793740272522, Test_Loss: 0.2226804792881012\n",
      "Epoch: 18, Train_Loss: 0.21935997903347015, Test_Loss: 0.28787198662757874\n",
      "Epoch: 18, Train_Loss: 0.20285366475582123, Test_Loss: 0.2153072953224182 *\n",
      "Epoch: 18, Train_Loss: 0.19864778220653534, Test_Loss: 0.6241676211357117\n",
      "Epoch: 18, Train_Loss: 0.19322501122951508, Test_Loss: 0.8200365304946899\n",
      "Epoch: 18, Train_Loss: 0.19584570825099945, Test_Loss: 0.43321794271469116 *\n",
      "Epoch: 18, Train_Loss: 0.22651349008083344, Test_Loss: 0.2439720332622528 *\n",
      "Epoch: 18, Train_Loss: 0.20598727464675903, Test_Loss: 0.22427105903625488 *\n",
      "Epoch: 18, Train_Loss: 0.19659265875816345, Test_Loss: 0.2044854462146759 *\n",
      "Epoch: 18, Train_Loss: 0.19247783720493317, Test_Loss: 0.2813800275325775\n",
      "Epoch: 18, Train_Loss: 0.2008630335330963, Test_Loss: 0.5484960079193115\n",
      "Epoch: 18, Train_Loss: 0.25588223338127136, Test_Loss: 0.648481011390686\n",
      "Epoch: 18, Train_Loss: 0.2402351200580597, Test_Loss: 0.2598876357078552 *\n",
      "Epoch: 18, Train_Loss: 0.24261949956417084, Test_Loss: 0.29836928844451904\n",
      "Epoch: 18, Train_Loss: 0.1926637887954712, Test_Loss: 0.1980471909046173 *\n",
      "Epoch: 18, Train_Loss: 0.24079486727714539, Test_Loss: 0.19551436603069305 *\n",
      "Epoch: 18, Train_Loss: 0.22477024793624878, Test_Loss: 0.2015305459499359\n",
      "Epoch: 18, Train_Loss: 0.19410669803619385, Test_Loss: 0.20266830921173096\n",
      "Epoch: 18, Train_Loss: 0.2045108526945114, Test_Loss: 0.22590231895446777\n",
      "Epoch: 18, Train_Loss: 0.22084711492061615, Test_Loss: 0.2149752378463745 *\n",
      "Epoch: 18, Train_Loss: 0.28038138151168823, Test_Loss: 0.1970239281654358 *\n",
      "Epoch: 18, Train_Loss: 0.26160821318626404, Test_Loss: 0.3206273317337036\n",
      "Epoch: 18, Train_Loss: 0.2365393042564392, Test_Loss: 0.6227125525474548\n",
      "Epoch: 18, Train_Loss: 0.21334806084632874, Test_Loss: 0.35399872064590454 *\n",
      "Epoch: 18, Train_Loss: 0.19947806000709534, Test_Loss: 0.3154996335506439 *\n",
      "Epoch: 18, Train_Loss: 0.2109675407409668, Test_Loss: 0.208484947681427 *\n",
      "Epoch: 18, Train_Loss: 0.19286353886127472, Test_Loss: 0.20849823951721191\n",
      "Epoch: 18, Train_Loss: 0.19607794284820557, Test_Loss: 0.20889368653297424\n",
      "Epoch: 18, Train_Loss: 0.210939422249794, Test_Loss: 0.21061818301677704\n",
      "Epoch: 18, Train_Loss: 0.21933549642562866, Test_Loss: 0.2637258768081665\n",
      "Epoch: 18, Train_Loss: 0.3040747046470642, Test_Loss: 5.175973415374756\n",
      "Epoch: 18, Train_Loss: 0.19247746467590332, Test_Loss: 0.3588137626647949 *\n",
      "Epoch: 18, Train_Loss: 0.2520121932029724, Test_Loss: 0.2039533108472824 *\n",
      "Epoch: 18, Train_Loss: 0.19849522411823273, Test_Loss: 0.19574327766895294 *\n",
      "Epoch: 18, Train_Loss: 0.24552536010742188, Test_Loss: 0.1999836415052414\n",
      "Epoch: 18, Train_Loss: 0.21689090132713318, Test_Loss: 0.20090384781360626\n",
      "Epoch: 18, Train_Loss: 0.4988820552825928, Test_Loss: 0.19726727902889252 *\n",
      "Epoch: 18, Train_Loss: 0.21822752058506012, Test_Loss: 0.1934739053249359 *\n",
      "Epoch: 18, Train_Loss: 0.22569036483764648, Test_Loss: 0.19308048486709595 *\n",
      "Epoch: 18, Train_Loss: 0.1924087256193161, Test_Loss: 0.19333991408348083\n",
      "Epoch: 18, Train_Loss: 0.19169938564300537, Test_Loss: 0.19358891248703003\n",
      "Epoch: 18, Train_Loss: 0.19236081838607788, Test_Loss: 0.2002282738685608\n",
      "Epoch: 18, Train_Loss: 0.1915869563817978, Test_Loss: 0.21069681644439697\n",
      "Epoch: 18, Train_Loss: 0.19938065111637115, Test_Loss: 0.23198220133781433\n",
      "Epoch: 18, Train_Loss: 0.20058074593544006, Test_Loss: 0.2136414349079132 *\n",
      "Epoch: 18, Train_Loss: 0.2082894891500473, Test_Loss: 0.1926652491092682 *\n",
      "Epoch: 18, Train_Loss: 0.19811920821666718, Test_Loss: 0.19186358153820038 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 18\n",
      "Epoch: 18, Train_Loss: 0.20290815830230713, Test_Loss: 0.19155646860599518 *\n",
      "Epoch: 18, Train_Loss: 0.20517650246620178, Test_Loss: 0.19273145496845245\n",
      "Epoch: 18, Train_Loss: 0.19374752044677734, Test_Loss: 0.1913607120513916 *\n",
      "Epoch: 18, Train_Loss: 0.19091621041297913, Test_Loss: 0.19224268198013306\n",
      "Epoch: 18, Train_Loss: 0.2072220742702484, Test_Loss: 0.19132983684539795 *\n",
      "Epoch: 18, Train_Loss: 0.21535934507846832, Test_Loss: 0.19189055263996124\n",
      "Epoch: 18, Train_Loss: 0.22741128504276276, Test_Loss: 0.19209907948970795\n",
      "Epoch: 18, Train_Loss: 0.1916920393705368, Test_Loss: 0.1921134740114212\n",
      "Epoch: 18, Train_Loss: 0.2239569127559662, Test_Loss: 0.19206422567367554 *\n",
      "Epoch: 18, Train_Loss: 0.23282815515995026, Test_Loss: 0.19121858477592468 *\n",
      "Epoch: 18, Train_Loss: 0.2331591546535492, Test_Loss: 0.19108352065086365 *\n",
      "Epoch: 18, Train_Loss: 0.19130383431911469, Test_Loss: 0.1911674290895462\n",
      "Epoch: 18, Train_Loss: 0.23278430104255676, Test_Loss: 0.19390951097011566\n",
      "Epoch: 18, Train_Loss: 0.19321201741695404, Test_Loss: 0.25179749727249146\n",
      "Epoch: 18, Train_Loss: 0.20549330115318298, Test_Loss: 1.723458170890808\n",
      "Epoch: 18, Train_Loss: 0.19243237376213074, Test_Loss: 4.203780651092529\n",
      "Epoch: 18, Train_Loss: 0.2200821340084076, Test_Loss: 0.19983458518981934 *\n",
      "Epoch: 18, Train_Loss: 0.2828099727630615, Test_Loss: 0.1915012151002884 *\n",
      "Epoch: 18, Train_Loss: 3.3082194328308105, Test_Loss: 0.24471059441566467\n",
      "Epoch: 18, Train_Loss: 2.521538019180298, Test_Loss: 0.25629135966300964\n",
      "Epoch: 18, Train_Loss: 0.20999053120613098, Test_Loss: 0.24736084043979645 *\n",
      "Epoch: 18, Train_Loss: 0.19181963801383972, Test_Loss: 0.21340152621269226 *\n",
      "Epoch: 18, Train_Loss: 0.3008924424648285, Test_Loss: 0.29740995168685913\n",
      "Epoch: 18, Train_Loss: 0.29484641551971436, Test_Loss: 0.20288126170635223 *\n",
      "Epoch: 18, Train_Loss: 0.21130400896072388, Test_Loss: 0.20169122517108917 *\n",
      "Epoch: 18, Train_Loss: 0.19119325280189514, Test_Loss: 0.22014905512332916\n",
      "Epoch: 18, Train_Loss: 0.245774045586586, Test_Loss: 0.21112877130508423 *\n",
      "Epoch: 18, Train_Loss: 0.22614318132400513, Test_Loss: 0.1979214996099472 *\n",
      "Epoch: 18, Train_Loss: 0.2051762044429779, Test_Loss: 0.2710728943347931\n",
      "Epoch: 18, Train_Loss: 0.3221738338470459, Test_Loss: 0.3136700689792633\n",
      "Epoch: 18, Train_Loss: 1.0143978595733643, Test_Loss: 0.24526141583919525 *\n",
      "Epoch: 18, Train_Loss: 1.3604695796966553, Test_Loss: 0.23016256093978882 *\n",
      "Epoch: 18, Train_Loss: 0.29230940341949463, Test_Loss: 0.2275184541940689 *\n",
      "Epoch: 18, Train_Loss: 0.2605760097503662, Test_Loss: 0.2816459834575653\n",
      "Epoch: 18, Train_Loss: 2.007567882537842, Test_Loss: 0.20721539855003357 *\n",
      "Epoch: 18, Train_Loss: 1.509576678276062, Test_Loss: 0.20486091077327728 *\n",
      "Epoch: 18, Train_Loss: 0.21646049618721008, Test_Loss: 0.2109675109386444\n",
      "Epoch: 18, Train_Loss: 0.1991850584745407, Test_Loss: 0.21415157616138458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train_Loss: 0.5547060966491699, Test_Loss: 0.1989464908838272 *\n",
      "Epoch: 18, Train_Loss: 0.7819659113883972, Test_Loss: 0.2031177282333374\n",
      "Epoch: 18, Train_Loss: 0.8918502330780029, Test_Loss: 0.20137110352516174 *\n",
      "Epoch: 18, Train_Loss: 0.19272196292877197, Test_Loss: 0.197819322347641 *\n",
      "Epoch: 18, Train_Loss: 0.2079818993806839, Test_Loss: 0.19926320016384125\n",
      "Epoch: 18, Train_Loss: 0.3190535604953766, Test_Loss: 0.2098616361618042\n",
      "Epoch: 18, Train_Loss: 0.5134961605072021, Test_Loss: 0.19270524382591248 *\n",
      "Epoch: 18, Train_Loss: 0.21050968766212463, Test_Loss: 0.20846444368362427\n",
      "Epoch: 18, Train_Loss: 0.23335804045200348, Test_Loss: 0.20881953835487366\n",
      "Epoch: 18, Train_Loss: 0.22443987429141998, Test_Loss: 0.6847777962684631\n",
      "Epoch: 18, Train_Loss: 0.24971917271614075, Test_Loss: 0.5084924697875977 *\n",
      "Epoch: 18, Train_Loss: 0.31805944442749023, Test_Loss: 0.30465036630630493 *\n",
      "Epoch: 18, Train_Loss: 0.3320506811141968, Test_Loss: 0.2141161412000656 *\n",
      "Epoch: 18, Train_Loss: 0.2433703988790512, Test_Loss: 0.22962918877601624\n",
      "Epoch: 18, Train_Loss: 0.32026052474975586, Test_Loss: 0.20368441939353943 *\n",
      "Epoch: 18, Train_Loss: 0.23480364680290222, Test_Loss: 0.252889484167099\n",
      "Epoch: 19, Train_Loss: 0.222333163022995, Test_Loss: 0.26454958319664 *\n",
      "Epoch: 19, Train_Loss: 0.2872803807258606, Test_Loss: 0.43950068950653076\n",
      "Epoch: 19, Train_Loss: 0.36150604486465454, Test_Loss: 0.2952948808670044 *\n",
      "Epoch: 19, Train_Loss: 0.22422900795936584, Test_Loss: 0.3010786473751068\n",
      "Epoch: 19, Train_Loss: 0.3023741841316223, Test_Loss: 0.21047453582286835 *\n",
      "Epoch: 19, Train_Loss: 0.3446089029312134, Test_Loss: 0.19668586552143097 *\n",
      "Epoch: 19, Train_Loss: 0.21555955708026886, Test_Loss: 0.20434677600860596\n",
      "Epoch: 19, Train_Loss: 0.2011854350566864, Test_Loss: 0.26131507754325867\n",
      "Epoch: 19, Train_Loss: 0.19147199392318726, Test_Loss: 0.20787528157234192 *\n",
      "Epoch: 19, Train_Loss: 0.193820059299469, Test_Loss: 0.21806347370147705\n",
      "Epoch: 19, Train_Loss: 0.1953437179327011, Test_Loss: 0.2701832056045532\n",
      "Epoch: 19, Train_Loss: 0.19823524355888367, Test_Loss: 0.42610567808151245\n",
      "Epoch: 19, Train_Loss: 0.21561183035373688, Test_Loss: 0.5671533346176147\n",
      "Epoch: 19, Train_Loss: 0.21413454413414001, Test_Loss: 0.9124507308006287\n",
      "Epoch: 19, Train_Loss: 0.20962364971637726, Test_Loss: 0.7760440111160278 *\n",
      "Epoch: 19, Train_Loss: 0.3464265465736389, Test_Loss: 0.8531001806259155\n",
      "Epoch: 19, Train_Loss: 0.4813261032104492, Test_Loss: 0.8483659625053406 *\n",
      "Epoch: 19, Train_Loss: 0.20432545244693756, Test_Loss: 0.8565669655799866\n",
      "Epoch: 19, Train_Loss: 0.25647252798080444, Test_Loss: 0.919396162033081\n",
      "Epoch: 19, Train_Loss: 0.2425963133573532, Test_Loss: 0.639139711856842 *\n",
      "Epoch: 19, Train_Loss: 0.21851301193237305, Test_Loss: 4.15079927444458\n",
      "Epoch: 19, Train_Loss: 0.3861689567565918, Test_Loss: 0.2650620639324188 *\n",
      "Epoch: 19, Train_Loss: 0.31021755933761597, Test_Loss: 0.2199253886938095 *\n",
      "Epoch: 19, Train_Loss: 0.5076649188995361, Test_Loss: 0.2337239384651184\n",
      "Epoch: 19, Train_Loss: 0.36425042152404785, Test_Loss: 0.20157857239246368 *\n",
      "Epoch: 19, Train_Loss: 0.31116926670074463, Test_Loss: 0.201570525765419 *\n",
      "Epoch: 19, Train_Loss: 0.21643437445163727, Test_Loss: 0.2582997977733612\n",
      "Epoch: 19, Train_Loss: 0.2239740788936615, Test_Loss: 0.25966760516166687\n",
      "Epoch: 19, Train_Loss: 0.40134766697883606, Test_Loss: 0.19927039742469788 *\n",
      "Epoch: 19, Train_Loss: 0.8343627452850342, Test_Loss: 0.22002559900283813\n",
      "Epoch: 19, Train_Loss: 0.5619587898254395, Test_Loss: 0.23462074995040894\n",
      "Epoch: 19, Train_Loss: 0.24917857348918915, Test_Loss: 0.39260953664779663\n",
      "Epoch: 19, Train_Loss: 0.20978032052516937, Test_Loss: 0.25919148325920105 *\n",
      "Epoch: 19, Train_Loss: 0.19548310339450836, Test_Loss: 0.2807268500328064\n",
      "Epoch: 19, Train_Loss: 0.45500653982162476, Test_Loss: 0.25328031182289124 *\n",
      "Epoch: 19, Train_Loss: 0.387154757976532, Test_Loss: 0.19863593578338623 *\n",
      "Epoch: 19, Train_Loss: 0.1996021717786789, Test_Loss: 0.19278615713119507 *\n",
      "Epoch: 19, Train_Loss: 0.4316675662994385, Test_Loss: 0.2192053347826004\n",
      "Epoch: 19, Train_Loss: 0.21549150347709656, Test_Loss: 0.2085515558719635 *\n",
      "Epoch: 19, Train_Loss: 0.20546090602874756, Test_Loss: 0.1947833150625229 *\n",
      "Epoch: 19, Train_Loss: 0.23429635167121887, Test_Loss: 0.22919602692127228\n",
      "Epoch: 19, Train_Loss: 0.3143312335014343, Test_Loss: 0.19469518959522247 *\n",
      "Epoch: 19, Train_Loss: 0.23464533686637878, Test_Loss: 0.22068697214126587\n",
      "Epoch: 19, Train_Loss: 0.27552980184555054, Test_Loss: 0.21545544266700745 *\n",
      "Epoch: 19, Train_Loss: 0.19718895852565765, Test_Loss: 0.2059110701084137 *\n",
      "Epoch: 19, Train_Loss: 0.3382301926612854, Test_Loss: 0.19186322391033173 *\n",
      "Epoch: 19, Train_Loss: 0.24063435196876526, Test_Loss: 0.2030254304409027\n",
      "Epoch: 19, Train_Loss: 0.2130148559808731, Test_Loss: 0.22301824390888214\n",
      "Epoch: 19, Train_Loss: 0.1945245862007141, Test_Loss: 0.19584645330905914 *\n",
      "Epoch: 19, Train_Loss: 0.2178770750761032, Test_Loss: 0.20008404552936554\n",
      "Epoch: 19, Train_Loss: 0.31078311800956726, Test_Loss: 0.30146968364715576\n",
      "Epoch: 19, Train_Loss: 0.42443007230758667, Test_Loss: 2.8038225173950195\n",
      "Epoch: 19, Train_Loss: 0.39057302474975586, Test_Loss: 2.611396074295044 *\n",
      "Epoch: 19, Train_Loss: 0.6005733013153076, Test_Loss: 0.21041500568389893 *\n",
      "Epoch: 19, Train_Loss: 0.496305376291275, Test_Loss: 0.20012028515338898 *\n",
      "Epoch: 19, Train_Loss: 0.3575872480869293, Test_Loss: 0.22192522883415222\n",
      "Epoch: 19, Train_Loss: 0.26488107442855835, Test_Loss: 0.19819071888923645 *\n",
      "Epoch: 19, Train_Loss: 0.20663578808307648, Test_Loss: 0.21091730892658234\n",
      "Epoch: 19, Train_Loss: 0.19722135365009308, Test_Loss: 0.24492493271827698\n",
      "Epoch: 19, Train_Loss: 0.20537084341049194, Test_Loss: 0.24462008476257324 *\n",
      "Epoch: 19, Train_Loss: 0.31835806369781494, Test_Loss: 0.1925683170557022 *\n",
      "Epoch: 19, Train_Loss: 0.45610174536705017, Test_Loss: 0.21629895269870758\n",
      "Epoch: 19, Train_Loss: 0.4694404602050781, Test_Loss: 0.21938031911849976\n",
      "Epoch: 19, Train_Loss: 1.2326180934906006, Test_Loss: 0.23670968413352966\n",
      "Epoch: 19, Train_Loss: 1.0226362943649292, Test_Loss: 0.1999245584011078 *\n",
      "Epoch: 19, Train_Loss: 0.35986095666885376, Test_Loss: 0.27163803577423096\n",
      "Epoch: 19, Train_Loss: 0.32464927434921265, Test_Loss: 0.25483569502830505 *\n",
      "Epoch: 19, Train_Loss: 0.19535019993782043, Test_Loss: 0.27354729175567627\n",
      "Epoch: 19, Train_Loss: 0.310714453458786, Test_Loss: 0.20347823202610016 *\n",
      "Epoch: 19, Train_Loss: 0.3507014513015747, Test_Loss: 0.238764226436615\n",
      "Epoch: 19, Train_Loss: 0.8328210115432739, Test_Loss: 0.21959730982780457 *\n",
      "Epoch: 19, Train_Loss: 0.21397745609283447, Test_Loss: 0.22510237991809845\n",
      "Epoch: 19, Train_Loss: 0.21879880130290985, Test_Loss: 0.2662220001220703\n",
      "Epoch: 19, Train_Loss: 0.2612910866737366, Test_Loss: 0.27470991015434265\n",
      "Epoch: 19, Train_Loss: 0.45823124051094055, Test_Loss: 0.22065621614456177 *\n",
      "Epoch: 19, Train_Loss: 0.32450348138809204, Test_Loss: 0.23647640645503998\n",
      "Epoch: 19, Train_Loss: 0.4059516489505768, Test_Loss: 0.2623230814933777\n",
      "Epoch: 19, Train_Loss: 0.3255138397216797, Test_Loss: 0.32888978719711304\n",
      "Epoch: 19, Train_Loss: 0.2895304560661316, Test_Loss: 0.33171284198760986\n",
      "Epoch: 19, Train_Loss: 0.20107004046440125, Test_Loss: 0.236098513007164 *\n",
      "Epoch: 19, Train_Loss: 0.21870088577270508, Test_Loss: 0.24534644186496735\n",
      "Epoch: 19, Train_Loss: 0.19524452090263367, Test_Loss: 0.196586012840271 *\n",
      "Epoch: 19, Train_Loss: 0.27011606097221375, Test_Loss: 0.20331034064292908\n",
      "Epoch: 19, Train_Loss: 0.24546468257904053, Test_Loss: 0.271426260471344\n",
      "Epoch: 19, Train_Loss: 0.3149459660053253, Test_Loss: 0.307292640209198\n",
      "Epoch: 19, Train_Loss: 15.553079605102539, Test_Loss: 0.3467298746109009\n",
      "Epoch: 19, Train_Loss: 0.2226479947566986, Test_Loss: 0.22329393029212952 *\n",
      "Epoch: 19, Train_Loss: 1.2582921981811523, Test_Loss: 0.28594377636909485\n",
      "Epoch: 19, Train_Loss: 1.0701978206634521, Test_Loss: 0.25712648034095764 *\n",
      "Epoch: 19, Train_Loss: 0.21372516453266144, Test_Loss: 0.23768027126789093 *\n",
      "Epoch: 19, Train_Loss: 0.5743738412857056, Test_Loss: 0.3614465594291687\n",
      "Epoch: 19, Train_Loss: 2.9547922611236572, Test_Loss: 0.2899380922317505 *\n",
      "Epoch: 19, Train_Loss: 3.9083852767944336, Test_Loss: 0.5685232281684875\n",
      "Epoch: 19, Train_Loss: 0.25208836793899536, Test_Loss: 0.3752620220184326 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.46944886445999146, Test_Loss: 0.33699142932891846 *\n",
      "Epoch: 19, Train_Loss: 4.843456268310547, Test_Loss: 0.357436865568161\n",
      "Epoch: 19, Train_Loss: 0.409260094165802, Test_Loss: 0.40727823972702026\n",
      "Epoch: 19, Train_Loss: 0.19663360714912415, Test_Loss: 0.6739392876625061\n",
      "Epoch: 19, Train_Loss: 0.1895127147436142, Test_Loss: 0.20467111468315125 *\n",
      "Epoch: 19, Train_Loss: 0.19170208275318146, Test_Loss: 0.2563464641571045\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.20518073439598083, Test_Loss: 0.19770047068595886 *\n",
      "Epoch: 19, Train_Loss: 0.19209535419940948, Test_Loss: 0.38605451583862305\n",
      "Epoch: 19, Train_Loss: 0.19133730232715607, Test_Loss: 1.5384613275527954\n",
      "Epoch: 19, Train_Loss: 0.18906623125076294, Test_Loss: 0.8432356715202332 *\n",
      "Epoch: 19, Train_Loss: 0.1884831339120865, Test_Loss: 0.4548553228378296 *\n",
      "Epoch: 19, Train_Loss: 0.20592908561229706, Test_Loss: 0.22592923045158386 *\n",
      "Epoch: 19, Train_Loss: 0.1932229846715927, Test_Loss: 0.21102233231067657 *\n",
      "Epoch: 19, Train_Loss: 0.22605273127555847, Test_Loss: 0.20827916264533997 *\n",
      "Epoch: 19, Train_Loss: 0.23700103163719177, Test_Loss: 0.20865505933761597\n",
      "Epoch: 19, Train_Loss: 0.20859351754188538, Test_Loss: 0.2375064641237259\n",
      "Epoch: 19, Train_Loss: 0.19989775121212006, Test_Loss: 1.893873691558838\n",
      "Epoch: 19, Train_Loss: 0.28023475408554077, Test_Loss: 6.025291919708252\n",
      "Epoch: 19, Train_Loss: 0.2113405466079712, Test_Loss: 0.662632942199707 *\n",
      "Epoch: 19, Train_Loss: 0.20988227427005768, Test_Loss: 1.0181858539581299\n",
      "Epoch: 19, Train_Loss: 0.19053113460540771, Test_Loss: 1.0231837034225464\n",
      "Epoch: 19, Train_Loss: 0.1882713884115219, Test_Loss: 0.4910406172275543 *\n",
      "Epoch: 19, Train_Loss: 0.18843668699264526, Test_Loss: 0.8987584114074707\n",
      "Epoch: 19, Train_Loss: 0.18872573971748352, Test_Loss: 1.7270768880844116\n",
      "Epoch: 19, Train_Loss: 0.18944966793060303, Test_Loss: 1.1734631061553955 *\n",
      "Epoch: 19, Train_Loss: 0.18839828670024872, Test_Loss: 0.40113943815231323 *\n",
      "Epoch: 19, Train_Loss: 0.1883924901485443, Test_Loss: 0.8722213506698608\n",
      "Epoch: 19, Train_Loss: 0.19628894329071045, Test_Loss: 0.6148287653923035 *\n",
      "Epoch: 19, Train_Loss: 0.21996228396892548, Test_Loss: 2.002796173095703\n",
      "Epoch: 19, Train_Loss: 0.25401270389556885, Test_Loss: 1.4092509746551514 *\n",
      "Epoch: 19, Train_Loss: 0.24418006837368011, Test_Loss: 1.815091848373413\n",
      "Epoch: 19, Train_Loss: 0.60089111328125, Test_Loss: 0.7607812285423279 *\n",
      "Epoch: 19, Train_Loss: 3.015296220779419, Test_Loss: 0.30851805210113525 *\n",
      "Epoch: 19, Train_Loss: 4.393957138061523, Test_Loss: 0.26960498094558716 *\n",
      "Epoch: 19, Train_Loss: 0.2243689000606537, Test_Loss: 0.22863557934761047 *\n",
      "Epoch: 19, Train_Loss: 0.3594076633453369, Test_Loss: 1.0897517204284668\n",
      "Epoch: 19, Train_Loss: 0.3606773018836975, Test_Loss: 0.24257837235927582 *\n",
      "Epoch: 19, Train_Loss: 0.36421704292297363, Test_Loss: 0.4159696400165558\n",
      "Epoch: 19, Train_Loss: 0.29628121852874756, Test_Loss: 0.213014617562294 *\n",
      "Epoch: 19, Train_Loss: 0.4256642758846283, Test_Loss: 0.6692839860916138\n",
      "Epoch: 19, Train_Loss: 0.33184099197387695, Test_Loss: 0.6933202743530273\n",
      "Epoch: 19, Train_Loss: 0.3128122091293335, Test_Loss: 0.25183558464050293 *\n",
      "Epoch: 19, Train_Loss: 0.2780286371707916, Test_Loss: 0.20224183797836304 *\n",
      "Epoch: 19, Train_Loss: 0.22857964038848877, Test_Loss: 0.21450074017047882\n",
      "Epoch: 19, Train_Loss: 0.21393482387065887, Test_Loss: 0.22374045848846436\n",
      "Epoch: 19, Train_Loss: 0.23393981158733368, Test_Loss: 0.19646790623664856 *\n",
      "Epoch: 19, Train_Loss: 0.29505884647369385, Test_Loss: 0.24895340204238892\n",
      "Epoch: 19, Train_Loss: 0.2631240487098694, Test_Loss: 0.2425282746553421 *\n",
      "Epoch: 19, Train_Loss: 0.20990870893001556, Test_Loss: 3.9105873107910156\n",
      "Epoch: 19, Train_Loss: 0.23336361348628998, Test_Loss: 1.902087688446045 *\n",
      "Epoch: 19, Train_Loss: 0.20205922424793243, Test_Loss: 0.1961018294095993 *\n",
      "Epoch: 19, Train_Loss: 0.2445346713066101, Test_Loss: 0.19604113698005676 *\n",
      "Epoch: 19, Train_Loss: 0.24303817749023438, Test_Loss: 0.20952415466308594\n",
      "Epoch: 19, Train_Loss: 0.2696540653705597, Test_Loss: 0.20837680995464325 *\n",
      "Epoch: 19, Train_Loss: 0.21560335159301758, Test_Loss: 0.2027946263551712 *\n",
      "Epoch: 19, Train_Loss: 0.19635671377182007, Test_Loss: 0.24665293097496033\n",
      "Epoch: 19, Train_Loss: 0.19944645464420319, Test_Loss: 0.2580944001674652\n",
      "Epoch: 19, Train_Loss: 2.750666856765747, Test_Loss: 0.1906372606754303 *\n",
      "Epoch: 19, Train_Loss: 3.0861127376556396, Test_Loss: 0.21535173058509827\n",
      "Epoch: 19, Train_Loss: 0.20128360390663147, Test_Loss: 0.21021486818790436 *\n",
      "Epoch: 19, Train_Loss: 0.1911037415266037, Test_Loss: 0.2073349952697754 *\n",
      "Epoch: 19, Train_Loss: 0.1902371346950531, Test_Loss: 0.19410470128059387 *\n",
      "Epoch: 19, Train_Loss: 0.18831351399421692, Test_Loss: 0.23602087795734406\n",
      "Epoch: 19, Train_Loss: 0.18981868028640747, Test_Loss: 0.22959335148334503 *\n",
      "Epoch: 19, Train_Loss: 0.19009250402450562, Test_Loss: 0.27134931087493896\n",
      "Epoch: 19, Train_Loss: 0.2146676480770111, Test_Loss: 0.22944487631320953 *\n",
      "Epoch: 19, Train_Loss: 0.20208898186683655, Test_Loss: 0.2300184965133667\n",
      "Epoch: 19, Train_Loss: 0.21167023479938507, Test_Loss: 0.202422097325325 *\n",
      "Epoch: 19, Train_Loss: 0.18954110145568848, Test_Loss: 0.18855561316013336 *\n",
      "Epoch: 19, Train_Loss: 0.18759669363498688, Test_Loss: 0.19059252738952637\n",
      "Epoch: 19, Train_Loss: 0.193685844540596, Test_Loss: 0.19086724519729614\n",
      "Epoch: 19, Train_Loss: 0.19860117137432098, Test_Loss: 0.1902390569448471 *\n",
      "Epoch: 19, Train_Loss: 0.18879267573356628, Test_Loss: 0.18997524678707123 *\n",
      "Epoch: 19, Train_Loss: 0.19197364151477814, Test_Loss: 0.18934592604637146 *\n",
      "Epoch: 19, Train_Loss: 0.21555083990097046, Test_Loss: 0.1908080279827118\n",
      "Epoch: 19, Train_Loss: 0.2131403237581253, Test_Loss: 0.1946643739938736\n",
      "Epoch: 19, Train_Loss: 0.18793967366218567, Test_Loss: 0.19796344637870789\n",
      "Epoch: 19, Train_Loss: 0.18778866529464722, Test_Loss: 0.2105264812707901\n",
      "Epoch: 19, Train_Loss: 0.22881793975830078, Test_Loss: 0.1911676675081253 *\n",
      "Epoch: 19, Train_Loss: 0.20509940385818481, Test_Loss: 0.20694684982299805\n",
      "Epoch: 19, Train_Loss: 0.2099728286266327, Test_Loss: 0.29031163454055786\n",
      "Epoch: 19, Train_Loss: 0.20684565603733063, Test_Loss: 0.4628022313117981\n",
      "Epoch: 19, Train_Loss: 0.25270897150039673, Test_Loss: 0.4262588620185852 *\n",
      "Epoch: 19, Train_Loss: 0.23873475193977356, Test_Loss: 0.23651930689811707 *\n",
      "Epoch: 19, Train_Loss: 0.21049973368644714, Test_Loss: 0.20948004722595215 *\n",
      "Epoch: 19, Train_Loss: 0.2442130446434021, Test_Loss: 0.23172247409820557\n",
      "Epoch: 19, Train_Loss: 0.2822432219982147, Test_Loss: 0.29814374446868896\n",
      "Epoch: 19, Train_Loss: 0.2928447425365448, Test_Loss: 0.4697585999965668\n",
      "Epoch: 19, Train_Loss: 0.21409913897514343, Test_Loss: 0.49730339646339417\n",
      "Epoch: 19, Train_Loss: 0.18755730986595154, Test_Loss: 0.5932809114456177\n",
      "Epoch: 19, Train_Loss: 0.18704433739185333, Test_Loss: 0.23976576328277588 *\n",
      "Epoch: 19, Train_Loss: 0.18693970143795013, Test_Loss: 0.20334777235984802 *\n",
      "Epoch: 19, Train_Loss: 0.1870499551296234, Test_Loss: 0.19785720109939575 *\n",
      "Epoch: 19, Train_Loss: 0.1885073184967041, Test_Loss: 0.19203397631645203 *\n",
      "Epoch: 19, Train_Loss: 3.190310001373291, Test_Loss: 0.20771987736225128\n",
      "Epoch: 19, Train_Loss: 1.973954677581787, Test_Loss: 0.20146559178829193 *\n",
      "Epoch: 19, Train_Loss: 0.1892559975385666, Test_Loss: 0.22413811087608337\n",
      "Epoch: 19, Train_Loss: 0.19402755796909332, Test_Loss: 0.19071023166179657 *\n",
      "Epoch: 19, Train_Loss: 0.194767028093338, Test_Loss: 0.29196637868881226\n",
      "Epoch: 19, Train_Loss: 0.1883242428302765, Test_Loss: 0.41141584515571594\n",
      "Epoch: 19, Train_Loss: 0.18745413422584534, Test_Loss: 0.4198266565799713\n",
      "Epoch: 19, Train_Loss: 0.18795673549175262, Test_Loss: 0.49200084805488586\n",
      "Epoch: 19, Train_Loss: 0.1877562552690506, Test_Loss: 0.24236077070236206 *\n",
      "Epoch: 19, Train_Loss: 0.18925872445106506, Test_Loss: 0.24524471163749695\n",
      "Epoch: 19, Train_Loss: 0.2026742845773697, Test_Loss: 0.24579142034053802\n",
      "Epoch: 19, Train_Loss: 0.21661408245563507, Test_Loss: 0.24439938366413116 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.22516512870788574, Test_Loss: 0.2749309837818146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.2502587139606476, Test_Loss: 2.750924587249756\n",
      "Epoch: 19, Train_Loss: 0.20985020697116852, Test_Loss: 2.6032533645629883 *\n",
      "Epoch: 19, Train_Loss: 0.1982519030570984, Test_Loss: 0.20056606829166412 *\n",
      "Epoch: 19, Train_Loss: 0.37505650520324707, Test_Loss: 0.19404776394367218 *\n",
      "Epoch: 19, Train_Loss: 0.345961332321167, Test_Loss: 0.19593635201454163\n",
      "Epoch: 19, Train_Loss: 0.32164186239242554, Test_Loss: 0.19105786085128784 *\n",
      "Epoch: 19, Train_Loss: 0.25438666343688965, Test_Loss: 0.19546644389629364\n",
      "Epoch: 19, Train_Loss: 0.18727950751781464, Test_Loss: 0.20209535956382751\n",
      "Epoch: 19, Train_Loss: 0.18668921291828156, Test_Loss: 0.21156804263591766\n",
      "Epoch: 19, Train_Loss: 0.18890568614006042, Test_Loss: 0.1969074308872223 *\n",
      "Epoch: 19, Train_Loss: 0.1878623515367508, Test_Loss: 0.20763380825519562\n",
      "Epoch: 19, Train_Loss: 0.19026120007038116, Test_Loss: 0.21265198290348053\n",
      "Epoch: 19, Train_Loss: 0.1910211443901062, Test_Loss: 0.23012405633926392\n",
      "Epoch: 19, Train_Loss: 0.18665221333503723, Test_Loss: 0.20289184153079987 *\n",
      "Epoch: 19, Train_Loss: 0.18743523955345154, Test_Loss: 0.21894985437393188\n",
      "Epoch: 19, Train_Loss: 0.19161659479141235, Test_Loss: 0.19802214205265045 *\n",
      "Epoch: 19, Train_Loss: 0.23719623684883118, Test_Loss: 0.1887923926115036 *\n",
      "Epoch: 19, Train_Loss: 0.38198453187942505, Test_Loss: 0.19094018638134003\n",
      "Epoch: 19, Train_Loss: 0.2760400176048279, Test_Loss: 0.19556233286857605\n",
      "Epoch: 19, Train_Loss: 0.2752572000026703, Test_Loss: 0.19909235835075378\n",
      "Epoch: 19, Train_Loss: 0.28703925013542175, Test_Loss: 0.18903015553951263 *\n",
      "Epoch: 19, Train_Loss: 0.3190661072731018, Test_Loss: 0.19441622495651245\n",
      "Epoch: 19, Train_Loss: 0.2043025642633438, Test_Loss: 0.18973219394683838 *\n",
      "Epoch: 19, Train_Loss: 0.32271015644073486, Test_Loss: 0.1990508735179901\n",
      "Epoch: 19, Train_Loss: 0.25729700922966003, Test_Loss: 0.20270203053951263\n",
      "Epoch: 19, Train_Loss: 0.4530635476112366, Test_Loss: 0.1990175098180771 *\n",
      "Epoch: 19, Train_Loss: 0.19763800501823425, Test_Loss: 0.1925172358751297 *\n",
      "Epoch: 19, Train_Loss: 0.5499590039253235, Test_Loss: 0.19548246264457703\n",
      "Epoch: 19, Train_Loss: 2.7424049377441406, Test_Loss: 0.1933167278766632 *\n",
      "Epoch: 19, Train_Loss: 0.24762524664402008, Test_Loss: 0.1876571625471115 *\n",
      "Epoch: 19, Train_Loss: 0.24021676182746887, Test_Loss: 0.2474430948495865\n",
      "Epoch: 19, Train_Loss: 0.21751010417938232, Test_Loss: 0.23596152663230896 *\n",
      "Epoch: 19, Train_Loss: 0.20142102241516113, Test_Loss: 5.254920959472656\n",
      "Epoch: 19, Train_Loss: 0.19044770300388336, Test_Loss: 0.7576659917831421 *\n",
      "Epoch: 19, Train_Loss: 0.20319798588752747, Test_Loss: 0.20136800408363342 *\n",
      "Epoch: 19, Train_Loss: 0.2654532492160797, Test_Loss: 0.2228095382452011\n",
      "Epoch: 19, Train_Loss: 0.2830359637737274, Test_Loss: 0.22904789447784424\n",
      "Epoch: 19, Train_Loss: 0.24753651022911072, Test_Loss: 0.21531358361244202 *\n",
      "Epoch: 19, Train_Loss: 0.22363996505737305, Test_Loss: 0.19822287559509277 *\n",
      "Epoch: 19, Train_Loss: 0.2129109501838684, Test_Loss: 0.22521458566188812\n",
      "Epoch: 19, Train_Loss: 0.19722023606300354, Test_Loss: 0.22369883954524994 *\n",
      "Epoch: 19, Train_Loss: 0.2091800570487976, Test_Loss: 0.19973300397396088 *\n",
      "Epoch: 19, Train_Loss: 0.21805503964424133, Test_Loss: 0.20260557532310486\n",
      "Epoch: 19, Train_Loss: 0.21535126864910126, Test_Loss: 0.24063536524772644\n",
      "Epoch: 19, Train_Loss: 0.19720536470413208, Test_Loss: 0.2166711390018463 *\n",
      "Epoch: 19, Train_Loss: 0.18641802668571472, Test_Loss: 0.21619442105293274 *\n",
      "Epoch: 19, Train_Loss: 0.20824946463108063, Test_Loss: 0.31198281049728394\n",
      "Epoch: 19, Train_Loss: 0.20163173973560333, Test_Loss: 0.23197773098945618 *\n",
      "Epoch: 19, Train_Loss: 0.199930340051651, Test_Loss: 0.22039389610290527 *\n",
      "Epoch: 19, Train_Loss: 0.1905694156885147, Test_Loss: 0.20938396453857422 *\n",
      "Epoch: 19, Train_Loss: 0.1872810274362564, Test_Loss: 0.3132496774196625\n",
      "Epoch: 19, Train_Loss: 0.1863469034433365, Test_Loss: 0.21792346239089966 *\n",
      "Epoch: 19, Train_Loss: 0.18887512385845184, Test_Loss: 0.18662726879119873 *\n",
      "Epoch: 19, Train_Loss: 0.18992288410663605, Test_Loss: 0.19540196657180786\n",
      "Epoch: 19, Train_Loss: 0.1912439614534378, Test_Loss: 0.19352102279663086 *\n",
      "Epoch: 19, Train_Loss: 0.19480861723423004, Test_Loss: 0.19171805679798126 *\n",
      "Epoch: 19, Train_Loss: 0.19086037576198578, Test_Loss: 0.19223539531230927\n",
      "Epoch: 19, Train_Loss: 0.18638478219509125, Test_Loss: 0.18634618818759918 *\n",
      "Epoch: 19, Train_Loss: 0.1877409815788269, Test_Loss: 0.18882036209106445\n",
      "Epoch: 19, Train_Loss: 0.20100603997707367, Test_Loss: 0.19944080710411072\n",
      "Epoch: 19, Train_Loss: 0.21330790221691132, Test_Loss: 0.19547103345394135 *\n",
      "Epoch: 19, Train_Loss: 0.21427369117736816, Test_Loss: 0.1973070651292801\n",
      "Epoch: 19, Train_Loss: 0.20925067365169525, Test_Loss: 0.20661137998104095\n",
      "Epoch: 19, Train_Loss: 0.2028411477804184, Test_Loss: 0.22299113869667053\n",
      "Epoch: 19, Train_Loss: 0.2243419587612152, Test_Loss: 0.44857969880104065\n",
      "Epoch: 19, Train_Loss: 0.19863265752792358, Test_Loss: 0.5316027998924255\n",
      "Epoch: 19, Train_Loss: 0.19012406468391418, Test_Loss: 0.4320349097251892 *\n",
      "Epoch: 19, Train_Loss: 0.20433686673641205, Test_Loss: 0.25385093688964844 *\n",
      "Epoch: 19, Train_Loss: 0.20450334250926971, Test_Loss: 0.19205231964588165 *\n",
      "Epoch: 19, Train_Loss: 0.18870626389980316, Test_Loss: 0.20482110977172852\n",
      "Epoch: 19, Train_Loss: 0.18924850225448608, Test_Loss: 0.2548033893108368\n",
      "Epoch: 19, Train_Loss: 0.19217567145824432, Test_Loss: 0.4721028208732605\n",
      "Epoch: 19, Train_Loss: 0.21691231429576874, Test_Loss: 0.39015495777130127 *\n",
      "Epoch: 19, Train_Loss: 0.2208467572927475, Test_Loss: 0.4425298273563385\n",
      "Epoch: 19, Train_Loss: 0.2285071611404419, Test_Loss: 0.25887778401374817 *\n",
      "Epoch: 19, Train_Loss: 0.18823425471782684, Test_Loss: 0.19296135008335114 *\n",
      "Epoch: 19, Train_Loss: 0.20615887641906738, Test_Loss: 0.1930980533361435\n",
      "Epoch: 19, Train_Loss: 0.23188498616218567, Test_Loss: 0.187685027718544 *\n",
      "Epoch: 19, Train_Loss: 0.19457580149173737, Test_Loss: 0.2173086404800415\n",
      "Epoch: 19, Train_Loss: 0.19124667346477509, Test_Loss: 0.19940662384033203 *\n",
      "Epoch: 19, Train_Loss: 0.20462089776992798, Test_Loss: 0.21419696509838104\n",
      "Epoch: 19, Train_Loss: 0.2800529897212982, Test_Loss: 0.189712792634964 *\n",
      "Epoch: 19, Train_Loss: 0.2542975842952728, Test_Loss: 0.31934335827827454\n",
      "Epoch: 19, Train_Loss: 0.22240222990512848, Test_Loss: 0.534909725189209\n",
      "Epoch: 19, Train_Loss: 0.21532076597213745, Test_Loss: 0.2767196595668793 *\n",
      "Epoch: 19, Train_Loss: 0.1913505345582962, Test_Loss: 0.42146480083465576\n",
      "Epoch: 19, Train_Loss: 0.21155628561973572, Test_Loss: 0.21251548826694489 *\n",
      "Epoch: 19, Train_Loss: 0.18686039745807648, Test_Loss: 0.21371716260910034\n",
      "Epoch: 19, Train_Loss: 0.19188137352466583, Test_Loss: 0.213094562292099 *\n",
      "Epoch: 19, Train_Loss: 0.19337251782417297, Test_Loss: 0.21076063811779022 *\n",
      "Epoch: 19, Train_Loss: 0.19814783334732056, Test_Loss: 0.23194721341133118\n",
      "Epoch: 19, Train_Loss: 0.24881622195243835, Test_Loss: 4.175954341888428\n",
      "Epoch: 19, Train_Loss: 0.22010667622089386, Test_Loss: 1.5260626077651978 *\n",
      "Epoch: 19, Train_Loss: 0.23071841895580292, Test_Loss: 0.19405926764011383 *\n",
      "Epoch: 19, Train_Loss: 0.19904737174510956, Test_Loss: 0.19178371131420135 *\n",
      "Epoch: 19, Train_Loss: 0.20932334661483765, Test_Loss: 0.1910775601863861 *\n",
      "Epoch: 19, Train_Loss: 0.197274848818779, Test_Loss: 0.18937444686889648 *\n",
      "Epoch: 19, Train_Loss: 0.41664496064186096, Test_Loss: 0.19128470122814178\n",
      "Epoch: 19, Train_Loss: 0.29409509897232056, Test_Loss: 0.2010667771100998\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.19421792030334473, Test_Loss: 0.2084246128797531\n",
      "Epoch: 19, Train_Loss: 0.20934970676898956, Test_Loss: 0.19338463246822357 *\n",
      "Epoch: 19, Train_Loss: 0.18723000586032867, Test_Loss: 0.20125877857208252\n",
      "Epoch: 19, Train_Loss: 0.1890464872121811, Test_Loss: 0.21442769467830658\n",
      "Epoch: 19, Train_Loss: 0.18697163462638855, Test_Loss: 0.21659329533576965\n",
      "Epoch: 19, Train_Loss: 0.18769638240337372, Test_Loss: 0.199627086520195 *\n",
      "Epoch: 19, Train_Loss: 0.18760700523853302, Test_Loss: 0.2250257134437561\n",
      "Epoch: 19, Train_Loss: 0.20393261313438416, Test_Loss: 0.19084209203720093 *\n",
      "Epoch: 19, Train_Loss: 0.18789465725421906, Test_Loss: 0.186030313372612 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.18942762911319733, Test_Loss: 0.18576839566230774 *\n",
      "Epoch: 19, Train_Loss: 0.1955753117799759, Test_Loss: 0.19272376596927643\n",
      "Epoch: 19, Train_Loss: 0.18578997254371643, Test_Loss: 0.1897503137588501 *\n",
      "Epoch: 19, Train_Loss: 0.18568697571754456, Test_Loss: 0.1854347437620163 *\n",
      "Epoch: 19, Train_Loss: 0.19200438261032104, Test_Loss: 0.1895321160554886\n",
      "Epoch: 19, Train_Loss: 0.20021435618400574, Test_Loss: 0.18674106895923615 *\n",
      "Epoch: 19, Train_Loss: 0.20063287019729614, Test_Loss: 0.18704906105995178\n",
      "Epoch: 19, Train_Loss: 0.19314156472682953, Test_Loss: 0.19341103732585907\n",
      "Epoch: 19, Train_Loss: 0.19784203171730042, Test_Loss: 0.1875249296426773 *\n",
      "Epoch: 19, Train_Loss: 0.207363560795784, Test_Loss: 0.18603284657001495 *\n",
      "Epoch: 19, Train_Loss: 0.211317241191864, Test_Loss: 0.18782977759838104\n",
      "Epoch: 19, Train_Loss: 0.18762674927711487, Test_Loss: 0.18628694117069244 *\n",
      "Epoch: 19, Train_Loss: 0.20513951778411865, Test_Loss: 0.18502944707870483 *\n",
      "Epoch: 19, Train_Loss: 0.19957400858402252, Test_Loss: 0.24671818315982819\n",
      "Epoch: 19, Train_Loss: 0.19440391659736633, Test_Loss: 0.27133890986442566\n",
      "Epoch: 19, Train_Loss: 0.18692761659622192, Test_Loss: 5.976551055908203\n",
      "Epoch: 19, Train_Loss: 0.20458044111728668, Test_Loss: 0.2565222680568695 *\n",
      "Epoch: 19, Train_Loss: 0.23965907096862793, Test_Loss: 0.18557144701480865 *\n",
      "Epoch: 19, Train_Loss: 2.455275058746338, Test_Loss: 0.2103249877691269\n",
      "Epoch: 19, Train_Loss: 3.185447931289673, Test_Loss: 0.21109597384929657\n",
      "Epoch: 19, Train_Loss: 0.23159503936767578, Test_Loss: 0.2093173861503601 *\n",
      "Epoch: 19, Train_Loss: 0.191338911652565, Test_Loss: 0.19042974710464478 *\n",
      "Epoch: 19, Train_Loss: 0.23763540387153625, Test_Loss: 0.28316566348075867\n",
      "Epoch: 19, Train_Loss: 0.30302172899246216, Test_Loss: 0.22848859429359436 *\n",
      "Epoch: 19, Train_Loss: 0.216548353433609, Test_Loss: 0.18640969693660736 *\n",
      "Epoch: 19, Train_Loss: 0.18823201954364777, Test_Loss: 0.21240577101707458\n",
      "Epoch: 19, Train_Loss: 0.20597055554389954, Test_Loss: 0.2043018788099289 *\n",
      "Epoch: 19, Train_Loss: 0.2583380341529846, Test_Loss: 0.18743160367012024 *\n",
      "Epoch: 19, Train_Loss: 0.1954827606678009, Test_Loss: 0.21528232097625732\n",
      "Epoch: 19, Train_Loss: 0.19684436917304993, Test_Loss: 0.23987853527069092\n",
      "Epoch: 19, Train_Loss: 0.6968600749969482, Test_Loss: 0.23146554827690125 *\n",
      "Epoch: 19, Train_Loss: 0.9208174347877502, Test_Loss: 0.24025286734104156\n",
      "Epoch: 19, Train_Loss: 0.5221236944198608, Test_Loss: 0.21723125874996185 *\n",
      "Epoch: 19, Train_Loss: 0.29304033517837524, Test_Loss: 0.24737250804901123\n",
      "Epoch: 19, Train_Loss: 1.130661964416504, Test_Loss: 0.1891227811574936 *\n",
      "Epoch: 19, Train_Loss: 1.267833948135376, Test_Loss: 0.19093596935272217\n",
      "Epoch: 19, Train_Loss: 0.24434292316436768, Test_Loss: 0.1924491971731186\n",
      "Epoch: 19, Train_Loss: 0.1895938366651535, Test_Loss: 0.1975027620792389\n",
      "Epoch: 19, Train_Loss: 0.3453792333602905, Test_Loss: 0.19362246990203857 *\n",
      "Epoch: 19, Train_Loss: 0.6819337606430054, Test_Loss: 0.1935826539993286 *\n",
      "Epoch: 19, Train_Loss: 0.7293514013290405, Test_Loss: 0.20314407348632812\n",
      "Epoch: 19, Train_Loss: 0.1932341754436493, Test_Loss: 0.22150272130966187\n",
      "Epoch: 19, Train_Loss: 0.20118308067321777, Test_Loss: 0.19910818338394165 *\n",
      "Epoch: 19, Train_Loss: 0.2108786553144455, Test_Loss: 0.20607562363147736\n",
      "Epoch: 19, Train_Loss: 0.31658875942230225, Test_Loss: 0.1930111050605774 *\n",
      "Epoch: 19, Train_Loss: 0.22030571103096008, Test_Loss: 0.20099671185016632\n",
      "Epoch: 19, Train_Loss: 0.2729358971118927, Test_Loss: 0.23076611757278442\n",
      "Epoch: 19, Train_Loss: 0.2362523376941681, Test_Loss: 0.42963162064552307\n",
      "Epoch: 19, Train_Loss: 0.2105667144060135, Test_Loss: 0.2992730736732483 *\n",
      "Epoch: 19, Train_Loss: 0.324943870306015, Test_Loss: 0.2919746935367584 *\n",
      "Epoch: 19, Train_Loss: 0.3025726079940796, Test_Loss: 0.24822241067886353 *\n",
      "Epoch: 19, Train_Loss: 0.21683946251869202, Test_Loss: 0.25623637437820435\n",
      "Epoch: 19, Train_Loss: 0.28131625056266785, Test_Loss: 0.19919414818286896 *\n",
      "Epoch: 19, Train_Loss: 0.2852148413658142, Test_Loss: 0.22790741920471191\n",
      "Epoch: 19, Train_Loss: 0.2056654393672943, Test_Loss: 0.44947418570518494\n",
      "Epoch: 19, Train_Loss: 0.21110716462135315, Test_Loss: 0.22905582189559937 *\n",
      "Epoch: 19, Train_Loss: 0.270463228225708, Test_Loss: 0.3528502583503723\n",
      "Epoch: 19, Train_Loss: 0.22886517643928528, Test_Loss: 0.3078846335411072 *\n",
      "Epoch: 19, Train_Loss: 0.251373827457428, Test_Loss: 0.23916545510292053 *\n",
      "Epoch: 19, Train_Loss: 0.34095168113708496, Test_Loss: 0.19432073831558228 *\n",
      "Epoch: 19, Train_Loss: 0.20291288197040558, Test_Loss: 0.19511184096336365\n",
      "Epoch: 19, Train_Loss: 0.1940942108631134, Test_Loss: 0.25227001309394836\n",
      "Epoch: 19, Train_Loss: 0.1842370480298996, Test_Loss: 0.1901218146085739 *\n",
      "Epoch: 19, Train_Loss: 0.1899581104516983, Test_Loss: 0.2044232338666916\n",
      "Epoch: 19, Train_Loss: 0.1911454051733017, Test_Loss: 0.19263355433940887 *\n",
      "Epoch: 19, Train_Loss: 0.19136078655719757, Test_Loss: 0.4540662169456482\n",
      "Epoch: 19, Train_Loss: 0.2071431577205658, Test_Loss: 0.5463730096817017\n",
      "Epoch: 19, Train_Loss: 0.20810231566429138, Test_Loss: 0.39220285415649414 *\n",
      "Epoch: 19, Train_Loss: 0.2089732438325882, Test_Loss: 0.5969651937484741\n",
      "Epoch: 19, Train_Loss: 0.3896392583847046, Test_Loss: 0.3854847550392151 *\n",
      "Epoch: 19, Train_Loss: 0.4118475914001465, Test_Loss: 0.3886517882347107\n",
      "Epoch: 19, Train_Loss: 0.1900162398815155, Test_Loss: 0.38704097270965576 *\n",
      "Epoch: 19, Train_Loss: 0.23877477645874023, Test_Loss: 0.41935646533966064\n",
      "Epoch: 19, Train_Loss: 0.24844524264335632, Test_Loss: 0.4173967242240906 *\n",
      "Epoch: 19, Train_Loss: 0.2200310081243515, Test_Loss: 5.064704895019531\n",
      "Epoch: 19, Train_Loss: 0.3998551368713379, Test_Loss: 0.5294734835624695 *\n",
      "Epoch: 19, Train_Loss: 0.25549405813217163, Test_Loss: 0.21705758571624756 *\n",
      "Epoch: 19, Train_Loss: 0.39874494075775146, Test_Loss: 0.21303603053092957 *\n",
      "Epoch: 19, Train_Loss: 0.30573412775993347, Test_Loss: 0.19817274808883667 *\n",
      "Epoch: 19, Train_Loss: 0.27715301513671875, Test_Loss: 0.19897344708442688\n",
      "Epoch: 19, Train_Loss: 0.20673176646232605, Test_Loss: 0.2187035232782364\n",
      "Epoch: 19, Train_Loss: 0.1994098573923111, Test_Loss: 0.22832871973514557\n",
      "Epoch: 19, Train_Loss: 0.3077612817287445, Test_Loss: 0.20219075679779053 *\n",
      "Epoch: 19, Train_Loss: 0.6814848780632019, Test_Loss: 0.20310157537460327\n",
      "Epoch: 19, Train_Loss: 0.5805759429931641, Test_Loss: 0.19616201519966125 *\n",
      "Epoch: 19, Train_Loss: 0.2159801572561264, Test_Loss: 0.2551424503326416\n",
      "Epoch: 19, Train_Loss: 0.2393224835395813, Test_Loss: 0.2306179404258728 *\n",
      "Epoch: 19, Train_Loss: 0.19001933932304382, Test_Loss: 0.1946476548910141 *\n",
      "Epoch: 19, Train_Loss: 0.3345288634300232, Test_Loss: 0.2209780067205429\n",
      "Epoch: 19, Train_Loss: 0.537945568561554, Test_Loss: 0.19786003232002258 *\n",
      "Epoch: 19, Train_Loss: 0.18811124563217163, Test_Loss: 0.1917399764060974 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 19\n",
      "Epoch: 19, Train_Loss: 0.3335699737071991, Test_Loss: 0.2313271313905716\n",
      "Epoch: 19, Train_Loss: 0.20971202850341797, Test_Loss: 0.22951479256153107 *\n",
      "Epoch: 19, Train_Loss: 0.22545936703681946, Test_Loss: 0.19243915379047394 *\n",
      "Epoch: 19, Train_Loss: 0.22421328723430634, Test_Loss: 0.19641418755054474\n",
      "Epoch: 19, Train_Loss: 0.3167264759540558, Test_Loss: 0.20044486224651337\n",
      "Epoch: 19, Train_Loss: 0.28845465183258057, Test_Loss: 0.21123383939266205\n",
      "Epoch: 19, Train_Loss: 0.2280866503715515, Test_Loss: 0.200321227312088 *\n",
      "Epoch: 19, Train_Loss: 0.20187069475650787, Test_Loss: 0.23938484489917755\n",
      "Epoch: 19, Train_Loss: 0.267299622297287, Test_Loss: 0.1894121766090393 *\n",
      "Epoch: 19, Train_Loss: 0.24179361760616302, Test_Loss: 0.20056192576885223\n",
      "Epoch: 19, Train_Loss: 0.20786356925964355, Test_Loss: 0.2413789927959442\n",
      "Epoch: 19, Train_Loss: 0.1935226023197174, Test_Loss: 0.1980564296245575 *\n",
      "Epoch: 19, Train_Loss: 0.21550826728343964, Test_Loss: 0.18527038395404816 *\n",
      "Epoch: 19, Train_Loss: 0.23918509483337402, Test_Loss: 0.2938978374004364\n",
      "Epoch: 19, Train_Loss: 0.42991751432418823, Test_Loss: 0.959112286567688\n",
      "Epoch: 19, Train_Loss: 0.38892388343811035, Test_Loss: 4.428282737731934\n",
      "Epoch: 19, Train_Loss: 0.5904704928398132, Test_Loss: 0.21446186304092407 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train_Loss: 0.4212099015712738, Test_Loss: 0.22012223303318024\n",
      "Epoch: 19, Train_Loss: 0.3794682025909424, Test_Loss: 0.24417710304260254\n",
      "Epoch: 19, Train_Loss: 0.2581782042980194, Test_Loss: 0.21812891960144043 *\n",
      "Epoch: 19, Train_Loss: 0.22151485085487366, Test_Loss: 0.20120367407798767 *\n",
      "Epoch: 19, Train_Loss: 0.19526544213294983, Test_Loss: 0.20104137063026428 *\n",
      "Epoch: 19, Train_Loss: 0.19579190015792847, Test_Loss: 0.2499958872795105\n",
      "Epoch: 19, Train_Loss: 0.27969250082969666, Test_Loss: 0.20573270320892334 *\n",
      "Epoch: 19, Train_Loss: 0.4929257333278656, Test_Loss: 0.19300474226474762 *\n",
      "Epoch: 19, Train_Loss: 0.6111495494842529, Test_Loss: 0.2175115942955017\n",
      "Epoch: 19, Train_Loss: 1.032492995262146, Test_Loss: 0.2392621636390686\n",
      "Epoch: 19, Train_Loss: 1.1542006731033325, Test_Loss: 0.19708877801895142 *\n",
      "Epoch: 19, Train_Loss: 0.3071635663509369, Test_Loss: 0.3097584843635559\n",
      "Epoch: 19, Train_Loss: 0.4151638150215149, Test_Loss: 0.30700188875198364 *\n",
      "Epoch: 19, Train_Loss: 0.18728885054588318, Test_Loss: 0.27211499214172363 *\n",
      "Epoch: 19, Train_Loss: 0.23171919584274292, Test_Loss: 0.20836488902568817 *\n",
      "Epoch: 19, Train_Loss: 0.3309439420700073, Test_Loss: 0.25105589628219604\n",
      "Epoch: 19, Train_Loss: 0.8518579006195068, Test_Loss: 0.24145320057868958 *\n",
      "Epoch: 19, Train_Loss: 0.222810760140419, Test_Loss: 0.2383003979921341 *\n",
      "Epoch: 19, Train_Loss: 0.21381816267967224, Test_Loss: 0.3820587992668152\n",
      "Epoch: 19, Train_Loss: 0.22153915464878082, Test_Loss: 0.3071003556251526 *\n",
      "Epoch: 19, Train_Loss: 0.3978395462036133, Test_Loss: 0.28686806559562683 *\n",
      "Epoch: 19, Train_Loss: 0.4074646234512329, Test_Loss: 0.2716028690338135 *\n",
      "Epoch: 19, Train_Loss: 0.500862717628479, Test_Loss: 0.29284030199050903\n",
      "Epoch: 19, Train_Loss: 0.3551000952720642, Test_Loss: 0.36475637555122375\n",
      "Epoch: 19, Train_Loss: 0.4226694107055664, Test_Loss: 0.3640193045139313 *\n",
      "Epoch: 19, Train_Loss: 0.18944506347179413, Test_Loss: 0.21818514168262482 *\n",
      "Epoch: 19, Train_Loss: 0.19832856953144073, Test_Loss: 0.243160218000412\n",
      "Epoch: 19, Train_Loss: 0.18990519642829895, Test_Loss: 0.18770985305309296 *\n",
      "Epoch: 19, Train_Loss: 0.23007944226264954, Test_Loss: 0.192714124917984\n",
      "Epoch: 19, Train_Loss: 0.200994074344635, Test_Loss: 0.24608160555362701\n",
      "Epoch: 19, Train_Loss: 0.2749558389186859, Test_Loss: 0.3611944317817688\n",
      "Epoch: 19, Train_Loss: 15.1561918258667, Test_Loss: 0.26965567469596863 *\n",
      "Epoch: 19, Train_Loss: 0.2102128267288208, Test_Loss: 0.24290844798088074 *\n",
      "Epoch: 19, Train_Loss: 1.4089583158493042, Test_Loss: 0.23713839054107666 *\n",
      "Epoch: 19, Train_Loss: 1.009019136428833, Test_Loss: 0.22656118869781494 *\n",
      "Epoch: 19, Train_Loss: 0.21420951187610626, Test_Loss: 0.19887901842594147 *\n",
      "Epoch: 19, Train_Loss: 0.2904707193374634, Test_Loss: 0.2509174346923828\n",
      "Epoch: 20, Train_Loss: 2.0138802528381348, Test_Loss: 0.3334410786628723 *\n",
      "Epoch: 20, Train_Loss: 4.3498969078063965, Test_Loss: 0.44961678981781006\n",
      "Epoch: 20, Train_Loss: 0.2945072650909424, Test_Loss: 0.29449933767318726 *\n",
      "Epoch: 20, Train_Loss: 0.2533472776412964, Test_Loss: 0.45144373178482056\n",
      "Epoch: 20, Train_Loss: 4.100104331970215, Test_Loss: 0.32841917872428894 *\n",
      "Epoch: 20, Train_Loss: 0.4852352440357208, Test_Loss: 0.3599587678909302\n",
      "Epoch: 20, Train_Loss: 0.36380577087402344, Test_Loss: 0.5847302675247192\n",
      "Epoch: 20, Train_Loss: 0.18435819447040558, Test_Loss: 0.47641927003860474 *\n",
      "Epoch: 20, Train_Loss: 0.18578550219535828, Test_Loss: 0.22088967263698578 *\n",
      "Epoch: 20, Train_Loss: 0.21907919645309448, Test_Loss: 0.20250724256038666 *\n",
      "Epoch: 20, Train_Loss: 0.18786439299583435, Test_Loss: 0.5459330081939697\n",
      "Epoch: 20, Train_Loss: 0.18740913271903992, Test_Loss: 1.2142903804779053\n",
      "Epoch: 20, Train_Loss: 0.18277199566364288, Test_Loss: 1.9442684650421143\n",
      "Epoch: 20, Train_Loss: 0.1817798912525177, Test_Loss: 0.4610934555530548 *\n",
      "Epoch: 20, Train_Loss: 0.19712266325950623, Test_Loss: 0.23171627521514893 *\n",
      "Epoch: 20, Train_Loss: 0.2219104915857315, Test_Loss: 0.18957142531871796 *\n",
      "Epoch: 20, Train_Loss: 0.25930526852607727, Test_Loss: 0.19350072741508484\n",
      "Epoch: 20, Train_Loss: 0.2837965488433838, Test_Loss: 0.18622009456157684 *\n",
      "Epoch: 20, Train_Loss: 0.19039130210876465, Test_Loss: 0.19054587185382843\n",
      "Epoch: 20, Train_Loss: 0.1910223364830017, Test_Loss: 0.41900545358657837\n",
      "Epoch: 20, Train_Loss: 0.2558216452598572, Test_Loss: 9.463264465332031\n",
      "Epoch: 20, Train_Loss: 0.19067628681659698, Test_Loss: 0.5352705121040344 *\n",
      "Epoch: 20, Train_Loss: 0.2131742238998413, Test_Loss: 1.4530792236328125\n",
      "Epoch: 20, Train_Loss: 0.18382035195827484, Test_Loss: 1.1864241361618042 *\n",
      "Epoch: 20, Train_Loss: 0.18211553990840912, Test_Loss: 0.8515152931213379 *\n",
      "Epoch: 20, Train_Loss: 0.18230390548706055, Test_Loss: 0.6140655875205994 *\n",
      "Epoch: 20, Train_Loss: 0.18282881379127502, Test_Loss: 2.1004295349121094\n",
      "Epoch: 20, Train_Loss: 0.1850726157426834, Test_Loss: 1.504718542098999 *\n",
      "Epoch: 20, Train_Loss: 0.18265336751937866, Test_Loss: 0.392782986164093 *\n",
      "Epoch: 20, Train_Loss: 0.18279145658016205, Test_Loss: 1.03848397731781\n",
      "Epoch: 20, Train_Loss: 0.1900842934846878, Test_Loss: 0.5761249661445618 *\n",
      "Epoch: 20, Train_Loss: 0.22619162499904633, Test_Loss: 1.9135305881500244\n",
      "Epoch: 20, Train_Loss: 0.21651652455329895, Test_Loss: 1.5088415145874023 *\n",
      "Epoch: 20, Train_Loss: 0.2770596444606781, Test_Loss: 1.6919264793395996\n",
      "Epoch: 20, Train_Loss: 0.3131765127182007, Test_Loss: 1.4340680837631226 *\n",
      "Epoch: 20, Train_Loss: 1.0527867078781128, Test_Loss: 0.26106446981430054 *\n",
      "Epoch: 20, Train_Loss: 4.722132682800293, Test_Loss: 0.3238903880119324\n",
      "Epoch: 20, Train_Loss: 0.22556143999099731, Test_Loss: 0.21473993360996246 *\n",
      "Epoch: 20, Train_Loss: 0.2737104892730713, Test_Loss: 0.9498712420463562\n",
      "Epoch: 20, Train_Loss: 0.23847582936286926, Test_Loss: 0.37041258811950684 *\n",
      "Epoch: 20, Train_Loss: 0.2963879108428955, Test_Loss: 0.611751914024353\n",
      "Epoch: 20, Train_Loss: 0.35232114791870117, Test_Loss: 0.2806330919265747 *\n",
      "Epoch: 20, Train_Loss: 0.4123607277870178, Test_Loss: 1.057686686515808\n",
      "Epoch: 20, Train_Loss: 0.33643001317977905, Test_Loss: 0.8149653673171997 *\n",
      "Epoch: 20, Train_Loss: 0.27024298906326294, Test_Loss: 0.5299622416496277 *\n",
      "Epoch: 20, Train_Loss: 0.2548128664493561, Test_Loss: 0.2718241810798645 *\n",
      "Epoch: 20, Train_Loss: 0.22714002430438995, Test_Loss: 0.24030622839927673 *\n",
      "Epoch: 20, Train_Loss: 0.1890193670988083, Test_Loss: 0.3023293614387512\n",
      "Epoch: 20, Train_Loss: 0.23595696687698364, Test_Loss: 0.21912138164043427 *\n",
      "Epoch: 20, Train_Loss: 0.20149539411067963, Test_Loss: 0.19165438413619995 *\n",
      "Epoch: 20, Train_Loss: 0.4779209494590759, Test_Loss: 0.33307603001594543\n",
      "Epoch: 20, Train_Loss: 0.18623679876327515, Test_Loss: 2.303426742553711\n",
      "Epoch: 20, Train_Loss: 0.22345948219299316, Test_Loss: 3.4364168643951416\n",
      "Epoch: 20, Train_Loss: 0.1883765459060669, Test_Loss: 0.20334085822105408 *\n",
      "Epoch: 20, Train_Loss: 0.2191634625196457, Test_Loss: 0.1904895305633545 *\n",
      "Epoch: 20, Train_Loss: 0.2539828419685364, Test_Loss: 0.2157231569290161\n",
      "Epoch: 20, Train_Loss: 0.24763010442256927, Test_Loss: 0.1882539987564087 *\n",
      "Epoch: 20, Train_Loss: 0.19993291795253754, Test_Loss: 0.20773115754127502\n",
      "Epoch: 20, Train_Loss: 0.18403558433055878, Test_Loss: 0.21727164089679718\n",
      "Epoch: 20, Train_Loss: 0.1826060712337494, Test_Loss: 0.23245909810066223\n",
      "Epoch: 20, Train_Loss: 1.3870583772659302, Test_Loss: 0.1912115216255188 *\n",
      "Epoch: 20, Train_Loss: 4.6634521484375, Test_Loss: 0.20013457536697388\n",
      "Epoch: 20, Train_Loss: 0.20126037299633026, Test_Loss: 0.20595593750476837\n",
      "Epoch: 20, Train_Loss: 0.18318188190460205, Test_Loss: 0.22113464772701263\n",
      "Epoch: 20, Train_Loss: 0.18337395787239075, Test_Loss: 0.19295355677604675 *\n",
      "Epoch: 20, Train_Loss: 0.1831231266260147, Test_Loss: 0.2349490523338318\n",
      "Epoch: 20, Train_Loss: 0.18274152278900146, Test_Loss: 0.21941983699798584 *\n",
      "Epoch: 20, Train_Loss: 0.18220746517181396, Test_Loss: 0.2514230012893677\n",
      "Epoch: 20, Train_Loss: 0.19573572278022766, Test_Loss: 0.20731256902217865 *\n",
      "Epoch: 20, Train_Loss: 0.2037571519613266, Test_Loss: 0.2131100296974182\n",
      "Epoch: 20, Train_Loss: 0.20790284872055054, Test_Loss: 0.21668142080307007\n",
      "Epoch: 20, Train_Loss: 0.18327213823795319, Test_Loss: 0.18363286554813385 *\n",
      "Epoch: 20, Train_Loss: 0.18219223618507385, Test_Loss: 0.19078338146209717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.18148115277290344, Test_Loss: 0.1945876181125641\n",
      "Epoch: 20, Train_Loss: 0.19736336171627045, Test_Loss: 0.1929255723953247 *\n",
      "Epoch: 20, Train_Loss: 0.1825072467327118, Test_Loss: 0.1901063323020935 *\n",
      "Epoch: 20, Train_Loss: 0.18382108211517334, Test_Loss: 0.18634328246116638 *\n",
      "Epoch: 20, Train_Loss: 0.2069905400276184, Test_Loss: 0.18798476457595825\n",
      "Epoch: 20, Train_Loss: 0.21520282328128815, Test_Loss: 0.19032973051071167\n",
      "Epoch: 20, Train_Loss: 0.18659664690494537, Test_Loss: 0.18496397137641907 *\n",
      "Epoch: 20, Train_Loss: 0.18108950555324554, Test_Loss: 0.20112933218479156\n",
      "Epoch: 20, Train_Loss: 0.19672702252864838, Test_Loss: 0.18406634032726288 *\n",
      "Epoch: 20, Train_Loss: 0.2144709825515747, Test_Loss: 0.20419283211231232\n",
      "Epoch: 20, Train_Loss: 0.20369993150234222, Test_Loss: 0.20227190852165222 *\n",
      "Epoch: 20, Train_Loss: 0.20495375990867615, Test_Loss: 0.47826287150382996\n",
      "Epoch: 20, Train_Loss: 0.22616097331047058, Test_Loss: 0.4180956482887268 *\n",
      "Epoch: 20, Train_Loss: 0.22121509909629822, Test_Loss: 0.2508818805217743 *\n",
      "Epoch: 20, Train_Loss: 0.2076001763343811, Test_Loss: 0.20888979732990265 *\n",
      "Epoch: 20, Train_Loss: 0.22846835851669312, Test_Loss: 0.213286891579628\n",
      "Epoch: 20, Train_Loss: 0.20863793790340424, Test_Loss: 0.21738804876804352\n",
      "Epoch: 20, Train_Loss: 0.4360617697238922, Test_Loss: 0.35462069511413574\n",
      "Epoch: 20, Train_Loss: 0.22873619198799133, Test_Loss: 0.41337960958480835\n",
      "Epoch: 20, Train_Loss: 0.18207836151123047, Test_Loss: 0.5698045492172241\n",
      "Epoch: 20, Train_Loss: 0.18095949292182922, Test_Loss: 0.2495497167110443 *\n",
      "Epoch: 20, Train_Loss: 0.18047408759593964, Test_Loss: 0.2242238074541092 *\n",
      "Epoch: 20, Train_Loss: 0.18045827746391296, Test_Loss: 0.1852283477783203 *\n",
      "Epoch: 20, Train_Loss: 0.18072645366191864, Test_Loss: 0.18618018925189972\n",
      "Epoch: 20, Train_Loss: 1.4577003717422485, Test_Loss: 0.1905871033668518\n",
      "Epoch: 20, Train_Loss: 3.417093276977539, Test_Loss: 0.19244670867919922\n",
      "Epoch: 20, Train_Loss: 0.18823710083961487, Test_Loss: 0.22249820828437805\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.18506823480129242, Test_Loss: 0.18651427328586578 *\n",
      "Epoch: 20, Train_Loss: 0.19167497754096985, Test_Loss: 0.2177256941795349\n",
      "Epoch: 20, Train_Loss: 0.18113450706005096, Test_Loss: 0.3188372552394867\n",
      "Epoch: 20, Train_Loss: 0.18271678686141968, Test_Loss: 0.5208878517150879\n",
      "Epoch: 20, Train_Loss: 0.18179847300052643, Test_Loss: 0.4825216233730316 *\n",
      "Epoch: 20, Train_Loss: 0.1809065341949463, Test_Loss: 0.23257440328598022 *\n",
      "Epoch: 20, Train_Loss: 0.18427687883377075, Test_Loss: 0.23082762956619263 *\n",
      "Epoch: 20, Train_Loss: 0.18451692163944244, Test_Loss: 0.23128320276737213\n",
      "Epoch: 20, Train_Loss: 0.21769165992736816, Test_Loss: 0.2344931960105896\n",
      "Epoch: 20, Train_Loss: 0.20777948200702667, Test_Loss: 0.2708089351654053\n",
      "Epoch: 20, Train_Loss: 0.2581940293312073, Test_Loss: 0.9578192234039307\n",
      "Epoch: 20, Train_Loss: 0.21681085228919983, Test_Loss: 4.352109909057617\n",
      "Epoch: 20, Train_Loss: 0.1810569167137146, Test_Loss: 0.21327002346515656 *\n",
      "Epoch: 20, Train_Loss: 0.3406432867050171, Test_Loss: 0.1896076500415802 *\n",
      "Epoch: 20, Train_Loss: 0.3478713631629944, Test_Loss: 0.19094784557819366\n",
      "Epoch: 20, Train_Loss: 0.32903197407722473, Test_Loss: 0.18411190807819366 *\n",
      "Epoch: 20, Train_Loss: 0.28891435265541077, Test_Loss: 0.18722879886627197\n",
      "Epoch: 20, Train_Loss: 0.1824396699666977, Test_Loss: 0.18944039940834045\n",
      "Epoch: 20, Train_Loss: 0.1805451363325119, Test_Loss: 0.2000841647386551\n",
      "Epoch: 20, Train_Loss: 0.18297699093818665, Test_Loss: 0.18630360066890717 *\n",
      "Epoch: 20, Train_Loss: 0.18247267603874207, Test_Loss: 0.19243073463439941\n",
      "Epoch: 20, Train_Loss: 0.18288929760456085, Test_Loss: 0.19140572845935822 *\n",
      "Epoch: 20, Train_Loss: 0.18433848023414612, Test_Loss: 0.24407359957695007\n",
      "Epoch: 20, Train_Loss: 0.18178042769432068, Test_Loss: 0.19788794219493866 *\n",
      "Epoch: 20, Train_Loss: 0.18137677013874054, Test_Loss: 0.1979338824748993\n",
      "Epoch: 20, Train_Loss: 0.1864953339099884, Test_Loss: 0.2085312455892563\n",
      "Epoch: 20, Train_Loss: 0.20268899202346802, Test_Loss: 0.18597596883773804 *\n",
      "Epoch: 20, Train_Loss: 0.34599900245666504, Test_Loss: 0.18418841063976288 *\n",
      "Epoch: 20, Train_Loss: 0.24574235081672668, Test_Loss: 0.19332051277160645\n",
      "Epoch: 20, Train_Loss: 0.2745888829231262, Test_Loss: 0.1960752159357071\n",
      "Epoch: 20, Train_Loss: 0.2473403811454773, Test_Loss: 0.1847449243068695 *\n",
      "Epoch: 20, Train_Loss: 0.2927902042865753, Test_Loss: 0.19525617361068726\n",
      "Epoch: 20, Train_Loss: 0.25293511152267456, Test_Loss: 0.1816408634185791 *\n",
      "Epoch: 20, Train_Loss: 0.27694201469421387, Test_Loss: 0.19181978702545166\n",
      "Epoch: 20, Train_Loss: 0.2616671919822693, Test_Loss: 0.19412168860435486\n",
      "Epoch: 20, Train_Loss: 0.42813628911972046, Test_Loss: 0.1912958025932312 *\n",
      "Epoch: 20, Train_Loss: 0.19078393280506134, Test_Loss: 0.1851179003715515 *\n",
      "Epoch: 20, Train_Loss: 0.19358845055103302, Test_Loss: 0.1885872632265091\n",
      "Epoch: 20, Train_Loss: 2.817545175552368, Test_Loss: 0.19605891406536102\n",
      "Epoch: 20, Train_Loss: 0.3715890347957611, Test_Loss: 0.18963798880577087 *\n",
      "Epoch: 20, Train_Loss: 0.23570294678211212, Test_Loss: 0.20087096095085144\n",
      "Epoch: 20, Train_Loss: 0.23280327022075653, Test_Loss: 0.25485965609550476\n",
      "Epoch: 20, Train_Loss: 0.19226175546646118, Test_Loss: 3.354708671569824\n",
      "Epoch: 20, Train_Loss: 0.19712646305561066, Test_Loss: 2.4725587368011475 *\n",
      "Epoch: 20, Train_Loss: 0.1895698606967926, Test_Loss: 0.20577530562877655 *\n",
      "Epoch: 20, Train_Loss: 0.21878044307231903, Test_Loss: 0.22067372500896454\n",
      "Epoch: 20, Train_Loss: 0.27164313197135925, Test_Loss: 0.28520694375038147\n",
      "Epoch: 20, Train_Loss: 0.2344556450843811, Test_Loss: 0.2080586850643158 *\n",
      "Epoch: 20, Train_Loss: 0.2195354551076889, Test_Loss: 0.20971085131168365\n",
      "Epoch: 20, Train_Loss: 0.1971363127231598, Test_Loss: 0.21131598949432373\n",
      "Epoch: 20, Train_Loss: 0.19198483228683472, Test_Loss: 0.1907777488231659 *\n",
      "Epoch: 20, Train_Loss: 0.1970318704843521, Test_Loss: 0.22705011069774628\n",
      "Epoch: 20, Train_Loss: 0.19488228857517242, Test_Loss: 0.2244495451450348 *\n",
      "Epoch: 20, Train_Loss: 0.22955462336540222, Test_Loss: 0.2587741017341614\n",
      "Epoch: 20, Train_Loss: 0.18952994048595428, Test_Loss: 0.30338966846466064\n",
      "Epoch: 20, Train_Loss: 0.18028424680233002, Test_Loss: 0.24310189485549927 *\n",
      "Epoch: 20, Train_Loss: 0.19297713041305542, Test_Loss: 0.3810248374938965\n",
      "Epoch: 20, Train_Loss: 0.19321474432945251, Test_Loss: 0.30944955348968506 *\n",
      "Epoch: 20, Train_Loss: 0.18476411700248718, Test_Loss: 0.1959380954504013 *\n",
      "Epoch: 20, Train_Loss: 0.18898381292819977, Test_Loss: 0.19247198104858398 *\n",
      "Epoch: 20, Train_Loss: 0.1843077689409256, Test_Loss: 0.3531661629676819\n",
      "Epoch: 20, Train_Loss: 0.18287093937397003, Test_Loss: 0.32037991285324097 *\n",
      "Epoch: 20, Train_Loss: 0.18581874668598175, Test_Loss: 0.18402157723903656 *\n",
      "Epoch: 20, Train_Loss: 0.1864972710609436, Test_Loss: 0.20411807298660278\n",
      "Epoch: 20, Train_Loss: 0.1864164024591446, Test_Loss: 0.19483041763305664 *\n",
      "Epoch: 20, Train_Loss: 0.18508510291576385, Test_Loss: 0.20173104107379913\n",
      "Epoch: 20, Train_Loss: 0.1868528127670288, Test_Loss: 0.19274161756038666 *\n",
      "Epoch: 20, Train_Loss: 0.18041381239891052, Test_Loss: 0.18128691613674164 *\n",
      "Epoch: 20, Train_Loss: 0.18058551847934723, Test_Loss: 0.18375784158706665\n",
      "Epoch: 20, Train_Loss: 0.1883115917444229, Test_Loss: 0.1921331137418747\n",
      "Epoch: 20, Train_Loss: 0.2024281769990921, Test_Loss: 0.19350767135620117\n",
      "Epoch: 20, Train_Loss: 0.2127223014831543, Test_Loss: 0.18968071043491364 *\n",
      "Epoch: 20, Train_Loss: 0.1921684443950653, Test_Loss: 0.18696680665016174 *\n",
      "Epoch: 20, Train_Loss: 0.2095569372177124, Test_Loss: 0.2442387342453003\n",
      "Epoch: 20, Train_Loss: 0.23042990267276764, Test_Loss: 0.26563310623168945\n",
      "Epoch: 20, Train_Loss: 0.19096574187278748, Test_Loss: 0.6054899096488953\n",
      "Epoch: 20, Train_Loss: 0.18702782690525055, Test_Loss: 0.5220521688461304 *\n",
      "Epoch: 20, Train_Loss: 0.19204656779766083, Test_Loss: 0.2672293186187744 *\n",
      "Epoch: 20, Train_Loss: 0.20416483283042908, Test_Loss: 0.1897808313369751 *\n",
      "Epoch: 20, Train_Loss: 0.18024523556232452, Test_Loss: 0.2055157721042633\n",
      "Epoch: 20, Train_Loss: 0.18579386174678802, Test_Loss: 0.24020737409591675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.18195486068725586, Test_Loss: 0.3984473943710327\n",
      "Epoch: 20, Train_Loss: 0.20615628361701965, Test_Loss: 0.3133702278137207 *\n",
      "Epoch: 20, Train_Loss: 0.22692564129829407, Test_Loss: 0.5330497026443481\n",
      "Epoch: 20, Train_Loss: 0.2216736376285553, Test_Loss: 0.2519405484199524 *\n",
      "Epoch: 20, Train_Loss: 0.18979297578334808, Test_Loss: 0.21539250016212463 *\n",
      "Epoch: 20, Train_Loss: 0.1836266815662384, Test_Loss: 0.1859544962644577 *\n",
      "Epoch: 20, Train_Loss: 0.22239547967910767, Test_Loss: 0.18115116655826569 *\n",
      "Epoch: 20, Train_Loss: 0.18853574991226196, Test_Loss: 0.19980238378047943\n",
      "Epoch: 20, Train_Loss: 0.18758882582187653, Test_Loss: 0.18977050483226776 *\n",
      "Epoch: 20, Train_Loss: 0.18696588277816772, Test_Loss: 0.21409617364406586\n",
      "Epoch: 20, Train_Loss: 0.2103535681962967, Test_Loss: 0.18614892661571503 *\n",
      "Epoch: 20, Train_Loss: 0.2830539345741272, Test_Loss: 0.25213611125946045\n",
      "Epoch: 20, Train_Loss: 0.21392954885959625, Test_Loss: 0.3108295202255249\n",
      "Epoch: 20, Train_Loss: 0.20381470024585724, Test_Loss: 0.4974867105484009\n",
      "Epoch: 20, Train_Loss: 0.19321736693382263, Test_Loss: 0.4236883521080017 *\n",
      "Epoch: 20, Train_Loss: 0.2043997347354889, Test_Loss: 0.20160722732543945 *\n",
      "Epoch: 20, Train_Loss: 0.18753758072853088, Test_Loss: 0.19801653921604156 *\n",
      "Epoch: 20, Train_Loss: 0.18644209206104279, Test_Loss: 0.19754138588905334 *\n",
      "Epoch: 20, Train_Loss: 0.1953733116388321, Test_Loss: 0.19775938987731934\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.18572664260864258, Test_Loss: 0.21511702239513397\n",
      "Epoch: 20, Train_Loss: 0.21670019626617432, Test_Loss: 2.1312575340270996\n",
      "Epoch: 20, Train_Loss: 0.2567581534385681, Test_Loss: 3.5079543590545654\n",
      "Epoch: 20, Train_Loss: 0.19892743229866028, Test_Loss: 0.1926184445619583 *\n",
      "Epoch: 20, Train_Loss: 0.2161288559436798, Test_Loss: 0.18542149662971497 *\n",
      "Epoch: 20, Train_Loss: 0.19779479503631592, Test_Loss: 0.1871929168701172\n",
      "Epoch: 20, Train_Loss: 0.1900443285703659, Test_Loss: 0.1822708398103714 *\n",
      "Epoch: 20, Train_Loss: 0.2883201837539673, Test_Loss: 0.18595902621746063\n",
      "Epoch: 20, Train_Loss: 0.3841565251350403, Test_Loss: 0.1910063922405243\n",
      "Epoch: 20, Train_Loss: 0.1795433610677719, Test_Loss: 0.20081159472465515\n",
      "Epoch: 20, Train_Loss: 0.20896673202514648, Test_Loss: 0.18962450325489044 *\n",
      "Epoch: 20, Train_Loss: 0.18265628814697266, Test_Loss: 0.1874265819787979 *\n",
      "Epoch: 20, Train_Loss: 0.18070274591445923, Test_Loss: 0.20004217326641083\n",
      "Epoch: 20, Train_Loss: 0.1816510260105133, Test_Loss: 0.22230088710784912\n",
      "Epoch: 20, Train_Loss: 0.18088772892951965, Test_Loss: 0.20385479927062988 *\n",
      "Epoch: 20, Train_Loss: 0.1803421825170517, Test_Loss: 0.22977250814437866\n",
      "Epoch: 20, Train_Loss: 0.19879963994026184, Test_Loss: 0.19404900074005127 *\n",
      "Epoch: 20, Train_Loss: 0.18153035640716553, Test_Loss: 0.1802040934562683 *\n",
      "Epoch: 20, Train_Loss: 0.18584288656711578, Test_Loss: 0.18058384954929352\n",
      "Epoch: 20, Train_Loss: 0.1883212774991989, Test_Loss: 0.18337997794151306\n",
      "Epoch: 20, Train_Loss: 0.17988432943820953, Test_Loss: 0.18924345076084137\n",
      "Epoch: 20, Train_Loss: 0.18010751903057098, Test_Loss: 0.18034416437149048 *\n",
      "Epoch: 20, Train_Loss: 0.1811373084783554, Test_Loss: 0.18493890762329102\n",
      "Epoch: 20, Train_Loss: 0.18986357748508453, Test_Loss: 0.1787087321281433 *\n",
      "Epoch: 20, Train_Loss: 0.1882462352514267, Test_Loss: 0.18296322226524353\n",
      "Epoch: 20, Train_Loss: 0.19320198893547058, Test_Loss: 0.1833135485649109\n",
      "Epoch: 20, Train_Loss: 0.18730422854423523, Test_Loss: 0.18245826661586761 *\n",
      "Epoch: 20, Train_Loss: 0.20482143759727478, Test_Loss: 0.18026889860630035 *\n",
      "Epoch: 20, Train_Loss: 0.19698023796081543, Test_Loss: 0.18143101036548615\n",
      "Epoch: 20, Train_Loss: 0.19829678535461426, Test_Loss: 0.18204420804977417\n",
      "Epoch: 20, Train_Loss: 0.19032084941864014, Test_Loss: 0.1794346421957016 *\n",
      "Epoch: 20, Train_Loss: 0.20685835182666779, Test_Loss: 0.22585581243038177\n",
      "Epoch: 20, Train_Loss: 0.18198075890541077, Test_Loss: 0.20957186818122864 *\n",
      "Epoch: 20, Train_Loss: 0.18929612636566162, Test_Loss: 4.701407432556152\n",
      "Epoch: 20, Train_Loss: 0.20142047107219696, Test_Loss: 1.6691735982894897 *\n",
      "Epoch: 20, Train_Loss: 0.21685943007469177, Test_Loss: 0.1837591975927353 *\n",
      "Epoch: 20, Train_Loss: 2.26421856880188, Test_Loss: 0.1914578378200531\n",
      "Epoch: 20, Train_Loss: 3.2075741291046143, Test_Loss: 0.21070893108844757\n",
      "Epoch: 20, Train_Loss: 0.20071028172969818, Test_Loss: 0.20340141654014587 *\n",
      "Epoch: 20, Train_Loss: 0.2028200477361679, Test_Loss: 0.18971039354801178 *\n",
      "Epoch: 20, Train_Loss: 0.2048349231481552, Test_Loss: 0.24423331022262573\n",
      "Epoch: 20, Train_Loss: 0.27108514308929443, Test_Loss: 0.22524118423461914 *\n",
      "Epoch: 20, Train_Loss: 0.20397260785102844, Test_Loss: 0.1851709634065628 *\n",
      "Epoch: 20, Train_Loss: 0.1908811628818512, Test_Loss: 0.2002224177122116\n",
      "Epoch: 20, Train_Loss: 0.18148435652256012, Test_Loss: 0.20419594645500183\n",
      "Epoch: 20, Train_Loss: 0.2664283514022827, Test_Loss: 0.20297756791114807 *\n",
      "Epoch: 20, Train_Loss: 0.1953602135181427, Test_Loss: 0.18829567730426788 *\n",
      "Epoch: 20, Train_Loss: 0.19283588230609894, Test_Loss: 0.23475508391857147\n",
      "Epoch: 20, Train_Loss: 0.6163505911827087, Test_Loss: 0.22312745451927185 *\n",
      "Epoch: 20, Train_Loss: 0.6603010892868042, Test_Loss: 0.22547368705272675\n",
      "Epoch: 20, Train_Loss: 0.7344194054603577, Test_Loss: 0.19707955420017242 *\n",
      "Epoch: 20, Train_Loss: 0.2585538625717163, Test_Loss: 0.2805200517177582\n",
      "Epoch: 20, Train_Loss: 0.7007738351821899, Test_Loss: 0.20784276723861694 *\n",
      "Epoch: 20, Train_Loss: 1.3129265308380127, Test_Loss: 0.1824183613061905 *\n",
      "Epoch: 20, Train_Loss: 0.47939765453338623, Test_Loss: 0.198960542678833\n",
      "Epoch: 20, Train_Loss: 0.18385757505893707, Test_Loss: 0.2034635692834854\n",
      "Epoch: 20, Train_Loss: 0.20444680750370026, Test_Loss: 0.19561028480529785 *\n",
      "Epoch: 20, Train_Loss: 0.5546269416809082, Test_Loss: 0.20046262443065643\n",
      "Epoch: 20, Train_Loss: 0.48191091418266296, Test_Loss: 0.20512992143630981\n",
      "Epoch: 20, Train_Loss: 0.3365912437438965, Test_Loss: 0.21675987541675568\n",
      "Epoch: 20, Train_Loss: 0.1816364824771881, Test_Loss: 0.21137236058712006 *\n",
      "Epoch: 20, Train_Loss: 0.19613300263881683, Test_Loss: 0.19225354492664337 *\n",
      "Epoch: 20, Train_Loss: 0.409121036529541, Test_Loss: 0.1925482302904129\n",
      "Epoch: 20, Train_Loss: 0.30878275632858276, Test_Loss: 0.18871697783470154 *\n",
      "Epoch: 20, Train_Loss: 0.22740282118320465, Test_Loss: 0.21680453419685364\n",
      "Epoch: 20, Train_Loss: 0.21625789999961853, Test_Loss: 0.38444045186042786\n",
      "Epoch: 20, Train_Loss: 0.24555596709251404, Test_Loss: 0.34460029006004333 *\n",
      "Epoch: 20, Train_Loss: 0.23898768424987793, Test_Loss: 0.42273464798927307\n",
      "Epoch: 20, Train_Loss: 0.32637742161750793, Test_Loss: 0.2327425181865692 *\n",
      "Epoch: 20, Train_Loss: 0.22262288630008698, Test_Loss: 0.2208741009235382 *\n",
      "Epoch: 20, Train_Loss: 0.26886266469955444, Test_Loss: 0.21253912150859833 *\n",
      "Epoch: 20, Train_Loss: 0.26672112941741943, Test_Loss: 0.21963411569595337\n",
      "Epoch: 20, Train_Loss: 0.20919567346572876, Test_Loss: 0.532305121421814\n",
      "Epoch: 20, Train_Loss: 0.23157240450382233, Test_Loss: 0.21057868003845215 *\n",
      "Epoch: 20, Train_Loss: 0.23412784934043884, Test_Loss: 0.5959257483482361\n",
      "Epoch: 20, Train_Loss: 0.2840893566608429, Test_Loss: 0.2637948989868164 *\n",
      "Epoch: 20, Train_Loss: 0.263248473405838, Test_Loss: 0.27153608202934265\n",
      "Epoch: 20, Train_Loss: 0.2944938540458679, Test_Loss: 0.1884239912033081 *\n",
      "Epoch: 20, Train_Loss: 0.21275565028190613, Test_Loss: 0.17928798496723175 *\n",
      "Epoch: 20, Train_Loss: 0.1860867142677307, Test_Loss: 0.2505285143852234\n",
      "Epoch: 20, Train_Loss: 0.17915338277816772, Test_Loss: 0.18383674323558807 *\n",
      "Epoch: 20, Train_Loss: 0.17996646463871002, Test_Loss: 0.19658546149730682\n",
      "Epoch: 20, Train_Loss: 0.18800094723701477, Test_Loss: 0.18635201454162598 *\n",
      "Epoch: 20, Train_Loss: 0.18674501776695251, Test_Loss: 0.4201980233192444\n",
      "Epoch: 20, Train_Loss: 0.2011520266532898, Test_Loss: 0.5698295831680298\n",
      "Epoch: 20, Train_Loss: 0.21066471934318542, Test_Loss: 0.36065322160720825 *\n",
      "Epoch: 20, Train_Loss: 0.20322421193122864, Test_Loss: 0.6047855615615845\n",
      "Epoch: 20, Train_Loss: 0.42582452297210693, Test_Loss: 0.3233134150505066 *\n",
      "Epoch: 20, Train_Loss: 0.3953409492969513, Test_Loss: 0.3392677903175354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.1926138550043106, Test_Loss: 0.3404298424720764\n",
      "Epoch: 20, Train_Loss: 0.20087124407291412, Test_Loss: 0.33742979168891907 *\n",
      "Epoch: 20, Train_Loss: 0.25051817297935486, Test_Loss: 0.47126203775405884\n",
      "Epoch: 20, Train_Loss: 0.26593485474586487, Test_Loss: 3.9830517768859863\n",
      "Epoch: 20, Train_Loss: 0.38056594133377075, Test_Loss: 1.9048621654510498 *\n",
      "Epoch: 20, Train_Loss: 0.23982128500938416, Test_Loss: 0.2146921157836914 *\n",
      "Epoch: 20, Train_Loss: 0.32185098528862, Test_Loss: 0.21325664222240448 *\n",
      "Epoch: 20, Train_Loss: 0.26788902282714844, Test_Loss: 0.20059983432292938 *\n",
      "Epoch: 20, Train_Loss: 0.24115094542503357, Test_Loss: 0.19882738590240479 *\n",
      "Epoch: 20, Train_Loss: 0.2351493239402771, Test_Loss: 0.18989360332489014 *\n",
      "Epoch: 20, Train_Loss: 0.20038902759552002, Test_Loss: 0.23144903779029846\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.2400328516960144, Test_Loss: 0.1993432641029358 *\n",
      "Epoch: 20, Train_Loss: 0.5267070531845093, Test_Loss: 0.19052839279174805 *\n",
      "Epoch: 20, Train_Loss: 0.545689046382904, Test_Loss: 0.18474861979484558 *\n",
      "Epoch: 20, Train_Loss: 0.21130268275737762, Test_Loss: 0.19069647789001465\n",
      "Epoch: 20, Train_Loss: 0.22094684839248657, Test_Loss: 0.23217108845710754\n",
      "Epoch: 20, Train_Loss: 0.19132670760154724, Test_Loss: 0.1866380274295807 *\n",
      "Epoch: 20, Train_Loss: 0.21482805907726288, Test_Loss: 0.20163904130458832\n",
      "Epoch: 20, Train_Loss: 0.5593752861022949, Test_Loss: 0.19832684099674225 *\n",
      "Epoch: 20, Train_Loss: 0.19317026436328888, Test_Loss: 0.20044155418872833\n",
      "Epoch: 20, Train_Loss: 0.27105826139450073, Test_Loss: 0.2498890608549118\n",
      "Epoch: 20, Train_Loss: 0.2464924454689026, Test_Loss: 0.24862860143184662 *\n",
      "Epoch: 20, Train_Loss: 0.24926531314849854, Test_Loss: 0.1878301501274109 *\n",
      "Epoch: 20, Train_Loss: 0.2443658411502838, Test_Loss: 0.204990953207016\n",
      "Epoch: 20, Train_Loss: 0.367658793926239, Test_Loss: 0.20085792243480682 *\n",
      "Epoch: 20, Train_Loss: 0.3314897418022156, Test_Loss: 0.1838405877351761 *\n",
      "Epoch: 20, Train_Loss: 0.1940927356481552, Test_Loss: 0.18845149874687195\n",
      "Epoch: 20, Train_Loss: 0.20792156457901, Test_Loss: 0.19745460152626038\n",
      "Epoch: 20, Train_Loss: 0.21351352334022522, Test_Loss: 0.18246223032474518 *\n",
      "Epoch: 20, Train_Loss: 0.2389148473739624, Test_Loss: 0.19171099364757538\n",
      "Epoch: 20, Train_Loss: 0.22035184502601624, Test_Loss: 0.22600558400154114\n",
      "Epoch: 20, Train_Loss: 0.19737744331359863, Test_Loss: 0.20480409264564514 *\n",
      "Epoch: 20, Train_Loss: 0.19172832369804382, Test_Loss: 0.18243834376335144 *\n",
      "Epoch: 20, Train_Loss: 0.20775046944618225, Test_Loss: 0.2551993727684021\n",
      "Epoch: 20, Train_Loss: 0.4653398394584656, Test_Loss: 0.27978265285491943\n",
      "Epoch: 20, Train_Loss: 0.4478238821029663, Test_Loss: 5.378347396850586\n",
      "Epoch: 20, Train_Loss: 0.6841076612472534, Test_Loss: 0.39359861612319946 *\n",
      "Epoch: 20, Train_Loss: 0.5103188157081604, Test_Loss: 0.19534513354301453 *\n",
      "Epoch: 20, Train_Loss: 0.3817328214645386, Test_Loss: 0.21896150708198547\n",
      "Epoch: 20, Train_Loss: 0.2542650103569031, Test_Loss: 0.1838253289461136 *\n",
      "Epoch: 20, Train_Loss: 0.22983543574810028, Test_Loss: 0.19528113305568695\n",
      "Epoch: 20, Train_Loss: 0.1904236078262329, Test_Loss: 0.1838538646697998 *\n",
      "Epoch: 20, Train_Loss: 0.1881449669599533, Test_Loss: 0.22543197870254517\n",
      "Epoch: 20, Train_Loss: 0.26143312454223633, Test_Loss: 0.22230133414268494 *\n",
      "Epoch: 20, Train_Loss: 0.44690972566604614, Test_Loss: 0.1792694330215454 *\n",
      "Epoch: 20, Train_Loss: 0.5231654644012451, Test_Loss: 0.19347822666168213\n",
      "Epoch: 20, Train_Loss: 0.7575947046279907, Test_Loss: 0.2151433527469635\n",
      "Epoch: 20, Train_Loss: 1.1514744758605957, Test_Loss: 0.18713051080703735 *\n",
      "Epoch: 20, Train_Loss: 0.4225732386112213, Test_Loss: 0.2069263458251953\n",
      "Epoch: 20, Train_Loss: 0.4678485095500946, Test_Loss: 0.1955210268497467 *\n",
      "Epoch: 20, Train_Loss: 0.18003970384597778, Test_Loss: 0.23389805853366852\n",
      "Epoch: 20, Train_Loss: 0.18915170431137085, Test_Loss: 0.22329464554786682 *\n",
      "Epoch: 20, Train_Loss: 0.4456745386123657, Test_Loss: 0.22802454233169556\n",
      "Epoch: 20, Train_Loss: 0.6562124490737915, Test_Loss: 0.2414841651916504\n",
      "Epoch: 20, Train_Loss: 0.26815760135650635, Test_Loss: 0.18937231600284576 *\n",
      "Epoch: 20, Train_Loss: 0.21953611075878143, Test_Loss: 0.24540074169635773\n",
      "Epoch: 20, Train_Loss: 0.18945923447608948, Test_Loss: 0.25347843766212463\n",
      "Epoch: 20, Train_Loss: 0.30518144369125366, Test_Loss: 0.3022921681404114\n",
      "Epoch: 20, Train_Loss: 0.4815300703048706, Test_Loss: 0.24984341859817505 *\n",
      "Epoch: 20, Train_Loss: 0.4709000587463379, Test_Loss: 0.2438400238752365 *\n",
      "Epoch: 20, Train_Loss: 0.2930580973625183, Test_Loss: 0.32324033975601196\n",
      "Epoch: 20, Train_Loss: 0.41600310802459717, Test_Loss: 0.2887022793292999 *\n",
      "Epoch: 20, Train_Loss: 0.18206186592578888, Test_Loss: 0.22779372334480286 *\n",
      "Epoch: 20, Train_Loss: 0.1826322227716446, Test_Loss: 0.22707760334014893 *\n",
      "Epoch: 20, Train_Loss: 0.20045523345470428, Test_Loss: 0.20194613933563232 *\n",
      "Epoch: 20, Train_Loss: 0.22565017640590668, Test_Loss: 0.1883859932422638 *\n",
      "Epoch: 20, Train_Loss: 0.19363561272621155, Test_Loss: 0.20250602066516876\n",
      "Epoch: 20, Train_Loss: 0.21278394758701324, Test_Loss: 0.33845263719558716\n",
      "Epoch: 20, Train_Loss: 13.724727630615234, Test_Loss: 0.2487773597240448 *\n",
      "Epoch: 20, Train_Loss: 2.010310649871826, Test_Loss: 0.26821810007095337\n",
      "Epoch: 20, Train_Loss: 1.1436388492584229, Test_Loss: 0.21730858087539673 *\n",
      "Epoch: 20, Train_Loss: 0.7720252871513367, Test_Loss: 0.22845004498958588\n",
      "Epoch: 20, Train_Loss: 0.23834851384162903, Test_Loss: 0.19255679845809937 *\n",
      "Epoch: 20, Train_Loss: 0.24404585361480713, Test_Loss: 0.24519413709640503\n",
      "Epoch: 20, Train_Loss: 1.2386181354522705, Test_Loss: 0.4436892867088318\n",
      "Epoch: 20, Train_Loss: 4.815245151519775, Test_Loss: 0.24864861369132996 *\n",
      "Epoch: 20, Train_Loss: 0.7402077913284302, Test_Loss: 0.4420939087867737\n",
      "Epoch: 20, Train_Loss: 0.2177460938692093, Test_Loss: 0.29958468675613403 *\n",
      "Epoch: 20, Train_Loss: 3.921116828918457, Test_Loss: 0.3139357566833496\n",
      "Epoch: 20, Train_Loss: 1.2334115505218506, Test_Loss: 0.3719474673271179\n",
      "Epoch: 20, Train_Loss: 0.48046645522117615, Test_Loss: 0.29158252477645874 *\n",
      "Epoch: 20, Train_Loss: 0.18070131540298462, Test_Loss: 0.40809041261672974\n",
      "Epoch: 20, Train_Loss: 0.1884549856185913, Test_Loss: 0.19831226766109467 *\n",
      "Epoch: 20, Train_Loss: 0.2151133120059967, Test_Loss: 0.20973283052444458\n",
      "Epoch: 20, Train_Loss: 0.18015912175178528, Test_Loss: 0.3092966675758362\n",
      "Epoch: 20, Train_Loss: 0.17831599712371826, Test_Loss: 0.6819995641708374\n",
      "Epoch: 20, Train_Loss: 0.17599163949489594, Test_Loss: 1.6411669254302979\n",
      "Epoch: 20, Train_Loss: 0.176186203956604, Test_Loss: 0.3679988980293274 *\n",
      "Epoch: 20, Train_Loss: 0.18137532472610474, Test_Loss: 0.3522021174430847 *\n",
      "Epoch: 20, Train_Loss: 0.19932839274406433, Test_Loss: 0.21620558202266693 *\n",
      "Epoch: 20, Train_Loss: 0.2700880169868469, Test_Loss: 0.22395361959934235\n",
      "Epoch: 20, Train_Loss: 0.25461545586586, Test_Loss: 0.2301938682794571\n",
      "Epoch: 20, Train_Loss: 0.27612096071243286, Test_Loss: 0.21303173899650574 *\n",
      "Epoch: 20, Train_Loss: 0.20836564898490906, Test_Loss: 0.21020451188087463 *\n",
      "Epoch: 20, Train_Loss: 0.21471205353736877, Test_Loss: 8.336885452270508\n",
      "Epoch: 20, Train_Loss: 0.17990680038928986, Test_Loss: 1.8935128450393677 *\n",
      "Epoch: 20, Train_Loss: 0.24426648020744324, Test_Loss: 0.9411725401878357 *\n",
      "Epoch: 20, Train_Loss: 0.17876793444156647, Test_Loss: 0.915535569190979 *\n",
      "Epoch: 20, Train_Loss: 0.1781989336013794, Test_Loss: 0.9868499636650085\n",
      "Epoch: 20, Train_Loss: 0.176018625497818, Test_Loss: 0.4107491374015808 *\n",
      "Epoch: 20, Train_Loss: 0.1773713082075119, Test_Loss: 1.7370502948760986\n",
      "Epoch: 20, Train_Loss: 0.17712721228599548, Test_Loss: 1.261474847793579 *\n",
      "Epoch: 20, Train_Loss: 0.1763780266046524, Test_Loss: 0.8107624053955078 *\n",
      "Epoch: 20, Train_Loss: 0.17612065374851227, Test_Loss: 1.1190210580825806\n",
      "Epoch: 20, Train_Loss: 0.17813768982887268, Test_Loss: 0.6515200138092041 *\n",
      "Epoch: 20, Train_Loss: 0.19893497228622437, Test_Loss: 0.9980506896972656\n",
      "Epoch: 20, Train_Loss: 0.20043553411960602, Test_Loss: 1.8594679832458496\n",
      "Epoch: 20, Train_Loss: 0.28181859850883484, Test_Loss: 1.06406569480896 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train_Loss: 0.21132132411003113, Test_Loss: 1.4057636260986328\n",
      "Epoch: 20, Train_Loss: 0.5599308609962463, Test_Loss: 0.4574076533317566 *\n",
      "Epoch: 20, Train_Loss: 5.35114049911499, Test_Loss: 0.29095423221588135 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 20\n",
      "Epoch: 20, Train_Loss: 0.3638431131839752, Test_Loss: 0.2480739951133728 *\n",
      "Epoch: 20, Train_Loss: 0.2288001924753189, Test_Loss: 0.564479649066925\n",
      "Epoch: 20, Train_Loss: 0.23098187148571014, Test_Loss: 0.5782082080841064\n",
      "Epoch: 20, Train_Loss: 0.2780751883983612, Test_Loss: 0.38044485449790955 *\n",
      "Epoch: 20, Train_Loss: 0.25597602128982544, Test_Loss: 0.4760197401046753\n",
      "Epoch: 20, Train_Loss: 0.3024696409702301, Test_Loss: 0.6166093349456787\n",
      "Epoch: 20, Train_Loss: 0.3601452708244324, Test_Loss: 0.47428780794143677 *\n",
      "Epoch: 20, Train_Loss: 0.29403746128082275, Test_Loss: 0.7606426477432251\n",
      "Epoch: 20, Train_Loss: 0.2698622941970825, Test_Loss: 0.27615809440612793 *\n",
      "Epoch: 20, Train_Loss: 0.2353193759918213, Test_Loss: 0.21426792442798615 *\n",
      "Epoch: 20, Train_Loss: 0.17708373069763184, Test_Loss: 0.33101797103881836\n",
      "Epoch: 20, Train_Loss: 0.2355850338935852, Test_Loss: 0.2327709048986435 *\n",
      "Epoch: 20, Train_Loss: 0.19305419921875, Test_Loss: 0.1803719699382782 *\n",
      "Epoch: 20, Train_Loss: 0.6248418092727661, Test_Loss: 0.3776257634162903\n",
      "Epoch: 20, Train_Loss: 0.18214845657348633, Test_Loss: 0.47048747539520264\n",
      "Epoch: 20, Train_Loss: 0.1916981339454651, Test_Loss: 5.050573348999023\n",
      "Epoch: 20, Train_Loss: 0.20334653556346893, Test_Loss: 0.20348672568798065 *\n",
      "Epoch: 20, Train_Loss: 0.19799548387527466, Test_Loss: 0.18393917381763458 *\n",
      "Epoch: 20, Train_Loss: 0.2575927674770355, Test_Loss: 0.21006155014038086\n",
      "Epoch: 20, Train_Loss: 0.2495366632938385, Test_Loss: 0.18451423943042755 *\n",
      "Epoch: 20, Train_Loss: 0.20846767723560333, Test_Loss: 0.19849041104316711\n",
      "Epoch: 20, Train_Loss: 0.1783309131860733, Test_Loss: 0.18508082628250122 *\n",
      "Epoch: 20, Train_Loss: 0.18777772784233093, Test_Loss: 0.23810029029846191\n",
      "Epoch: 20, Train_Loss: 0.3404690623283386, Test_Loss: 0.21156467497348785 *\n",
      "Epoch: 20, Train_Loss: 4.875978946685791, Test_Loss: 0.17851229012012482 *\n",
      "Epoch: 20, Train_Loss: 0.20440490543842316, Test_Loss: 0.1973341405391693\n",
      "Epoch: 20, Train_Loss: 0.17782920598983765, Test_Loss: 0.19813206791877747\n",
      "Epoch: 20, Train_Loss: 0.18087740242481232, Test_Loss: 0.18046286702156067 *\n",
      "Epoch: 20, Train_Loss: 0.17913995683193207, Test_Loss: 0.21580903232097626\n",
      "Epoch: 20, Train_Loss: 0.17688074707984924, Test_Loss: 0.1928597092628479 *\n",
      "Epoch: 20, Train_Loss: 0.17619715631008148, Test_Loss: 0.23829764127731323\n",
      "Epoch: 20, Train_Loss: 0.1802760511636734, Test_Loss: 0.2265443652868271 *\n",
      "Epoch: 20, Train_Loss: 0.21302302181720734, Test_Loss: 0.21689645946025848 *\n",
      "Epoch: 20, Train_Loss: 0.19917763769626617, Test_Loss: 0.21585318446159363 *\n",
      "Epoch: 20, Train_Loss: 0.18978790938854218, Test_Loss: 0.17909903824329376 *\n",
      "Epoch: 20, Train_Loss: 0.1791212111711502, Test_Loss: 0.18776865303516388\n",
      "Epoch: 20, Train_Loss: 0.17667603492736816, Test_Loss: 0.18734265863895416 *\n",
      "Epoch: 20, Train_Loss: 0.19080989062786102, Test_Loss: 0.19345104694366455\n",
      "Epoch: 20, Train_Loss: 0.18049979209899902, Test_Loss: 0.18704241514205933 *\n",
      "Epoch: 20, Train_Loss: 0.1801987737417221, Test_Loss: 0.1836864948272705 *\n",
      "Epoch: 20, Train_Loss: 0.19884903728961945, Test_Loss: 0.1913861185312271\n",
      "Epoch: 20, Train_Loss: 0.20387482643127441, Test_Loss: 0.18327145278453827 *\n",
      "Epoch: 20, Train_Loss: 0.189189150929451, Test_Loss: 0.179331436753273 *\n",
      "Epoch: 20, Train_Loss: 0.1763668656349182, Test_Loss: 0.19541360437870026\n",
      "Epoch: 20, Train_Loss: 0.17622171342372894, Test_Loss: 0.18263228237628937 *\n",
      "Epoch: 20, Train_Loss: 0.22520823776721954, Test_Loss: 0.2059713453054428\n",
      "Epoch: 20, Train_Loss: 0.19556325674057007, Test_Loss: 0.21308043599128723\n",
      "Epoch: 20, Train_Loss: 0.22516122460365295, Test_Loss: 0.3912475109100342\n",
      "Epoch: 20, Train_Loss: 0.19247299432754517, Test_Loss: 0.33430442214012146 *\n",
      "Epoch: 20, Train_Loss: 0.19277459383010864, Test_Loss: 0.2700278162956238 *\n",
      "Epoch: 20, Train_Loss: 0.20641763508319855, Test_Loss: 0.20410193502902985 *\n",
      "Epoch: 20, Train_Loss: 0.20710699260234833, Test_Loss: 0.1931603103876114 *\n",
      "Epoch: 20, Train_Loss: 0.19519126415252686, Test_Loss: 0.19674570858478546\n",
      "Epoch: 20, Train_Loss: 0.43655669689178467, Test_Loss: 0.28411537408828735\n",
      "Epoch: 21, Train_Loss: 0.24632303416728973, Test_Loss: 0.3756296634674072 *\n",
      "Epoch: 21, Train_Loss: 0.19769032299518585, Test_Loss: 0.4631893038749695\n",
      "Epoch: 21, Train_Loss: 0.1756412833929062, Test_Loss: 0.2850128412246704 *\n",
      "Epoch: 21, Train_Loss: 0.1758299618959427, Test_Loss: 0.2347407042980194 *\n",
      "Epoch: 21, Train_Loss: 0.1757633537054062, Test_Loss: 0.18284855782985687 *\n",
      "Epoch: 21, Train_Loss: 0.17531056702136993, Test_Loss: 0.184041365981102\n",
      "Epoch: 21, Train_Loss: 0.3666432499885559, Test_Loss: 0.18530809879302979\n",
      "Epoch: 21, Train_Loss: 4.267367839813232, Test_Loss: 0.1916663497686386\n",
      "Epoch: 21, Train_Loss: 0.22615699470043182, Test_Loss: 0.19787834584712982\n",
      "Epoch: 21, Train_Loss: 0.1803077757358551, Test_Loss: 0.19609901309013367 *\n",
      "Epoch: 21, Train_Loss: 0.18503116071224213, Test_Loss: 0.18209034204483032 *\n",
      "Epoch: 21, Train_Loss: 0.17644666135311127, Test_Loss: 0.31843507289886475\n",
      "Epoch: 21, Train_Loss: 0.17953962087631226, Test_Loss: 0.5445662140846252\n",
      "Epoch: 21, Train_Loss: 0.17727446556091309, Test_Loss: 0.3399944305419922 *\n",
      "Epoch: 21, Train_Loss: 0.17712248861789703, Test_Loss: 0.4610568881034851\n",
      "Epoch: 21, Train_Loss: 0.1832388937473297, Test_Loss: 0.30586883425712585 *\n",
      "Epoch: 21, Train_Loss: 0.1779772937297821, Test_Loss: 0.31291663646698\n",
      "Epoch: 21, Train_Loss: 0.2044111043214798, Test_Loss: 0.31844791769981384\n",
      "Epoch: 21, Train_Loss: 0.18307851254940033, Test_Loss: 0.34881147742271423\n",
      "Epoch: 21, Train_Loss: 0.2240399271249771, Test_Loss: 0.3127560019493103 *\n",
      "Epoch: 21, Train_Loss: 0.19797185063362122, Test_Loss: 4.762459754943848\n",
      "Epoch: 21, Train_Loss: 0.17962117493152618, Test_Loss: 0.3832359313964844 *\n",
      "Epoch: 21, Train_Loss: 0.3224472403526306, Test_Loss: 0.1908288598060608 *\n",
      "Epoch: 21, Train_Loss: 0.3575802147388458, Test_Loss: 0.19131022691726685\n",
      "Epoch: 21, Train_Loss: 0.32602888345718384, Test_Loss: 0.18603134155273438 *\n",
      "Epoch: 21, Train_Loss: 0.3137403726577759, Test_Loss: 0.18302969634532928 *\n",
      "Epoch: 21, Train_Loss: 0.17960840463638306, Test_Loss: 0.18751509487628937\n",
      "Epoch: 21, Train_Loss: 0.17594477534294128, Test_Loss: 0.21585142612457275\n",
      "Epoch: 21, Train_Loss: 0.17803817987442017, Test_Loss: 0.1884123831987381 *\n",
      "Epoch: 21, Train_Loss: 0.17706869542598724, Test_Loss: 0.1860308051109314 *\n",
      "Epoch: 21, Train_Loss: 0.17599895596504211, Test_Loss: 0.1891155242919922\n",
      "Epoch: 21, Train_Loss: 0.17827284336090088, Test_Loss: 0.2617079019546509\n",
      "Epoch: 21, Train_Loss: 0.17932261526584625, Test_Loss: 0.19111379981040955 *\n",
      "Epoch: 21, Train_Loss: 0.17823472619056702, Test_Loss: 0.20328930020332336\n",
      "Epoch: 21, Train_Loss: 0.18513619899749756, Test_Loss: 0.21428918838500977\n",
      "Epoch: 21, Train_Loss: 0.1812218278646469, Test_Loss: 0.1839173436164856 *\n",
      "Epoch: 21, Train_Loss: 0.2659337520599365, Test_Loss: 0.18098008632659912 *\n",
      "Epoch: 21, Train_Loss: 0.2651311755180359, Test_Loss: 0.1824461966753006\n",
      "Epoch: 21, Train_Loss: 0.2169320285320282, Test_Loss: 0.191161647439003\n",
      "Epoch: 21, Train_Loss: 0.21049268543720245, Test_Loss: 0.18100914359092712 *\n",
      "Epoch: 21, Train_Loss: 0.2886744439601898, Test_Loss: 0.2011062204837799\n",
      "Epoch: 21, Train_Loss: 0.2867007851600647, Test_Loss: 0.18199901282787323 *\n",
      "Epoch: 21, Train_Loss: 0.24911323189735413, Test_Loss: 0.18224786221981049\n",
      "Epoch: 21, Train_Loss: 0.28238773345947266, Test_Loss: 0.1879204511642456\n",
      "Epoch: 21, Train_Loss: 0.3814513087272644, Test_Loss: 0.1866220086812973 *\n",
      "Epoch: 21, Train_Loss: 0.21371877193450928, Test_Loss: 0.18242019414901733 *\n",
      "Epoch: 21, Train_Loss: 0.18310530483722687, Test_Loss: 0.18052655458450317 *\n",
      "Epoch: 21, Train_Loss: 2.2554306983947754, Test_Loss: 0.196848064661026\n",
      "Epoch: 21, Train_Loss: 0.7474181056022644, Test_Loss: 0.18299156427383423 *\n",
      "Epoch: 21, Train_Loss: 0.22006195783615112, Test_Loss: 0.17899194359779358 *\n",
      "Epoch: 21, Train_Loss: 0.2042248249053955, Test_Loss: 0.26735156774520874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.18311025202274323, Test_Loss: 1.5823118686676025\n",
      "Epoch: 21, Train_Loss: 0.2025085687637329, Test_Loss: 3.9507029056549072\n",
      "Epoch: 21, Train_Loss: 0.18433837592601776, Test_Loss: 0.20591257512569427 *\n",
      "Epoch: 21, Train_Loss: 0.20400722324848175, Test_Loss: 0.21975770592689514\n",
      "Epoch: 21, Train_Loss: 0.2535277009010315, Test_Loss: 0.25078532099723816\n",
      "Epoch: 21, Train_Loss: 0.2158496379852295, Test_Loss: 0.1984579712152481 *\n",
      "Epoch: 21, Train_Loss: 0.20478512346744537, Test_Loss: 0.19135239720344543 *\n",
      "Epoch: 21, Train_Loss: 0.18488170206546783, Test_Loss: 0.1994522511959076\n",
      "Epoch: 21, Train_Loss: 0.1895580142736435, Test_Loss: 0.18305827677249908 *\n",
      "Epoch: 21, Train_Loss: 0.1884365826845169, Test_Loss: 0.21513710916042328\n",
      "Epoch: 21, Train_Loss: 0.19811610877513885, Test_Loss: 0.2196437567472458\n",
      "Epoch: 21, Train_Loss: 0.26503726840019226, Test_Loss: 0.23783275485038757\n",
      "Epoch: 21, Train_Loss: 0.18897762894630432, Test_Loss: 0.32569098472595215\n",
      "Epoch: 21, Train_Loss: 0.1790972203016281, Test_Loss: 0.24983735382556915 *\n",
      "Epoch: 21, Train_Loss: 0.18035602569580078, Test_Loss: 0.3050461411476135\n",
      "Epoch: 21, Train_Loss: 0.18457123637199402, Test_Loss: 0.24939188361167908 *\n",
      "Epoch: 21, Train_Loss: 0.17769035696983337, Test_Loss: 0.21166333556175232 *\n",
      "Epoch: 21, Train_Loss: 0.1884431540966034, Test_Loss: 0.18960833549499512 *\n",
      "Epoch: 21, Train_Loss: 0.1775083839893341, Test_Loss: 0.29053565859794617\n",
      "Epoch: 21, Train_Loss: 0.1757824420928955, Test_Loss: 0.3599677085876465\n",
      "Epoch: 21, Train_Loss: 0.1747158318758011, Test_Loss: 0.1768554449081421 *\n",
      "Epoch: 21, Train_Loss: 0.1813260018825531, Test_Loss: 0.18251551687717438\n",
      "Epoch: 21, Train_Loss: 0.17751361429691315, Test_Loss: 0.1911909282207489\n",
      "Epoch: 21, Train_Loss: 0.1790483146905899, Test_Loss: 0.1968172788619995\n",
      "Epoch: 21, Train_Loss: 0.18208299577236176, Test_Loss: 0.18037769198417664 *\n",
      "Epoch: 21, Train_Loss: 0.17657412588596344, Test_Loss: 0.1853867918252945\n",
      "Epoch: 21, Train_Loss: 0.17474693059921265, Test_Loss: 0.17779019474983215 *\n",
      "Epoch: 21, Train_Loss: 0.18219321966171265, Test_Loss: 0.1887614130973816\n",
      "Epoch: 21, Train_Loss: 0.1894492655992508, Test_Loss: 0.18242181837558746 *\n",
      "Epoch: 21, Train_Loss: 0.19371484220027924, Test_Loss: 0.199335515499115\n",
      "Epoch: 21, Train_Loss: 0.1806120127439499, Test_Loss: 0.17702870070934296 *\n",
      "Epoch: 21, Train_Loss: 0.20027285814285278, Test_Loss: 0.20128953456878662\n",
      "Epoch: 21, Train_Loss: 0.2151428759098053, Test_Loss: 0.1948653608560562 *\n",
      "Epoch: 21, Train_Loss: 0.20985524356365204, Test_Loss: 0.5592819452285767\n",
      "Epoch: 21, Train_Loss: 0.19448566436767578, Test_Loss: 0.4381489157676697 *\n",
      "Epoch: 21, Train_Loss: 0.18597473204135895, Test_Loss: 0.28082799911499023 *\n",
      "Epoch: 21, Train_Loss: 0.20369943976402283, Test_Loss: 0.20012997090816498 *\n",
      "Epoch: 21, Train_Loss: 0.17921394109725952, Test_Loss: 0.19809490442276 *\n",
      "Epoch: 21, Train_Loss: 0.18556804955005646, Test_Loss: 0.18978340923786163 *\n",
      "Epoch: 21, Train_Loss: 0.17940674722194672, Test_Loss: 0.24346400797367096\n",
      "Epoch: 21, Train_Loss: 0.18682309985160828, Test_Loss: 0.24571344256401062\n",
      "Epoch: 21, Train_Loss: 0.22637684643268585, Test_Loss: 0.39621204137802124\n",
      "Epoch: 21, Train_Loss: 0.22129835188388824, Test_Loss: 0.2428153157234192 *\n",
      "Epoch: 21, Train_Loss: 0.18828147649765015, Test_Loss: 0.2516351342201233\n",
      "Epoch: 21, Train_Loss: 0.17471429705619812, Test_Loss: 0.1814562827348709 *\n",
      "Epoch: 21, Train_Loss: 0.2133556455373764, Test_Loss: 0.17951571941375732 *\n",
      "Epoch: 21, Train_Loss: 0.1909707635641098, Test_Loss: 0.18388786911964417\n",
      "Epoch: 21, Train_Loss: 0.17903371155261993, Test_Loss: 0.19583167135715485\n",
      "Epoch: 21, Train_Loss: 0.18681108951568604, Test_Loss: 0.20870283246040344\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.23609954118728638, Test_Loss: 0.1854715198278427 *\n",
      "Epoch: 21, Train_Loss: 0.25862377882003784, Test_Loss: 0.19577132165431976\n",
      "Epoch: 21, Train_Loss: 0.20906281471252441, Test_Loss: 0.28084295988082886\n",
      "Epoch: 21, Train_Loss: 0.19960498809814453, Test_Loss: 0.5177097916603088\n",
      "Epoch: 21, Train_Loss: 0.19366104900836945, Test_Loss: 0.4164779782295227 *\n",
      "Epoch: 21, Train_Loss: 0.18566371500492096, Test_Loss: 0.2060529738664627 *\n",
      "Epoch: 21, Train_Loss: 0.19375565648078918, Test_Loss: 0.1966189593076706 *\n",
      "Epoch: 21, Train_Loss: 0.17898647487163544, Test_Loss: 0.19554568827152252 *\n",
      "Epoch: 21, Train_Loss: 0.19647493958473206, Test_Loss: 0.19586263597011566\n",
      "Epoch: 21, Train_Loss: 0.18005400896072388, Test_Loss: 0.2154151052236557\n",
      "Epoch: 21, Train_Loss: 0.18262770771980286, Test_Loss: 0.5589963793754578\n",
      "Epoch: 21, Train_Loss: 0.2623375654220581, Test_Loss: 5.090826034545898\n",
      "Epoch: 21, Train_Loss: 0.17816348373889923, Test_Loss: 0.24420998990535736 *\n",
      "Epoch: 21, Train_Loss: 0.22654326260089874, Test_Loss: 0.18525023758411407 *\n",
      "Epoch: 21, Train_Loss: 0.19098882377147675, Test_Loss: 0.18614639341831207\n",
      "Epoch: 21, Train_Loss: 0.20130445063114166, Test_Loss: 0.17561914026737213 *\n",
      "Epoch: 21, Train_Loss: 0.27008694410324097, Test_Loss: 0.18023031949996948\n",
      "Epoch: 21, Train_Loss: 0.3682507872581482, Test_Loss: 0.1872716248035431\n",
      "Epoch: 21, Train_Loss: 0.17946627736091614, Test_Loss: 0.2213045358657837\n",
      "Epoch: 21, Train_Loss: 0.2002743035554886, Test_Loss: 0.19040748476982117 *\n",
      "Epoch: 21, Train_Loss: 0.17641183733940125, Test_Loss: 0.1834976226091385 *\n",
      "Epoch: 21, Train_Loss: 0.17594978213310242, Test_Loss: 0.18848615884780884\n",
      "Epoch: 21, Train_Loss: 0.17736370861530304, Test_Loss: 0.2540048360824585\n",
      "Epoch: 21, Train_Loss: 0.1765817403793335, Test_Loss: 0.18848858773708344 *\n",
      "Epoch: 21, Train_Loss: 0.17716127634048462, Test_Loss: 0.1960497349500656\n",
      "Epoch: 21, Train_Loss: 0.18475505709648132, Test_Loss: 0.20368517935276031\n",
      "Epoch: 21, Train_Loss: 0.1816118359565735, Test_Loss: 0.179754376411438 *\n",
      "Epoch: 21, Train_Loss: 0.17788845300674438, Test_Loss: 0.17510512471199036 *\n",
      "Epoch: 21, Train_Loss: 0.1794697344303131, Test_Loss: 0.17639291286468506\n",
      "Epoch: 21, Train_Loss: 0.18084178864955902, Test_Loss: 0.1858619898557663\n",
      "Epoch: 21, Train_Loss: 0.17486827075481415, Test_Loss: 0.17517095804214478 *\n",
      "Epoch: 21, Train_Loss: 0.17380252480506897, Test_Loss: 0.1864180862903595\n",
      "Epoch: 21, Train_Loss: 0.1877291053533554, Test_Loss: 0.17428913712501526 *\n",
      "Epoch: 21, Train_Loss: 0.17967431247234344, Test_Loss: 0.17668670415878296\n",
      "Epoch: 21, Train_Loss: 0.19610115885734558, Test_Loss: 0.18060626089572906\n",
      "Epoch: 21, Train_Loss: 0.17603851854801178, Test_Loss: 0.1797722727060318 *\n",
      "Epoch: 21, Train_Loss: 0.19184570014476776, Test_Loss: 0.17586398124694824 *\n",
      "Epoch: 21, Train_Loss: 0.18788489699363708, Test_Loss: 0.17615868151187897\n",
      "Epoch: 21, Train_Loss: 0.20120759308338165, Test_Loss: 0.17741647362709045\n",
      "Epoch: 21, Train_Loss: 0.17989279329776764, Test_Loss: 0.1743970811367035 *\n",
      "Epoch: 21, Train_Loss: 0.20301756262779236, Test_Loss: 0.18243634700775146\n",
      "Epoch: 21, Train_Loss: 0.17337121069431305, Test_Loss: 0.2477220594882965\n",
      "Epoch: 21, Train_Loss: 0.18824288249015808, Test_Loss: 2.895441770553589\n",
      "Epoch: 21, Train_Loss: 0.1869288980960846, Test_Loss: 3.452025890350342\n",
      "Epoch: 21, Train_Loss: 0.19533118605613708, Test_Loss: 0.19106291234493256 *\n",
      "Epoch: 21, Train_Loss: 1.5279712677001953, Test_Loss: 0.17467421293258667 *\n",
      "Epoch: 21, Train_Loss: 4.0396881103515625, Test_Loss: 0.19483399391174316\n",
      "Epoch: 21, Train_Loss: 0.36385124921798706, Test_Loss: 0.1872217208147049 *\n",
      "Epoch: 21, Train_Loss: 0.21632948517799377, Test_Loss: 0.19645652174949646\n",
      "Epoch: 21, Train_Loss: 0.1828104555606842, Test_Loss: 0.2303093671798706\n",
      "Epoch: 21, Train_Loss: 0.2385430932044983, Test_Loss: 0.25806647539138794\n",
      "Epoch: 21, Train_Loss: 0.2248300313949585, Test_Loss: 0.17737679183483124 *\n",
      "Epoch: 21, Train_Loss: 0.1919582337141037, Test_Loss: 0.19426777958869934\n",
      "Epoch: 21, Train_Loss: 0.1735355705022812, Test_Loss: 0.19416771829128265 *\n",
      "Epoch: 21, Train_Loss: 0.24051380157470703, Test_Loss: 0.19714486598968506\n",
      "Epoch: 21, Train_Loss: 0.2015955001115799, Test_Loss: 0.18135744333267212 *\n",
      "Epoch: 21, Train_Loss: 0.18160438537597656, Test_Loss: 0.22300562262535095\n",
      "Epoch: 21, Train_Loss: 0.48126012086868286, Test_Loss: 0.22053220868110657 *\n",
      "Epoch: 21, Train_Loss: 0.5482155084609985, Test_Loss: 0.24294406175613403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.9450653791427612, Test_Loss: 0.18973027169704437 *\n",
      "Epoch: 21, Train_Loss: 0.24999076128005981, Test_Loss: 0.2204936444759369\n",
      "Epoch: 21, Train_Loss: 0.37298280000686646, Test_Loss: 0.22704729437828064\n",
      "Epoch: 21, Train_Loss: 1.3286516666412354, Test_Loss: 0.17373639345169067 *\n",
      "Epoch: 21, Train_Loss: 0.747643768787384, Test_Loss: 0.1876092553138733\n",
      "Epoch: 21, Train_Loss: 0.18246449530124664, Test_Loss: 0.18209753930568695 *\n",
      "Epoch: 21, Train_Loss: 0.18408378958702087, Test_Loss: 0.18497277796268463\n",
      "Epoch: 21, Train_Loss: 0.6265331506729126, Test_Loss: 0.18349014222621918 *\n",
      "Epoch: 21, Train_Loss: 0.4458264410495758, Test_Loss: 0.18249689042568207 *\n",
      "Epoch: 21, Train_Loss: 0.7086852788925171, Test_Loss: 0.18798834085464478\n",
      "Epoch: 21, Train_Loss: 0.1770184487104416, Test_Loss: 0.2142610400915146\n",
      "Epoch: 21, Train_Loss: 0.21210111677646637, Test_Loss: 0.1845281422138214 *\n",
      "Epoch: 21, Train_Loss: 0.38674795627593994, Test_Loss: 0.1962522566318512\n",
      "Epoch: 21, Train_Loss: 0.4314612150192261, Test_Loss: 0.17579801380634308 *\n",
      "Epoch: 21, Train_Loss: 0.1973043978214264, Test_Loss: 0.21754778921604156\n",
      "Epoch: 21, Train_Loss: 0.2136392891407013, Test_Loss: 0.23197078704833984\n",
      "Epoch: 21, Train_Loss: 0.240565687417984, Test_Loss: 0.562309741973877\n",
      "Epoch: 21, Train_Loss: 0.262937068939209, Test_Loss: 0.4415721297264099 *\n",
      "Epoch: 21, Train_Loss: 0.30876439809799194, Test_Loss: 0.256009042263031 *\n",
      "Epoch: 21, Train_Loss: 0.28546565771102905, Test_Loss: 0.20020493865013123 *\n",
      "Epoch: 21, Train_Loss: 0.23106898367404938, Test_Loss: 0.21176767349243164\n",
      "Epoch: 21, Train_Loss: 0.2646915912628174, Test_Loss: 0.19070108234882355 *\n",
      "Epoch: 21, Train_Loss: 0.2326267510652542, Test_Loss: 0.3167027235031128\n",
      "Epoch: 21, Train_Loss: 0.271470308303833, Test_Loss: 0.23024804890155792 *\n",
      "Epoch: 21, Train_Loss: 0.32449841499328613, Test_Loss: 0.5576274991035461\n",
      "Epoch: 21, Train_Loss: 0.3720276951789856, Test_Loss: 0.25894543528556824 *\n",
      "Epoch: 21, Train_Loss: 0.21412837505340576, Test_Loss: 0.2297600507736206 *\n",
      "Epoch: 21, Train_Loss: 0.2460024356842041, Test_Loss: 0.1828330159187317 *\n",
      "Epoch: 21, Train_Loss: 0.2529922127723694, Test_Loss: 0.17494620382785797 *\n",
      "Epoch: 21, Train_Loss: 0.18893828988075256, Test_Loss: 0.18543265759944916\n",
      "Epoch: 21, Train_Loss: 0.17429658770561218, Test_Loss: 0.19677944481372833\n",
      "Epoch: 21, Train_Loss: 0.17803235352039337, Test_Loss: 0.18642067909240723 *\n",
      "Epoch: 21, Train_Loss: 0.18378415703773499, Test_Loss: 0.19202320277690887\n",
      "Epoch: 21, Train_Loss: 0.1821763962507248, Test_Loss: 0.2879634499549866\n",
      "Epoch: 21, Train_Loss: 0.1849454939365387, Test_Loss: 0.3288905620574951\n",
      "Epoch: 21, Train_Loss: 0.19504308700561523, Test_Loss: 0.44409042596817017\n",
      "Epoch: 21, Train_Loss: 0.19238929450511932, Test_Loss: 0.5893592238426208\n",
      "Epoch: 21, Train_Loss: 0.24125564098358154, Test_Loss: 0.35253697633743286 *\n",
      "Epoch: 21, Train_Loss: 0.3393264412879944, Test_Loss: 0.34195172786712646 *\n",
      "Epoch: 21, Train_Loss: 0.31392544507980347, Test_Loss: 0.33924955129623413 *\n",
      "Epoch: 21, Train_Loss: 0.1895921230316162, Test_Loss: 0.3438068926334381\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.24611619114875793, Test_Loss: 0.46779966354370117\n",
      "Epoch: 21, Train_Loss: 0.2739570140838623, Test_Loss: 1.53813636302948\n",
      "Epoch: 21, Train_Loss: 0.2814468741416931, Test_Loss: 3.963561534881592\n",
      "Epoch: 21, Train_Loss: 0.2310384213924408, Test_Loss: 0.2036570906639099 *\n",
      "Epoch: 21, Train_Loss: 0.24915456771850586, Test_Loss: 0.20096522569656372 *\n",
      "Epoch: 21, Train_Loss: 0.26510944962501526, Test_Loss: 0.20269134640693665\n",
      "Epoch: 21, Train_Loss: 0.23740507662296295, Test_Loss: 0.18457239866256714 *\n",
      "Epoch: 21, Train_Loss: 0.31807559728622437, Test_Loss: 0.18358156085014343 *\n",
      "Epoch: 21, Train_Loss: 0.1976204514503479, Test_Loss: 0.21533958613872528\n",
      "Epoch: 21, Train_Loss: 0.20017588138580322, Test_Loss: 0.21066069602966309 *\n",
      "Epoch: 21, Train_Loss: 0.5324847102165222, Test_Loss: 0.17932574450969696 *\n",
      "Epoch: 21, Train_Loss: 0.5253311991691589, Test_Loss: 0.1884496510028839\n",
      "Epoch: 21, Train_Loss: 0.2929043769836426, Test_Loss: 0.18461191654205322 *\n",
      "Epoch: 21, Train_Loss: 0.25269806385040283, Test_Loss: 0.2612552046775818\n",
      "Epoch: 21, Train_Loss: 0.1875901222229004, Test_Loss: 0.18778811395168304 *\n",
      "Epoch: 21, Train_Loss: 0.18458831310272217, Test_Loss: 0.18316829204559326 *\n",
      "Epoch: 21, Train_Loss: 0.5094971060752869, Test_Loss: 0.20082011818885803\n",
      "Epoch: 21, Train_Loss: 0.2125249058008194, Test_Loss: 0.18149209022521973 *\n",
      "Epoch: 21, Train_Loss: 0.21524804830551147, Test_Loss: 0.19462956488132477\n",
      "Epoch: 21, Train_Loss: 0.3337881565093994, Test_Loss: 0.22465947270393372\n",
      "Epoch: 21, Train_Loss: 0.2170475721359253, Test_Loss: 0.18151964247226715 *\n",
      "Epoch: 21, Train_Loss: 0.19450417160987854, Test_Loss: 0.19274236261844635\n",
      "Epoch: 21, Train_Loss: 0.24011671543121338, Test_Loss: 0.21162918210029602\n",
      "Epoch: 21, Train_Loss: 0.3605306148529053, Test_Loss: 0.17799437046051025 *\n",
      "Epoch: 21, Train_Loss: 0.20031842589378357, Test_Loss: 0.184524267911911\n",
      "Epoch: 21, Train_Loss: 0.2654974162578583, Test_Loss: 0.19266267120838165\n",
      "Epoch: 21, Train_Loss: 0.2039211243391037, Test_Loss: 0.17901091277599335 *\n",
      "Epoch: 21, Train_Loss: 0.2566595673561096, Test_Loss: 0.17620274424552917 *\n",
      "Epoch: 21, Train_Loss: 0.23691461980342865, Test_Loss: 0.21365207433700562\n",
      "Epoch: 21, Train_Loss: 0.1984577476978302, Test_Loss: 0.2050127238035202 *\n",
      "Epoch: 21, Train_Loss: 0.1822473108768463, Test_Loss: 0.18409346044063568 *\n",
      "Epoch: 21, Train_Loss: 0.20599475502967834, Test_Loss: 0.20776724815368652\n",
      "Epoch: 21, Train_Loss: 0.4583587050437927, Test_Loss: 0.29893672466278076\n",
      "Epoch: 21, Train_Loss: 0.410613089799881, Test_Loss: 3.6694605350494385\n",
      "Epoch: 21, Train_Loss: 0.5464134216308594, Test_Loss: 1.8650448322296143 *\n",
      "Epoch: 21, Train_Loss: 0.6000930666923523, Test_Loss: 0.19902503490447998 *\n",
      "Epoch: 21, Train_Loss: 0.4039316177368164, Test_Loss: 0.19118261337280273 *\n",
      "Epoch: 21, Train_Loss: 0.21711309254169464, Test_Loss: 0.18700018525123596 *\n",
      "Epoch: 21, Train_Loss: 0.23769435286521912, Test_Loss: 0.1821065992116928 *\n",
      "Epoch: 21, Train_Loss: 0.1872428059577942, Test_Loss: 0.18752703070640564\n",
      "Epoch: 21, Train_Loss: 0.180683434009552, Test_Loss: 0.21812286972999573\n",
      "Epoch: 21, Train_Loss: 0.2080339640378952, Test_Loss: 0.2082236409187317 *\n",
      "Epoch: 21, Train_Loss: 0.42751044034957886, Test_Loss: 0.17749428749084473 *\n",
      "Epoch: 21, Train_Loss: 0.41752731800079346, Test_Loss: 0.2002640813589096\n",
      "Epoch: 21, Train_Loss: 0.5079067945480347, Test_Loss: 0.21088309586048126\n",
      "Epoch: 21, Train_Loss: 1.1546250581741333, Test_Loss: 0.21542946994304657\n",
      "Epoch: 21, Train_Loss: 0.6977694630622864, Test_Loss: 0.18195348978042603 *\n",
      "Epoch: 21, Train_Loss: 0.4230167865753174, Test_Loss: 0.192806676030159\n",
      "Epoch: 21, Train_Loss: 0.19491875171661377, Test_Loss: 0.2239801585674286\n",
      "Epoch: 21, Train_Loss: 0.17932923138141632, Test_Loss: 0.22832384705543518\n",
      "Epoch: 21, Train_Loss: 0.3967575430870056, Test_Loss: 0.1898854821920395 *\n",
      "Epoch: 21, Train_Loss: 0.44983336329460144, Test_Loss: 0.24586892127990723\n",
      "Epoch: 21, Train_Loss: 0.5434170365333557, Test_Loss: 0.19231994450092316 *\n",
      "Epoch: 21, Train_Loss: 0.21663455665111542, Test_Loss: 0.1891179084777832 *\n",
      "Epoch: 21, Train_Loss: 0.18681268393993378, Test_Loss: 0.21274718642234802\n",
      "Epoch: 21, Train_Loss: 0.2657858431339264, Test_Loss: 0.2287486493587494\n",
      "Epoch: 21, Train_Loss: 0.44454216957092285, Test_Loss: 0.20596002042293549 *\n",
      "Epoch: 21, Train_Loss: 0.350000262260437, Test_Loss: 0.20830747485160828\n",
      "Epoch: 21, Train_Loss: 0.2293211966753006, Test_Loss: 0.22076691687107086\n",
      "Epoch: 21, Train_Loss: 0.310453325510025, Test_Loss: 0.22367452085018158\n",
      "Epoch: 21, Train_Loss: 0.18183258175849915, Test_Loss: 0.2132713794708252 *\n",
      "Epoch: 21, Train_Loss: 0.180569589138031, Test_Loss: 0.20378507673740387 *\n",
      "Epoch: 21, Train_Loss: 0.20676834881305695, Test_Loss: 0.2016594558954239 *\n",
      "Epoch: 21, Train_Loss: 0.1799253672361374, Test_Loss: 0.17607340216636658 *\n",
      "Epoch: 21, Train_Loss: 0.23198413848876953, Test_Loss: 0.19153828918933868\n",
      "Epoch: 21, Train_Loss: 0.248434379696846, Test_Loss: 0.26169055700302124\n",
      "Epoch: 21, Train_Loss: 2.600236654281616, Test_Loss: 0.30122268199920654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 14.059591293334961, Test_Loss: 0.3200053572654724\n",
      "Epoch: 21, Train_Loss: 0.559853732585907, Test_Loss: 0.20827855169773102 *\n",
      "Epoch: 21, Train_Loss: 0.6851567625999451, Test_Loss: 0.22205638885498047\n",
      "Epoch: 21, Train_Loss: 0.6840001344680786, Test_Loss: 0.2086678147315979 *\n",
      "Epoch: 21, Train_Loss: 0.22210845351219177, Test_Loss: 0.21974356472492218\n",
      "Epoch: 21, Train_Loss: 0.39676398038864136, Test_Loss: 0.41659221053123474\n",
      "Epoch: 21, Train_Loss: 4.739076137542725, Test_Loss: 0.2357119917869568 *\n",
      "Epoch: 21, Train_Loss: 2.2185065746307373, Test_Loss: 0.5990280508995056\n",
      "Epoch: 21, Train_Loss: 0.24279996752738953, Test_Loss: 0.2567654848098755 *\n",
      "Epoch: 21, Train_Loss: 1.7257936000823975, Test_Loss: 0.24628490209579468 *\n",
      "Epoch: 21, Train_Loss: 3.7760281562805176, Test_Loss: 0.21450620889663696 *\n",
      "Epoch: 21, Train_Loss: 0.5045313835144043, Test_Loss: 0.19714921712875366 *\n",
      "Epoch: 21, Train_Loss: 0.1824951469898224, Test_Loss: 0.22508226335048676\n",
      "Epoch: 21, Train_Loss: 0.20157450437545776, Test_Loss: 0.19145207107067108 *\n",
      "Epoch: 21, Train_Loss: 0.232407808303833, Test_Loss: 0.20871764421463013\n",
      "Epoch: 21, Train_Loss: 0.18284890055656433, Test_Loss: 0.17644542455673218 *\n",
      "Epoch: 21, Train_Loss: 0.17211098968982697, Test_Loss: 0.27261027693748474\n",
      "Epoch: 21, Train_Loss: 0.17114266753196716, Test_Loss: 0.6716591715812683\n",
      "Epoch: 21, Train_Loss: 0.1709735095500946, Test_Loss: 0.3988022804260254 *\n",
      "Epoch: 21, Train_Loss: 0.17198584973812103, Test_Loss: 0.3548069894313812 *\n",
      "Epoch: 21, Train_Loss: 0.17753247916698456, Test_Loss: 0.1753457933664322 *\n",
      "Epoch: 21, Train_Loss: 0.17667709290981293, Test_Loss: 0.17190933227539062 *\n",
      "Epoch: 21, Train_Loss: 0.28043583035469055, Test_Loss: 0.171656534075737 *\n",
      "Epoch: 21, Train_Loss: 0.2599937617778778, Test_Loss: 0.17239120602607727\n",
      "Epoch: 21, Train_Loss: 0.1935853511095047, Test_Loss: 0.1819787323474884\n",
      "Epoch: 21, Train_Loss: 0.1810474395751953, Test_Loss: 4.306158542633057\n",
      "Epoch: 21, Train_Loss: 0.18309742212295532, Test_Loss: 4.4851484298706055\n",
      "Epoch: 21, Train_Loss: 0.18469861149787903, Test_Loss: 0.4625862240791321 *\n",
      "Epoch: 21, Train_Loss: 0.1745680719614029, Test_Loss: 0.37538468837738037 *\n",
      "Epoch: 21, Train_Loss: 0.17269833385944366, Test_Loss: 0.5656479001045227\n",
      "Epoch: 21, Train_Loss: 0.17094650864601135, Test_Loss: 0.24708375334739685 *\n",
      "Epoch: 21, Train_Loss: 0.17552052438259125, Test_Loss: 0.5510985851287842\n",
      "Epoch: 21, Train_Loss: 0.1745981127023697, Test_Loss: 0.9941141605377197\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.17139467597007751, Test_Loss: 0.7585607171058655 *\n",
      "Epoch: 21, Train_Loss: 0.17099715769290924, Test_Loss: 0.5892379283905029 *\n",
      "Epoch: 21, Train_Loss: 0.17370684444904327, Test_Loss: 0.7630418539047241\n",
      "Epoch: 21, Train_Loss: 0.18220815062522888, Test_Loss: 0.4908849596977234 *\n",
      "Epoch: 21, Train_Loss: 0.19832879304885864, Test_Loss: 1.2309824228286743\n",
      "Epoch: 21, Train_Loss: 0.22317010164260864, Test_Loss: 0.5733311772346497 *\n",
      "Epoch: 21, Train_Loss: 0.18612174689769745, Test_Loss: 0.650094747543335\n",
      "Epoch: 21, Train_Loss: 0.36979424953460693, Test_Loss: 0.5293393731117249 *\n",
      "Epoch: 21, Train_Loss: 5.655069351196289, Test_Loss: 0.23230639100074768 *\n",
      "Epoch: 21, Train_Loss: 1.077826738357544, Test_Loss: 0.2115185558795929 *\n",
      "Epoch: 21, Train_Loss: 0.2116675078868866, Test_Loss: 0.2996918559074402\n",
      "Epoch: 21, Train_Loss: 0.22815479338169098, Test_Loss: 0.4486985206604004\n",
      "Epoch: 21, Train_Loss: 0.24633711576461792, Test_Loss: 0.24268585443496704 *\n",
      "Epoch: 21, Train_Loss: 0.20967352390289307, Test_Loss: 0.46673259139060974\n",
      "Epoch: 21, Train_Loss: 0.2702668011188507, Test_Loss: 0.22495540976524353 *\n",
      "Epoch: 21, Train_Loss: 0.25454819202423096, Test_Loss: 0.43031853437423706\n",
      "Epoch: 21, Train_Loss: 0.3144570291042328, Test_Loss: 0.48337364196777344\n",
      "Epoch: 21, Train_Loss: 0.31566619873046875, Test_Loss: 0.24430063366889954 *\n",
      "Epoch: 21, Train_Loss: 0.2824864983558655, Test_Loss: 0.18688416481018066 *\n",
      "Epoch: 21, Train_Loss: 0.18465624749660492, Test_Loss: 0.2733950912952423\n",
      "Epoch: 21, Train_Loss: 0.2115473747253418, Test_Loss: 0.21733257174491882 *\n",
      "Epoch: 21, Train_Loss: 0.18820597231388092, Test_Loss: 0.19360840320587158 *\n",
      "Epoch: 21, Train_Loss: 0.554607093334198, Test_Loss: 0.4022979736328125\n",
      "Epoch: 21, Train_Loss: 0.17584368586540222, Test_Loss: 0.2865629196166992 *\n",
      "Epoch: 21, Train_Loss: 0.1957765817642212, Test_Loss: 5.2616777420043945\n",
      "Epoch: 21, Train_Loss: 0.2284272313117981, Test_Loss: 1.1952364444732666 *\n",
      "Epoch: 21, Train_Loss: 0.19598166644573212, Test_Loss: 0.17314161360263824 *\n",
      "Epoch: 21, Train_Loss: 0.21302580833435059, Test_Loss: 0.19180573523044586\n",
      "Epoch: 21, Train_Loss: 0.2102685272693634, Test_Loss: 0.22289074957370758\n",
      "Epoch: 21, Train_Loss: 0.21675115823745728, Test_Loss: 0.19137419760227203 *\n",
      "Epoch: 21, Train_Loss: 0.2016390860080719, Test_Loss: 0.17985288798809052 *\n",
      "Epoch: 21, Train_Loss: 0.17623953521251678, Test_Loss: 0.25744256377220154\n",
      "Epoch: 21, Train_Loss: 0.24940690398216248, Test_Loss: 0.23617753386497498 *\n",
      "Epoch: 21, Train_Loss: 4.013592720031738, Test_Loss: 0.17309030890464783 *\n",
      "Epoch: 21, Train_Loss: 0.388423353433609, Test_Loss: 0.20118986070156097\n",
      "Epoch: 21, Train_Loss: 0.17268414795398712, Test_Loss: 0.18571257591247559 *\n",
      "Epoch: 21, Train_Loss: 0.176373690366745, Test_Loss: 0.18230339884757996 *\n",
      "Epoch: 21, Train_Loss: 0.175201416015625, Test_Loss: 0.1846429407596588\n",
      "Epoch: 21, Train_Loss: 0.17106421291828156, Test_Loss: 0.22314943373203278\n",
      "Epoch: 21, Train_Loss: 0.17388656735420227, Test_Loss: 0.22564290463924408\n",
      "Epoch: 21, Train_Loss: 0.17268146574497223, Test_Loss: 0.27272629737854004\n",
      "Epoch: 21, Train_Loss: 0.2039775848388672, Test_Loss: 0.20505918562412262 *\n",
      "Epoch: 21, Train_Loss: 0.18940499424934387, Test_Loss: 0.2062923014163971\n",
      "Epoch: 21, Train_Loss: 0.19268447160720825, Test_Loss: 0.22152215242385864\n",
      "Epoch: 21, Train_Loss: 0.17155803740024567, Test_Loss: 0.23336660861968994\n",
      "Epoch: 21, Train_Loss: 0.1711965799331665, Test_Loss: 0.24945244193077087\n",
      "Epoch: 21, Train_Loss: 0.1812271922826767, Test_Loss: 0.28458482027053833\n",
      "Epoch: 21, Train_Loss: 0.18143881857395172, Test_Loss: 0.24066445231437683 *\n",
      "Epoch: 21, Train_Loss: 0.17559204995632172, Test_Loss: 0.22412481904029846 *\n",
      "Epoch: 21, Train_Loss: 0.18849509954452515, Test_Loss: 0.26776015758514404\n",
      "Epoch: 21, Train_Loss: 0.19222600758075714, Test_Loss: 0.25214385986328125 *\n",
      "Epoch: 21, Train_Loss: 0.1874629110097885, Test_Loss: 0.19795888662338257 *\n",
      "Epoch: 21, Train_Loss: 0.17199845612049103, Test_Loss: 0.18707016110420227 *\n",
      "Epoch: 21, Train_Loss: 0.17090781033039093, Test_Loss: 0.18484628200531006 *\n",
      "Epoch: 21, Train_Loss: 0.23201842606067657, Test_Loss: 0.19716480374336243\n",
      "Epoch: 21, Train_Loss: 0.2664882242679596, Test_Loss: 0.19079311192035675 *\n",
      "Epoch: 21, Train_Loss: 0.2661804258823395, Test_Loss: 0.3293911814689636\n",
      "Epoch: 21, Train_Loss: 0.2168724089860916, Test_Loss: 0.2553461194038391 *\n",
      "Epoch: 21, Train_Loss: 0.220509335398674, Test_Loss: 0.32016175985336304\n",
      "Epoch: 21, Train_Loss: 0.2076149582862854, Test_Loss: 0.21419131755828857 *\n",
      "Epoch: 21, Train_Loss: 0.19283467531204224, Test_Loss: 0.18994499742984772 *\n",
      "Epoch: 21, Train_Loss: 0.21424664556980133, Test_Loss: 0.19663721323013306\n",
      "Epoch: 21, Train_Loss: 0.46848824620246887, Test_Loss: 0.24700570106506348\n",
      "Epoch: 21, Train_Loss: 0.25901105999946594, Test_Loss: 0.4549191892147064\n",
      "Epoch: 21, Train_Loss: 0.23167669773101807, Test_Loss: 0.29357242584228516 *\n",
      "Epoch: 21, Train_Loss: 0.1704014092683792, Test_Loss: 0.47806668281555176\n",
      "Epoch: 21, Train_Loss: 0.17118582129478455, Test_Loss: 0.22117973864078522 *\n",
      "Epoch: 21, Train_Loss: 0.1710129678249359, Test_Loss: 0.2315579056739807\n",
      "Epoch: 21, Train_Loss: 0.17084181308746338, Test_Loss: 0.18388043344020844 *\n",
      "Epoch: 21, Train_Loss: 0.1826760172843933, Test_Loss: 0.17489475011825562 *\n",
      "Epoch: 21, Train_Loss: 5.177247047424316, Test_Loss: 0.19754710793495178\n",
      "Epoch: 21, Train_Loss: 0.7197253108024597, Test_Loss: 0.19409039616584778 *\n",
      "Epoch: 21, Train_Loss: 0.17234084010124207, Test_Loss: 0.20637840032577515\n",
      "Epoch: 21, Train_Loss: 0.1859489530324936, Test_Loss: 0.17621803283691406 *\n",
      "Epoch: 21, Train_Loss: 0.17695167660713196, Test_Loss: 0.2805323600769043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train_Loss: 0.17161424458026886, Test_Loss: 0.5111426711082458\n",
      "Epoch: 21, Train_Loss: 0.1717638373374939, Test_Loss: 0.2359544336795807 *\n",
      "Epoch: 21, Train_Loss: 0.1713806539773941, Test_Loss: 0.3933418393135071\n",
      "Epoch: 21, Train_Loss: 0.17250679433345795, Test_Loss: 0.2020074725151062 *\n",
      "Epoch: 21, Train_Loss: 0.17220398783683777, Test_Loss: 0.2031555473804474\n",
      "Epoch: 21, Train_Loss: 0.20342442393302917, Test_Loss: 0.20314925909042358 *\n",
      "Epoch: 21, Train_Loss: 0.21452531218528748, Test_Loss: 0.20207825303077698 *\n",
      "Epoch: 21, Train_Loss: 0.24047723412513733, Test_Loss: 0.20183342695236206 *\n",
      "Epoch: 21, Train_Loss: 0.21011823415756226, Test_Loss: 4.198207378387451\n",
      "Epoch: 21, Train_Loss: 0.174940824508667, Test_Loss: 1.6296923160552979 *\n",
      "Epoch: 21, Train_Loss: 0.2483663707971573, Test_Loss: 0.1855403482913971 *\n",
      "Epoch: 21, Train_Loss: 0.394319087266922, Test_Loss: 0.18282830715179443 *\n",
      "Epoch: 21, Train_Loss: 0.3965338468551636, Test_Loss: 0.1797827184200287 *\n",
      "Epoch: 21, Train_Loss: 0.36751508712768555, Test_Loss: 0.18630018830299377\n",
      "Epoch: 21, Train_Loss: 0.17976346611976624, Test_Loss: 0.1756742149591446 *\n",
      "Epoch: 21, Train_Loss: 0.17100566625595093, Test_Loss: 0.1779857575893402\n",
      "Epoch: 21, Train_Loss: 0.17163021862506866, Test_Loss: 0.1769886016845703 *\n",
      "Epoch: 21, Train_Loss: 0.17537766695022583, Test_Loss: 0.17455029487609863 *\n",
      "Epoch: 21, Train_Loss: 0.17387895286083221, Test_Loss: 0.17283833026885986 *\n",
      "Epoch: 21, Train_Loss: 0.18367786705493927, Test_Loss: 0.17365576326847076\n",
      "Epoch: 21, Train_Loss: 0.18099768459796906, Test_Loss: 0.18071748316287994\n",
      "Epoch: 21, Train_Loss: 0.17082124948501587, Test_Loss: 0.19038783013820648\n",
      "Epoch: 21, Train_Loss: 0.17596130073070526, Test_Loss: 0.19508568942546844\n",
      "Epoch: 21, Train_Loss: 0.18267089128494263, Test_Loss: 0.17417824268341064 *\n",
      "Epoch: 21, Train_Loss: 0.2649811804294586, Test_Loss: 0.17331378161907196 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 21\n",
      "Epoch: 21, Train_Loss: 0.2613455057144165, Test_Loss: 0.17285418510437012 *\n",
      "Epoch: 21, Train_Loss: 0.23458252847194672, Test_Loss: 0.17919889092445374\n",
      "Epoch: 21, Train_Loss: 0.23449476063251495, Test_Loss: 0.1737014800310135 *\n",
      "Epoch: 21, Train_Loss: 0.27935856580734253, Test_Loss: 0.17297278344631195 *\n",
      "Epoch: 21, Train_Loss: 0.27074527740478516, Test_Loss: 0.17220810055732727 *\n",
      "Epoch: 21, Train_Loss: 0.19046536087989807, Test_Loss: 0.1717560887336731 *\n",
      "Epoch: 21, Train_Loss: 0.2768249809741974, Test_Loss: 0.17598727345466614\n",
      "Epoch: 21, Train_Loss: 0.20575743913650513, Test_Loss: 0.18201766908168793\n",
      "Epoch: 21, Train_Loss: 0.3954940140247345, Test_Loss: 0.17626601457595825 *\n",
      "Epoch: 21, Train_Loss: 0.17606791853904724, Test_Loss: 0.17379771173000336 *\n",
      "Epoch: 21, Train_Loss: 1.3042380809783936, Test_Loss: 0.1844186633825302\n",
      "Epoch: 21, Train_Loss: 1.7981300354003906, Test_Loss: 0.18785806000232697\n",
      "Epoch: 21, Train_Loss: 0.25104403495788574, Test_Loss: 0.17237703502178192 *\n",
      "Epoch: 21, Train_Loss: 0.21609723567962646, Test_Loss: 0.24345247447490692\n",
      "Epoch: 21, Train_Loss: 0.19676725566387177, Test_Loss: 0.23545265197753906 *\n",
      "Epoch: 21, Train_Loss: 0.19435277581214905, Test_Loss: 5.194764137268066\n",
      "Epoch: 21, Train_Loss: 0.17381808161735535, Test_Loss: 0.23799151182174683 *\n",
      "Epoch: 21, Train_Loss: 0.18552754819393158, Test_Loss: 0.19846753776073456 *\n",
      "Epoch: 21, Train_Loss: 0.2377229779958725, Test_Loss: 0.21039779484272003\n",
      "Epoch: 21, Train_Loss: 0.21599282324314117, Test_Loss: 0.18845966458320618 *\n",
      "Epoch: 21, Train_Loss: 0.18927493691444397, Test_Loss: 0.19658999145030975\n",
      "Epoch: 21, Train_Loss: 0.18376606702804565, Test_Loss: 0.17969441413879395 *\n",
      "Epoch: 21, Train_Loss: 0.18503737449645996, Test_Loss: 0.21763810515403748\n",
      "Epoch: 21, Train_Loss: 0.17963548004627228, Test_Loss: 0.2096346765756607 *\n",
      "Epoch: 21, Train_Loss: 0.1865231692790985, Test_Loss: 0.1805918961763382 *\n",
      "Epoch: 21, Train_Loss: 0.2484385073184967, Test_Loss: 0.18438713252544403\n",
      "Epoch: 21, Train_Loss: 0.18795469403266907, Test_Loss: 0.2432200014591217\n",
      "Epoch: 21, Train_Loss: 0.1862698495388031, Test_Loss: 0.19424745440483093 *\n",
      "Epoch: 21, Train_Loss: 0.17148824036121368, Test_Loss: 0.23326918482780457\n",
      "Epoch: 21, Train_Loss: 0.18494221568107605, Test_Loss: 0.23030711710453033 *\n",
      "Epoch: 21, Train_Loss: 0.17593605816364288, Test_Loss: 0.21523427963256836 *\n",
      "Epoch: 21, Train_Loss: 0.18536920845508575, Test_Loss: 0.1915174424648285 *\n",
      "Epoch: 21, Train_Loss: 0.1721792072057724, Test_Loss: 0.2132556289434433\n",
      "Epoch: 21, Train_Loss: 0.17006632685661316, Test_Loss: 0.3103553056716919\n",
      "Epoch: 21, Train_Loss: 0.1707858443260193, Test_Loss: 0.1851658970117569 *\n",
      "Epoch: 21, Train_Loss: 0.1743144989013672, Test_Loss: 0.1705394834280014 *\n",
      "Epoch: 21, Train_Loss: 0.17172150313854218, Test_Loss: 0.17373168468475342\n",
      "Epoch: 21, Train_Loss: 0.17317399382591248, Test_Loss: 0.17257905006408691 *\n",
      "Epoch: 21, Train_Loss: 0.17331811785697937, Test_Loss: 0.17061187326908112 *\n",
      "Epoch: 21, Train_Loss: 0.17585337162017822, Test_Loss: 0.17147153615951538\n",
      "Epoch: 21, Train_Loss: 0.16985580325126648, Test_Loss: 0.17110246419906616 *\n",
      "Epoch: 21, Train_Loss: 0.17284981906414032, Test_Loss: 0.17231552302837372\n",
      "Epoch: 21, Train_Loss: 0.1790434569120407, Test_Loss: 0.1762617975473404\n",
      "Epoch: 21, Train_Loss: 0.18333598971366882, Test_Loss: 0.18483299016952515\n",
      "Epoch: 21, Train_Loss: 0.17577143013477325, Test_Loss: 0.17781512439250946 *\n",
      "Epoch: 21, Train_Loss: 0.18861626088619232, Test_Loss: 0.19891197979450226\n",
      "Epoch: 21, Train_Loss: 0.18269215524196625, Test_Loss: 0.19720876216888428 *\n",
      "Epoch: 21, Train_Loss: 0.19800522923469543, Test_Loss: 0.47903144359588623\n",
      "Epoch: 21, Train_Loss: 0.18276657164096832, Test_Loss: 0.4827955365180969\n",
      "Epoch: 21, Train_Loss: 0.17238567769527435, Test_Loss: 0.3384326100349426 *\n",
      "Epoch: 21, Train_Loss: 0.18464088439941406, Test_Loss: 0.21408969163894653 *\n",
      "Epoch: 21, Train_Loss: 0.17503692209720612, Test_Loss: 0.178079754114151 *\n",
      "Epoch: 21, Train_Loss: 0.1729835718870163, Test_Loss: 0.17852294445037842\n",
      "Epoch: 21, Train_Loss: 0.17004501819610596, Test_Loss: 0.2183711975812912\n",
      "Epoch: 22, Train_Loss: 0.17992089688777924, Test_Loss: 0.3739073574542999 *\n",
      "Epoch: 22, Train_Loss: 0.2029799073934555, Test_Loss: 0.3462951183319092 *\n",
      "Epoch: 22, Train_Loss: 0.20105309784412384, Test_Loss: 0.3250468969345093 *\n",
      "Epoch: 22, Train_Loss: 0.18559251725673676, Test_Loss: 0.2364814579486847 *\n",
      "Epoch: 22, Train_Loss: 0.17101767659187317, Test_Loss: 0.18087980151176453 *\n",
      "Epoch: 22, Train_Loss: 0.19970941543579102, Test_Loss: 0.18157252669334412\n",
      "Epoch: 22, Train_Loss: 0.18772198259830475, Test_Loss: 0.17795759439468384 *\n",
      "Epoch: 22, Train_Loss: 0.17411091923713684, Test_Loss: 0.20070333778858185\n",
      "Epoch: 22, Train_Loss: 0.17709249258041382, Test_Loss: 0.19059526920318604 *\n",
      "Epoch: 22, Train_Loss: 0.2279227077960968, Test_Loss: 0.19568301737308502\n",
      "Epoch: 22, Train_Loss: 0.2647159993648529, Test_Loss: 0.17604045569896698 *\n",
      "Epoch: 22, Train_Loss: 0.2223568707704544, Test_Loss: 0.2813909947872162\n",
      "Epoch: 22, Train_Loss: 0.18597547709941864, Test_Loss: 0.5262353420257568\n",
      "Epoch: 22, Train_Loss: 0.19521605968475342, Test_Loss: 0.2210492640733719 *\n",
      "Epoch: 22, Train_Loss: 0.1773504912853241, Test_Loss: 0.36051973700523376\n",
      "Epoch: 22, Train_Loss: 0.1910957545042038, Test_Loss: 0.20037046074867249 *\n",
      "Epoch: 22, Train_Loss: 0.17275452613830566, Test_Loss: 0.20139841735363007\n",
      "Epoch: 22, Train_Loss: 0.18725807964801788, Test_Loss: 0.20084357261657715 *\n",
      "Epoch: 22, Train_Loss: 0.17336374521255493, Test_Loss: 0.19731831550598145 *\n",
      "Epoch: 22, Train_Loss: 0.17377185821533203, Test_Loss: 0.19430269300937653 *\n",
      "Epoch: 22, Train_Loss: 0.24013368785381317, Test_Loss: 5.267526626586914\n",
      "Epoch: 22, Train_Loss: 0.17076458036899567, Test_Loss: 0.6329922676086426 *\n",
      "Epoch: 22, Train_Loss: 0.22278866171836853, Test_Loss: 0.17910552024841309 *\n",
      "Epoch: 22, Train_Loss: 0.18274521827697754, Test_Loss: 0.17689938843250275 *\n",
      "Epoch: 22, Train_Loss: 0.1895422488451004, Test_Loss: 0.17399515211582184 *\n",
      "Epoch: 22, Train_Loss: 0.209672749042511, Test_Loss: 0.17569006979465485\n",
      "Epoch: 22, Train_Loss: 0.4540930986404419, Test_Loss: 0.17485913634300232 *\n",
      "Epoch: 22, Train_Loss: 0.1816197782754898, Test_Loss: 0.18157488107681274\n",
      "Epoch: 22, Train_Loss: 0.1895398199558258, Test_Loss: 0.18224221467971802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.16950276494026184, Test_Loss: 0.1729000210762024 *\n",
      "Epoch: 22, Train_Loss: 0.1698109656572342, Test_Loss: 0.171494260430336 *\n",
      "Epoch: 22, Train_Loss: 0.17300854623317719, Test_Loss: 0.18235927820205688\n",
      "Epoch: 22, Train_Loss: 0.17535753548145294, Test_Loss: 0.17570267617702484 *\n",
      "Epoch: 22, Train_Loss: 0.17511703073978424, Test_Loss: 0.18939122557640076\n",
      "Epoch: 22, Train_Loss: 0.17286980152130127, Test_Loss: 0.20718443393707275\n",
      "Epoch: 22, Train_Loss: 0.1808953434228897, Test_Loss: 0.1721857786178589 *\n",
      "Epoch: 22, Train_Loss: 0.17146208882331848, Test_Loss: 0.17185088992118835 *\n",
      "Epoch: 22, Train_Loss: 0.17123405635356903, Test_Loss: 0.1712632179260254 *\n",
      "Epoch: 22, Train_Loss: 0.17669576406478882, Test_Loss: 0.17973734438419342\n",
      "Epoch: 22, Train_Loss: 0.17206019163131714, Test_Loss: 0.1714676022529602 *\n",
      "Epoch: 22, Train_Loss: 0.16980929672718048, Test_Loss: 0.17338673770427704\n",
      "Epoch: 22, Train_Loss: 0.18450377881526947, Test_Loss: 0.17103183269500732 *\n",
      "Epoch: 22, Train_Loss: 0.17541103065013885, Test_Loss: 0.17136161029338837\n",
      "Epoch: 22, Train_Loss: 0.19616533815860748, Test_Loss: 0.17386986315250397\n",
      "Epoch: 22, Train_Loss: 0.1712273508310318, Test_Loss: 0.18108588457107544\n",
      "Epoch: 22, Train_Loss: 0.17906765639781952, Test_Loss: 0.17309080064296722 *\n",
      "Epoch: 22, Train_Loss: 0.18221688270568848, Test_Loss: 0.17313197255134583\n",
      "Epoch: 22, Train_Loss: 0.19200080633163452, Test_Loss: 0.18114255368709564\n",
      "Epoch: 22, Train_Loss: 0.1690799593925476, Test_Loss: 0.17323563992977142 *\n",
      "Epoch: 22, Train_Loss: 0.1990702748298645, Test_Loss: 0.17057278752326965 *\n",
      "Epoch: 22, Train_Loss: 0.17253197729587555, Test_Loss: 0.2461475133895874\n",
      "Epoch: 22, Train_Loss: 0.17952384054660797, Test_Loss: 0.9260300397872925\n",
      "Epoch: 22, Train_Loss: 0.17129087448120117, Test_Loss: 5.275330543518066\n",
      "Epoch: 22, Train_Loss: 0.18691033124923706, Test_Loss: 0.20456889271736145 *\n",
      "Epoch: 22, Train_Loss: 0.31777554750442505, Test_Loss: 0.1700630784034729 *\n",
      "Epoch: 22, Train_Loss: 3.711541175842285, Test_Loss: 0.18558447062969208\n",
      "Epoch: 22, Train_Loss: 2.0893702507019043, Test_Loss: 0.18204601109027863 *\n",
      "Epoch: 22, Train_Loss: 0.22266130149364471, Test_Loss: 0.18930742144584656\n",
      "Epoch: 22, Train_Loss: 0.1741488128900528, Test_Loss: 0.19258572161197662\n",
      "Epoch: 22, Train_Loss: 0.22224092483520508, Test_Loss: 0.313932329416275\n",
      "Epoch: 22, Train_Loss: 0.2657921612262726, Test_Loss: 0.20671464502811432 *\n",
      "Epoch: 22, Train_Loss: 0.2049637734889984, Test_Loss: 0.17356793582439423 *\n",
      "Epoch: 22, Train_Loss: 0.1727241724729538, Test_Loss: 0.2142457216978073\n",
      "Epoch: 22, Train_Loss: 0.22502201795578003, Test_Loss: 0.17532536387443542 *\n",
      "Epoch: 22, Train_Loss: 0.22273924946784973, Test_Loss: 0.1848480999469757\n",
      "Epoch: 22, Train_Loss: 0.17535842955112457, Test_Loss: 0.20160427689552307\n",
      "Epoch: 22, Train_Loss: 0.32143867015838623, Test_Loss: 0.21844585239887238\n",
      "Epoch: 22, Train_Loss: 0.5448948740959167, Test_Loss: 0.24457554519176483\n",
      "Epoch: 22, Train_Loss: 0.892368733882904, Test_Loss: 0.23473450541496277 *\n",
      "Epoch: 22, Train_Loss: 0.2704707980155945, Test_Loss: 0.19914038479328156 *\n",
      "Epoch: 22, Train_Loss: 0.25789493322372437, Test_Loss: 0.21266669034957886\n",
      "Epoch: 22, Train_Loss: 1.4141343832015991, Test_Loss: 0.1696786880493164 *\n",
      "Epoch: 22, Train_Loss: 0.9427049160003662, Test_Loss: 0.16908229887485504 *\n",
      "Epoch: 22, Train_Loss: 0.1794370859861374, Test_Loss: 0.17159634828567505\n",
      "Epoch: 22, Train_Loss: 0.17464086413383484, Test_Loss: 0.1730375438928604\n",
      "Epoch: 22, Train_Loss: 0.5695580244064331, Test_Loss: 0.17015434801578522 *\n",
      "Epoch: 22, Train_Loss: 0.6166729927062988, Test_Loss: 0.17039507627487183\n",
      "Epoch: 22, Train_Loss: 0.8011907935142517, Test_Loss: 0.17101921141147614\n",
      "Epoch: 22, Train_Loss: 0.1756429821252823, Test_Loss: 0.17421945929527283\n",
      "Epoch: 22, Train_Loss: 0.19045376777648926, Test_Loss: 0.1731036752462387 *\n",
      "Epoch: 22, Train_Loss: 0.2958889305591583, Test_Loss: 0.18746240437030792\n",
      "Epoch: 22, Train_Loss: 0.3876563310623169, Test_Loss: 0.17338040471076965 *\n",
      "Epoch: 22, Train_Loss: 0.18295186758041382, Test_Loss: 0.19506612420082092\n",
      "Epoch: 22, Train_Loss: 0.2330181896686554, Test_Loss: 0.18963533639907837 *\n",
      "Epoch: 22, Train_Loss: 0.21406972408294678, Test_Loss: 0.5577165484428406\n",
      "Epoch: 22, Train_Loss: 0.22636118531227112, Test_Loss: 0.4535828232765198 *\n",
      "Epoch: 22, Train_Loss: 0.3144986927509308, Test_Loss: 0.316686749458313 *\n",
      "Epoch: 22, Train_Loss: 0.27703338861465454, Test_Loss: 0.19549977779388428 *\n",
      "Epoch: 22, Train_Loss: 0.21616286039352417, Test_Loss: 0.19492658972740173 *\n",
      "Epoch: 22, Train_Loss: 0.2241864949464798, Test_Loss: 0.17298264801502228 *\n",
      "Epoch: 22, Train_Loss: 0.25474613904953003, Test_Loss: 0.1954777091741562\n",
      "Epoch: 22, Train_Loss: 0.26370710134506226, Test_Loss: 0.2818242311477661\n",
      "Epoch: 22, Train_Loss: 0.503284752368927, Test_Loss: 0.31422826647758484\n",
      "Epoch: 22, Train_Loss: 0.381198525428772, Test_Loss: 0.25102493166923523 *\n",
      "Epoch: 22, Train_Loss: 0.2023548185825348, Test_Loss: 0.26553982496261597\n",
      "Epoch: 22, Train_Loss: 0.254798948764801, Test_Loss: 0.18675819039344788 *\n",
      "Epoch: 22, Train_Loss: 0.2681012451648712, Test_Loss: 0.17294301092624664 *\n",
      "Epoch: 22, Train_Loss: 0.19081176817417145, Test_Loss: 0.17987611889839172\n",
      "Epoch: 22, Train_Loss: 0.17536483705043793, Test_Loss: 0.2317306250333786\n",
      "Epoch: 22, Train_Loss: 0.17030489444732666, Test_Loss: 0.17472825944423676 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.18077322840690613, Test_Loss: 0.19054295122623444\n",
      "Epoch: 22, Train_Loss: 0.17265194654464722, Test_Loss: 0.19154377281665802\n",
      "Epoch: 22, Train_Loss: 0.17723804712295532, Test_Loss: 0.3958936929702759\n",
      "Epoch: 22, Train_Loss: 0.18647682666778564, Test_Loss: 0.5205519199371338\n",
      "Epoch: 22, Train_Loss: 0.18863849341869354, Test_Loss: 0.49672749638557434 *\n",
      "Epoch: 22, Train_Loss: 0.18065303564071655, Test_Loss: 0.5198216438293457\n",
      "Epoch: 22, Train_Loss: 0.28723663091659546, Test_Loss: 0.4637465476989746 *\n",
      "Epoch: 22, Train_Loss: 0.4570070505142212, Test_Loss: 0.46257591247558594 *\n",
      "Epoch: 22, Train_Loss: 0.18301407992839813, Test_Loss: 0.46859562397003174\n",
      "Epoch: 22, Train_Loss: 0.22151681780815125, Test_Loss: 0.5177919268608093\n",
      "Epoch: 22, Train_Loss: 0.21515612304210663, Test_Loss: 0.3657020330429077 *\n",
      "Epoch: 22, Train_Loss: 0.2036035656929016, Test_Loss: 5.597131729125977\n",
      "Epoch: 22, Train_Loss: 0.2721596360206604, Test_Loss: 0.27088120579719543 *\n",
      "Epoch: 22, Train_Loss: 0.208892822265625, Test_Loss: 0.19736003875732422 *\n",
      "Epoch: 22, Train_Loss: 0.39312949776649475, Test_Loss: 0.20561429858207703\n",
      "Epoch: 22, Train_Loss: 0.24085596203804016, Test_Loss: 0.18233530223369598 *\n",
      "Epoch: 22, Train_Loss: 0.2839095890522003, Test_Loss: 0.17770543694496155 *\n",
      "Epoch: 22, Train_Loss: 0.1925937831401825, Test_Loss: 0.19190315902233124\n",
      "Epoch: 22, Train_Loss: 0.1934611201286316, Test_Loss: 0.2148578017950058\n",
      "Epoch: 22, Train_Loss: 0.4480901062488556, Test_Loss: 0.1792925000190735 *\n",
      "Epoch: 22, Train_Loss: 0.5940767526626587, Test_Loss: 0.18026302754878998\n",
      "Epoch: 22, Train_Loss: 0.491089403629303, Test_Loss: 0.18413153290748596\n",
      "Epoch: 22, Train_Loss: 0.24055002629756927, Test_Loss: 0.2435053586959839\n",
      "Epoch: 22, Train_Loss: 0.20397639274597168, Test_Loss: 0.17685748636722565 *\n",
      "Epoch: 22, Train_Loss: 0.17807318270206451, Test_Loss: 0.17349712550640106 *\n",
      "Epoch: 22, Train_Loss: 0.46361595392227173, Test_Loss: 0.19673699140548706\n",
      "Epoch: 22, Train_Loss: 0.345004141330719, Test_Loss: 0.17795000970363617 *\n",
      "Epoch: 22, Train_Loss: 0.17486391961574554, Test_Loss: 0.1742873340845108 *\n",
      "Epoch: 22, Train_Loss: 0.34760454297065735, Test_Loss: 0.20883587002754211\n",
      "Epoch: 22, Train_Loss: 0.19967347383499146, Test_Loss: 0.1888573169708252 *\n",
      "Epoch: 22, Train_Loss: 0.1897740513086319, Test_Loss: 0.17103926837444305 *\n",
      "Epoch: 22, Train_Loss: 0.2196560502052307, Test_Loss: 0.2037779688835144\n",
      "Epoch: 22, Train_Loss: 0.3313232362270355, Test_Loss: 0.17875079810619354 *\n",
      "Epoch: 22, Train_Loss: 0.20825642347335815, Test_Loss: 0.180331289768219\n",
      "Epoch: 22, Train_Loss: 0.25989317893981934, Test_Loss: 0.18058423697948456\n",
      "Epoch: 22, Train_Loss: 0.1825461983680725, Test_Loss: 0.18939895927906036\n",
      "Epoch: 22, Train_Loss: 0.25438129901885986, Test_Loss: 0.1749851405620575 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.2006060779094696, Test_Loss: 0.1902533620595932\n",
      "Epoch: 22, Train_Loss: 0.20013515651226044, Test_Loss: 0.22326332330703735\n",
      "Epoch: 22, Train_Loss: 0.18191216886043549, Test_Loss: 0.18957994878292084 *\n",
      "Epoch: 22, Train_Loss: 0.20153817534446716, Test_Loss: 0.17191654443740845 *\n",
      "Epoch: 22, Train_Loss: 0.3099702000617981, Test_Loss: 0.27111756801605225\n",
      "Epoch: 22, Train_Loss: 0.39802050590515137, Test_Loss: 2.0513343811035156\n",
      "Epoch: 22, Train_Loss: 0.3970765471458435, Test_Loss: 3.2789931297302246\n",
      "Epoch: 22, Train_Loss: 0.6923697590827942, Test_Loss: 0.1927466094493866 *\n",
      "Epoch: 22, Train_Loss: 0.4840540289878845, Test_Loss: 0.18307338654994965 *\n",
      "Epoch: 22, Train_Loss: 0.3112126290798187, Test_Loss: 0.20259404182434082\n",
      "Epoch: 22, Train_Loss: 0.2483767569065094, Test_Loss: 0.17264114320278168 *\n",
      "Epoch: 22, Train_Loss: 0.1872372329235077, Test_Loss: 0.1902754008769989\n",
      "Epoch: 22, Train_Loss: 0.17688043415546417, Test_Loss: 0.20390954613685608\n",
      "Epoch: 22, Train_Loss: 0.18687640130519867, Test_Loss: 0.19844751060009003 *\n",
      "Epoch: 22, Train_Loss: 0.3221067190170288, Test_Loss: 0.17743495106697083 *\n",
      "Epoch: 22, Train_Loss: 0.4163658022880554, Test_Loss: 0.18939070403575897\n",
      "Epoch: 22, Train_Loss: 0.4203330874443054, Test_Loss: 0.204437255859375\n",
      "Epoch: 22, Train_Loss: 1.076992154121399, Test_Loss: 0.2303660362958908\n",
      "Epoch: 22, Train_Loss: 1.0575610399246216, Test_Loss: 0.1953991800546646 *\n",
      "Epoch: 22, Train_Loss: 0.2855425179004669, Test_Loss: 0.22548048198223114\n",
      "Epoch: 22, Train_Loss: 0.26362550258636475, Test_Loss: 0.19923169910907745 *\n",
      "Epoch: 22, Train_Loss: 0.1735765039920807, Test_Loss: 0.22681571543216705\n",
      "Epoch: 22, Train_Loss: 0.2915921211242676, Test_Loss: 0.1803448349237442 *\n",
      "Epoch: 22, Train_Loss: 0.41072750091552734, Test_Loss: 0.24015404284000397\n",
      "Epoch: 22, Train_Loss: 0.6928337812423706, Test_Loss: 0.2243110090494156 *\n",
      "Epoch: 22, Train_Loss: 0.19614925980567932, Test_Loss: 0.17125779390335083 *\n",
      "Epoch: 22, Train_Loss: 0.19213025271892548, Test_Loss: 0.199116051197052\n",
      "Epoch: 22, Train_Loss: 0.2338743507862091, Test_Loss: 0.2094830423593521\n",
      "Epoch: 22, Train_Loss: 0.42704910039901733, Test_Loss: 0.1988784372806549 *\n",
      "Epoch: 22, Train_Loss: 0.30650660395622253, Test_Loss: 0.1980934739112854 *\n",
      "Epoch: 22, Train_Loss: 0.3449752926826477, Test_Loss: 0.1919250190258026 *\n",
      "Epoch: 22, Train_Loss: 0.3108161389827728, Test_Loss: 0.19302204251289368\n",
      "Epoch: 22, Train_Loss: 0.24510285258293152, Test_Loss: 0.20483717322349548\n",
      "Epoch: 22, Train_Loss: 0.1788777858018875, Test_Loss: 0.18931490182876587 *\n",
      "Epoch: 22, Train_Loss: 0.19821012020111084, Test_Loss: 0.21088793873786926\n",
      "Epoch: 22, Train_Loss: 0.1711597591638565, Test_Loss: 0.17569081485271454 *\n",
      "Epoch: 22, Train_Loss: 0.2214440405368805, Test_Loss: 0.17350918054580688 *\n",
      "Epoch: 22, Train_Loss: 0.20528718829154968, Test_Loss: 0.21121668815612793\n",
      "Epoch: 22, Train_Loss: 0.28497201204299927, Test_Loss: 0.3656994104385376\n",
      "Epoch: 22, Train_Loss: 15.991118431091309, Test_Loss: 0.3076566457748413 *\n",
      "Epoch: 22, Train_Loss: 0.22175520658493042, Test_Loss: 0.20005875825881958 *\n",
      "Epoch: 22, Train_Loss: 0.9240856766700745, Test_Loss: 0.22405588626861572\n",
      "Epoch: 22, Train_Loss: 1.0126103162765503, Test_Loss: 0.21839392185211182 *\n",
      "Epoch: 22, Train_Loss: 0.18971791863441467, Test_Loss: 0.19021984934806824 *\n",
      "Epoch: 22, Train_Loss: 0.46418678760528564, Test_Loss: 0.27440106868743896\n",
      "Epoch: 22, Train_Loss: 3.3796162605285645, Test_Loss: 0.25030165910720825 *\n",
      "Epoch: 22, Train_Loss: 4.277068614959717, Test_Loss: 0.4663659930229187\n",
      "Epoch: 22, Train_Loss: 0.23454996943473816, Test_Loss: 0.28363120555877686 *\n",
      "Epoch: 22, Train_Loss: 0.4920424222946167, Test_Loss: 0.23933544754981995 *\n",
      "Epoch: 22, Train_Loss: 4.7274088859558105, Test_Loss: 0.18518665432929993 *\n",
      "Epoch: 22, Train_Loss: 0.46190255880355835, Test_Loss: 0.1879355013370514\n",
      "Epoch: 22, Train_Loss: 0.18220891058444977, Test_Loss: 0.19829927384853363\n",
      "Epoch: 22, Train_Loss: 0.17545846104621887, Test_Loss: 0.19613106548786163 *\n",
      "Epoch: 22, Train_Loss: 0.22400078177452087, Test_Loss: 0.19638583064079285\n",
      "Epoch: 22, Train_Loss: 0.21938474476337433, Test_Loss: 0.17549706995487213 *\n",
      "Epoch: 22, Train_Loss: 0.18707488477230072, Test_Loss: 0.24041847884655\n",
      "Epoch: 22, Train_Loss: 0.19030128419399261, Test_Loss: 0.39016592502593994\n",
      "Epoch: 22, Train_Loss: 0.1696423441171646, Test_Loss: 0.44340288639068604\n",
      "Epoch: 22, Train_Loss: 0.16721516847610474, Test_Loss: 0.35164427757263184 *\n",
      "Epoch: 22, Train_Loss: 0.18244096636772156, Test_Loss: 0.19039727747440338 *\n",
      "Epoch: 22, Train_Loss: 0.1753843128681183, Test_Loss: 0.1675894856452942 *\n",
      "Epoch: 22, Train_Loss: 0.2597197890281677, Test_Loss: 0.16734032332897186 *\n",
      "Epoch: 22, Train_Loss: 0.18891699612140656, Test_Loss: 0.16743160784244537\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.21158713102340698, Test_Loss: 0.21528172492980957\n",
      "Epoch: 22, Train_Loss: 0.17772328853607178, Test_Loss: 1.1923596858978271\n",
      "Epoch: 22, Train_Loss: 0.17389588057994843, Test_Loss: 6.1685028076171875\n",
      "Epoch: 22, Train_Loss: 0.16928431391716003, Test_Loss: 0.2873217463493347 *\n",
      "Epoch: 22, Train_Loss: 0.16938701272010803, Test_Loss: 0.5527241826057434\n",
      "Epoch: 22, Train_Loss: 0.17006489634513855, Test_Loss: 0.5853555202484131\n",
      "Epoch: 22, Train_Loss: 0.16660526394844055, Test_Loss: 0.41011345386505127 *\n",
      "Epoch: 22, Train_Loss: 0.16677944362163544, Test_Loss: 0.39116936922073364 *\n",
      "Epoch: 22, Train_Loss: 0.1667489856481552, Test_Loss: 0.9374878406524658\n",
      "Epoch: 22, Train_Loss: 0.16808371245861053, Test_Loss: 0.8674755096435547 *\n",
      "Epoch: 22, Train_Loss: 0.16656504571437836, Test_Loss: 0.5855712890625 *\n",
      "Epoch: 22, Train_Loss: 0.16658751666545868, Test_Loss: 0.7971808314323425\n",
      "Epoch: 22, Train_Loss: 0.17840802669525146, Test_Loss: 0.6197947263717651 *\n",
      "Epoch: 22, Train_Loss: 0.19575761258602142, Test_Loss: 1.3352668285369873\n",
      "Epoch: 22, Train_Loss: 0.22714780271053314, Test_Loss: 0.5683231949806213 *\n",
      "Epoch: 22, Train_Loss: 0.1942169964313507, Test_Loss: 0.5607585906982422 *\n",
      "Epoch: 22, Train_Loss: 0.2820228934288025, Test_Loss: 0.4970804452896118 *\n",
      "Epoch: 22, Train_Loss: 3.36601185798645, Test_Loss: 0.26527684926986694 *\n",
      "Epoch: 22, Train_Loss: 4.1276397705078125, Test_Loss: 0.20169587433338165 *\n",
      "Epoch: 22, Train_Loss: 0.1968085914850235, Test_Loss: 0.17487336695194244 *\n",
      "Epoch: 22, Train_Loss: 0.22619980573654175, Test_Loss: 0.3384224772453308\n",
      "Epoch: 22, Train_Loss: 0.22837769985198975, Test_Loss: 0.21026691794395447 *\n",
      "Epoch: 22, Train_Loss: 0.26229721307754517, Test_Loss: 0.4753304421901703\n",
      "Epoch: 22, Train_Loss: 0.29230403900146484, Test_Loss: 0.2019372135400772 *\n",
      "Epoch: 22, Train_Loss: 0.39454522728919983, Test_Loss: 0.265739381313324\n",
      "Epoch: 22, Train_Loss: 0.3074594736099243, Test_Loss: 0.32069891691207886\n",
      "Epoch: 22, Train_Loss: 0.2797703742980957, Test_Loss: 0.2199636697769165 *\n",
      "Epoch: 22, Train_Loss: 0.2631269097328186, Test_Loss: 0.17681658267974854 *\n",
      "Epoch: 22, Train_Loss: 0.19729569554328918, Test_Loss: 0.18367931246757507\n",
      "Epoch: 22, Train_Loss: 0.18837317824363708, Test_Loss: 0.20256465673446655\n",
      "Epoch: 22, Train_Loss: 0.18865561485290527, Test_Loss: 0.191483274102211 *\n",
      "Epoch: 22, Train_Loss: 0.2530897557735443, Test_Loss: 0.21509279310703278\n",
      "Epoch: 22, Train_Loss: 0.22807376086711884, Test_Loss: 0.32550525665283203\n",
      "Epoch: 22, Train_Loss: 0.18427841365337372, Test_Loss: 3.525275230407715\n",
      "Epoch: 22, Train_Loss: 0.20981445908546448, Test_Loss: 2.8723340034484863 *\n",
      "Epoch: 22, Train_Loss: 0.17059117555618286, Test_Loss: 0.18479430675506592 *\n",
      "Epoch: 22, Train_Loss: 0.21216851472854614, Test_Loss: 0.1707281619310379 *\n",
      "Epoch: 22, Train_Loss: 0.22504006326198578, Test_Loss: 0.18513087928295135\n",
      "Epoch: 22, Train_Loss: 0.21300002932548523, Test_Loss: 0.23011620342731476\n",
      "Epoch: 22, Train_Loss: 0.17852561175823212, Test_Loss: 0.18791992962360382 *\n",
      "Epoch: 22, Train_Loss: 0.16938206553459167, Test_Loss: 0.25647106766700745\n",
      "Epoch: 22, Train_Loss: 0.18252429366111755, Test_Loss: 0.268700510263443\n",
      "Epoch: 22, Train_Loss: 2.557314395904541, Test_Loss: 0.16881604492664337 *\n",
      "Epoch: 22, Train_Loss: 2.511107921600342, Test_Loss: 0.19187043607234955\n",
      "Epoch: 22, Train_Loss: 0.17004084587097168, Test_Loss: 0.18757592141628265 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.17745999991893768, Test_Loss: 0.18339300155639648 *\n",
      "Epoch: 22, Train_Loss: 0.177414208650589, Test_Loss: 0.17484208941459656 *\n",
      "Epoch: 22, Train_Loss: 0.16771182417869568, Test_Loss: 0.3297915458679199\n",
      "Epoch: 22, Train_Loss: 0.17262957990169525, Test_Loss: 0.30497264862060547 *\n",
      "Epoch: 22, Train_Loss: 0.1691596359014511, Test_Loss: 0.272817999124527 *\n",
      "Epoch: 22, Train_Loss: 0.19114075601100922, Test_Loss: 0.22263368964195251 *\n",
      "Epoch: 22, Train_Loss: 0.17330841720104218, Test_Loss: 0.19020521640777588 *\n",
      "Epoch: 22, Train_Loss: 0.1940656453371048, Test_Loss: 0.19671958684921265\n",
      "Epoch: 22, Train_Loss: 0.16846828162670135, Test_Loss: 0.2658640146255493\n",
      "Epoch: 22, Train_Loss: 0.17124773561954498, Test_Loss: 0.27586233615875244\n",
      "Epoch: 22, Train_Loss: 0.17725834250450134, Test_Loss: 0.2966725528240204\n",
      "Epoch: 22, Train_Loss: 0.17400629818439484, Test_Loss: 0.233527272939682 *\n",
      "Epoch: 22, Train_Loss: 0.16681714355945587, Test_Loss: 0.23688191175460815\n",
      "Epoch: 22, Train_Loss: 0.17229680716991425, Test_Loss: 0.272883802652359\n",
      "Epoch: 22, Train_Loss: 0.19292303919792175, Test_Loss: 0.28573742508888245\n",
      "Epoch: 22, Train_Loss: 0.1891622692346573, Test_Loss: 0.34598350524902344\n",
      "Epoch: 22, Train_Loss: 0.1684761494398117, Test_Loss: 0.20239245891571045 *\n",
      "Epoch: 22, Train_Loss: 0.16946394741535187, Test_Loss: 0.19284722208976746 *\n",
      "Epoch: 22, Train_Loss: 0.22303545475006104, Test_Loss: 0.17214380204677582 *\n",
      "Epoch: 22, Train_Loss: 0.20180118083953857, Test_Loss: 0.19758538901805878\n",
      "Epoch: 22, Train_Loss: 0.23583900928497314, Test_Loss: 0.23194685578346252\n",
      "Epoch: 22, Train_Loss: 0.19035062193870544, Test_Loss: 0.2880235016345978\n",
      "Epoch: 22, Train_Loss: 0.22375696897506714, Test_Loss: 0.34030359983444214\n",
      "Epoch: 22, Train_Loss: 0.22440797090530396, Test_Loss: 0.21385525166988373 *\n",
      "Epoch: 22, Train_Loss: 0.18737240135669708, Test_Loss: 0.20055487751960754 *\n",
      "Epoch: 22, Train_Loss: 0.20979538559913635, Test_Loss: 0.19958733022212982 *\n",
      "Epoch: 22, Train_Loss: 0.26780012249946594, Test_Loss: 0.2197370082139969\n",
      "Epoch: 22, Train_Loss: 0.2852862477302551, Test_Loss: 0.408583402633667\n",
      "Epoch: 22, Train_Loss: 0.20758230984210968, Test_Loss: 0.3389204144477844 *\n",
      "Epoch: 22, Train_Loss: 0.16617871820926666, Test_Loss: 0.5662874579429626\n",
      "Epoch: 22, Train_Loss: 0.16554266214370728, Test_Loss: 0.2365543246269226 *\n",
      "Epoch: 22, Train_Loss: 0.16564854979515076, Test_Loss: 0.21973292529582977 *\n",
      "Epoch: 22, Train_Loss: 0.16572874784469604, Test_Loss: 0.1768934279680252 *\n",
      "Epoch: 22, Train_Loss: 0.1680537760257721, Test_Loss: 0.1764100193977356 *\n",
      "Epoch: 22, Train_Loss: 3.4739818572998047, Test_Loss: 0.1772998720407486\n",
      "Epoch: 22, Train_Loss: 1.9518436193466187, Test_Loss: 0.17786294221878052\n",
      "Epoch: 22, Train_Loss: 0.1670001745223999, Test_Loss: 0.20824038982391357\n",
      "Epoch: 22, Train_Loss: 0.17265212535858154, Test_Loss: 0.16784432530403137 *\n",
      "Epoch: 22, Train_Loss: 0.17073732614517212, Test_Loss: 0.23252460360527039\n",
      "Epoch: 22, Train_Loss: 0.16781999170780182, Test_Loss: 0.29314398765563965\n",
      "Epoch: 22, Train_Loss: 0.16690491139888763, Test_Loss: 0.4681103825569153\n",
      "Epoch: 22, Train_Loss: 0.16846244037151337, Test_Loss: 0.4048110246658325 *\n",
      "Epoch: 22, Train_Loss: 0.1670515090227127, Test_Loss: 0.19080618023872375 *\n",
      "Epoch: 22, Train_Loss: 0.16806000471115112, Test_Loss: 0.18628700077533722 *\n",
      "Epoch: 22, Train_Loss: 0.18210503458976746, Test_Loss: 0.1860792338848114 *\n",
      "Epoch: 22, Train_Loss: 0.20296357572078705, Test_Loss: 0.18643979728221893\n",
      "Epoch: 22, Train_Loss: 0.2058868259191513, Test_Loss: 0.1960962563753128\n",
      "Epoch: 22, Train_Loss: 0.21945807337760925, Test_Loss: 1.9660993814468384\n",
      "Epoch: 22, Train_Loss: 0.1836286187171936, Test_Loss: 3.843958616256714\n",
      "Epoch: 22, Train_Loss: 0.18842695653438568, Test_Loss: 0.18317566812038422 *\n",
      "Epoch: 22, Train_Loss: 0.40524160861968994, Test_Loss: 0.17640547454357147 *\n",
      "Epoch: 22, Train_Loss: 0.3865739703178406, Test_Loss: 0.17811115086078644\n",
      "Epoch: 22, Train_Loss: 0.38961315155029297, Test_Loss: 0.1790173500776291\n",
      "Epoch: 22, Train_Loss: 0.23157291114330292, Test_Loss: 0.16992220282554626 *\n",
      "Epoch: 22, Train_Loss: 0.16637873649597168, Test_Loss: 0.18119823932647705\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.1659850776195526, Test_Loss: 0.17212997376918793 *\n",
      "Epoch: 22, Train_Loss: 0.17177951335906982, Test_Loss: 0.16700027883052826 *\n",
      "Epoch: 22, Train_Loss: 0.1717311441898346, Test_Loss: 0.16983535885810852\n",
      "Epoch: 22, Train_Loss: 0.18224094808101654, Test_Loss: 0.16957131028175354 *\n",
      "Epoch: 22, Train_Loss: 0.18083234131336212, Test_Loss: 0.19356036186218262\n",
      "Epoch: 22, Train_Loss: 0.16522572934627533, Test_Loss: 0.1816544085741043 *\n",
      "Epoch: 22, Train_Loss: 0.16716870665550232, Test_Loss: 0.19270172715187073\n",
      "Epoch: 22, Train_Loss: 0.18316525220870972, Test_Loss: 0.17463567852973938 *\n",
      "Epoch: 22, Train_Loss: 0.25693750381469727, Test_Loss: 0.167842835187912 *\n",
      "Epoch: 22, Train_Loss: 0.2886786162853241, Test_Loss: 0.16867424547672272\n",
      "Epoch: 22, Train_Loss: 0.20162752270698547, Test_Loss: 0.17649970948696136\n",
      "Epoch: 22, Train_Loss: 0.26901933550834656, Test_Loss: 0.17771558463573456\n",
      "Epoch: 22, Train_Loss: 0.2534635663032532, Test_Loss: 0.16639576852321625 *\n",
      "Epoch: 22, Train_Loss: 0.276185542345047, Test_Loss: 0.16879792511463165\n",
      "Epoch: 22, Train_Loss: 0.17150041460990906, Test_Loss: 0.1650901585817337 *\n",
      "Epoch: 22, Train_Loss: 0.2645648121833801, Test_Loss: 0.17164252698421478\n",
      "Epoch: 22, Train_Loss: 0.2118941843509674, Test_Loss: 0.18168936669826508\n",
      "Epoch: 22, Train_Loss: 0.41211241483688354, Test_Loss: 0.16901883482933044 *\n",
      "Epoch: 22, Train_Loss: 0.1727515161037445, Test_Loss: 0.16660071909427643 *\n",
      "Epoch: 22, Train_Loss: 0.6183961629867554, Test_Loss: 0.17636550962924957\n",
      "Epoch: 22, Train_Loss: 2.5969550609588623, Test_Loss: 0.17747566103935242\n",
      "Epoch: 22, Train_Loss: 0.21002955734729767, Test_Loss: 0.16797155141830444 *\n",
      "Epoch: 22, Train_Loss: 0.23299391567707062, Test_Loss: 0.2168455421924591\n",
      "Epoch: 22, Train_Loss: 0.19847306609153748, Test_Loss: 0.21213509142398834 *\n",
      "Epoch: 22, Train_Loss: 0.18017566204071045, Test_Loss: 4.291796684265137\n",
      "Epoch: 22, Train_Loss: 0.1686890423297882, Test_Loss: 1.6042416095733643 *\n",
      "Epoch: 22, Train_Loss: 0.18065929412841797, Test_Loss: 0.1693129986524582 *\n",
      "Epoch: 22, Train_Loss: 0.2476482093334198, Test_Loss: 0.1788274496793747\n",
      "Epoch: 22, Train_Loss: 0.24885183572769165, Test_Loss: 0.19085858762264252\n",
      "Epoch: 22, Train_Loss: 0.21273159980773926, Test_Loss: 0.17884796857833862 *\n",
      "Epoch: 22, Train_Loss: 0.18712806701660156, Test_Loss: 0.1776171326637268 *\n",
      "Epoch: 22, Train_Loss: 0.17950516939163208, Test_Loss: 0.2400597482919693\n",
      "Epoch: 22, Train_Loss: 0.1731904000043869, Test_Loss: 0.22707730531692505 *\n",
      "Epoch: 22, Train_Loss: 0.18094368278980255, Test_Loss: 0.16993822157382965 *\n",
      "Epoch: 22, Train_Loss: 0.18930621445178986, Test_Loss: 0.18879635632038116\n",
      "Epoch: 22, Train_Loss: 0.19520549476146698, Test_Loss: 0.18964074552059174\n",
      "Epoch: 22, Train_Loss: 0.18011638522148132, Test_Loss: 0.18097087740898132 *\n",
      "Epoch: 22, Train_Loss: 0.16757051646709442, Test_Loss: 0.17114043235778809 *\n",
      "Epoch: 22, Train_Loss: 0.18267667293548584, Test_Loss: 0.1889224350452423\n",
      "Epoch: 22, Train_Loss: 0.17846080660820007, Test_Loss: 0.21613624691963196\n",
      "Epoch: 22, Train_Loss: 0.18001757562160492, Test_Loss: 0.2313326895236969\n",
      "Epoch: 22, Train_Loss: 0.16858161985874176, Test_Loss: 0.18764927983283997 *\n",
      "Epoch: 22, Train_Loss: 0.16553565859794617, Test_Loss: 0.22777143120765686\n",
      "Epoch: 22, Train_Loss: 0.1645786613225937, Test_Loss: 0.18822555243968964 *\n",
      "Epoch: 22, Train_Loss: 0.16784223914146423, Test_Loss: 0.17389707267284393 *\n",
      "Epoch: 22, Train_Loss: 0.16700179874897003, Test_Loss: 0.18503372371196747\n",
      "Epoch: 22, Train_Loss: 0.16787894070148468, Test_Loss: 0.17737261950969696 *\n",
      "Epoch: 22, Train_Loss: 0.1713985651731491, Test_Loss: 0.19053858518600464\n",
      "Epoch: 22, Train_Loss: 0.1666489541530609, Test_Loss: 0.18345484137535095 *\n",
      "Epoch: 22, Train_Loss: 0.16441476345062256, Test_Loss: 0.1705668866634369 *\n",
      "Epoch: 22, Train_Loss: 0.16861103475093842, Test_Loss: 0.1739538162946701\n",
      "Epoch: 22, Train_Loss: 0.17501506209373474, Test_Loss: 0.1886507272720337\n",
      "Epoch: 22, Train_Loss: 0.17848806083202362, Test_Loss: 0.17513278126716614 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train_Loss: 0.1776517927646637, Test_Loss: 0.1759994775056839\n",
      "Epoch: 22, Train_Loss: 0.1908491551876068, Test_Loss: 0.20180056989192963\n",
      "Epoch: 22, Train_Loss: 0.18297253549098969, Test_Loss: 0.2274172306060791\n",
      "Epoch: 22, Train_Loss: 0.19179901480674744, Test_Loss: 0.33877748250961304\n",
      "Epoch: 22, Train_Loss: 0.17307785153388977, Test_Loss: 0.43950390815734863\n",
      "Epoch: 22, Train_Loss: 0.16800929605960846, Test_Loss: 0.4323870837688446 *\n",
      "Epoch: 22, Train_Loss: 0.18384601175785065, Test_Loss: 0.2368210107088089 *\n",
      "Epoch: 22, Train_Loss: 0.18328067660331726, Test_Loss: 0.17511916160583496 *\n",
      "Epoch: 22, Train_Loss: 0.1717883050441742, Test_Loss: 0.18194794654846191\n",
      "Epoch: 22, Train_Loss: 0.17322133481502533, Test_Loss: 0.20854148268699646\n",
      "Epoch: 22, Train_Loss: 0.17828024923801422, Test_Loss: 0.4329664707183838\n",
      "Epoch: 22, Train_Loss: 0.17905744910240173, Test_Loss: 0.2249515950679779 *\n",
      "Epoch: 22, Train_Loss: 0.21096958220005035, Test_Loss: 0.49961572885513306\n",
      "Epoch: 22, Train_Loss: 0.21271637082099915, Test_Loss: 0.24182069301605225 *\n",
      "Epoch: 22, Train_Loss: 0.16692091524600983, Test_Loss: 0.19245092570781708 *\n",
      "Epoch: 22, Train_Loss: 0.19160181283950806, Test_Loss: 0.1716819852590561 *\n",
      "Epoch: 22, Train_Loss: 0.21716263890266418, Test_Loss: 0.16587446630001068 *\n",
      "Epoch: 22, Train_Loss: 0.17010201513767242, Test_Loss: 0.19901975989341736\n",
      "Epoch: 22, Train_Loss: 0.17082686722278595, Test_Loss: 0.17847907543182373 *\n",
      "Epoch: 22, Train_Loss: 0.18437343835830688, Test_Loss: 0.19682711362838745\n",
      "Epoch: 22, Train_Loss: 0.24868115782737732, Test_Loss: 0.16791388392448425 *\n",
      "Epoch: 22, Train_Loss: 0.21467995643615723, Test_Loss: 0.27746498584747314\n",
      "Epoch: 22, Train_Loss: 0.1943933516740799, Test_Loss: 0.443407267332077\n",
      "Epoch: 22, Train_Loss: 0.1866590976715088, Test_Loss: 0.31559520959854126 *\n",
      "Epoch: 22, Train_Loss: 0.16886259615421295, Test_Loss: 0.37511569261550903\n",
      "Epoch: 22, Train_Loss: 0.19005931913852692, Test_Loss: 0.17799533903598785 *\n",
      "Epoch: 22, Train_Loss: 0.16603177785873413, Test_Loss: 0.17807310819625854\n",
      "Epoch: 22, Train_Loss: 0.1718486100435257, Test_Loss: 0.17741389572620392 *\n",
      "Epoch: 22, Train_Loss: 0.17069000005722046, Test_Loss: 0.1775536835193634\n",
      "Epoch: 22, Train_Loss: 0.17569807171821594, Test_Loss: 0.19214242696762085\n",
      "Epoch: 22, Train_Loss: 0.21298223733901978, Test_Loss: 3.4702985286712646\n",
      "Epoch: 22, Train_Loss: 0.19102533161640167, Test_Loss: 2.305257797241211 *\n",
      "Epoch: 22, Train_Loss: 0.2081032395362854, Test_Loss: 0.17670731246471405 *\n",
      "Epoch: 22, Train_Loss: 0.17638657987117767, Test_Loss: 0.17506489157676697 *\n",
      "Epoch: 22, Train_Loss: 0.18484970927238464, Test_Loss: 0.1741127222776413 *\n",
      "Epoch: 22, Train_Loss: 0.17276404798030853, Test_Loss: 0.1726325899362564 *\n",
      "Epoch: 22, Train_Loss: 0.4267714321613312, Test_Loss: 0.16904543340206146 *\n",
      "Epoch: 22, Train_Loss: 0.23941218852996826, Test_Loss: 0.18382981419563293\n",
      "Epoch: 22, Train_Loss: 0.17585361003875732, Test_Loss: 0.17238378524780273 *\n",
      "Epoch: 22, Train_Loss: 0.1846267729997635, Test_Loss: 0.1672138124704361 *\n",
      "Epoch: 22, Train_Loss: 0.16455739736557007, Test_Loss: 0.16974058747291565\n",
      "Epoch: 22, Train_Loss: 0.16823779046535492, Test_Loss: 0.1687593162059784 *\n",
      "Epoch: 22, Train_Loss: 0.17269033193588257, Test_Loss: 0.19154483079910278\n",
      "Epoch: 22, Train_Loss: 0.17287907004356384, Test_Loss: 0.182398721575737 *\n",
      "Epoch: 22, Train_Loss: 0.16560162603855133, Test_Loss: 0.2132209688425064\n",
      "Epoch: 22, Train_Loss: 0.1780485063791275, Test_Loss: 0.16683974862098694 *\n",
      "Epoch: 22, Train_Loss: 0.16725888848304749, Test_Loss: 0.1683483123779297\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 22\n",
      "Epoch: 22, Train_Loss: 0.16800600290298462, Test_Loss: 0.16938632726669312\n",
      "Epoch: 22, Train_Loss: 0.171743243932724, Test_Loss: 0.18341806530952454\n",
      "Epoch: 22, Train_Loss: 0.1660022735595703, Test_Loss: 0.16907256841659546 *\n",
      "Epoch: 22, Train_Loss: 0.16522210836410522, Test_Loss: 0.16528385877609253 *\n",
      "Epoch: 22, Train_Loss: 0.17332008481025696, Test_Loss: 0.16749116778373718\n",
      "Epoch: 22, Train_Loss: 0.18082059919834137, Test_Loss: 0.1654876470565796 *\n",
      "Epoch: 22, Train_Loss: 0.1828242540359497, Test_Loss: 0.16726967692375183\n",
      "Epoch: 22, Train_Loss: 0.1755998581647873, Test_Loss: 0.17693164944648743\n",
      "Epoch: 22, Train_Loss: 0.17743128538131714, Test_Loss: 0.16678133606910706 *\n",
      "Epoch: 22, Train_Loss: 0.1736820936203003, Test_Loss: 0.16544191539287567 *\n",
      "Epoch: 22, Train_Loss: 0.18254047632217407, Test_Loss: 0.18152305483818054\n",
      "Epoch: 22, Train_Loss: 0.1649879366159439, Test_Loss: 0.18103940784931183 *\n",
      "Epoch: 22, Train_Loss: 0.18503805994987488, Test_Loss: 0.16413310170173645 *\n",
      "Epoch: 22, Train_Loss: 0.17730242013931274, Test_Loss: 0.2434895932674408\n",
      "Epoch: 22, Train_Loss: 0.17434458434581757, Test_Loss: 0.20774491131305695 *\n",
      "Epoch: 22, Train_Loss: 0.16569863259792328, Test_Loss: 5.531639099121094\n",
      "Epoch: 22, Train_Loss: 0.18474671244621277, Test_Loss: 0.5237835049629211 *\n",
      "Epoch: 22, Train_Loss: 0.22520658373832703, Test_Loss: 0.16458101570606232 *\n",
      "Epoch: 22, Train_Loss: 2.523695945739746, Test_Loss: 0.18410858511924744\n",
      "Epoch: 22, Train_Loss: 2.9270896911621094, Test_Loss: 0.1729930341243744 *\n",
      "Epoch: 22, Train_Loss: 0.19436350464820862, Test_Loss: 0.17832504212856293\n",
      "Epoch: 22, Train_Loss: 0.16604457795619965, Test_Loss: 0.1734554022550583 *\n",
      "Epoch: 22, Train_Loss: 0.19998864829540253, Test_Loss: 0.2703227698802948\n",
      "Epoch: 22, Train_Loss: 0.25612181425094604, Test_Loss: 0.22962144017219543 *\n",
      "Epoch: 22, Train_Loss: 0.1906035840511322, Test_Loss: 0.1642691195011139 *\n",
      "Epoch: 22, Train_Loss: 0.16761399805545807, Test_Loss: 0.192422553896904\n",
      "Epoch: 22, Train_Loss: 0.19036370515823364, Test_Loss: 0.17483286559581757 *\n",
      "Epoch: 22, Train_Loss: 0.2320469617843628, Test_Loss: 0.17159022390842438 *\n",
      "Epoch: 22, Train_Loss: 0.17590013146400452, Test_Loss: 0.17827098071575165\n",
      "Epoch: 22, Train_Loss: 0.18302224576473236, Test_Loss: 0.16918303072452545 *\n",
      "Epoch: 22, Train_Loss: 0.5050809383392334, Test_Loss: 0.21018999814987183\n",
      "Epoch: 22, Train_Loss: 0.57362300157547, Test_Loss: 0.2371760457754135\n",
      "Epoch: 22, Train_Loss: 0.354985773563385, Test_Loss: 0.1988394856452942 *\n",
      "Epoch: 22, Train_Loss: 0.268142968416214, Test_Loss: 0.21779899299144745\n",
      "Epoch: 22, Train_Loss: 1.0061479806900024, Test_Loss: 0.16997045278549194 *\n",
      "Epoch: 22, Train_Loss: 0.6910731196403503, Test_Loss: 0.16763588786125183 *\n",
      "Epoch: 22, Train_Loss: 0.19131042063236237, Test_Loss: 0.17931751906871796\n",
      "Epoch: 22, Train_Loss: 0.16519558429718018, Test_Loss: 0.18081067502498627\n",
      "Epoch: 22, Train_Loss: 0.4133155643939972, Test_Loss: 0.18177227675914764\n",
      "Epoch: 22, Train_Loss: 0.4499853551387787, Test_Loss: 0.17478471994400024 *\n",
      "Epoch: 22, Train_Loss: 0.521548867225647, Test_Loss: 0.18334205448627472\n",
      "Epoch: 22, Train_Loss: 0.16562844812870026, Test_Loss: 0.1927853524684906\n",
      "Epoch: 22, Train_Loss: 0.18417564034461975, Test_Loss: 0.20890988409519196\n",
      "Epoch: 22, Train_Loss: 0.19900894165039062, Test_Loss: 0.18757441639900208 *\n",
      "Epoch: 22, Train_Loss: 0.3386789560317993, Test_Loss: 0.1745847761631012 *\n",
      "Epoch: 22, Train_Loss: 0.17478728294372559, Test_Loss: 0.1895703822374344\n",
      "Epoch: 22, Train_Loss: 0.2217465490102768, Test_Loss: 0.19366635382175446\n",
      "Epoch: 22, Train_Loss: 0.19909018278121948, Test_Loss: 0.40310752391815186\n",
      "Epoch: 22, Train_Loss: 0.2269042432308197, Test_Loss: 0.3416726589202881 *\n",
      "Epoch: 22, Train_Loss: 0.35865986347198486, Test_Loss: 0.3418799042701721\n",
      "Epoch: 22, Train_Loss: 0.2736465036869049, Test_Loss: 0.21932353079319 *\n",
      "Epoch: 22, Train_Loss: 0.20444218814373016, Test_Loss: 0.18795405328273773 *\n",
      "Epoch: 22, Train_Loss: 0.24297966063022614, Test_Loss: 0.17962034046649933 *\n",
      "Epoch: 22, Train_Loss: 0.25170448422431946, Test_Loss: 0.20053178071975708\n",
      "Epoch: 23, Train_Loss: 0.2139962613582611, Test_Loss: 0.5238237380981445 *\n",
      "Epoch: 23, Train_Loss: 0.27416935563087463, Test_Loss: 0.20007562637329102 *\n",
      "Epoch: 23, Train_Loss: 0.2799232602119446, Test_Loss: 0.4080215096473694\n",
      "Epoch: 23, Train_Loss: 0.20329682528972626, Test_Loss: 0.21334245800971985 *\n",
      "Epoch: 23, Train_Loss: 0.23112937808036804, Test_Loss: 0.21353843808174133\n",
      "Epoch: 23, Train_Loss: 0.2769048511981964, Test_Loss: 0.17537306249141693 *\n",
      "Epoch: 23, Train_Loss: 0.17963626980781555, Test_Loss: 0.169327974319458 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.17414240539073944, Test_Loss: 0.2535848617553711\n",
      "Epoch: 23, Train_Loss: 0.16700883209705353, Test_Loss: 0.17125244438648224 *\n",
      "Epoch: 23, Train_Loss: 0.18000532686710358, Test_Loss: 0.18670721352100372\n",
      "Epoch: 23, Train_Loss: 0.17316266894340515, Test_Loss: 0.16956518590450287 *\n",
      "Epoch: 23, Train_Loss: 0.17734268307685852, Test_Loss: 0.3969491124153137\n",
      "Epoch: 23, Train_Loss: 0.18685761094093323, Test_Loss: 0.5106195211410522\n",
      "Epoch: 23, Train_Loss: 0.1875527948141098, Test_Loss: 0.23386895656585693 *\n",
      "Epoch: 23, Train_Loss: 0.1869945526123047, Test_Loss: 0.4636993408203125\n",
      "Epoch: 23, Train_Loss: 0.33296364545822144, Test_Loss: 0.2198604941368103 *\n",
      "Epoch: 23, Train_Loss: 0.3961198329925537, Test_Loss: 0.2225547432899475\n",
      "Epoch: 23, Train_Loss: 0.17464593052864075, Test_Loss: 0.22133322060108185 *\n",
      "Epoch: 23, Train_Loss: 0.22045551240444183, Test_Loss: 0.2201453596353531 *\n",
      "Epoch: 23, Train_Loss: 0.24355155229568481, Test_Loss: 0.3309747278690338\n",
      "Epoch: 23, Train_Loss: 0.21775951981544495, Test_Loss: 4.829802513122559\n",
      "Epoch: 23, Train_Loss: 0.39053159952163696, Test_Loss: 1.0306800603866577 *\n",
      "Epoch: 23, Train_Loss: 0.2396356761455536, Test_Loss: 0.18226295709609985 *\n",
      "Epoch: 23, Train_Loss: 0.46779778599739075, Test_Loss: 0.18690751492977142\n",
      "Epoch: 23, Train_Loss: 0.23099550604820251, Test_Loss: 0.18388310074806213 *\n",
      "Epoch: 23, Train_Loss: 0.2624952793121338, Test_Loss: 0.169457346200943 *\n",
      "Epoch: 23, Train_Loss: 0.17711107432842255, Test_Loss: 0.2048720270395279\n",
      "Epoch: 23, Train_Loss: 0.17433671653270721, Test_Loss: 0.19738084077835083 *\n",
      "Epoch: 23, Train_Loss: 0.2839125990867615, Test_Loss: 0.18798047304153442 *\n",
      "Epoch: 23, Train_Loss: 0.5255351066589355, Test_Loss: 0.1742609739303589 *\n",
      "Epoch: 23, Train_Loss: 0.5599296689033508, Test_Loss: 0.17797937989234924\n",
      "Epoch: 23, Train_Loss: 0.19287928938865662, Test_Loss: 0.2130357027053833\n",
      "Epoch: 23, Train_Loss: 0.19648633897304535, Test_Loss: 0.21007412672042847 *\n",
      "Epoch: 23, Train_Loss: 0.16793738305568695, Test_Loss: 0.1772845983505249 *\n",
      "Epoch: 23, Train_Loss: 0.32413724064826965, Test_Loss: 0.19021320343017578\n",
      "Epoch: 23, Train_Loss: 0.47902238368988037, Test_Loss: 0.17116793990135193 *\n",
      "Epoch: 23, Train_Loss: 0.16743843257427216, Test_Loss: 0.1739228069782257\n",
      "Epoch: 23, Train_Loss: 0.3145160675048828, Test_Loss: 0.19768007099628448\n",
      "Epoch: 23, Train_Loss: 0.18968889117240906, Test_Loss: 0.199482724070549\n",
      "Epoch: 23, Train_Loss: 0.19097688794136047, Test_Loss: 0.1666560024023056 *\n",
      "Epoch: 23, Train_Loss: 0.21886609494686127, Test_Loss: 0.17126518487930298\n",
      "Epoch: 23, Train_Loss: 0.2753492295742035, Test_Loss: 0.18137328326702118\n",
      "Epoch: 23, Train_Loss: 0.2708930969238281, Test_Loss: 0.17003819346427917 *\n",
      "Epoch: 23, Train_Loss: 0.21832428872585297, Test_Loss: 0.17563560605049133\n",
      "Epoch: 23, Train_Loss: 0.18139146268367767, Test_Loss: 0.19285650551319122\n",
      "Epoch: 23, Train_Loss: 0.23444440960884094, Test_Loss: 0.1698918342590332 *\n",
      "Epoch: 23, Train_Loss: 0.2202633023262024, Test_Loss: 0.17403925955295563\n",
      "Epoch: 23, Train_Loss: 0.18406391143798828, Test_Loss: 0.2247796356678009\n",
      "Epoch: 23, Train_Loss: 0.18201853334903717, Test_Loss: 0.19194021821022034 *\n",
      "Epoch: 23, Train_Loss: 0.1786925047636032, Test_Loss: 0.1647110879421234 *\n",
      "Epoch: 23, Train_Loss: 0.22533398866653442, Test_Loss: 0.2641851305961609\n",
      "Epoch: 23, Train_Loss: 0.3334120810031891, Test_Loss: 0.3854738175868988\n",
      "Epoch: 23, Train_Loss: 0.36268866062164307, Test_Loss: 5.00714111328125\n",
      "Epoch: 23, Train_Loss: 0.5980725288391113, Test_Loss: 0.21599988639354706 *\n",
      "Epoch: 23, Train_Loss: 0.45103734731674194, Test_Loss: 0.17968858778476715 *\n",
      "Epoch: 23, Train_Loss: 0.3641424775123596, Test_Loss: 0.21991252899169922\n",
      "Epoch: 23, Train_Loss: 0.22277502715587616, Test_Loss: 0.17271766066551208 *\n",
      "Epoch: 23, Train_Loss: 0.19519682228565216, Test_Loss: 0.17212878167629242 *\n",
      "Epoch: 23, Train_Loss: 0.17233231663703918, Test_Loss: 0.17713680863380432\n",
      "Epoch: 23, Train_Loss: 0.1760851889848709, Test_Loss: 0.20496812462806702\n",
      "Epoch: 23, Train_Loss: 0.26781612634658813, Test_Loss: 0.2027634084224701 *\n",
      "Epoch: 23, Train_Loss: 0.4753754138946533, Test_Loss: 0.16481679677963257 *\n",
      "Epoch: 23, Train_Loss: 0.579097330570221, Test_Loss: 0.18801641464233398\n",
      "Epoch: 23, Train_Loss: 1.0018709897994995, Test_Loss: 0.19627080857753754\n",
      "Epoch: 23, Train_Loss: 1.1533204317092896, Test_Loss: 0.17887799441814423 *\n",
      "Epoch: 23, Train_Loss: 0.2732253074645996, Test_Loss: 0.2222822904586792\n",
      "Epoch: 23, Train_Loss: 0.3716498017311096, Test_Loss: 0.2044423520565033 *\n",
      "Epoch: 23, Train_Loss: 0.16868016123771667, Test_Loss: 0.23916953802108765\n",
      "Epoch: 23, Train_Loss: 0.21388031542301178, Test_Loss: 0.19343937933444977 *\n",
      "Epoch: 23, Train_Loss: 0.3645603656768799, Test_Loss: 0.22943374514579773\n",
      "Epoch: 23, Train_Loss: 0.6921583414077759, Test_Loss: 0.22598421573638916 *\n",
      "Epoch: 23, Train_Loss: 0.19998504221439362, Test_Loss: 0.17017383873462677 *\n",
      "Epoch: 23, Train_Loss: 0.19856706261634827, Test_Loss: 0.2539921700954437\n",
      "Epoch: 23, Train_Loss: 0.19330348074436188, Test_Loss: 0.24172770977020264 *\n",
      "Epoch: 23, Train_Loss: 0.37461745738983154, Test_Loss: 0.2669770419597626\n",
      "Epoch: 23, Train_Loss: 0.355602502822876, Test_Loss: 0.24607253074645996 *\n",
      "Epoch: 23, Train_Loss: 0.4769781231880188, Test_Loss: 0.23174479603767395 *\n",
      "Epoch: 23, Train_Loss: 0.33175531029701233, Test_Loss: 0.2601016163825989\n",
      "Epoch: 23, Train_Loss: 0.3858325481414795, Test_Loss: 0.24785885214805603 *\n",
      "Epoch: 23, Train_Loss: 0.17099922895431519, Test_Loss: 0.19505567848682404 *\n",
      "Epoch: 23, Train_Loss: 0.18078073859214783, Test_Loss: 0.21812568604946136\n",
      "Epoch: 23, Train_Loss: 0.16716225445270538, Test_Loss: 0.17607256770133972 *\n",
      "Epoch: 23, Train_Loss: 0.2139892280101776, Test_Loss: 0.17262127995491028 *\n",
      "Epoch: 23, Train_Loss: 0.1849060207605362, Test_Loss: 0.20529523491859436\n",
      "Epoch: 23, Train_Loss: 0.2458217442035675, Test_Loss: 0.34277111291885376\n",
      "Epoch: 23, Train_Loss: 15.677409172058105, Test_Loss: 0.2728267312049866 *\n",
      "Epoch: 23, Train_Loss: 0.22706252336502075, Test_Loss: 0.2381785809993744 *\n",
      "Epoch: 23, Train_Loss: 1.3146491050720215, Test_Loss: 0.20712436735630035 *\n",
      "Epoch: 23, Train_Loss: 1.0096124410629272, Test_Loss: 0.18514195084571838 *\n",
      "Epoch: 23, Train_Loss: 0.20589496195316315, Test_Loss: 0.17194314301013947 *\n",
      "Epoch: 23, Train_Loss: 0.3247978091239929, Test_Loss: 0.2122635841369629\n",
      "Epoch: 23, Train_Loss: 2.0992624759674072, Test_Loss: 0.33587712049484253\n",
      "Epoch: 23, Train_Loss: 5.245011806488037, Test_Loss: 0.3482474386692047\n",
      "Epoch: 23, Train_Loss: 0.23885762691497803, Test_Loss: 0.28848448395729065 *\n",
      "Epoch: 23, Train_Loss: 0.3212133049964905, Test_Loss: 0.2159014344215393 *\n",
      "Epoch: 23, Train_Loss: 4.557511329650879, Test_Loss: 0.17867761850357056 *\n",
      "Epoch: 23, Train_Loss: 0.3422953486442566, Test_Loss: 0.19101573526859283\n",
      "Epoch: 23, Train_Loss: 0.3354746103286743, Test_Loss: 0.19338853657245636\n",
      "Epoch: 23, Train_Loss: 0.16998161375522614, Test_Loss: 0.2165655791759491\n",
      "Epoch: 23, Train_Loss: 0.21519726514816284, Test_Loss: 0.18214230239391327 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.2040272057056427, Test_Loss: 0.18417425453662872\n",
      "Epoch: 23, Train_Loss: 0.16687171161174774, Test_Loss: 0.18513083457946777\n",
      "Epoch: 23, Train_Loss: 0.17754104733467102, Test_Loss: 0.2919686734676361\n",
      "Epoch: 23, Train_Loss: 0.1614999622106552, Test_Loss: 0.6725249886512756\n",
      "Epoch: 23, Train_Loss: 0.1618657410144806, Test_Loss: 0.2448524832725525 *\n",
      "Epoch: 23, Train_Loss: 0.1677541434764862, Test_Loss: 0.3071804642677307\n",
      "Epoch: 23, Train_Loss: 0.17917199432849884, Test_Loss: 0.16234858334064484 *\n",
      "Epoch: 23, Train_Loss: 0.22966298460960388, Test_Loss: 0.16220171749591827 *\n",
      "Epoch: 23, Train_Loss: 0.1852252036333084, Test_Loss: 0.16212964057922363 *\n",
      "Epoch: 23, Train_Loss: 0.18384906649589539, Test_Loss: 0.18190553784370422\n",
      "Epoch: 23, Train_Loss: 0.17797617614269257, Test_Loss: 0.23723849654197693\n",
      "Epoch: 23, Train_Loss: 0.16798116266727448, Test_Loss: 6.895133018493652\n",
      "Epoch: 23, Train_Loss: 0.16417379677295685, Test_Loss: 0.478875994682312 *\n",
      "Epoch: 23, Train_Loss: 0.16343536972999573, Test_Loss: 0.3546464443206787 *\n",
      "Epoch: 23, Train_Loss: 0.16315166652202606, Test_Loss: 0.45635104179382324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.161601260304451, Test_Loss: 0.44431281089782715 *\n",
      "Epoch: 23, Train_Loss: 0.16151513159275055, Test_Loss: 0.3016231656074524 *\n",
      "Epoch: 23, Train_Loss: 0.16239197552204132, Test_Loss: 0.8041205406188965\n",
      "Epoch: 23, Train_Loss: 0.16233322024345398, Test_Loss: 0.5711013078689575 *\n",
      "Epoch: 23, Train_Loss: 0.16155950725078583, Test_Loss: 0.3889757990837097 *\n",
      "Epoch: 23, Train_Loss: 0.16147927939891815, Test_Loss: 0.47577595710754395\n",
      "Epoch: 23, Train_Loss: 0.16766415536403656, Test_Loss: 0.44705164432525635 *\n",
      "Epoch: 23, Train_Loss: 0.18969829380512238, Test_Loss: 0.9620335102081299\n",
      "Epoch: 23, Train_Loss: 0.19577744603157043, Test_Loss: 0.6185344457626343 *\n",
      "Epoch: 23, Train_Loss: 0.22143729031085968, Test_Loss: 0.5779427289962769 *\n",
      "Epoch: 23, Train_Loss: 0.3717370927333832, Test_Loss: 0.4340449571609497 *\n",
      "Epoch: 23, Train_Loss: 1.0429705381393433, Test_Loss: 0.2379153072834015 *\n",
      "Epoch: 23, Train_Loss: 4.19498872756958, Test_Loss: 0.1967908889055252 *\n",
      "Epoch: 23, Train_Loss: 0.28238463401794434, Test_Loss: 0.18082085251808167 *\n",
      "Epoch: 23, Train_Loss: 0.265776127576828, Test_Loss: 0.3433186411857605\n",
      "Epoch: 23, Train_Loss: 0.23013439774513245, Test_Loss: 0.2357410192489624 *\n",
      "Epoch: 23, Train_Loss: 0.317624032497406, Test_Loss: 0.4940335750579834\n",
      "Epoch: 23, Train_Loss: 0.36073219776153564, Test_Loss: 0.28833261132240295 *\n",
      "Epoch: 23, Train_Loss: 0.2958548069000244, Test_Loss: 0.3681606650352478\n",
      "Epoch: 23, Train_Loss: 0.26381221413612366, Test_Loss: 0.31056809425354004 *\n",
      "Epoch: 23, Train_Loss: 0.25700658559799194, Test_Loss: 0.3468952178955078\n",
      "Epoch: 23, Train_Loss: 0.24256856739521027, Test_Loss: 0.18973220884799957 *\n",
      "Epoch: 23, Train_Loss: 0.19880236685276031, Test_Loss: 0.18581964075565338 *\n",
      "Epoch: 23, Train_Loss: 0.17136728763580322, Test_Loss: 0.22452831268310547\n",
      "Epoch: 23, Train_Loss: 0.19454973936080933, Test_Loss: 0.20344820618629456 *\n",
      "Epoch: 23, Train_Loss: 0.17848379909992218, Test_Loss: 0.17165741324424744 *\n",
      "Epoch: 23, Train_Loss: 0.40263307094573975, Test_Loss: 0.4499761462211609\n",
      "Epoch: 23, Train_Loss: 0.1675657033920288, Test_Loss: 1.7448811531066895\n",
      "Epoch: 23, Train_Loss: 0.20442236959934235, Test_Loss: 4.728128910064697\n",
      "Epoch: 23, Train_Loss: 0.1679660677909851, Test_Loss: 0.22736793756484985 *\n",
      "Epoch: 23, Train_Loss: 0.19052022695541382, Test_Loss: 0.16395065188407898 *\n",
      "Epoch: 23, Train_Loss: 0.24254730343818665, Test_Loss: 0.24023830890655518\n",
      "Epoch: 23, Train_Loss: 0.23917365074157715, Test_Loss: 0.32721301913261414\n",
      "Epoch: 23, Train_Loss: 0.1906249225139618, Test_Loss: 0.19708746671676636 *\n",
      "Epoch: 23, Train_Loss: 0.1650131791830063, Test_Loss: 0.19670674204826355 *\n",
      "Epoch: 23, Train_Loss: 0.16896551847457886, Test_Loss: 0.2743837833404541\n",
      "Epoch: 23, Train_Loss: 1.2161152362823486, Test_Loss: 0.18188291788101196 *\n",
      "Epoch: 23, Train_Loss: 4.054124355316162, Test_Loss: 0.17119936645030975 *\n",
      "Epoch: 23, Train_Loss: 0.1740531176328659, Test_Loss: 0.19425010681152344\n",
      "Epoch: 23, Train_Loss: 0.16331075131893158, Test_Loss: 0.17534512281417847 *\n",
      "Epoch: 23, Train_Loss: 0.16734662652015686, Test_Loss: 0.17198215425014496 *\n",
      "Epoch: 23, Train_Loss: 0.16398343443870544, Test_Loss: 0.3651666045188904\n",
      "Epoch: 23, Train_Loss: 0.16368354856967926, Test_Loss: 0.3383508622646332 *\n",
      "Epoch: 23, Train_Loss: 0.16306552290916443, Test_Loss: 0.25316479802131653 *\n",
      "Epoch: 23, Train_Loss: 0.17642781138420105, Test_Loss: 0.22579848766326904 *\n",
      "Epoch: 23, Train_Loss: 0.17199520766735077, Test_Loss: 0.19034740328788757 *\n",
      "Epoch: 23, Train_Loss: 0.18927644193172455, Test_Loss: 0.18413233757019043 *\n",
      "Epoch: 23, Train_Loss: 0.16307570040225983, Test_Loss: 0.23146289587020874\n",
      "Epoch: 23, Train_Loss: 0.161678746342659, Test_Loss: 0.2836087942123413\n",
      "Epoch: 23, Train_Loss: 0.1625329554080963, Test_Loss: 0.24774587154388428 *\n",
      "Epoch: 23, Train_Loss: 0.17833995819091797, Test_Loss: 0.21374475955963135 *\n",
      "Epoch: 23, Train_Loss: 0.1632174402475357, Test_Loss: 0.21217864751815796 *\n",
      "Epoch: 23, Train_Loss: 0.16383063793182373, Test_Loss: 0.20778793096542358 *\n",
      "Epoch: 23, Train_Loss: 0.1836923062801361, Test_Loss: 0.29914695024490356\n",
      "Epoch: 23, Train_Loss: 0.18921902775764465, Test_Loss: 0.30469945073127747\n",
      "Epoch: 23, Train_Loss: 0.16662569344043732, Test_Loss: 0.17075763642787933 *\n",
      "Epoch: 23, Train_Loss: 0.16162759065628052, Test_Loss: 0.19056865572929382\n",
      "Epoch: 23, Train_Loss: 0.19963675737380981, Test_Loss: 0.1704767495393753 *\n",
      "Epoch: 23, Train_Loss: 0.2226160764694214, Test_Loss: 0.2160525918006897\n",
      "Epoch: 23, Train_Loss: 0.23863458633422852, Test_Loss: 0.2190556675195694\n",
      "Epoch: 23, Train_Loss: 0.18652066588401794, Test_Loss: 0.34147754311561584\n",
      "Epoch: 23, Train_Loss: 0.208800807595253, Test_Loss: 0.2922474145889282 *\n",
      "Epoch: 23, Train_Loss: 0.2044583410024643, Test_Loss: 0.217020645737648 *\n",
      "Epoch: 23, Train_Loss: 0.18369942903518677, Test_Loss: 0.22474683821201324\n",
      "Epoch: 23, Train_Loss: 0.19862937927246094, Test_Loss: 0.19545644521713257 *\n",
      "Epoch: 23, Train_Loss: 0.1684836596250534, Test_Loss: 0.18214541673660278 *\n",
      "Epoch: 23, Train_Loss: 0.3246009051799774, Test_Loss: 0.2847934365272522\n",
      "Epoch: 23, Train_Loss: 0.18877586722373962, Test_Loss: 0.40771231055259705\n",
      "Epoch: 23, Train_Loss: 0.1622467190027237, Test_Loss: 0.5500874519348145\n",
      "Epoch: 23, Train_Loss: 0.1609005182981491, Test_Loss: 0.22917670011520386 *\n",
      "Epoch: 23, Train_Loss: 0.16078370809555054, Test_Loss: 0.2389993965625763\n",
      "Epoch: 23, Train_Loss: 0.1608736217021942, Test_Loss: 0.16650192439556122 *\n",
      "Epoch: 23, Train_Loss: 0.16093049943447113, Test_Loss: 0.169722780585289\n",
      "Epoch: 23, Train_Loss: 1.7039201259613037, Test_Loss: 0.17029178142547607\n",
      "Epoch: 23, Train_Loss: 3.2917580604553223, Test_Loss: 0.1719236820936203\n",
      "Epoch: 23, Train_Loss: 0.16722872853279114, Test_Loss: 0.20104540884494781\n",
      "Epoch: 23, Train_Loss: 0.16743528842926025, Test_Loss: 0.1705373227596283 *\n",
      "Epoch: 23, Train_Loss: 0.1683903932571411, Test_Loss: 0.17671862244606018\n",
      "Epoch: 23, Train_Loss: 0.16202031075954437, Test_Loss: 0.27734044194221497\n",
      "Epoch: 23, Train_Loss: 0.16318649053573608, Test_Loss: 0.5010111331939697\n",
      "Epoch: 23, Train_Loss: 0.16387225687503815, Test_Loss: 0.39428091049194336 *\n",
      "Epoch: 23, Train_Loss: 0.16195116937160492, Test_Loss: 0.18798011541366577 *\n",
      "Epoch: 23, Train_Loss: 0.16307692229747772, Test_Loss: 0.18025970458984375 *\n",
      "Epoch: 23, Train_Loss: 0.1668955534696579, Test_Loss: 0.18032687902450562\n",
      "Epoch: 23, Train_Loss: 0.2031889259815216, Test_Loss: 0.18131253123283386\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.1879872828722, Test_Loss: 0.18360266089439392\n",
      "Epoch: 23, Train_Loss: 0.2171134203672409, Test_Loss: 0.4741672873497009\n",
      "Epoch: 23, Train_Loss: 0.19062575697898865, Test_Loss: 5.680106163024902\n",
      "Epoch: 23, Train_Loss: 0.16351447999477386, Test_Loss: 0.21982647478580475 *\n",
      "Epoch: 23, Train_Loss: 0.3742685616016388, Test_Loss: 0.17714299261569977 *\n",
      "Epoch: 23, Train_Loss: 0.35023611783981323, Test_Loss: 0.18207800388336182\n",
      "Epoch: 23, Train_Loss: 0.3676825165748596, Test_Loss: 0.1754845678806305 *\n",
      "Epoch: 23, Train_Loss: 0.2769339978694916, Test_Loss: 0.1695517897605896 *\n",
      "Epoch: 23, Train_Loss: 0.16188186407089233, Test_Loss: 0.18001684546470642\n",
      "Epoch: 23, Train_Loss: 0.16179314255714417, Test_Loss: 0.17680250108242035 *\n",
      "Epoch: 23, Train_Loss: 0.16561734676361084, Test_Loss: 0.1628393679857254 *\n",
      "Epoch: 23, Train_Loss: 0.16519108414649963, Test_Loss: 0.1641257256269455\n",
      "Epoch: 23, Train_Loss: 0.17465277016162872, Test_Loss: 0.1707884669303894\n",
      "Epoch: 23, Train_Loss: 0.17632077634334564, Test_Loss: 0.24755749106407166\n",
      "Epoch: 23, Train_Loss: 0.1626013219356537, Test_Loss: 0.17666491866111755 *\n",
      "Epoch: 23, Train_Loss: 0.1620023548603058, Test_Loss: 0.169623002409935 *\n",
      "Epoch: 23, Train_Loss: 0.17551392316818237, Test_Loss: 0.18331846594810486\n",
      "Epoch: 23, Train_Loss: 0.2183825522661209, Test_Loss: 0.16300512850284576 *\n",
      "Epoch: 23, Train_Loss: 0.27380576729774475, Test_Loss: 0.16352815926074982\n",
      "Epoch: 23, Train_Loss: 0.1727295219898224, Test_Loss: 0.16717751324176788\n",
      "Epoch: 23, Train_Loss: 0.2548012137413025, Test_Loss: 0.18831638991832733\n",
      "Epoch: 23, Train_Loss: 0.22729367017745972, Test_Loss: 0.1613558828830719 *\n",
      "Epoch: 23, Train_Loss: 0.2612488567829132, Test_Loss: 0.16686470806598663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.21055234968662262, Test_Loss: 0.1607741415500641 *\n",
      "Epoch: 23, Train_Loss: 0.2483929544687271, Test_Loss: 0.19038867950439453\n",
      "Epoch: 23, Train_Loss: 0.21484050154685974, Test_Loss: 0.19794069230556488\n",
      "Epoch: 23, Train_Loss: 0.3897242546081543, Test_Loss: 0.19245252013206482 *\n",
      "Epoch: 23, Train_Loss: 0.17147748172283173, Test_Loss: 0.1631542593240738 *\n",
      "Epoch: 23, Train_Loss: 0.18456922471523285, Test_Loss: 0.16346517205238342\n",
      "Epoch: 23, Train_Loss: 2.963505506515503, Test_Loss: 0.16947253048419952\n",
      "Epoch: 23, Train_Loss: 0.3408559560775757, Test_Loss: 0.16751734912395477 *\n",
      "Epoch: 23, Train_Loss: 0.21842610836029053, Test_Loss: 0.16943727433681488\n",
      "Epoch: 23, Train_Loss: 0.19993990659713745, Test_Loss: 0.2584153413772583\n",
      "Epoch: 23, Train_Loss: 0.16749750077724457, Test_Loss: 2.551632881164551\n",
      "Epoch: 23, Train_Loss: 0.17400918900966644, Test_Loss: 3.0898406505584717\n",
      "Epoch: 23, Train_Loss: 0.17713364958763123, Test_Loss: 0.17150309681892395 *\n",
      "Epoch: 23, Train_Loss: 0.2177821695804596, Test_Loss: 0.16195403039455414 *\n",
      "Epoch: 23, Train_Loss: 0.26045146584510803, Test_Loss: 0.19036568701267242\n",
      "Epoch: 23, Train_Loss: 0.21111378073692322, Test_Loss: 0.1720583438873291 *\n",
      "Epoch: 23, Train_Loss: 0.1845570206642151, Test_Loss: 0.19868861138820648\n",
      "Epoch: 23, Train_Loss: 0.16672761738300323, Test_Loss: 0.20288196206092834\n",
      "Epoch: 23, Train_Loss: 0.1689247190952301, Test_Loss: 0.23679868876934052\n",
      "Epoch: 23, Train_Loss: 0.16801905632019043, Test_Loss: 0.1654733568429947 *\n",
      "Epoch: 23, Train_Loss: 0.17720209062099457, Test_Loss: 0.1828007996082306\n",
      "Epoch: 23, Train_Loss: 0.20629124343395233, Test_Loss: 0.1823362112045288 *\n",
      "Epoch: 23, Train_Loss: 0.18451112508773804, Test_Loss: 0.1901751458644867\n",
      "Epoch: 23, Train_Loss: 0.16397246718406677, Test_Loss: 0.1688380092382431 *\n",
      "Epoch: 23, Train_Loss: 0.16995353996753693, Test_Loss: 0.2496103048324585\n",
      "Epoch: 23, Train_Loss: 0.17399479448795319, Test_Loss: 0.2255028784275055 *\n",
      "Epoch: 23, Train_Loss: 0.17316585779190063, Test_Loss: 0.21976570785045624 *\n",
      "Epoch: 23, Train_Loss: 0.1643776297569275, Test_Loss: 0.1787145882844925 *\n",
      "Epoch: 23, Train_Loss: 0.1617906093597412, Test_Loss: 0.20457586646080017\n",
      "Epoch: 23, Train_Loss: 0.16051705181598663, Test_Loss: 0.2010677605867386 *\n",
      "Epoch: 23, Train_Loss: 0.16378973424434662, Test_Loss: 0.1652732640504837 *\n",
      "Epoch: 23, Train_Loss: 0.16362597048282623, Test_Loss: 0.17648915946483612\n",
      "Epoch: 23, Train_Loss: 0.1631496697664261, Test_Loss: 0.17475326359272003 *\n",
      "Epoch: 23, Train_Loss: 0.1658286303281784, Test_Loss: 0.1874876618385315\n",
      "Epoch: 23, Train_Loss: 0.16328684985637665, Test_Loss: 0.17859208583831787 *\n",
      "Epoch: 23, Train_Loss: 0.16063959896564484, Test_Loss: 0.166659876704216 *\n",
      "Epoch: 23, Train_Loss: 0.16321180760860443, Test_Loss: 0.1745416671037674\n",
      "Epoch: 23, Train_Loss: 0.16557592153549194, Test_Loss: 0.1901770830154419\n",
      "Epoch: 23, Train_Loss: 0.1723845899105072, Test_Loss: 0.16446207463741302 *\n",
      "Epoch: 23, Train_Loss: 0.17714494466781616, Test_Loss: 0.1660277545452118\n",
      "Epoch: 23, Train_Loss: 0.17718574404716492, Test_Loss: 0.19796542823314667\n",
      "Epoch: 23, Train_Loss: 0.18853983283042908, Test_Loss: 0.2502213716506958\n",
      "Epoch: 23, Train_Loss: 0.18629029393196106, Test_Loss: 0.19632913172245026 *\n",
      "Epoch: 23, Train_Loss: 0.17046454548835754, Test_Loss: 0.4506816864013672\n",
      "Epoch: 23, Train_Loss: 0.16571368277072906, Test_Loss: 0.4234201908111572 *\n",
      "Epoch: 23, Train_Loss: 0.18058620393276215, Test_Loss: 0.2391810417175293 *\n",
      "Epoch: 23, Train_Loss: 0.186786949634552, Test_Loss: 0.18553376197814941 *\n",
      "Epoch: 23, Train_Loss: 0.16390852630138397, Test_Loss: 0.18496620655059814 *\n",
      "Epoch: 23, Train_Loss: 0.17177018523216248, Test_Loss: 0.1787722110748291 *\n",
      "Epoch: 23, Train_Loss: 0.17201325297355652, Test_Loss: 0.314576119184494\n",
      "Epoch: 23, Train_Loss: 0.1751709282398224, Test_Loss: 0.23766584694385529 *\n",
      "Epoch: 23, Train_Loss: 0.21114487946033478, Test_Loss: 0.49152112007141113\n",
      "Epoch: 23, Train_Loss: 0.21500852704048157, Test_Loss: 0.22745853662490845 *\n",
      "Epoch: 23, Train_Loss: 0.17195384204387665, Test_Loss: 0.20885884761810303 *\n",
      "Epoch: 23, Train_Loss: 0.16635572910308838, Test_Loss: 0.16536778211593628 *\n",
      "Epoch: 23, Train_Loss: 0.22454454004764557, Test_Loss: 0.16208021342754364 *\n",
      "Epoch: 23, Train_Loss: 0.17021191120147705, Test_Loss: 0.16891631484031677\n",
      "Epoch: 23, Train_Loss: 0.16865389049053192, Test_Loss: 0.16731734573841095 *\n",
      "Epoch: 23, Train_Loss: 0.16578860580921173, Test_Loss: 0.1981380581855774\n",
      "Epoch: 23, Train_Loss: 0.18674544990062714, Test_Loss: 0.16316698491573334 *\n",
      "Epoch: 23, Train_Loss: 0.24197998642921448, Test_Loss: 0.2081674188375473\n",
      "Epoch: 23, Train_Loss: 0.18891289830207825, Test_Loss: 0.29645368456840515\n",
      "Epoch: 23, Train_Loss: 0.17977756261825562, Test_Loss: 0.4327751398086548\n",
      "Epoch: 23, Train_Loss: 0.1707337349653244, Test_Loss: 0.38470950722694397 *\n",
      "Epoch: 23, Train_Loss: 0.18724945187568665, Test_Loss: 0.17730405926704407 *\n",
      "Epoch: 23, Train_Loss: 0.16244226694107056, Test_Loss: 0.17162556946277618 *\n",
      "Epoch: 23, Train_Loss: 0.16590988636016846, Test_Loss: 0.1711336374282837 *\n",
      "Epoch: 23, Train_Loss: 0.17740122973918915, Test_Loss: 0.17140734195709229\n",
      "Epoch: 23, Train_Loss: 0.1665700227022171, Test_Loss: 0.17926722764968872\n",
      "Epoch: 23, Train_Loss: 0.18982870876789093, Test_Loss: 1.3137402534484863\n",
      "Epoch: 23, Train_Loss: 0.22954276204109192, Test_Loss: 4.619781970977783\n",
      "Epoch: 23, Train_Loss: 0.18175184726715088, Test_Loss: 0.1769111454486847 *\n",
      "Epoch: 23, Train_Loss: 0.19409863650798798, Test_Loss: 0.17017874121665955 *\n",
      "Epoch: 23, Train_Loss: 0.18383492529392242, Test_Loss: 0.17049039900302887\n",
      "Epoch: 23, Train_Loss: 0.16988331079483032, Test_Loss: 0.16715571284294128 *\n",
      "Epoch: 23, Train_Loss: 0.28338801860809326, Test_Loss: 0.1642758995294571 *\n",
      "Epoch: 23, Train_Loss: 0.3281315565109253, Test_Loss: 0.18183358013629913\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.15992984175682068, Test_Loss: 0.1726747453212738 *\n",
      "Epoch: 23, Train_Loss: 0.19385509192943573, Test_Loss: 0.16228672862052917 *\n",
      "Epoch: 23, Train_Loss: 0.15941782295703888, Test_Loss: 0.16255153715610504\n",
      "Epoch: 23, Train_Loss: 0.16122838854789734, Test_Loss: 0.17230653762817383\n",
      "Epoch: 23, Train_Loss: 0.16681793332099915, Test_Loss: 0.2280033379793167\n",
      "Epoch: 23, Train_Loss: 0.16631318628787994, Test_Loss: 0.17559704184532166 *\n",
      "Epoch: 23, Train_Loss: 0.16043081879615784, Test_Loss: 0.18162235617637634\n",
      "Epoch: 23, Train_Loss: 0.1718517690896988, Test_Loss: 0.17865091562271118 *\n",
      "Epoch: 23, Train_Loss: 0.16335584223270416, Test_Loss: 0.161183163523674 *\n",
      "Epoch: 23, Train_Loss: 0.16400212049484253, Test_Loss: 0.16250759363174438\n",
      "Epoch: 23, Train_Loss: 0.16520676016807556, Test_Loss: 0.16628143191337585\n",
      "Epoch: 23, Train_Loss: 0.16008004546165466, Test_Loss: 0.17878161370754242\n",
      "Epoch: 23, Train_Loss: 0.16149583458900452, Test_Loss: 0.16037268936634064 *\n",
      "Epoch: 23, Train_Loss: 0.16108188033103943, Test_Loss: 0.167595773935318\n",
      "Epoch: 23, Train_Loss: 0.1742180734872818, Test_Loss: 0.15889796614646912 *\n",
      "Epoch: 23, Train_Loss: 0.1732213944196701, Test_Loss: 0.1700872778892517\n",
      "Epoch: 23, Train_Loss: 0.18332168459892273, Test_Loss: 0.1886802762746811\n",
      "Epoch: 23, Train_Loss: 0.17598459124565125, Test_Loss: 0.16753986477851868 *\n",
      "Epoch: 23, Train_Loss: 0.16679716110229492, Test_Loss: 0.1606999635696411 *\n",
      "Epoch: 23, Train_Loss: 0.1709490418434143, Test_Loss: 0.16850833594799042\n",
      "Epoch: 23, Train_Loss: 0.17542105913162231, Test_Loss: 0.16318614780902863 *\n",
      "Epoch: 23, Train_Loss: 0.17260585725307465, Test_Loss: 0.1642778217792511\n",
      "Epoch: 23, Train_Loss: 0.1855965554714203, Test_Loss: 0.19673192501068115\n",
      "Epoch: 23, Train_Loss: 0.1635652482509613, Test_Loss: 0.22470608353614807\n",
      "Epoch: 23, Train_Loss: 0.16903071105480194, Test_Loss: 3.6353187561035156\n",
      "Epoch: 23, Train_Loss: 0.18361206352710724, Test_Loss: 2.2895970344543457 *\n",
      "Epoch: 23, Train_Loss: 0.2028629183769226, Test_Loss: 0.16956762969493866 *\n",
      "Epoch: 23, Train_Loss: 2.2966041564941406, Test_Loss: 0.16347505152225494 *\n",
      "Epoch: 23, Train_Loss: 2.9127018451690674, Test_Loss: 0.1741916537284851\n",
      "Epoch: 23, Train_Loss: 0.17158682644367218, Test_Loss: 0.17177598178386688 *\n",
      "Epoch: 23, Train_Loss: 0.16649143397808075, Test_Loss: 0.1763424575328827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.18608792126178741, Test_Loss: 0.2338123321533203\n",
      "Epoch: 23, Train_Loss: 0.23806467652320862, Test_Loss: 0.24912358820438385\n",
      "Epoch: 23, Train_Loss: 0.17855997383594513, Test_Loss: 0.16088326275348663 *\n",
      "Epoch: 23, Train_Loss: 0.16595257818698883, Test_Loss: 0.18833689391613007\n",
      "Epoch: 23, Train_Loss: 0.1645956039428711, Test_Loss: 0.1795753836631775 *\n",
      "Epoch: 23, Train_Loss: 0.2334049940109253, Test_Loss: 0.17579157650470734 *\n",
      "Epoch: 23, Train_Loss: 0.17900606989860535, Test_Loss: 0.16619613766670227 *\n",
      "Epoch: 23, Train_Loss: 0.173546701669693, Test_Loss: 0.21757182478904724\n",
      "Epoch: 23, Train_Loss: 0.5722095370292664, Test_Loss: 0.2181486189365387\n",
      "Epoch: 23, Train_Loss: 0.42958778142929077, Test_Loss: 0.2426488697528839\n",
      "Epoch: 23, Train_Loss: 0.6035184264183044, Test_Loss: 0.18187573552131653 *\n",
      "Epoch: 23, Train_Loss: 0.23398557305335999, Test_Loss: 0.19750656187534332\n",
      "Epoch: 23, Train_Loss: 0.6996333599090576, Test_Loss: 0.17631396651268005 *\n",
      "Epoch: 23, Train_Loss: 0.5914828777313232, Test_Loss: 0.1680152267217636 *\n",
      "Epoch: 23, Train_Loss: 0.4526106119155884, Test_Loss: 0.19630596041679382\n",
      "Epoch: 23, Train_Loss: 0.1594211608171463, Test_Loss: 0.20254549384117126\n",
      "Epoch: 23, Train_Loss: 0.18466657400131226, Test_Loss: 0.17558279633522034 *\n",
      "Epoch: 23, Train_Loss: 0.47408419847488403, Test_Loss: 0.18844524025917053\n",
      "Epoch: 23, Train_Loss: 0.36800849437713623, Test_Loss: 0.19042915105819702\n",
      "Epoch: 23, Train_Loss: 0.27523019909858704, Test_Loss: 0.21736198663711548\n",
      "Epoch: 23, Train_Loss: 0.1659163236618042, Test_Loss: 0.24379530549049377\n",
      "Epoch: 23, Train_Loss: 0.1680600345134735, Test_Loss: 0.17238681018352509 *\n",
      "Epoch: 23, Train_Loss: 0.3402898907661438, Test_Loss: 0.17241275310516357\n",
      "Epoch: 23, Train_Loss: 0.24305501580238342, Test_Loss: 0.17326682806015015\n",
      "Epoch: 23, Train_Loss: 0.23080965876579285, Test_Loss: 0.1994088739156723\n",
      "Epoch: 23, Train_Loss: 0.21146813035011292, Test_Loss: 0.2792404592037201\n",
      "Epoch: 23, Train_Loss: 0.1975637674331665, Test_Loss: 0.3226325511932373\n",
      "Epoch: 23, Train_Loss: 0.2680947780609131, Test_Loss: 0.3734830915927887\n",
      "Epoch: 23, Train_Loss: 0.288230299949646, Test_Loss: 0.22422876954078674 *\n",
      "Epoch: 23, Train_Loss: 0.20581476390361786, Test_Loss: 0.21569547057151794 *\n",
      "Epoch: 23, Train_Loss: 0.20457932353019714, Test_Loss: 0.2010646015405655 *\n",
      "Epoch: 23, Train_Loss: 0.2520393133163452, Test_Loss: 0.17474448680877686 *\n",
      "Epoch: 23, Train_Loss: 0.18818409740924835, Test_Loss: 0.4467587471008301\n",
      "Epoch: 23, Train_Loss: 0.2025250494480133, Test_Loss: 0.24675443768501282 *\n",
      "Epoch: 23, Train_Loss: 0.2196858823299408, Test_Loss: 0.6272017359733582\n",
      "Epoch: 23, Train_Loss: 0.2831565737724304, Test_Loss: 0.23549756407737732 *\n",
      "Epoch: 23, Train_Loss: 0.19636277854442596, Test_Loss: 0.23280984163284302 *\n",
      "Epoch: 23, Train_Loss: 0.22184491157531738, Test_Loss: 0.17177152633666992 *\n",
      "Epoch: 23, Train_Loss: 0.19052863121032715, Test_Loss: 0.1612900048494339 *\n",
      "Epoch: 23, Train_Loss: 0.16851381957530975, Test_Loss: 0.22187775373458862\n",
      "Epoch: 23, Train_Loss: 0.16063399612903595, Test_Loss: 0.18256989121437073 *\n",
      "Epoch: 23, Train_Loss: 0.16686752438545227, Test_Loss: 0.17599374055862427 *\n",
      "Epoch: 23, Train_Loss: 0.17093348503112793, Test_Loss: 0.17953073978424072\n",
      "Epoch: 23, Train_Loss: 0.173345148563385, Test_Loss: 0.3402400016784668\n",
      "Epoch: 23, Train_Loss: 0.17433127760887146, Test_Loss: 0.35469818115234375\n",
      "Epoch: 23, Train_Loss: 0.17008936405181885, Test_Loss: 0.3804131746292114\n",
      "Epoch: 23, Train_Loss: 0.16613084077835083, Test_Loss: 0.459245502948761\n",
      "Epoch: 23, Train_Loss: 0.32622307538986206, Test_Loss: 0.20139095187187195 *\n",
      "Epoch: 23, Train_Loss: 0.306080162525177, Test_Loss: 0.1928386688232422 *\n",
      "Epoch: 23, Train_Loss: 0.1722124218940735, Test_Loss: 0.18979565799236298 *\n",
      "Epoch: 23, Train_Loss: 0.18754960596561432, Test_Loss: 0.18940536677837372 *\n",
      "Epoch: 23, Train_Loss: 0.2122219204902649, Test_Loss: 0.3041788935661316\n",
      "Epoch: 23, Train_Loss: 0.20809948444366455, Test_Loss: 3.0034990310668945\n",
      "Epoch: 23, Train_Loss: 0.3971269726753235, Test_Loss: 2.6614577770233154 *\n",
      "Epoch: 23, Train_Loss: 0.1948975920677185, Test_Loss: 0.18534398078918457 *\n",
      "Epoch: 23, Train_Loss: 0.38334572315216064, Test_Loss: 0.1825345754623413 *\n",
      "Epoch: 23, Train_Loss: 0.22737224400043488, Test_Loss: 0.18625003099441528\n",
      "Epoch: 23, Train_Loss: 0.23592540621757507, Test_Loss: 0.16344453394412994 *\n",
      "Epoch: 23, Train_Loss: 0.21857190132141113, Test_Loss: 0.17661507427692413\n",
      "Epoch: 23, Train_Loss: 0.17042312026023865, Test_Loss: 0.2282172292470932\n",
      "Epoch: 23, Train_Loss: 0.20249690115451813, Test_Loss: 0.1939920336008072 *\n",
      "Epoch: 23, Train_Loss: 0.47124624252319336, Test_Loss: 0.16412538290023804 *\n",
      "Epoch: 23, Train_Loss: 0.4930841326713562, Test_Loss: 0.17611084878444672\n",
      "Epoch: 23, Train_Loss: 0.17400631308555603, Test_Loss: 0.1854119449853897\n",
      "Epoch: 23, Train_Loss: 0.1953570544719696, Test_Loss: 0.24681392312049866\n",
      "Epoch: 23, Train_Loss: 0.16901063919067383, Test_Loss: 0.17216959595680237 *\n",
      "Epoch: 23, Train_Loss: 0.19493909180164337, Test_Loss: 0.17325720191001892\n",
      "Epoch: 23, Train_Loss: 0.46716663241386414, Test_Loss: 0.17428044974803925\n",
      "Epoch: 23, Train_Loss: 0.164631187915802, Test_Loss: 0.17176324129104614 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 23\n",
      "Epoch: 23, Train_Loss: 0.27299827337265015, Test_Loss: 0.20293396711349487\n",
      "Epoch: 23, Train_Loss: 0.22040744125843048, Test_Loss: 0.21322785317897797\n",
      "Epoch: 23, Train_Loss: 0.19356562197208405, Test_Loss: 0.1698436588048935 *\n",
      "Epoch: 23, Train_Loss: 0.20137153565883636, Test_Loss: 0.16553382575511932 *\n",
      "Epoch: 23, Train_Loss: 0.25479021668434143, Test_Loss: 0.18482255935668945\n",
      "Epoch: 23, Train_Loss: 0.32012349367141724, Test_Loss: 0.1616823673248291 *\n",
      "Epoch: 23, Train_Loss: 0.17835332453250885, Test_Loss: 0.17310024797916412\n",
      "Epoch: 23, Train_Loss: 0.19226732850074768, Test_Loss: 0.1912037879228592\n",
      "Epoch: 23, Train_Loss: 0.19959399104118347, Test_Loss: 0.16618825495243073 *\n",
      "Epoch: 23, Train_Loss: 0.222619891166687, Test_Loss: 0.16803142428398132\n",
      "Epoch: 23, Train_Loss: 0.19869713485240936, Test_Loss: 0.20827996730804443\n",
      "Epoch: 23, Train_Loss: 0.18338091671466827, Test_Loss: 0.2109612375497818\n",
      "Epoch: 23, Train_Loss: 0.17148461937904358, Test_Loss: 0.1627243161201477 *\n",
      "Epoch: 23, Train_Loss: 0.18733759224414825, Test_Loss: 0.2338009476661682\n",
      "Epoch: 23, Train_Loss: 0.3889045715332031, Test_Loss: 0.2904195785522461\n",
      "Epoch: 23, Train_Loss: 0.44765883684158325, Test_Loss: 5.029772758483887\n",
      "Epoch: 23, Train_Loss: 0.6101736426353455, Test_Loss: 1.0064787864685059 *\n",
      "Epoch: 23, Train_Loss: 0.4067687392234802, Test_Loss: 0.16501469910144806 *\n",
      "Epoch: 23, Train_Loss: 0.37812161445617676, Test_Loss: 0.18015266954898834\n",
      "Epoch: 23, Train_Loss: 0.2335325926542282, Test_Loss: 0.17329397797584534 *\n",
      "Epoch: 23, Train_Loss: 0.2176893949508667, Test_Loss: 0.16711559891700745 *\n",
      "Epoch: 23, Train_Loss: 0.17190304398536682, Test_Loss: 0.16977374255657196\n",
      "Epoch: 23, Train_Loss: 0.16742637753486633, Test_Loss: 0.21210609376430511\n",
      "Epoch: 23, Train_Loss: 0.2460552155971527, Test_Loss: 0.21550418436527252\n",
      "Epoch: 23, Train_Loss: 0.4359753131866455, Test_Loss: 0.1597326248884201 *\n",
      "Epoch: 23, Train_Loss: 0.5013970136642456, Test_Loss: 0.18755513429641724\n",
      "Epoch: 23, Train_Loss: 0.7525231242179871, Test_Loss: 0.17280462384223938 *\n",
      "Epoch: 23, Train_Loss: 1.0276111364364624, Test_Loss: 0.17700982093811035\n",
      "Epoch: 23, Train_Loss: 0.37183547019958496, Test_Loss: 0.183757483959198\n",
      "Epoch: 23, Train_Loss: 0.40269672870635986, Test_Loss: 0.19113779067993164\n",
      "Epoch: 23, Train_Loss: 0.1621648073196411, Test_Loss: 0.22259142994880676\n",
      "Epoch: 23, Train_Loss: 0.17125742137432098, Test_Loss: 0.22338451445102692\n",
      "Epoch: 23, Train_Loss: 0.35119229555130005, Test_Loss: 0.19169564545154572 *\n",
      "Epoch: 23, Train_Loss: 0.5091822147369385, Test_Loss: 0.200494647026062\n",
      "Epoch: 23, Train_Loss: 0.2356797456741333, Test_Loss: 0.17843778431415558 *\n",
      "Epoch: 23, Train_Loss: 0.19674545526504517, Test_Loss: 0.25384506583213806\n",
      "Epoch: 23, Train_Loss: 0.16662345826625824, Test_Loss: 0.264026403427124\n",
      "Epoch: 23, Train_Loss: 0.261652410030365, Test_Loss: 0.3320614695549011\n",
      "Epoch: 23, Train_Loss: 0.46619439125061035, Test_Loss: 0.27375537157058716 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train_Loss: 0.4617025852203369, Test_Loss: 0.26110684871673584 *\n",
      "Epoch: 23, Train_Loss: 0.28140056133270264, Test_Loss: 0.32357659935951233\n",
      "Epoch: 23, Train_Loss: 0.41269201040267944, Test_Loss: 0.27518796920776367 *\n",
      "Epoch: 23, Train_Loss: 0.16720300912857056, Test_Loss: 0.2574942708015442 *\n",
      "Epoch: 23, Train_Loss: 0.17013122141361237, Test_Loss: 0.20267267525196075 *\n",
      "Epoch: 23, Train_Loss: 0.17853927612304688, Test_Loss: 0.1892237663269043 *\n",
      "Epoch: 23, Train_Loss: 0.21656270325183868, Test_Loss: 0.1632036417722702 *\n",
      "Epoch: 23, Train_Loss: 0.18413269519805908, Test_Loss: 0.17744359374046326\n",
      "Epoch: 23, Train_Loss: 0.2218342423439026, Test_Loss: 0.3067007064819336\n",
      "Epoch: 23, Train_Loss: 14.5479736328125, Test_Loss: 0.24010714888572693 *\n",
      "Epoch: 23, Train_Loss: 1.0614910125732422, Test_Loss: 0.2639560401439667\n",
      "Epoch: 23, Train_Loss: 1.2251825332641602, Test_Loss: 0.1980849802494049 *\n",
      "Epoch: 23, Train_Loss: 0.7366155385971069, Test_Loss: 0.20523089170455933\n",
      "Epoch: 23, Train_Loss: 0.21400412917137146, Test_Loss: 0.17901389300823212 *\n",
      "Epoch: 23, Train_Loss: 0.21960127353668213, Test_Loss: 0.20340192317962646\n",
      "Epoch: 24, Train_Loss: 1.3659712076187134, Test_Loss: 0.4065927267074585 *\n",
      "Epoch: 24, Train_Loss: 5.038519382476807, Test_Loss: 0.273277223110199 *\n",
      "Epoch: 24, Train_Loss: 0.49583899974823, Test_Loss: 0.4785959720611572\n",
      "Epoch: 24, Train_Loss: 0.24546509981155396, Test_Loss: 0.2075667381286621 *\n",
      "Epoch: 24, Train_Loss: 4.237157821655273, Test_Loss: 0.20855583250522614\n",
      "Epoch: 24, Train_Loss: 0.8343917727470398, Test_Loss: 0.21480174362659454\n",
      "Epoch: 24, Train_Loss: 0.3857084810733795, Test_Loss: 0.19069060683250427 *\n",
      "Epoch: 24, Train_Loss: 0.17452922463417053, Test_Loss: 0.2414705604314804\n",
      "Epoch: 24, Train_Loss: 0.21831150352954865, Test_Loss: 0.179701030254364 *\n",
      "Epoch: 24, Train_Loss: 0.20056737959384918, Test_Loss: 0.19871480762958527\n",
      "Epoch: 24, Train_Loss: 0.16375622153282166, Test_Loss: 0.18356521427631378 *\n",
      "Epoch: 24, Train_Loss: 0.16558775305747986, Test_Loss: 0.27163833379745483\n",
      "Epoch: 24, Train_Loss: 0.15767958760261536, Test_Loss: 0.8574386835098267\n",
      "Epoch: 24, Train_Loss: 0.15818479657173157, Test_Loss: 0.2698872685432434 *\n",
      "Epoch: 24, Train_Loss: 0.16071712970733643, Test_Loss: 0.3498510718345642\n",
      "Epoch: 24, Train_Loss: 0.16496267914772034, Test_Loss: 0.1599484533071518 *\n",
      "Epoch: 24, Train_Loss: 0.1973915845155716, Test_Loss: 0.15779131650924683 *\n",
      "Epoch: 24, Train_Loss: 0.18434758484363556, Test_Loss: 0.15772050619125366 *\n",
      "Epoch: 24, Train_Loss: 0.17753322422504425, Test_Loss: 0.15884605050086975\n",
      "Epoch: 24, Train_Loss: 0.16865447163581848, Test_Loss: 0.1885811686515808\n",
      "Epoch: 24, Train_Loss: 0.16446706652641296, Test_Loss: 5.609299659729004\n",
      "Epoch: 24, Train_Loss: 0.1608642190694809, Test_Loss: 1.978400468826294 *\n",
      "Epoch: 24, Train_Loss: 0.16192451119422913, Test_Loss: 0.29918739199638367 *\n",
      "Epoch: 24, Train_Loss: 0.15840673446655273, Test_Loss: 0.3784521520137787\n",
      "Epoch: 24, Train_Loss: 0.15809109807014465, Test_Loss: 0.48482927680015564\n",
      "Epoch: 24, Train_Loss: 0.1572563797235489, Test_Loss: 0.25771331787109375 *\n",
      "Epoch: 24, Train_Loss: 0.1581788957118988, Test_Loss: 0.5613553524017334\n",
      "Epoch: 24, Train_Loss: 0.15766659379005432, Test_Loss: 0.7943077087402344\n",
      "Epoch: 24, Train_Loss: 0.15734444558620453, Test_Loss: 0.6577045321464539 *\n",
      "Epoch: 24, Train_Loss: 0.1572057157754898, Test_Loss: 0.570768415927887 *\n",
      "Epoch: 24, Train_Loss: 0.16063186526298523, Test_Loss: 0.607149064540863\n",
      "Epoch: 24, Train_Loss: 0.17600798606872559, Test_Loss: 0.6319224238395691\n",
      "Epoch: 24, Train_Loss: 0.17610923945903778, Test_Loss: 1.1289234161376953\n",
      "Epoch: 24, Train_Loss: 0.21520784497261047, Test_Loss: 0.5079435110092163 *\n",
      "Epoch: 24, Train_Loss: 0.1851939857006073, Test_Loss: 0.5118612051010132\n",
      "Epoch: 24, Train_Loss: 0.32744842767715454, Test_Loss: 0.33861255645751953 *\n",
      "Epoch: 24, Train_Loss: 4.884171009063721, Test_Loss: 0.1841997504234314 *\n",
      "Epoch: 24, Train_Loss: 0.3275889754295349, Test_Loss: 0.17423106729984283 *\n",
      "Epoch: 24, Train_Loss: 0.1887645423412323, Test_Loss: 0.307935506105423\n",
      "Epoch: 24, Train_Loss: 0.2030380815267563, Test_Loss: 0.27916455268859863 *\n",
      "Epoch: 24, Train_Loss: 0.27698370814323425, Test_Loss: 0.3940751552581787\n",
      "Epoch: 24, Train_Loss: 0.2635829746723175, Test_Loss: 0.5317327976226807\n",
      "Epoch: 24, Train_Loss: 0.253171443939209, Test_Loss: 0.271940141916275 *\n",
      "Epoch: 24, Train_Loss: 0.27098146080970764, Test_Loss: 0.33919858932495117\n",
      "Epoch: 24, Train_Loss: 0.3172086477279663, Test_Loss: 0.37762394547462463\n",
      "Epoch: 24, Train_Loss: 0.2722978889942169, Test_Loss: 0.2031039148569107 *\n",
      "Epoch: 24, Train_Loss: 0.20282189548015594, Test_Loss: 0.17409884929656982 *\n",
      "Epoch: 24, Train_Loss: 0.16241274774074554, Test_Loss: 0.24097442626953125\n",
      "Epoch: 24, Train_Loss: 0.19176344573497772, Test_Loss: 0.1908182054758072 *\n",
      "Epoch: 24, Train_Loss: 0.16080600023269653, Test_Loss: 0.1597987562417984 *\n",
      "Epoch: 24, Train_Loss: 0.2766750454902649, Test_Loss: 0.43161988258361816\n",
      "Epoch: 24, Train_Loss: 0.16173051297664642, Test_Loss: 0.2796679139137268 *\n",
      "Epoch: 24, Train_Loss: 0.17822282016277313, Test_Loss: 6.23418664932251\n",
      "Epoch: 24, Train_Loss: 0.18015475571155548, Test_Loss: 0.34889620542526245 *\n",
      "Epoch: 24, Train_Loss: 0.18841823935508728, Test_Loss: 0.15890516340732574 *\n",
      "Epoch: 24, Train_Loss: 0.2476334422826767, Test_Loss: 0.2161649465560913\n",
      "Epoch: 24, Train_Loss: 0.2588774263858795, Test_Loss: 0.39333486557006836\n",
      "Epoch: 24, Train_Loss: 0.18346214294433594, Test_Loss: 0.220597043633461 *\n",
      "Epoch: 24, Train_Loss: 0.15844431519508362, Test_Loss: 0.16876643896102905 *\n",
      "Epoch: 24, Train_Loss: 0.16178978979587555, Test_Loss: 0.259736567735672\n",
      "Epoch: 24, Train_Loss: 0.39458101987838745, Test_Loss: 0.21596527099609375 *\n",
      "Epoch: 24, Train_Loss: 4.374339580535889, Test_Loss: 0.15850086510181427 *\n",
      "Epoch: 24, Train_Loss: 0.165498748421669, Test_Loss: 0.1863020360469818\n",
      "Epoch: 24, Train_Loss: 0.16031914949417114, Test_Loss: 0.17175358533859253 *\n",
      "Epoch: 24, Train_Loss: 0.16538572311401367, Test_Loss: 0.16585272550582886 *\n",
      "Epoch: 24, Train_Loss: 0.16116009652614594, Test_Loss: 0.21944469213485718\n",
      "Epoch: 24, Train_Loss: 0.15873093903064728, Test_Loss: 0.3975663185119629\n",
      "Epoch: 24, Train_Loss: 0.15880148112773895, Test_Loss: 0.23921480774879456 *\n",
      "Epoch: 24, Train_Loss: 0.16365273296833038, Test_Loss: 0.26476994156837463\n",
      "Epoch: 24, Train_Loss: 0.1700238585472107, Test_Loss: 0.19694072008132935 *\n",
      "Epoch: 24, Train_Loss: 0.18605566024780273, Test_Loss: 0.19040906429290771 *\n",
      "Epoch: 24, Train_Loss: 0.17427262663841248, Test_Loss: 0.22020182013511658\n",
      "Epoch: 24, Train_Loss: 0.15875060856342316, Test_Loss: 0.2714955806732178\n",
      "Epoch: 24, Train_Loss: 0.15934360027313232, Test_Loss: 0.25625473260879517 *\n",
      "Epoch: 24, Train_Loss: 0.1803554743528366, Test_Loss: 0.28729933500289917\n",
      "Epoch: 24, Train_Loss: 0.16050297021865845, Test_Loss: 0.22235113382339478 *\n",
      "Epoch: 24, Train_Loss: 0.16041837632656097, Test_Loss: 0.2189505398273468 *\n",
      "Epoch: 24, Train_Loss: 0.18017549812793732, Test_Loss: 0.31341078877449036\n",
      "Epoch: 24, Train_Loss: 0.18467190861701965, Test_Loss: 0.24118074774742126 *\n",
      "Epoch: 24, Train_Loss: 0.17341431975364685, Test_Loss: 0.17450180649757385 *\n",
      "Epoch: 24, Train_Loss: 0.15970104932785034, Test_Loss: 0.1885487586259842\n",
      "Epoch: 24, Train_Loss: 0.17178231477737427, Test_Loss: 0.16670483350753784 *\n",
      "Epoch: 24, Train_Loss: 0.2599756121635437, Test_Loss: 0.19989559054374695\n",
      "Epoch: 24, Train_Loss: 0.2735055983066559, Test_Loss: 0.20073261857032776\n",
      "Epoch: 24, Train_Loss: 0.23797279596328735, Test_Loss: 0.3128238022327423\n",
      "Epoch: 24, Train_Loss: 0.20780541002750397, Test_Loss: 0.24228957295417786 *\n",
      "Epoch: 24, Train_Loss: 0.19658178091049194, Test_Loss: 0.25953149795532227\n",
      "Epoch: 24, Train_Loss: 0.19027966260910034, Test_Loss: 0.19728074967861176 *\n",
      "Epoch: 24, Train_Loss: 0.19950054585933685, Test_Loss: 0.17578169703483582 *\n",
      "Epoch: 24, Train_Loss: 0.16713561117649078, Test_Loss: 0.16785374283790588 *\n",
      "Epoch: 24, Train_Loss: 0.5011506080627441, Test_Loss: 0.2375119924545288\n",
      "Epoch: 24, Train_Loss: 0.22474411129951477, Test_Loss: 0.43512243032455444\n",
      "Epoch: 24, Train_Loss: 0.16394498944282532, Test_Loss: 0.4649730324745178\n",
      "Epoch: 24, Train_Loss: 0.15693965554237366, Test_Loss: 0.30678221583366394 *\n",
      "Epoch: 24, Train_Loss: 0.15707136690616608, Test_Loss: 0.21018190681934357 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.15708085894584656, Test_Loss: 0.2436145544052124\n",
      "Epoch: 24, Train_Loss: 0.1571662425994873, Test_Loss: 0.1645176112651825 *\n",
      "Epoch: 24, Train_Loss: 0.3951092064380646, Test_Loss: 0.16312801837921143 *\n",
      "Epoch: 24, Train_Loss: 4.432866096496582, Test_Loss: 0.17038486897945404\n",
      "Epoch: 24, Train_Loss: 0.18820419907569885, Test_Loss: 0.18035179376602173\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.16515059769153595, Test_Loss: 0.18775352835655212\n",
      "Epoch: 24, Train_Loss: 0.1669834852218628, Test_Loss: 0.16281764209270477 *\n",
      "Epoch: 24, Train_Loss: 0.15743117034435272, Test_Loss: 0.28157690167427063\n",
      "Epoch: 24, Train_Loss: 0.1593160480260849, Test_Loss: 0.5330826044082642\n",
      "Epoch: 24, Train_Loss: 0.15912121534347534, Test_Loss: 0.2503332197666168 *\n",
      "Epoch: 24, Train_Loss: 0.1578720360994339, Test_Loss: 0.3389683961868286\n",
      "Epoch: 24, Train_Loss: 0.15900053083896637, Test_Loss: 0.1796574592590332 *\n",
      "Epoch: 24, Train_Loss: 0.15906132757663727, Test_Loss: 0.18069741129875183\n",
      "Epoch: 24, Train_Loss: 0.1955089420080185, Test_Loss: 0.18111830949783325\n",
      "Epoch: 24, Train_Loss: 0.1907961517572403, Test_Loss: 0.18051470816135406 *\n",
      "Epoch: 24, Train_Loss: 0.20722709596157074, Test_Loss: 0.19386866688728333\n",
      "Epoch: 24, Train_Loss: 0.1831096112728119, Test_Loss: 4.98378324508667\n",
      "Epoch: 24, Train_Loss: 0.16504789888858795, Test_Loss: 0.6221393346786499 *\n",
      "Epoch: 24, Train_Loss: 0.338077574968338, Test_Loss: 0.18326425552368164 *\n",
      "Epoch: 24, Train_Loss: 0.40629273653030396, Test_Loss: 0.1808461993932724 *\n",
      "Epoch: 24, Train_Loss: 0.40650811791419983, Test_Loss: 0.17923809587955475 *\n",
      "Epoch: 24, Train_Loss: 0.35397613048553467, Test_Loss: 0.1794404536485672\n",
      "Epoch: 24, Train_Loss: 0.1590692698955536, Test_Loss: 0.16877438127994537 *\n",
      "Epoch: 24, Train_Loss: 0.15917910635471344, Test_Loss: 0.15810202062129974 *\n",
      "Epoch: 24, Train_Loss: 0.1641133725643158, Test_Loss: 0.1582307368516922\n",
      "Epoch: 24, Train_Loss: 0.1693398803472519, Test_Loss: 0.1581754833459854 *\n",
      "Epoch: 24, Train_Loss: 0.1749744564294815, Test_Loss: 0.15830224752426147\n",
      "Epoch: 24, Train_Loss: 0.17794407904148102, Test_Loss: 0.1609385758638382\n",
      "Epoch: 24, Train_Loss: 0.16603678464889526, Test_Loss: 0.1694253385066986\n",
      "Epoch: 24, Train_Loss: 0.1606106013059616, Test_Loss: 0.1858213245868683\n",
      "Epoch: 24, Train_Loss: 0.1732928454875946, Test_Loss: 0.17908719182014465 *\n",
      "Epoch: 24, Train_Loss: 0.19082733988761902, Test_Loss: 0.1574440449476242 *\n",
      "Epoch: 24, Train_Loss: 0.35526803135871887, Test_Loss: 0.1589835286140442\n",
      "Epoch: 24, Train_Loss: 0.26165056228637695, Test_Loss: 0.15855588018894196 *\n",
      "Epoch: 24, Train_Loss: 0.2346092313528061, Test_Loss: 0.16330654919147491\n",
      "Epoch: 24, Train_Loss: 0.18865200877189636, Test_Loss: 0.15748071670532227 *\n",
      "Epoch: 24, Train_Loss: 0.25812599062919617, Test_Loss: 0.15767082571983337\n",
      "Epoch: 24, Train_Loss: 0.2347651720046997, Test_Loss: 0.15681202709674835 *\n",
      "Epoch: 24, Train_Loss: 0.22344240546226501, Test_Loss: 0.1579141467809677\n",
      "Epoch: 24, Train_Loss: 0.22757095098495483, Test_Loss: 0.15956750512123108\n",
      "Epoch: 24, Train_Loss: 0.32925477623939514, Test_Loss: 0.17269454896450043\n",
      "Epoch: 24, Train_Loss: 0.17677854001522064, Test_Loss: 0.15887749195098877 *\n",
      "Epoch: 24, Train_Loss: 0.16256973147392273, Test_Loss: 0.1576782464981079 *\n",
      "Epoch: 24, Train_Loss: 2.5160932540893555, Test_Loss: 0.1660444140434265\n",
      "Epoch: 24, Train_Loss: 0.6801202893257141, Test_Loss: 0.16161076724529266 *\n",
      "Epoch: 24, Train_Loss: 0.2132265418767929, Test_Loss: 0.158143550157547 *\n",
      "Epoch: 24, Train_Loss: 0.21922828257083893, Test_Loss: 0.2341819554567337\n",
      "Epoch: 24, Train_Loss: 0.18628215789794922, Test_Loss: 0.7141106724739075\n",
      "Epoch: 24, Train_Loss: 0.18763241171836853, Test_Loss: 4.652085304260254\n",
      "Epoch: 24, Train_Loss: 0.15856273472309113, Test_Loss: 0.16700558364391327 *\n",
      "Epoch: 24, Train_Loss: 0.18986952304840088, Test_Loss: 0.16274550557136536 *\n",
      "Epoch: 24, Train_Loss: 0.2373315691947937, Test_Loss: 0.19529882073402405\n",
      "Epoch: 24, Train_Loss: 0.2039707452058792, Test_Loss: 0.17306147515773773 *\n",
      "Epoch: 24, Train_Loss: 0.17423665523529053, Test_Loss: 0.2078944742679596\n",
      "Epoch: 24, Train_Loss: 0.16272838413715363, Test_Loss: 0.163570374250412 *\n",
      "Epoch: 24, Train_Loss: 0.17338846623897552, Test_Loss: 0.23818473517894745\n",
      "Epoch: 24, Train_Loss: 0.1584123969078064, Test_Loss: 0.1847487837076187 *\n",
      "Epoch: 24, Train_Loss: 0.17164960503578186, Test_Loss: 0.16125254333019257 *\n",
      "Epoch: 24, Train_Loss: 0.2100706696510315, Test_Loss: 0.1807720810174942\n",
      "Epoch: 24, Train_Loss: 0.1840689778327942, Test_Loss: 0.19342349469661713\n",
      "Epoch: 24, Train_Loss: 0.16283729672431946, Test_Loss: 0.16231930255889893 *\n",
      "Epoch: 24, Train_Loss: 0.16243916749954224, Test_Loss: 0.21192291378974915\n",
      "Epoch: 24, Train_Loss: 0.17374871671199799, Test_Loss: 0.19437670707702637 *\n",
      "Epoch: 24, Train_Loss: 0.17576374113559723, Test_Loss: 0.21241194009780884\n",
      "Epoch: 24, Train_Loss: 0.16211333870887756, Test_Loss: 0.18524451553821564 *\n",
      "Epoch: 24, Train_Loss: 0.15642136335372925, Test_Loss: 0.21317078173160553\n",
      "Epoch: 24, Train_Loss: 0.1562940925359726, Test_Loss: 0.2283962368965149\n",
      "Epoch: 24, Train_Loss: 0.1561238169670105, Test_Loss: 0.16147851943969727 *\n",
      "Epoch: 24, Train_Loss: 0.16694380342960358, Test_Loss: 0.16269096732139587\n",
      "Epoch: 24, Train_Loss: 0.1570451855659485, Test_Loss: 0.16696758568286896\n",
      "Epoch: 24, Train_Loss: 0.15729881823062897, Test_Loss: 0.16417476534843445 *\n",
      "Epoch: 24, Train_Loss: 0.15894652903079987, Test_Loss: 0.1593516767024994 *\n",
      "Epoch: 24, Train_Loss: 0.15652576088905334, Test_Loss: 0.16480663418769836\n",
      "Epoch: 24, Train_Loss: 0.1568623036146164, Test_Loss: 0.16130848228931427 *\n",
      "Epoch: 24, Train_Loss: 0.15829995274543762, Test_Loss: 0.15921735763549805 *\n",
      "Epoch: 24, Train_Loss: 0.17138759791851044, Test_Loss: 0.16948343813419342\n",
      "Epoch: 24, Train_Loss: 0.16782765090465546, Test_Loss: 0.1669618785381317 *\n",
      "Epoch: 24, Train_Loss: 0.1651342362165451, Test_Loss: 0.1758934110403061\n",
      "Epoch: 24, Train_Loss: 0.18490266799926758, Test_Loss: 0.23584312200546265\n",
      "Epoch: 24, Train_Loss: 0.18096883594989777, Test_Loss: 0.17849449813365936 *\n",
      "Epoch: 24, Train_Loss: 0.16894663870334625, Test_Loss: 0.45480161905288696\n",
      "Epoch: 24, Train_Loss: 0.16394908726215363, Test_Loss: 0.40608006715774536 *\n",
      "Epoch: 24, Train_Loss: 0.17743387818336487, Test_Loss: 0.2751716077327728 *\n",
      "Epoch: 24, Train_Loss: 0.18408243358135223, Test_Loss: 0.17905178666114807 *\n",
      "Epoch: 24, Train_Loss: 0.16237497329711914, Test_Loss: 0.1750650703907013 *\n",
      "Epoch: 24, Train_Loss: 0.1656845211982727, Test_Loss: 0.16542556881904602 *\n",
      "Epoch: 24, Train_Loss: 0.1627453863620758, Test_Loss: 0.2189616560935974\n",
      "Epoch: 24, Train_Loss: 0.16956479847431183, Test_Loss: 0.2910047173500061\n",
      "Epoch: 24, Train_Loss: 0.2093592882156372, Test_Loss: 0.4454524517059326\n",
      "Epoch: 24, Train_Loss: 0.2106766551733017, Test_Loss: 0.22120648622512817 *\n",
      "Epoch: 24, Train_Loss: 0.17348039150238037, Test_Loss: 0.22046756744384766 *\n",
      "Epoch: 24, Train_Loss: 0.15579338371753693, Test_Loss: 0.1583475023508072 *\n",
      "Epoch: 24, Train_Loss: 0.20231539011001587, Test_Loss: 0.15998272597789764\n",
      "Epoch: 24, Train_Loss: 0.171573668718338, Test_Loss: 0.16457045078277588\n",
      "Epoch: 24, Train_Loss: 0.161055326461792, Test_Loss: 0.1658841371536255\n",
      "Epoch: 24, Train_Loss: 0.16380062699317932, Test_Loss: 0.183503195643425\n",
      "Epoch: 24, Train_Loss: 0.17921751737594604, Test_Loss: 0.1743001937866211 *\n",
      "Epoch: 24, Train_Loss: 0.24599885940551758, Test_Loss: 0.16500291228294373 *\n",
      "Epoch: 24, Train_Loss: 0.19900618493556976, Test_Loss: 0.2648019790649414\n",
      "Epoch: 24, Train_Loss: 0.17997127771377563, Test_Loss: 0.5117735266685486\n",
      "Epoch: 24, Train_Loss: 0.16894933581352234, Test_Loss: 0.3056122362613678 *\n",
      "Epoch: 24, Train_Loss: 0.16920407116413116, Test_Loss: 0.233656644821167 *\n",
      "Epoch: 24, Train_Loss: 0.17030052840709686, Test_Loss: 0.1710282564163208 *\n",
      "Epoch: 24, Train_Loss: 0.1594669669866562, Test_Loss: 0.17120620608329773\n",
      "Epoch: 24, Train_Loss: 0.17409908771514893, Test_Loss: 0.17159904539585114\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.16229715943336487, Test_Loss: 0.17144079506397247 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.16434134542942047, Test_Loss: 0.27246513962745667\n",
      "Epoch: 24, Train_Loss: 0.239101380109787, Test_Loss: 5.457823753356934\n",
      "Epoch: 24, Train_Loss: 0.16075634956359863, Test_Loss: 0.278339147567749 *\n",
      "Epoch: 24, Train_Loss: 0.20777550339698792, Test_Loss: 0.16783300042152405 *\n",
      "Epoch: 24, Train_Loss: 0.17326635122299194, Test_Loss: 0.16822834312915802\n",
      "Epoch: 24, Train_Loss: 0.18172475695610046, Test_Loss: 0.16720345616340637 *\n",
      "Epoch: 24, Train_Loss: 0.2556724548339844, Test_Loss: 0.1644919067621231 *\n",
      "Epoch: 24, Train_Loss: 0.3209836781024933, Test_Loss: 0.1646595299243927\n",
      "Epoch: 24, Train_Loss: 0.1605333685874939, Test_Loss: 0.16535331308841705\n",
      "Epoch: 24, Train_Loss: 0.18514467775821686, Test_Loss: 0.1591065526008606 *\n",
      "Epoch: 24, Train_Loss: 0.15555880963802338, Test_Loss: 0.15772268176078796 *\n",
      "Epoch: 24, Train_Loss: 0.15566083788871765, Test_Loss: 0.16060039401054382\n",
      "Epoch: 24, Train_Loss: 0.16130636632442474, Test_Loss: 0.17950834333896637\n",
      "Epoch: 24, Train_Loss: 0.15939533710479736, Test_Loss: 0.16592615842819214 *\n",
      "Epoch: 24, Train_Loss: 0.15823039412498474, Test_Loss: 0.16538621485233307 *\n",
      "Epoch: 24, Train_Loss: 0.16246654093265533, Test_Loss: 0.18371710181236267\n",
      "Epoch: 24, Train_Loss: 0.163031205534935, Test_Loss: 0.15711160004138947 *\n",
      "Epoch: 24, Train_Loss: 0.15913663804531097, Test_Loss: 0.15815523266792297\n",
      "Epoch: 24, Train_Loss: 0.16153503954410553, Test_Loss: 0.15871727466583252\n",
      "Epoch: 24, Train_Loss: 0.16148635745048523, Test_Loss: 0.16661657392978668\n",
      "Epoch: 24, Train_Loss: 0.1570436954498291, Test_Loss: 0.1567244529724121 *\n",
      "Epoch: 24, Train_Loss: 0.15635810792446136, Test_Loss: 0.15904845297336578\n",
      "Epoch: 24, Train_Loss: 0.17121927440166473, Test_Loss: 0.15560446679592133 *\n",
      "Epoch: 24, Train_Loss: 0.16510708630084991, Test_Loss: 0.15878421068191528\n",
      "Epoch: 24, Train_Loss: 0.18479110300540924, Test_Loss: 0.1613641232252121\n",
      "Epoch: 24, Train_Loss: 0.16037769615650177, Test_Loss: 0.1709088236093521\n",
      "Epoch: 24, Train_Loss: 0.16149254143238068, Test_Loss: 0.15813294053077698 *\n",
      "Epoch: 24, Train_Loss: 0.1681526005268097, Test_Loss: 0.15817680954933167\n",
      "Epoch: 24, Train_Loss: 0.17907321453094482, Test_Loss: 0.16568350791931152\n",
      "Epoch: 24, Train_Loss: 0.16318222880363464, Test_Loss: 0.16010206937789917 *\n",
      "Epoch: 24, Train_Loss: 0.18433983623981476, Test_Loss: 0.16071029007434845\n",
      "Epoch: 24, Train_Loss: 0.15558533370494843, Test_Loss: 0.23824958503246307\n",
      "Epoch: 24, Train_Loss: 0.17053082585334778, Test_Loss: 1.9224698543548584\n",
      "Epoch: 24, Train_Loss: 0.17042744159698486, Test_Loss: 3.731802225112915\n",
      "Epoch: 24, Train_Loss: 0.1823682337999344, Test_Loss: 0.17500099539756775 *\n",
      "Epoch: 24, Train_Loss: 1.7354366779327393, Test_Loss: 0.1560508757829666 *\n",
      "Epoch: 24, Train_Loss: 3.8832125663757324, Test_Loss: 0.17862096428871155\n",
      "Epoch: 24, Train_Loss: 0.24833634495735168, Test_Loss: 0.16409975290298462 *\n",
      "Epoch: 24, Train_Loss: 0.16908490657806396, Test_Loss: 0.18649835884571075\n",
      "Epoch: 24, Train_Loss: 0.16528072953224182, Test_Loss: 0.19589172303676605\n",
      "Epoch: 24, Train_Loss: 0.21873445808887482, Test_Loss: 0.25583744049072266\n",
      "Epoch: 24, Train_Loss: 0.192667618393898, Test_Loss: 0.16440927982330322 *\n",
      "Epoch: 24, Train_Loss: 0.16532346606254578, Test_Loss: 0.1714148223400116\n",
      "Epoch: 24, Train_Loss: 0.15525884926319122, Test_Loss: 0.18389376997947693\n",
      "Epoch: 24, Train_Loss: 0.22206707298755646, Test_Loss: 0.17000772058963776 *\n",
      "Epoch: 24, Train_Loss: 0.18575677275657654, Test_Loss: 0.16581259667873383 *\n",
      "Epoch: 24, Train_Loss: 0.16410315036773682, Test_Loss: 0.1982842981815338\n",
      "Epoch: 24, Train_Loss: 0.4974461793899536, Test_Loss: 0.19152770936489105 *\n",
      "Epoch: 24, Train_Loss: 0.4688475728034973, Test_Loss: 0.23311138153076172\n",
      "Epoch: 24, Train_Loss: 0.788590669631958, Test_Loss: 0.1881888210773468 *\n",
      "Epoch: 24, Train_Loss: 0.22492516040802002, Test_Loss: 0.18827371299266815\n",
      "Epoch: 24, Train_Loss: 0.39072585105895996, Test_Loss: 0.18701721727848053 *\n",
      "Epoch: 24, Train_Loss: 1.0051615238189697, Test_Loss: 0.1563548892736435 *\n",
      "Epoch: 24, Train_Loss: 0.6517778635025024, Test_Loss: 0.16125276684761047\n",
      "Epoch: 24, Train_Loss: 0.16024024784564972, Test_Loss: 0.16786834597587585\n",
      "Epoch: 24, Train_Loss: 0.1645941436290741, Test_Loss: 0.16773176193237305 *\n",
      "Epoch: 24, Train_Loss: 0.6161396503448486, Test_Loss: 0.16554246842861176 *\n",
      "Epoch: 24, Train_Loss: 0.47214481234550476, Test_Loss: 0.1606808602809906 *\n",
      "Epoch: 24, Train_Loss: 0.6205354332923889, Test_Loss: 0.16654770076274872\n",
      "Epoch: 24, Train_Loss: 0.16139407455921173, Test_Loss: 0.17252449691295624\n",
      "Epoch: 24, Train_Loss: 0.1701507866382599, Test_Loss: 0.15856754779815674 *\n",
      "Epoch: 24, Train_Loss: 0.3881661891937256, Test_Loss: 0.1731315404176712\n",
      "Epoch: 24, Train_Loss: 0.35530227422714233, Test_Loss: 0.16112680733203888 *\n",
      "Epoch: 24, Train_Loss: 0.219536691904068, Test_Loss: 0.18622465431690216\n",
      "Epoch: 24, Train_Loss: 0.24808764457702637, Test_Loss: 0.19360168278217316\n",
      "Epoch: 24, Train_Loss: 0.18647749722003937, Test_Loss: 0.41730523109436035\n",
      "Epoch: 24, Train_Loss: 0.1958697885274887, Test_Loss: 0.34678032994270325 *\n",
      "Epoch: 24, Train_Loss: 0.2700195908546448, Test_Loss: 0.23192423582077026 *\n",
      "Epoch: 24, Train_Loss: 0.25960469245910645, Test_Loss: 0.189531609416008 *\n",
      "Epoch: 24, Train_Loss: 0.20637433230876923, Test_Loss: 0.19037701189517975\n",
      "Epoch: 24, Train_Loss: 0.2522434890270233, Test_Loss: 0.16429626941680908 *\n",
      "Epoch: 24, Train_Loss: 0.20478758215904236, Test_Loss: 0.21625542640686035\n",
      "Epoch: 24, Train_Loss: 0.21811944246292114, Test_Loss: 0.22600163519382477\n",
      "Epoch: 24, Train_Loss: 0.32192760705947876, Test_Loss: 0.43844711780548096\n",
      "Epoch: 24, Train_Loss: 0.37458983063697815, Test_Loss: 0.24198827147483826 *\n",
      "Epoch: 24, Train_Loss: 0.1823997050523758, Test_Loss: 0.26014167070388794\n",
      "Epoch: 24, Train_Loss: 0.20660051703453064, Test_Loss: 0.17234942317008972 *\n",
      "Epoch: 24, Train_Loss: 0.19795894622802734, Test_Loss: 0.1583513766527176 *\n",
      "Epoch: 24, Train_Loss: 0.1725165992975235, Test_Loss: 0.16523805260658264\n",
      "Epoch: 24, Train_Loss: 0.15533655881881714, Test_Loss: 0.21094904839992523\n",
      "Epoch: 24, Train_Loss: 0.15850195288658142, Test_Loss: 0.1717654913663864 *\n",
      "Epoch: 24, Train_Loss: 0.15890145301818848, Test_Loss: 0.17956162989139557\n",
      "Epoch: 24, Train_Loss: 0.16119983792304993, Test_Loss: 0.20524008572101593\n",
      "Epoch: 24, Train_Loss: 0.16755759716033936, Test_Loss: 0.3174416422843933\n",
      "Epoch: 24, Train_Loss: 0.17426449060440063, Test_Loss: 0.4404789209365845\n",
      "Epoch: 24, Train_Loss: 0.16818267107009888, Test_Loss: 0.4735602140426636\n",
      "Epoch: 24, Train_Loss: 0.2187425047159195, Test_Loss: 0.28177204728126526 *\n",
      "Epoch: 24, Train_Loss: 0.3047196865081787, Test_Loss: 0.27671507000923157 *\n",
      "Epoch: 24, Train_Loss: 0.2515847682952881, Test_Loss: 0.2721468508243561 *\n",
      "Epoch: 24, Train_Loss: 0.1699659675359726, Test_Loss: 0.2738593816757202\n",
      "Epoch: 24, Train_Loss: 0.19518721103668213, Test_Loss: 0.306723028421402\n",
      "Epoch: 24, Train_Loss: 0.2578414976596832, Test_Loss: 0.7803198099136353\n",
      "Epoch: 24, Train_Loss: 0.27743855118751526, Test_Loss: 4.919793605804443\n",
      "Epoch: 24, Train_Loss: 0.20614400506019592, Test_Loss: 0.21189555525779724 *\n",
      "Epoch: 24, Train_Loss: 0.22227944433689117, Test_Loss: 0.17451515793800354 *\n",
      "Epoch: 24, Train_Loss: 0.22057756781578064, Test_Loss: 0.19690462946891785\n",
      "Epoch: 24, Train_Loss: 0.21518594026565552, Test_Loss: 0.15991920232772827 *\n",
      "Epoch: 24, Train_Loss: 0.24908539652824402, Test_Loss: 0.1652136892080307\n",
      "Epoch: 24, Train_Loss: 0.1721993088722229, Test_Loss: 0.20513728260993958\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.20890262722969055, Test_Loss: 0.20694908499717712\n",
      "Epoch: 24, Train_Loss: 0.483009934425354, Test_Loss: 0.16481724381446838 *\n",
      "Epoch: 24, Train_Loss: 0.4198977053165436, Test_Loss: 0.1636086106300354 *\n",
      "Epoch: 24, Train_Loss: 0.2535736560821533, Test_Loss: 0.17751790583133698\n",
      "Epoch: 24, Train_Loss: 0.1993112564086914, Test_Loss: 0.2769961357116699\n",
      "Epoch: 24, Train_Loss: 0.17660078406333923, Test_Loss: 0.1714611053466797 *\n",
      "Epoch: 24, Train_Loss: 0.16417133808135986, Test_Loss: 0.1592818647623062 *\n",
      "Epoch: 24, Train_Loss: 0.4244239926338196, Test_Loss: 0.18405482172966003\n",
      "Epoch: 24, Train_Loss: 0.1897066980600357, Test_Loss: 0.18292884528636932 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.2102571725845337, Test_Loss: 0.1703878939151764 *\n",
      "Epoch: 24, Train_Loss: 0.2899421453475952, Test_Loss: 0.22642986476421356\n",
      "Epoch: 24, Train_Loss: 0.1993204951286316, Test_Loss: 0.18144603073596954 *\n",
      "Epoch: 24, Train_Loss: 0.17802882194519043, Test_Loss: 0.16009874641895294 *\n",
      "Epoch: 24, Train_Loss: 0.22215057909488678, Test_Loss: 0.1799539178609848\n",
      "Epoch: 24, Train_Loss: 0.31696751713752747, Test_Loss: 0.15718895196914673 *\n",
      "Epoch: 24, Train_Loss: 0.18976113200187683, Test_Loss: 0.1699085533618927\n",
      "Epoch: 24, Train_Loss: 0.2261621654033661, Test_Loss: 0.1729251742362976\n",
      "Epoch: 24, Train_Loss: 0.18593458831310272, Test_Loss: 0.16991332173347473 *\n",
      "Epoch: 24, Train_Loss: 0.2148992419242859, Test_Loss: 0.1617489755153656 *\n",
      "Epoch: 24, Train_Loss: 0.2055211365222931, Test_Loss: 0.1880284547805786\n",
      "Epoch: 24, Train_Loss: 0.18009236454963684, Test_Loss: 0.19134710729122162\n",
      "Epoch: 24, Train_Loss: 0.1634349822998047, Test_Loss: 0.17610472440719604 *\n",
      "Epoch: 24, Train_Loss: 0.17785508930683136, Test_Loss: 0.17343711853027344 *\n",
      "Epoch: 24, Train_Loss: 0.45558810234069824, Test_Loss: 0.2608111500740051\n",
      "Epoch: 24, Train_Loss: 0.4486401081085205, Test_Loss: 2.9475181102752686\n",
      "Epoch: 24, Train_Loss: 0.5097835063934326, Test_Loss: 2.6471710205078125 *\n",
      "Epoch: 24, Train_Loss: 0.5181560516357422, Test_Loss: 0.17482981085777283 *\n",
      "Epoch: 24, Train_Loss: 0.3653731346130371, Test_Loss: 0.16217052936553955 *\n",
      "Epoch: 24, Train_Loss: 0.23657941818237305, Test_Loss: 0.1632208377122879\n",
      "Epoch: 24, Train_Loss: 0.21795588731765747, Test_Loss: 0.16177619993686676 *\n",
      "Epoch: 24, Train_Loss: 0.16575628519058228, Test_Loss: 0.17345373332500458\n",
      "Epoch: 24, Train_Loss: 0.1628723293542862, Test_Loss: 0.21537964046001434\n",
      "Epoch: 24, Train_Loss: 0.19122442603111267, Test_Loss: 0.2114793211221695 *\n",
      "Epoch: 24, Train_Loss: 0.39125925302505493, Test_Loss: 0.15671339631080627 *\n",
      "Epoch: 24, Train_Loss: 0.4120587110519409, Test_Loss: 0.18491633236408234\n",
      "Epoch: 24, Train_Loss: 0.5256317853927612, Test_Loss: 0.17981038987636566 *\n",
      "Epoch: 24, Train_Loss: 0.8838827610015869, Test_Loss: 0.1859041154384613\n",
      "Epoch: 24, Train_Loss: 0.583594024181366, Test_Loss: 0.16441646218299866 *\n",
      "Epoch: 24, Train_Loss: 0.38570934534072876, Test_Loss: 0.20762598514556885\n",
      "Epoch: 24, Train_Loss: 0.17177121341228485, Test_Loss: 0.21068991720676422\n",
      "Epoch: 24, Train_Loss: 0.16241994500160217, Test_Loss: 0.22930613160133362\n",
      "Epoch: 24, Train_Loss: 0.37985920906066895, Test_Loss: 0.16875708103179932 *\n",
      "Epoch: 24, Train_Loss: 0.35562050342559814, Test_Loss: 0.1941760778427124\n",
      "Epoch: 24, Train_Loss: 0.44694983959198, Test_Loss: 0.17810480296611786 *\n",
      "Epoch: 24, Train_Loss: 0.20109526813030243, Test_Loss: 0.19395892322063446\n",
      "Epoch: 24, Train_Loss: 0.16722843050956726, Test_Loss: 0.2531301975250244\n",
      "Epoch: 24, Train_Loss: 0.22771885991096497, Test_Loss: 0.30918800830841064\n",
      "Epoch: 24, Train_Loss: 0.4153937101364136, Test_Loss: 0.26859164237976074 *\n",
      "Epoch: 24, Train_Loss: 0.31003421545028687, Test_Loss: 0.2512340545654297 *\n",
      "Epoch: 24, Train_Loss: 0.21623195707798004, Test_Loss: 0.2680095136165619\n",
      "Epoch: 24, Train_Loss: 0.30755484104156494, Test_Loss: 0.24167394638061523 *\n",
      "Epoch: 24, Train_Loss: 0.16503310203552246, Test_Loss: 0.24140530824661255 *\n",
      "Epoch: 24, Train_Loss: 0.16366317868232727, Test_Loss: 0.17935404181480408 *\n",
      "Epoch: 24, Train_Loss: 0.18881067633628845, Test_Loss: 0.18367990851402283\n",
      "Epoch: 24, Train_Loss: 0.1635218858718872, Test_Loss: 0.15682841837406158 *\n",
      "Epoch: 24, Train_Loss: 0.20100854337215424, Test_Loss: 0.16829773783683777\n",
      "Epoch: 24, Train_Loss: 0.2496483027935028, Test_Loss: 0.20431400835514069\n",
      "Epoch: 24, Train_Loss: 4.692911148071289, Test_Loss: 0.2773262560367584\n",
      "Epoch: 24, Train_Loss: 11.240946769714355, Test_Loss: 0.2916492223739624\n",
      "Epoch: 24, Train_Loss: 0.7052448391914368, Test_Loss: 0.18673694133758545 *\n",
      "Epoch: 24, Train_Loss: 0.4786792993545532, Test_Loss: 0.2223651111125946\n",
      "Epoch: 24, Train_Loss: 0.6031582355499268, Test_Loss: 0.20591402053833008 *\n",
      "Epoch: 24, Train_Loss: 0.20091243088245392, Test_Loss: 0.1782238781452179 *\n",
      "Epoch: 24, Train_Loss: 0.565554678440094, Test_Loss: 0.3220483064651489\n",
      "Epoch: 24, Train_Loss: 3.6381521224975586, Test_Loss: 0.21590353548526764 *\n",
      "Epoch: 24, Train_Loss: 1.9025154113769531, Test_Loss: 0.5297877788543701\n",
      "Epoch: 24, Train_Loss: 0.2469167411327362, Test_Loss: 0.2271871566772461 *\n",
      "Epoch: 24, Train_Loss: 2.054415225982666, Test_Loss: 0.22786089777946472\n",
      "Epoch: 24, Train_Loss: 3.176185369491577, Test_Loss: 0.18509650230407715 *\n",
      "Epoch: 24, Train_Loss: 0.44415780901908875, Test_Loss: 0.18534785509109497\n",
      "Epoch: 24, Train_Loss: 0.18838053941726685, Test_Loss: 0.19278407096862793\n",
      "Epoch: 24, Train_Loss: 0.23491518199443817, Test_Loss: 0.16930915415287018 *\n",
      "Epoch: 24, Train_Loss: 0.22288201749324799, Test_Loss: 0.19358883798122406\n",
      "Epoch: 24, Train_Loss: 0.17668406665325165, Test_Loss: 0.16486719250679016 *\n",
      "Epoch: 24, Train_Loss: 0.1690530776977539, Test_Loss: 0.22655446827411652\n",
      "Epoch: 24, Train_Loss: 0.15380021929740906, Test_Loss: 0.4858098030090332\n",
      "Epoch: 24, Train_Loss: 0.15369273722171783, Test_Loss: 0.3848283886909485 *\n",
      "Epoch: 24, Train_Loss: 0.15481474995613098, Test_Loss: 0.3546380400657654 *\n",
      "Epoch: 24, Train_Loss: 0.16052545607089996, Test_Loss: 0.16979773342609406 *\n",
      "Epoch: 24, Train_Loss: 0.17026062309741974, Test_Loss: 0.1579224318265915 *\n",
      "Epoch: 24, Train_Loss: 0.20586322247982025, Test_Loss: 0.1567244827747345 *\n",
      "Epoch: 24, Train_Loss: 0.17895425856113434, Test_Loss: 0.15725544095039368\n",
      "Epoch: 24, Train_Loss: 0.1633031666278839, Test_Loss: 0.18995651602745056\n",
      "Epoch: 24, Train_Loss: 0.16158485412597656, Test_Loss: 2.694730281829834\n",
      "Epoch: 24, Train_Loss: 0.1649276316165924, Test_Loss: 4.5526580810546875\n",
      "Epoch: 24, Train_Loss: 0.17604896426200867, Test_Loss: 0.21874479949474335 *\n",
      "Epoch: 24, Train_Loss: 0.15621428191661835, Test_Loss: 0.2857307195663452\n",
      "Epoch: 24, Train_Loss: 0.15591783821582794, Test_Loss: 0.42177262902259827\n",
      "Epoch: 24, Train_Loss: 0.15379557013511658, Test_Loss: 0.23892858624458313 *\n",
      "Epoch: 24, Train_Loss: 0.15503230690956116, Test_Loss: 0.3523099422454834\n",
      "Epoch: 24, Train_Loss: 0.1546742171049118, Test_Loss: 0.649709939956665\n",
      "Epoch: 24, Train_Loss: 0.15396882593631744, Test_Loss: 0.5337822437286377 *\n",
      "Epoch: 24, Train_Loss: 0.1537209451198578, Test_Loss: 0.4781811535358429 *\n",
      "Epoch: 24, Train_Loss: 0.15639816224575043, Test_Loss: 0.7238103151321411\n",
      "Epoch: 24, Train_Loss: 0.1749013364315033, Test_Loss: 0.5382291078567505 *\n",
      "Epoch: 24, Train_Loss: 0.177752286195755, Test_Loss: 1.0004291534423828\n",
      "Epoch: 24, Train_Loss: 0.20964692533016205, Test_Loss: 0.41063475608825684 *\n",
      "Epoch: 24, Train_Loss: 0.1818399876356125, Test_Loss: 0.4419856667518616\n",
      "Epoch: 24, Train_Loss: 0.3080483078956604, Test_Loss: 0.39389336109161377 *\n",
      "Epoch: 24, Train_Loss: 5.301253795623779, Test_Loss: 0.175453320145607 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 24\n",
      "Epoch: 24, Train_Loss: 0.8069497346878052, Test_Loss: 0.1647283136844635 *\n",
      "Epoch: 24, Train_Loss: 0.17769698798656464, Test_Loss: 0.16674759984016418\n",
      "Epoch: 24, Train_Loss: 0.1820012778043747, Test_Loss: 0.31993013620376587\n",
      "Epoch: 24, Train_Loss: 0.21232455968856812, Test_Loss: 0.2143058329820633 *\n",
      "Epoch: 24, Train_Loss: 0.1942375749349594, Test_Loss: 0.7455588579177856\n",
      "Epoch: 24, Train_Loss: 0.24864734709262848, Test_Loss: 0.16803669929504395 *\n",
      "Epoch: 24, Train_Loss: 0.21376566588878632, Test_Loss: 0.22766494750976562\n",
      "Epoch: 24, Train_Loss: 0.2580568194389343, Test_Loss: 0.26334670186042786\n",
      "Epoch: 24, Train_Loss: 0.2729019522666931, Test_Loss: 0.2006014734506607 *\n",
      "Epoch: 24, Train_Loss: 0.22963592410087585, Test_Loss: 0.16758641600608826 *\n",
      "Epoch: 24, Train_Loss: 0.16282077133655548, Test_Loss: 0.17540296912193298\n",
      "Epoch: 24, Train_Loss: 0.17822067439556122, Test_Loss: 0.1779215782880783\n",
      "Epoch: 24, Train_Loss: 0.15893183648586273, Test_Loss: 0.1591370850801468 *\n",
      "Epoch: 24, Train_Loss: 0.1853417307138443, Test_Loss: 0.26865148544311523\n",
      "Epoch: 24, Train_Loss: 0.15718494355678558, Test_Loss: 0.37475287914276123\n",
      "Epoch: 24, Train_Loss: 0.17154817283153534, Test_Loss: 5.459713935852051\n",
      "Epoch: 24, Train_Loss: 0.21310633420944214, Test_Loss: 2.9017934799194336 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train_Loss: 0.21874737739562988, Test_Loss: 0.20612776279449463 *\n",
      "Epoch: 24, Train_Loss: 0.2058095782995224, Test_Loss: 0.22286151349544525\n",
      "Epoch: 24, Train_Loss: 0.228785902261734, Test_Loss: 0.26744359731674194\n",
      "Epoch: 24, Train_Loss: 0.18844254314899445, Test_Loss: 0.3081345856189728\n",
      "Epoch: 24, Train_Loss: 0.18470415472984314, Test_Loss: 0.19759921729564667 *\n",
      "Epoch: 24, Train_Loss: 0.1622837483882904, Test_Loss: 0.29381537437438965\n",
      "Epoch: 24, Train_Loss: 0.3267096281051636, Test_Loss: 0.3574362099170685\n",
      "Epoch: 24, Train_Loss: 3.312706470489502, Test_Loss: 0.15671081840991974 *\n",
      "Epoch: 24, Train_Loss: 0.27505093812942505, Test_Loss: 0.18700310587882996\n",
      "Epoch: 24, Train_Loss: 0.16430363059043884, Test_Loss: 0.18458566069602966 *\n",
      "Epoch: 24, Train_Loss: 0.16395790874958038, Test_Loss: 0.1729297935962677 *\n",
      "Epoch: 24, Train_Loss: 0.1566748470067978, Test_Loss: 0.19351543486118317\n",
      "Epoch: 24, Train_Loss: 0.15380901098251343, Test_Loss: 0.3752942681312561\n",
      "Epoch: 24, Train_Loss: 0.15871605277061462, Test_Loss: 0.30008453130722046 *\n",
      "Epoch: 24, Train_Loss: 0.1569671332836151, Test_Loss: 0.27699288725852966 *\n",
      "Epoch: 24, Train_Loss: 0.1741686314344406, Test_Loss: 0.19973911345005035 *\n",
      "Epoch: 24, Train_Loss: 0.16454169154167175, Test_Loss: 0.179756760597229 *\n",
      "Epoch: 24, Train_Loss: 0.16822440922260284, Test_Loss: 0.2560535967350006\n",
      "Epoch: 24, Train_Loss: 0.15432773530483246, Test_Loss: 0.40864676237106323\n",
      "Epoch: 24, Train_Loss: 0.15577802062034607, Test_Loss: 0.37723875045776367 *\n",
      "Epoch: 24, Train_Loss: 0.16616828739643097, Test_Loss: 0.4009886384010315\n",
      "Epoch: 24, Train_Loss: 0.16165967285633087, Test_Loss: 0.3256146311759949 *\n",
      "Epoch: 24, Train_Loss: 0.1577262282371521, Test_Loss: 0.33740174770355225\n",
      "Epoch: 24, Train_Loss: 0.16997064650058746, Test_Loss: 0.4368132948875427\n",
      "Epoch: 24, Train_Loss: 0.17302632331848145, Test_Loss: 0.4233162999153137 *\n",
      "Epoch: 24, Train_Loss: 0.1703244000673294, Test_Loss: 0.44516223669052124\n",
      "Epoch: 24, Train_Loss: 0.15713553130626678, Test_Loss: 0.20146258175373077 *\n",
      "Epoch: 24, Train_Loss: 0.15990346670150757, Test_Loss: 0.1790267378091812 *\n",
      "Epoch: 24, Train_Loss: 0.2477886825799942, Test_Loss: 0.1639731079339981 *\n",
      "Epoch: 24, Train_Loss: 0.24157920479774475, Test_Loss: 0.17694680392742157\n",
      "Epoch: 24, Train_Loss: 0.2116081714630127, Test_Loss: 0.28007280826568604\n",
      "Epoch: 24, Train_Loss: 0.20700600743293762, Test_Loss: 0.233352392911911 *\n",
      "Epoch: 24, Train_Loss: 0.18685214221477509, Test_Loss: 0.288030207157135\n",
      "Epoch: 24, Train_Loss: 0.19015324115753174, Test_Loss: 0.20385466516017914 *\n",
      "Epoch: 24, Train_Loss: 0.17781513929367065, Test_Loss: 0.20233729481697083 *\n",
      "Epoch: 24, Train_Loss: 0.17916180193424225, Test_Loss: 0.17998343706130981 *\n",
      "Epoch: 24, Train_Loss: 0.28018635511398315, Test_Loss: 0.19617024064064026\n",
      "Epoch: 25, Train_Loss: 0.21048934757709503, Test_Loss: 0.4236604869365692 *\n",
      "Epoch: 25, Train_Loss: 0.18604111671447754, Test_Loss: 0.2499854564666748 *\n",
      "Epoch: 25, Train_Loss: 0.15381957590579987, Test_Loss: 0.513480544090271\n",
      "Epoch: 25, Train_Loss: 0.15386390686035156, Test_Loss: 0.21943369507789612 *\n",
      "Epoch: 25, Train_Loss: 0.15392707288265228, Test_Loss: 0.23078805208206177\n",
      "Epoch: 25, Train_Loss: 0.15399175882339478, Test_Loss: 0.16932138800621033 *\n",
      "Epoch: 25, Train_Loss: 0.17185619473457336, Test_Loss: 0.16207145154476166 *\n",
      "Epoch: 25, Train_Loss: 4.246045112609863, Test_Loss: 0.17142856121063232\n",
      "Epoch: 25, Train_Loss: 0.5042847394943237, Test_Loss: 0.17051368951797485 *\n",
      "Epoch: 25, Train_Loss: 0.1558067798614502, Test_Loss: 0.18940234184265137\n",
      "Epoch: 25, Train_Loss: 0.16425760090351105, Test_Loss: 0.15701016783714294 *\n",
      "Epoch: 25, Train_Loss: 0.1566362977027893, Test_Loss: 0.24391482770442963\n",
      "Epoch: 25, Train_Loss: 0.15742227435112, Test_Loss: 0.42769140005111694\n",
      "Epoch: 25, Train_Loss: 0.15587230026721954, Test_Loss: 0.33857035636901855 *\n",
      "Epoch: 25, Train_Loss: 0.15466450154781342, Test_Loss: 0.3917255401611328\n",
      "Epoch: 25, Train_Loss: 0.15580444037914276, Test_Loss: 0.1804647147655487 *\n",
      "Epoch: 25, Train_Loss: 0.15476840734481812, Test_Loss: 0.18177810311317444\n",
      "Epoch: 25, Train_Loss: 0.18052716553211212, Test_Loss: 0.18180562555789948\n",
      "Epoch: 25, Train_Loss: 0.1684994101524353, Test_Loss: 0.1823064684867859\n",
      "Epoch: 25, Train_Loss: 0.18887373805046082, Test_Loss: 0.19624105095863342\n",
      "Epoch: 25, Train_Loss: 0.17933636903762817, Test_Loss: 3.103472948074341\n",
      "Epoch: 25, Train_Loss: 0.1627027988433838, Test_Loss: 2.345250368118286 *\n",
      "Epoch: 25, Train_Loss: 0.245931476354599, Test_Loss: 0.17594747245311737 *\n",
      "Epoch: 25, Train_Loss: 0.3479476273059845, Test_Loss: 0.16998326778411865 *\n",
      "Epoch: 25, Train_Loss: 0.35728615522384644, Test_Loss: 0.16411712765693665 *\n",
      "Epoch: 25, Train_Loss: 0.31511908769607544, Test_Loss: 0.17205935716629028\n",
      "Epoch: 25, Train_Loss: 0.15802836418151855, Test_Loss: 0.1571161150932312 *\n",
      "Epoch: 25, Train_Loss: 0.15392114222049713, Test_Loss: 0.167166069149971\n",
      "Epoch: 25, Train_Loss: 0.1552361696958542, Test_Loss: 0.15812145173549652 *\n",
      "Epoch: 25, Train_Loss: 0.15742428600788116, Test_Loss: 0.15483470261096954 *\n",
      "Epoch: 25, Train_Loss: 0.1600801795721054, Test_Loss: 0.15751981735229492\n",
      "Epoch: 25, Train_Loss: 0.16807064414024353, Test_Loss: 0.154667466878891 *\n",
      "Epoch: 25, Train_Loss: 0.16570653021335602, Test_Loss: 0.19281908869743347\n",
      "Epoch: 25, Train_Loss: 0.1549813449382782, Test_Loss: 0.16886626183986664 *\n",
      "Epoch: 25, Train_Loss: 0.16631808876991272, Test_Loss: 0.17126308381557465\n",
      "Epoch: 25, Train_Loss: 0.17787864804267883, Test_Loss: 0.15550604462623596 *\n",
      "Epoch: 25, Train_Loss: 0.2717004716396332, Test_Loss: 0.15584680438041687\n",
      "Epoch: 25, Train_Loss: 0.21642982959747314, Test_Loss: 0.15727518498897552\n",
      "Epoch: 25, Train_Loss: 0.1680978238582611, Test_Loss: 0.1759966015815735\n",
      "Epoch: 25, Train_Loss: 0.22486120462417603, Test_Loss: 0.1666216105222702 *\n",
      "Epoch: 25, Train_Loss: 0.25332218408584595, Test_Loss: 0.15512266755104065 *\n",
      "Epoch: 25, Train_Loss: 0.24191202223300934, Test_Loss: 0.15497557818889618 *\n",
      "Epoch: 25, Train_Loss: 0.18124127388000488, Test_Loss: 0.15814447402954102\n",
      "Epoch: 25, Train_Loss: 0.26031196117401123, Test_Loss: 0.16898716986179352\n",
      "Epoch: 25, Train_Loss: 0.21889516711235046, Test_Loss: 0.20662792026996613\n",
      "Epoch: 25, Train_Loss: 0.35527944564819336, Test_Loss: 0.1633753925561905 *\n",
      "Epoch: 25, Train_Loss: 0.16064901649951935, Test_Loss: 0.1554122418165207 *\n",
      "Epoch: 25, Train_Loss: 1.5225387811660767, Test_Loss: 0.16482265293598175\n",
      "Epoch: 25, Train_Loss: 1.49154794216156, Test_Loss: 0.1610894352197647 *\n",
      "Epoch: 25, Train_Loss: 0.21292729675769806, Test_Loss: 0.15369628369808197 *\n",
      "Epoch: 25, Train_Loss: 0.18665574491024017, Test_Loss: 0.2538021504878998\n",
      "Epoch: 25, Train_Loss: 0.16081452369689941, Test_Loss: 0.21199823915958405 *\n",
      "Epoch: 25, Train_Loss: 0.17689447104930878, Test_Loss: 4.63915491104126\n",
      "Epoch: 25, Train_Loss: 0.15872721374034882, Test_Loss: 0.5281506776809692 *\n",
      "Epoch: 25, Train_Loss: 0.18448419868946075, Test_Loss: 0.15881717205047607 *\n",
      "Epoch: 25, Train_Loss: 0.2509932518005371, Test_Loss: 0.17907632887363434\n",
      "Epoch: 25, Train_Loss: 0.19961446523666382, Test_Loss: 0.16631142795085907 *\n",
      "Epoch: 25, Train_Loss: 0.16902756690979004, Test_Loss: 0.17560666799545288\n",
      "Epoch: 25, Train_Loss: 0.16979889571666718, Test_Loss: 0.16062456369400024 *\n",
      "Epoch: 25, Train_Loss: 0.17209392786026, Test_Loss: 0.21516628563404083\n",
      "Epoch: 25, Train_Loss: 0.15881963074207306, Test_Loss: 0.2032863199710846 *\n",
      "Epoch: 25, Train_Loss: 0.17174962162971497, Test_Loss: 0.15759751200675964 *\n",
      "Epoch: 25, Train_Loss: 0.20577751100063324, Test_Loss: 0.1738615781068802\n",
      "Epoch: 25, Train_Loss: 0.17481794953346252, Test_Loss: 0.19026118516921997\n",
      "Epoch: 25, Train_Loss: 0.17139451205730438, Test_Loss: 0.17251721024513245 *\n",
      "Epoch: 25, Train_Loss: 0.1580968201160431, Test_Loss: 0.17452150583267212\n",
      "Epoch: 25, Train_Loss: 0.16695186495780945, Test_Loss: 0.1637602001428604 *\n",
      "Epoch: 25, Train_Loss: 0.16203853487968445, Test_Loss: 0.19777090847492218\n",
      "Epoch: 25, Train_Loss: 0.16785791516304016, Test_Loss: 0.20404118299484253\n",
      "Epoch: 25, Train_Loss: 0.15483586490154266, Test_Loss: 0.19128647446632385 *\n",
      "Epoch: 25, Train_Loss: 0.15336140990257263, Test_Loss: 0.25568535923957825\n",
      "Epoch: 25, Train_Loss: 0.15267913043498993, Test_Loss: 0.17532643675804138 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.16806861758232117, Test_Loss: 0.15895918011665344 *\n",
      "Epoch: 25, Train_Loss: 0.15404978394508362, Test_Loss: 0.17732422053813934\n",
      "Epoch: 25, Train_Loss: 0.15603657066822052, Test_Loss: 0.16978199779987335 *\n",
      "Epoch: 25, Train_Loss: 0.15943607687950134, Test_Loss: 0.16568008065223694 *\n",
      "Epoch: 25, Train_Loss: 0.1551472395658493, Test_Loss: 0.16682828962802887\n",
      "Epoch: 25, Train_Loss: 0.1528724581003189, Test_Loss: 0.16888508200645447\n",
      "Epoch: 25, Train_Loss: 0.16044586896896362, Test_Loss: 0.18244755268096924\n",
      "Epoch: 25, Train_Loss: 0.16833892464637756, Test_Loss: 0.17461861670017242 *\n",
      "Epoch: 25, Train_Loss: 0.1668408066034317, Test_Loss: 0.16310450434684753 *\n",
      "Epoch: 25, Train_Loss: 0.1620895266532898, Test_Loss: 0.1643170714378357\n",
      "Epoch: 25, Train_Loss: 0.183824822306633, Test_Loss: 0.21105067431926727\n",
      "Epoch: 25, Train_Loss: 0.17148813605308533, Test_Loss: 0.19714368879795074 *\n",
      "Epoch: 25, Train_Loss: 0.1774522066116333, Test_Loss: 0.3888726234436035\n",
      "Epoch: 25, Train_Loss: 0.16628724336624146, Test_Loss: 0.29951751232147217 *\n",
      "Epoch: 25, Train_Loss: 0.1713382601737976, Test_Loss: 0.3189433515071869\n",
      "Epoch: 25, Train_Loss: 0.18443360924720764, Test_Loss: 0.20199933648109436 *\n",
      "Epoch: 25, Train_Loss: 0.1665465086698532, Test_Loss: 0.1672016829252243 *\n",
      "Epoch: 25, Train_Loss: 0.16186338663101196, Test_Loss: 0.1608261615037918 *\n",
      "Epoch: 25, Train_Loss: 0.15941578149795532, Test_Loss: 0.18922974169254303\n",
      "Epoch: 25, Train_Loss: 0.16333463788032532, Test_Loss: 0.4110032916069031\n",
      "Epoch: 25, Train_Loss: 0.19057492911815643, Test_Loss: 0.2246416062116623 *\n",
      "Epoch: 25, Train_Loss: 0.2014417201280594, Test_Loss: 0.3582102656364441\n",
      "Epoch: 25, Train_Loss: 0.18022875487804413, Test_Loss: 0.22568151354789734 *\n",
      "Epoch: 25, Train_Loss: 0.15274560451507568, Test_Loss: 0.162933811545372 *\n",
      "Epoch: 25, Train_Loss: 0.1890399158000946, Test_Loss: 0.16014017164707184 *\n",
      "Epoch: 25, Train_Loss: 0.17445334792137146, Test_Loss: 0.1563427597284317 *\n",
      "Epoch: 25, Train_Loss: 0.15608090162277222, Test_Loss: 0.17232489585876465\n",
      "Epoch: 25, Train_Loss: 0.15701811015605927, Test_Loss: 0.1696251928806305 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.20626381039619446, Test_Loss: 0.18425452709197998\n",
      "Epoch: 25, Train_Loss: 0.24969281256198883, Test_Loss: 0.16071760654449463 *\n",
      "Epoch: 25, Train_Loss: 0.19691504538059235, Test_Loss: 0.24201154708862305\n",
      "Epoch: 25, Train_Loss: 0.18356984853744507, Test_Loss: 0.5004918575286865\n",
      "Epoch: 25, Train_Loss: 0.1774381697177887, Test_Loss: 0.21582576632499695 *\n",
      "Epoch: 25, Train_Loss: 0.15750746428966522, Test_Loss: 0.3381931185722351\n",
      "Epoch: 25, Train_Loss: 0.17027004063129425, Test_Loss: 0.16507196426391602 *\n",
      "Epoch: 25, Train_Loss: 0.15499535202980042, Test_Loss: 0.16577810049057007\n",
      "Epoch: 25, Train_Loss: 0.171289324760437, Test_Loss: 0.16577304899692535 *\n",
      "Epoch: 25, Train_Loss: 0.15811146795749664, Test_Loss: 0.16541649401187897 *\n",
      "Epoch: 25, Train_Loss: 0.15971560776233673, Test_Loss: 0.17651227116584778\n",
      "Epoch: 25, Train_Loss: 0.21690762042999268, Test_Loss: 4.576467990875244\n",
      "Epoch: 25, Train_Loss: 0.15408329665660858, Test_Loss: 1.2067549228668213 *\n",
      "Epoch: 25, Train_Loss: 0.20745135843753815, Test_Loss: 0.16381506621837616 *\n",
      "Epoch: 25, Train_Loss: 0.1681443154811859, Test_Loss: 0.15843220055103302 *\n",
      "Epoch: 25, Train_Loss: 0.197081059217453, Test_Loss: 0.15661108493804932 *\n",
      "Epoch: 25, Train_Loss: 0.2008449137210846, Test_Loss: 0.1575394570827484\n",
      "Epoch: 25, Train_Loss: 0.4132081866264343, Test_Loss: 0.16062873601913452\n",
      "Epoch: 25, Train_Loss: 0.16084839403629303, Test_Loss: 0.15656925737857819 *\n",
      "Epoch: 25, Train_Loss: 0.17653527855873108, Test_Loss: 0.15650756657123566 *\n",
      "Epoch: 25, Train_Loss: 0.15238650143146515, Test_Loss: 0.15405307710170746 *\n",
      "Epoch: 25, Train_Loss: 0.1523890346288681, Test_Loss: 0.15727175772190094\n",
      "Epoch: 25, Train_Loss: 0.15534211695194244, Test_Loss: 0.16216041147708893\n",
      "Epoch: 25, Train_Loss: 0.15478962659835815, Test_Loss: 0.1680353581905365\n",
      "Epoch: 25, Train_Loss: 0.15818573534488678, Test_Loss: 0.17263643443584442\n",
      "Epoch: 25, Train_Loss: 0.15581414103507996, Test_Loss: 0.17691531777381897\n",
      "Epoch: 25, Train_Loss: 0.1674226075410843, Test_Loss: 0.15309631824493408 *\n",
      "Epoch: 25, Train_Loss: 0.1566440612077713, Test_Loss: 0.15359172224998474\n",
      "Epoch: 25, Train_Loss: 0.15803028643131256, Test_Loss: 0.15237972140312195 *\n",
      "Epoch: 25, Train_Loss: 0.16218045353889465, Test_Loss: 0.16252505779266357\n",
      "Epoch: 25, Train_Loss: 0.15447771549224854, Test_Loss: 0.15561895072460175 *\n",
      "Epoch: 25, Train_Loss: 0.1521092802286148, Test_Loss: 0.15282581746578217 *\n",
      "Epoch: 25, Train_Loss: 0.17036812007427216, Test_Loss: 0.15364325046539307\n",
      "Epoch: 25, Train_Loss: 0.15888679027557373, Test_Loss: 0.15416893362998962\n",
      "Epoch: 25, Train_Loss: 0.177016481757164, Test_Loss: 0.1556604653596878\n",
      "Epoch: 25, Train_Loss: 0.15388324856758118, Test_Loss: 0.1694028675556183\n",
      "Epoch: 25, Train_Loss: 0.1613989770412445, Test_Loss: 0.1551169753074646 *\n",
      "Epoch: 25, Train_Loss: 0.16673319041728973, Test_Loss: 0.15326304733753204 *\n",
      "Epoch: 25, Train_Loss: 0.17909565567970276, Test_Loss: 0.15427519381046295\n",
      "Epoch: 25, Train_Loss: 0.15351498126983643, Test_Loss: 0.15307600796222687 *\n",
      "Epoch: 25, Train_Loss: 0.17402756214141846, Test_Loss: 0.15324698388576508\n",
      "Epoch: 25, Train_Loss: 0.153520405292511, Test_Loss: 0.24320602416992188\n",
      "Epoch: 25, Train_Loss: 0.1688431203365326, Test_Loss: 0.3104715943336487\n",
      "Epoch: 25, Train_Loss: 0.15722958743572235, Test_Loss: 5.552005290985107\n",
      "Epoch: 25, Train_Loss: 0.16924726963043213, Test_Loss: 0.20919984579086304 *\n",
      "Epoch: 25, Train_Loss: 0.38635027408599854, Test_Loss: 0.15311191976070404 *\n",
      "Epoch: 25, Train_Loss: 3.9482955932617188, Test_Loss: 0.17220911383628845\n",
      "Epoch: 25, Train_Loss: 1.764738917350769, Test_Loss: 0.15697520971298218 *\n",
      "Epoch: 25, Train_Loss: 0.16725607216358185, Test_Loss: 0.16630595922470093\n",
      "Epoch: 25, Train_Loss: 0.15676188468933105, Test_Loss: 0.1625632345676422 *\n",
      "Epoch: 25, Train_Loss: 0.19564853608608246, Test_Loss: 0.26836761832237244\n",
      "Epoch: 25, Train_Loss: 0.21361833810806274, Test_Loss: 0.20254792273044586 *\n",
      "Epoch: 25, Train_Loss: 0.16466914117336273, Test_Loss: 0.15262804925441742 *\n",
      "Epoch: 25, Train_Loss: 0.15181134641170502, Test_Loss: 0.18973372876644135\n",
      "Epoch: 25, Train_Loss: 0.19948308169841766, Test_Loss: 0.1603141725063324 *\n",
      "Epoch: 25, Train_Loss: 0.19024983048439026, Test_Loss: 0.16295339167118073\n",
      "Epoch: 25, Train_Loss: 0.15975214540958405, Test_Loss: 0.18679045140743256\n",
      "Epoch: 25, Train_Loss: 0.3206065595149994, Test_Loss: 0.16757996380329132 *\n",
      "Epoch: 25, Train_Loss: 0.407953143119812, Test_Loss: 0.21362608671188354\n",
      "Epoch: 25, Train_Loss: 0.7848491072654724, Test_Loss: 0.2190903127193451\n",
      "Epoch: 25, Train_Loss: 0.2162000983953476, Test_Loss: 0.19335505366325378 *\n",
      "Epoch: 25, Train_Loss: 0.26678037643432617, Test_Loss: 0.19032058119773865 *\n",
      "Epoch: 25, Train_Loss: 1.1106300354003906, Test_Loss: 0.15820008516311646 *\n",
      "Epoch: 25, Train_Loss: 0.693156898021698, Test_Loss: 0.16509774327278137\n",
      "Epoch: 25, Train_Loss: 0.1573439985513687, Test_Loss: 0.16552457213401794\n",
      "Epoch: 25, Train_Loss: 0.15651831030845642, Test_Loss: 0.17014998197555542\n",
      "Epoch: 25, Train_Loss: 0.6084378957748413, Test_Loss: 0.16331730782985687 *\n",
      "Epoch: 25, Train_Loss: 0.4532075524330139, Test_Loss: 0.16783544421195984\n",
      "Epoch: 25, Train_Loss: 0.7630974650382996, Test_Loss: 0.18990054726600647\n",
      "Epoch: 25, Train_Loss: 0.15672567486763, Test_Loss: 0.21285216510295868\n",
      "Epoch: 25, Train_Loss: 0.17213788628578186, Test_Loss: 0.16480137407779694 *\n",
      "Epoch: 25, Train_Loss: 0.32584255933761597, Test_Loss: 0.17107868194580078\n",
      "Epoch: 25, Train_Loss: 0.37818142771720886, Test_Loss: 0.1593484878540039 *\n",
      "Epoch: 25, Train_Loss: 0.16437876224517822, Test_Loss: 0.18450945615768433\n",
      "Epoch: 25, Train_Loss: 0.2217148244380951, Test_Loss: 0.1760241836309433 *\n",
      "Epoch: 25, Train_Loss: 0.19481965899467468, Test_Loss: 0.4126499593257904\n",
      "Epoch: 25, Train_Loss: 0.19078828394412994, Test_Loss: 0.28994202613830566 *\n",
      "Epoch: 25, Train_Loss: 0.3040348291397095, Test_Loss: 0.2737327814102173 *\n",
      "Epoch: 25, Train_Loss: 0.2280135154724121, Test_Loss: 0.2070470005273819 *\n",
      "Epoch: 25, Train_Loss: 0.18727020919322968, Test_Loss: 0.17571412026882172 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.21347114443778992, Test_Loss: 0.15917949378490448 *\n",
      "Epoch: 25, Train_Loss: 0.2335592806339264, Test_Loss: 0.1672711968421936\n",
      "Epoch: 25, Train_Loss: 0.19446146488189697, Test_Loss: 0.37276744842529297\n",
      "Epoch: 25, Train_Loss: 0.29453983902931213, Test_Loss: 0.22994323074817657 *\n",
      "Epoch: 25, Train_Loss: 0.3660317361354828, Test_Loss: 0.251960426568985\n",
      "Epoch: 25, Train_Loss: 0.18420417606830597, Test_Loss: 0.2834387421607971\n",
      "Epoch: 25, Train_Loss: 0.18309110403060913, Test_Loss: 0.1657313108444214 *\n",
      "Epoch: 25, Train_Loss: 0.2034679800271988, Test_Loss: 0.15738588571548462 *\n",
      "Epoch: 25, Train_Loss: 0.1728190779685974, Test_Loss: 0.16199231147766113\n",
      "Epoch: 25, Train_Loss: 0.15600495040416718, Test_Loss: 0.19964326918125153\n",
      "Epoch: 25, Train_Loss: 0.15274937450885773, Test_Loss: 0.15874850749969482 *\n",
      "Epoch: 25, Train_Loss: 0.15322083234786987, Test_Loss: 0.17724478244781494\n",
      "Epoch: 25, Train_Loss: 0.15577593445777893, Test_Loss: 0.15836021304130554 *\n",
      "Epoch: 25, Train_Loss: 0.16031108796596527, Test_Loss: 0.3503694534301758\n",
      "Epoch: 25, Train_Loss: 0.16464564204216003, Test_Loss: 0.5071303844451904\n",
      "Epoch: 25, Train_Loss: 0.163430854678154, Test_Loss: 0.26078566908836365 *\n",
      "Epoch: 25, Train_Loss: 0.1723751723766327, Test_Loss: 0.4465070068836212\n",
      "Epoch: 25, Train_Loss: 0.28985023498535156, Test_Loss: 0.2637103199958801 *\n",
      "Epoch: 25, Train_Loss: 0.44818615913391113, Test_Loss: 0.2652841806411743\n",
      "Epoch: 25, Train_Loss: 0.17053599655628204, Test_Loss: 0.2647998332977295 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.2072770893573761, Test_Loss: 0.2619381844997406 *\n",
      "Epoch: 25, Train_Loss: 0.1990944743156433, Test_Loss: 0.2685248851776123\n",
      "Epoch: 25, Train_Loss: 0.19578154385089874, Test_Loss: 5.220847129821777\n",
      "Epoch: 25, Train_Loss: 0.24795016646385193, Test_Loss: 0.4263001084327698 *\n",
      "Epoch: 25, Train_Loss: 0.1907743513584137, Test_Loss: 0.16399802267551422 *\n",
      "Epoch: 25, Train_Loss: 0.3571479320526123, Test_Loss: 0.17732401192188263\n",
      "Epoch: 25, Train_Loss: 0.21716687083244324, Test_Loss: 0.16486254334449768 *\n",
      "Epoch: 25, Train_Loss: 0.2685348689556122, Test_Loss: 0.16663795709609985\n",
      "Epoch: 25, Train_Loss: 0.16997525095939636, Test_Loss: 0.18598003685474396\n",
      "Epoch: 25, Train_Loss: 0.18059858679771423, Test_Loss: 0.1758326292037964 *\n",
      "Epoch: 25, Train_Loss: 0.37467700242996216, Test_Loss: 0.17212212085723877 *\n",
      "Epoch: 25, Train_Loss: 0.5682811141014099, Test_Loss: 0.1587953269481659 *\n",
      "Epoch: 25, Train_Loss: 0.43531087040901184, Test_Loss: 0.1713697761297226\n",
      "Epoch: 25, Train_Loss: 0.20090289413928986, Test_Loss: 0.2482559084892273\n",
      "Epoch: 25, Train_Loss: 0.1860729604959488, Test_Loss: 0.1763419508934021 *\n",
      "Epoch: 25, Train_Loss: 0.16073361039161682, Test_Loss: 0.165956512093544 *\n",
      "Epoch: 25, Train_Loss: 0.43227678537368774, Test_Loss: 0.1784113645553589\n",
      "Epoch: 25, Train_Loss: 0.2939000129699707, Test_Loss: 0.15712052583694458 *\n",
      "Epoch: 25, Train_Loss: 0.16348180174827576, Test_Loss: 0.1577543318271637\n",
      "Epoch: 25, Train_Loss: 0.3407214879989624, Test_Loss: 0.17599545419216156\n",
      "Epoch: 25, Train_Loss: 0.17590327560901642, Test_Loss: 0.17386209964752197 *\n",
      "Epoch: 25, Train_Loss: 0.16987016797065735, Test_Loss: 0.1541600525379181 *\n",
      "Epoch: 25, Train_Loss: 0.19382601976394653, Test_Loss: 0.1774573177099228\n",
      "Epoch: 25, Train_Loss: 0.25104376673698425, Test_Loss: 0.15757274627685547 *\n",
      "Epoch: 25, Train_Loss: 0.19687986373901367, Test_Loss: 0.164451465010643\n",
      "Epoch: 25, Train_Loss: 0.2515197992324829, Test_Loss: 0.16115355491638184 *\n",
      "Epoch: 25, Train_Loss: 0.16162344813346863, Test_Loss: 0.18757468461990356\n",
      "Epoch: 25, Train_Loss: 0.22390314936637878, Test_Loss: 0.15837985277175903 *\n",
      "Epoch: 25, Train_Loss: 0.1879907101392746, Test_Loss: 0.15980347990989685\n",
      "Epoch: 25, Train_Loss: 0.18074648082256317, Test_Loss: 0.19087721407413483\n",
      "Epoch: 25, Train_Loss: 0.16748663783073425, Test_Loss: 0.16446423530578613 *\n",
      "Epoch: 25, Train_Loss: 0.17254242300987244, Test_Loss: 0.1550990343093872 *\n",
      "Epoch: 25, Train_Loss: 0.32097750902175903, Test_Loss: 0.26294389367103577\n",
      "Epoch: 25, Train_Loss: 0.3761424124240875, Test_Loss: 1.1630792617797852\n",
      "Epoch: 25, Train_Loss: 0.3665023446083069, Test_Loss: 3.8252663612365723\n",
      "Epoch: 25, Train_Loss: 0.575897753238678, Test_Loss: 0.18762624263763428 *\n",
      "Epoch: 25, Train_Loss: 0.4439406394958496, Test_Loss: 0.1602693647146225 *\n",
      "Epoch: 25, Train_Loss: 0.27513188123703003, Test_Loss: 0.1880965232849121\n",
      "Epoch: 25, Train_Loss: 0.23298346996307373, Test_Loss: 0.17458681762218475 *\n",
      "Epoch: 25, Train_Loss: 0.16628485918045044, Test_Loss: 0.1616557538509369 *\n",
      "Epoch: 25, Train_Loss: 0.15756943821907043, Test_Loss: 0.1877245008945465\n",
      "Epoch: 25, Train_Loss: 0.1681145578622818, Test_Loss: 0.20256048440933228\n",
      "Epoch: 25, Train_Loss: 0.30191096663475037, Test_Loss: 0.17276720702648163 *\n",
      "Epoch: 25, Train_Loss: 0.38252460956573486, Test_Loss: 0.16338030993938446 *\n",
      "Epoch: 25, Train_Loss: 0.40157338976860046, Test_Loss: 0.1913314312696457\n",
      "Epoch: 25, Train_Loss: 0.8075286149978638, Test_Loss: 0.20102559030056\n",
      "Epoch: 25, Train_Loss: 1.0956075191497803, Test_Loss: 0.17909826338291168 *\n",
      "Epoch: 25, Train_Loss: 0.2989458441734314, Test_Loss: 0.245828777551651\n",
      "Epoch: 25, Train_Loss: 0.2341729998588562, Test_Loss: 0.19629892706871033 *\n",
      "Epoch: 25, Train_Loss: 0.15696962177753448, Test_Loss: 0.23803992569446564\n",
      "Epoch: 25, Train_Loss: 0.28693926334381104, Test_Loss: 0.175633043050766 *\n",
      "Epoch: 25, Train_Loss: 0.32405591011047363, Test_Loss: 0.20441070199012756\n",
      "Epoch: 25, Train_Loss: 0.7084752321243286, Test_Loss: 0.20668785274028778\n",
      "Epoch: 25, Train_Loss: 0.18129460513591766, Test_Loss: 0.20177499949932098 *\n",
      "Epoch: 25, Train_Loss: 0.17477798461914062, Test_Loss: 0.33003661036491394\n",
      "Epoch: 25, Train_Loss: 0.21719315648078918, Test_Loss: 0.31729328632354736 *\n",
      "Epoch: 25, Train_Loss: 0.40188315510749817, Test_Loss: 0.318331241607666\n",
      "Epoch: 25, Train_Loss: 0.28944411873817444, Test_Loss: 0.3151906132698059 *\n",
      "Epoch: 25, Train_Loss: 0.2828499674797058, Test_Loss: 0.2800126075744629 *\n",
      "Epoch: 25, Train_Loss: 0.2833068370819092, Test_Loss: 0.2752975523471832 *\n",
      "Epoch: 25, Train_Loss: 0.2123061716556549, Test_Loss: 0.2867615222930908\n",
      "Epoch: 25, Train_Loss: 0.15977159142494202, Test_Loss: 0.17195433378219604 *\n",
      "Epoch: 25, Train_Loss: 0.1750418096780777, Test_Loss: 0.19594475626945496\n",
      "Epoch: 25, Train_Loss: 0.1545097976922989, Test_Loss: 0.15629789233207703 *\n",
      "Epoch: 25, Train_Loss: 0.19840705394744873, Test_Loss: 0.15863727033138275\n",
      "Epoch: 25, Train_Loss: 0.2096989005804062, Test_Loss: 0.17788878083229065\n",
      "Epoch: 25, Train_Loss: 0.42960500717163086, Test_Loss: 0.3382985293865204\n",
      "Epoch: 25, Train_Loss: 15.199943542480469, Test_Loss: 0.25604745745658875 *\n",
      "Epoch: 25, Train_Loss: 0.28291741013526917, Test_Loss: 0.18895791471004486 *\n",
      "Epoch: 25, Train_Loss: 0.7029321789741516, Test_Loss: 0.20690113306045532\n",
      "Epoch: 25, Train_Loss: 0.9515136480331421, Test_Loss: 0.19552727043628693 *\n",
      "Epoch: 25, Train_Loss: 0.18964320421218872, Test_Loss: 0.1572289764881134 *\n",
      "Epoch: 25, Train_Loss: 0.5864883661270142, Test_Loss: 0.19735080003738403\n",
      "Epoch: 25, Train_Loss: 2.5897862911224365, Test_Loss: 0.23076042532920837\n",
      "Epoch: 25, Train_Loss: 3.522822856903076, Test_Loss: 0.3899022340774536\n",
      "Epoch: 25, Train_Loss: 0.24607747793197632, Test_Loss: 0.22830438613891602 *\n",
      "Epoch: 25, Train_Loss: 0.5554860234260559, Test_Loss: 0.250903457403183\n",
      "Epoch: 25, Train_Loss: 4.523388385772705, Test_Loss: 0.17216628789901733 *\n",
      "Epoch: 25, Train_Loss: 0.32373982667922974, Test_Loss: 0.1945837140083313\n",
      "Epoch: 25, Train_Loss: 0.17164786159992218, Test_Loss: 0.1828954666852951 *\n",
      "Epoch: 25, Train_Loss: 0.17130377888679504, Test_Loss: 0.19495224952697754\n",
      "Epoch: 25, Train_Loss: 0.22987881302833557, Test_Loss: 0.1816607564687729 *\n",
      "Epoch: 25, Train_Loss: 0.19451817870140076, Test_Loss: 0.17010697722434998 *\n",
      "Epoch: 25, Train_Loss: 0.16319434344768524, Test_Loss: 0.18848063051700592\n",
      "Epoch: 25, Train_Loss: 0.16333474218845367, Test_Loss: 0.2977668046951294\n",
      "Epoch: 25, Train_Loss: 0.15014611184597015, Test_Loss: 0.5851401090621948\n",
      "Epoch: 25, Train_Loss: 0.1510220170021057, Test_Loss: 0.28130030632019043 *\n",
      "Epoch: 25, Train_Loss: 0.1686261147260666, Test_Loss: 0.17880935966968536 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.1646365374326706, Test_Loss: 0.15739671885967255 *\n",
      "Epoch: 25, Train_Loss: 0.19429439306259155, Test_Loss: 0.15662920475006104 *\n",
      "Epoch: 25, Train_Loss: 0.19290608167648315, Test_Loss: 0.157308429479599\n",
      "Epoch: 25, Train_Loss: 0.19006460905075073, Test_Loss: 0.17985358834266663\n",
      "Epoch: 25, Train_Loss: 0.1620582789182663, Test_Loss: 0.4856550693511963\n",
      "Epoch: 25, Train_Loss: 0.16303735971450806, Test_Loss: 6.190290451049805\n",
      "Epoch: 25, Train_Loss: 0.16912606358528137, Test_Loss: 0.25203627347946167 *\n",
      "Epoch: 25, Train_Loss: 0.1527169644832611, Test_Loss: 0.25657305121421814\n",
      "Epoch: 25, Train_Loss: 0.15276756882667542, Test_Loss: 0.34575992822647095\n",
      "Epoch: 25, Train_Loss: 0.15010295808315277, Test_Loss: 0.2894156575202942 *\n",
      "Epoch: 25, Train_Loss: 0.15034009516239166, Test_Loss: 0.28826701641082764 *\n",
      "Epoch: 25, Train_Loss: 0.15030094981193542, Test_Loss: 0.480161190032959\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.15051297843456268, Test_Loss: 0.47220683097839355 *\n",
      "Epoch: 25, Train_Loss: 0.15003041923046112, Test_Loss: 0.25011035799980164 *\n",
      "Epoch: 25, Train_Loss: 0.15043556690216064, Test_Loss: 0.29877692461013794\n",
      "Epoch: 25, Train_Loss: 0.16871562600135803, Test_Loss: 0.3516805171966553\n",
      "Epoch: 25, Train_Loss: 0.18564538657665253, Test_Loss: 0.8213897347450256\n",
      "Epoch: 25, Train_Loss: 0.20316380262374878, Test_Loss: 0.38031530380249023 *\n",
      "Epoch: 25, Train_Loss: 0.18460151553153992, Test_Loss: 0.34542539715766907 *\n",
      "Epoch: 25, Train_Loss: 0.27984189987182617, Test_Loss: 0.26905301213264465 *\n",
      "Epoch: 25, Train_Loss: 3.600764036178589, Test_Loss: 0.15791624784469604 *\n",
      "Epoch: 25, Train_Loss: 3.5204575061798096, Test_Loss: 0.1587834358215332\n",
      "Epoch: 25, Train_Loss: 0.1742829978466034, Test_Loss: 0.15828509628772736 *\n",
      "Epoch: 25, Train_Loss: 0.17599524557590485, Test_Loss: 0.30554160475730896\n",
      "Epoch: 25, Train_Loss: 0.19411444664001465, Test_Loss: 0.15864434838294983 *\n",
      "Epoch: 25, Train_Loss: 0.2314678132534027, Test_Loss: 0.2984435260295868\n",
      "Epoch: 25, Train_Loss: 0.22043484449386597, Test_Loss: 0.1606578826904297 *\n",
      "Epoch: 25, Train_Loss: 0.19086746871471405, Test_Loss: 0.20219069719314575\n",
      "Epoch: 25, Train_Loss: 0.21249374747276306, Test_Loss: 0.22966627776622772\n",
      "Epoch: 25, Train_Loss: 0.25064903497695923, Test_Loss: 0.23375999927520752\n",
      "Epoch: 25, Train_Loss: 0.26185619831085205, Test_Loss: 0.16261285543441772 *\n",
      "Epoch: 25, Train_Loss: 0.17208093404769897, Test_Loss: 0.16084259748458862 *\n",
      "Epoch: 25, Train_Loss: 0.1671140342950821, Test_Loss: 0.1719662994146347\n",
      "Epoch: 25, Train_Loss: 0.16145579516887665, Test_Loss: 0.16535437107086182 *\n",
      "Epoch: 25, Train_Loss: 0.16258475184440613, Test_Loss: 0.16960886120796204\n",
      "Epoch: 25, Train_Loss: 0.15469716489315033, Test_Loss: 0.3936621844768524\n",
      "Epoch: 25, Train_Loss: 0.16858065128326416, Test_Loss: 3.283696174621582\n",
      "Epoch: 25, Train_Loss: 0.20676276087760925, Test_Loss: 4.746851921081543\n",
      "Epoch: 25, Train_Loss: 0.16113390028476715, Test_Loss: 0.28687426447868347 *\n",
      "Epoch: 25, Train_Loss: 0.19184277951717377, Test_Loss: 0.17369762063026428 *\n",
      "Epoch: 25, Train_Loss: 0.20599402487277985, Test_Loss: 0.20892669260501862\n",
      "Epoch: 25, Train_Loss: 0.20394188165664673, Test_Loss: 0.2836025357246399\n",
      "Epoch: 25, Train_Loss: 0.16827227175235748, Test_Loss: 0.25065019726753235 *\n",
      "Epoch: 25, Train_Loss: 0.15071478486061096, Test_Loss: 0.2782489061355591\n",
      "Epoch: 25, Train_Loss: 0.18977567553520203, Test_Loss: 0.6137956380844116\n",
      "Epoch: 25, Train_Loss: 1.8573046922683716, Test_Loss: 0.15831734240055084 *\n",
      "Epoch: 25, Train_Loss: 2.183297634124756, Test_Loss: 0.16887402534484863\n",
      "Epoch: 25, Train_Loss: 0.20932474732398987, Test_Loss: 0.18701113760471344\n",
      "Epoch: 25, Train_Loss: 0.20544396340847015, Test_Loss: 0.17798250913619995 *\n",
      "Epoch: 25, Train_Loss: 0.1562543511390686, Test_Loss: 0.15861469507217407 *\n",
      "Epoch: 25, Train_Loss: 0.15020054578781128, Test_Loss: 0.25960952043533325\n",
      "Epoch: 25, Train_Loss: 0.15273773670196533, Test_Loss: 0.2472027987241745 *\n",
      "Epoch: 25, Train_Loss: 0.15195980668067932, Test_Loss: 0.2548089027404785\n",
      "Epoch: 25, Train_Loss: 0.18882903456687927, Test_Loss: 0.1822163611650467 *\n",
      "Epoch: 25, Train_Loss: 0.16944333910942078, Test_Loss: 0.18195496499538422 *\n",
      "Epoch: 25, Train_Loss: 0.16172204911708832, Test_Loss: 0.1850326657295227\n",
      "Epoch: 25, Train_Loss: 0.15175284445285797, Test_Loss: 0.3525758385658264\n",
      "Epoch: 25, Train_Loss: 0.15352052450180054, Test_Loss: 0.2730576992034912 *\n",
      "Epoch: 25, Train_Loss: 0.1606832891702652, Test_Loss: 0.23615474998950958 *\n",
      "Epoch: 25, Train_Loss: 0.1700323224067688, Test_Loss: 0.19438698887825012 *\n",
      "Epoch: 25, Train_Loss: 0.15318186581134796, Test_Loss: 0.21838641166687012\n",
      "Epoch: 25, Train_Loss: 0.15640965104103088, Test_Loss: 0.21929945051670074\n",
      "Epoch: 25, Train_Loss: 0.16659170389175415, Test_Loss: 0.31108635663986206\n",
      "Epoch: 25, Train_Loss: 0.1648036688566208, Test_Loss: 0.47744786739349365\n",
      "Epoch: 25, Train_Loss: 0.15254557132720947, Test_Loss: 0.2013809233903885 *\n",
      "Epoch: 25, Train_Loss: 0.150253027677536, Test_Loss: 0.16758745908737183 *\n",
      "Epoch: 25, Train_Loss: 0.20418576896190643, Test_Loss: 0.15915535390377045 *\n",
      "Epoch: 25, Train_Loss: 0.214713454246521, Test_Loss: 0.1953521966934204\n",
      "Epoch: 25, Train_Loss: 0.17790114879608154, Test_Loss: 0.1846252828836441 *\n",
      "Epoch: 25, Train_Loss: 0.1790895015001297, Test_Loss: 0.2848869562149048\n",
      "Epoch: 25, Train_Loss: 0.18940459191799164, Test_Loss: 0.3061833381652832\n",
      "Epoch: 25, Train_Loss: 0.18767964839935303, Test_Loss: 0.1840665340423584 *\n",
      "Epoch: 25, Train_Loss: 0.1632576435804367, Test_Loss: 0.2503581643104553\n",
      "Epoch: 25, Train_Loss: 0.18641750514507294, Test_Loss: 0.19320863485336304 *\n",
      "Epoch: 25, Train_Loss: 0.21456216275691986, Test_Loss: 0.17294177412986755 *\n",
      "Epoch: 25, Train_Loss: 0.21463562548160553, Test_Loss: 0.32049763202667236\n",
      "Epoch: 25, Train_Loss: 0.169727623462677, Test_Loss: 0.3539819121360779\n",
      "Epoch: 25, Train_Loss: 0.1502767950296402, Test_Loss: 0.5476007461547852\n",
      "Epoch: 25, Train_Loss: 0.14951159060001373, Test_Loss: 0.25740036368370056 *\n",
      "Epoch: 25, Train_Loss: 0.14931389689445496, Test_Loss: 0.2273063063621521 *\n",
      "Epoch: 25, Train_Loss: 0.1493588536977768, Test_Loss: 0.16574716567993164 *\n",
      "Epoch: 25, Train_Loss: 0.1511530727148056, Test_Loss: 0.16101910173892975 *\n",
      "Epoch: 25, Train_Loss: 3.6404173374176025, Test_Loss: 0.16293634474277496\n",
      "Epoch: 25, Train_Loss: 1.2448420524597168, Test_Loss: 0.15898537635803223 *\n",
      "Epoch: 25, Train_Loss: 0.1535273939371109, Test_Loss: 0.18461447954177856\n",
      "Epoch: 25, Train_Loss: 0.15524044632911682, Test_Loss: 0.15537193417549133 *\n",
      "Epoch: 25, Train_Loss: 0.1543496698141098, Test_Loss: 0.1916773021221161\n",
      "Epoch: 25, Train_Loss: 0.15358708798885345, Test_Loss: 0.2911248207092285\n",
      "Epoch: 25, Train_Loss: 0.15275682508945465, Test_Loss: 0.4393804371356964\n",
      "Epoch: 25, Train_Loss: 0.15402443706989288, Test_Loss: 0.3983688950538635 *\n",
      "Epoch: 25, Train_Loss: 0.15135660767555237, Test_Loss: 0.1769096553325653 *\n",
      "Epoch: 25, Train_Loss: 0.1517890989780426, Test_Loss: 0.16764883697032928 *\n",
      "Epoch: 25, Train_Loss: 0.16786563396453857, Test_Loss: 0.16741076111793518 *\n",
      "Epoch: 25, Train_Loss: 0.17641788721084595, Test_Loss: 0.16817867755889893\n",
      "Epoch: 25, Train_Loss: 0.18211333453655243, Test_Loss: 0.18918335437774658\n",
      "Epoch: 25, Train_Loss: 0.19020061194896698, Test_Loss: 1.1484911441802979\n",
      "Epoch: 25, Train_Loss: 0.1654948592185974, Test_Loss: 4.451903820037842\n",
      "Epoch: 25, Train_Loss: 0.18068718910217285, Test_Loss: 0.16970288753509521 *\n",
      "Epoch: 25, Train_Loss: 0.32397276163101196, Test_Loss: 0.16121992468833923 *\n",
      "Epoch: 25, Train_Loss: 0.27334165573120117, Test_Loss: 0.16337572038173676\n",
      "Epoch: 25, Train_Loss: 0.23165738582611084, Test_Loss: 0.15753786265850067 *\n",
      "Epoch: 25, Train_Loss: 0.20197144150733948, Test_Loss: 0.15626733005046844 *\n",
      "Epoch: 25, Train_Loss: 0.1527048647403717, Test_Loss: 0.1779259592294693\n",
      "Epoch: 25, Train_Loss: 0.1494576334953308, Test_Loss: 0.16210249066352844 *\n",
      "Epoch: 25, Train_Loss: 0.15353207290172577, Test_Loss: 0.15391886234283447 *\n",
      "Epoch: 25, Train_Loss: 0.15046285092830658, Test_Loss: 0.15376456081867218 *\n",
      "Epoch: 25, Train_Loss: 0.15606872737407684, Test_Loss: 0.15758177638053894\n",
      "Epoch: 25, Train_Loss: 0.1581835150718689, Test_Loss: 0.2527207136154175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train_Loss: 0.1492573618888855, Test_Loss: 0.17007112503051758 *\n",
      "Epoch: 25, Train_Loss: 0.15058036148548126, Test_Loss: 0.15807181596755981 *\n",
      "Epoch: 25, Train_Loss: 0.16010542213916779, Test_Loss: 0.17229489982128143\n",
      "Epoch: 25, Train_Loss: 0.217047780752182, Test_Loss: 0.1526888906955719 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 25\n",
      "Epoch: 25, Train_Loss: 0.19914421439170837, Test_Loss: 0.15269120037555695\n",
      "Epoch: 25, Train_Loss: 0.18840065598487854, Test_Loss: 0.1553162783384323\n",
      "Epoch: 25, Train_Loss: 0.33484843373298645, Test_Loss: 0.18719078600406647\n",
      "Epoch: 25, Train_Loss: 0.2490793764591217, Test_Loss: 0.15018439292907715 *\n",
      "Epoch: 25, Train_Loss: 0.2582437992095947, Test_Loss: 0.17998985946178436\n",
      "Epoch: 25, Train_Loss: 0.161603182554245, Test_Loss: 0.14958085119724274 *\n",
      "Epoch: 25, Train_Loss: 0.2611757516860962, Test_Loss: 0.17636191844940186\n",
      "Epoch: 25, Train_Loss: 0.19416557252407074, Test_Loss: 0.18531832098960876\n",
      "Epoch: 25, Train_Loss: 0.42631834745407104, Test_Loss: 0.16445623338222504 *\n",
      "Epoch: 25, Train_Loss: 0.15812739729881287, Test_Loss: 0.1520107388496399 *\n",
      "Epoch: 25, Train_Loss: 0.6736140847206116, Test_Loss: 0.15498994290828705\n",
      "Epoch: 25, Train_Loss: 2.3464956283569336, Test_Loss: 0.15258611738681793 *\n",
      "Epoch: 25, Train_Loss: 0.19982558488845825, Test_Loss: 0.15319466590881348\n",
      "Epoch: 25, Train_Loss: 0.20767441391944885, Test_Loss: 0.18923026323318481\n",
      "Epoch: 25, Train_Loss: 0.16945087909698486, Test_Loss: 0.24419942498207092\n",
      "Epoch: 25, Train_Loss: 0.1653417944908142, Test_Loss: 3.5722830295562744\n",
      "Epoch: 25, Train_Loss: 0.15115882456302643, Test_Loss: 2.563349485397339 *\n",
      "Epoch: 25, Train_Loss: 0.17374905943870544, Test_Loss: 0.19187012314796448 *\n",
      "Epoch: 25, Train_Loss: 0.21804356575012207, Test_Loss: 0.17473474144935608 *\n",
      "Epoch: 25, Train_Loss: 0.22074243426322937, Test_Loss: 0.17777585983276367\n",
      "Epoch: 25, Train_Loss: 0.18273751437664032, Test_Loss: 0.2087322175502777\n",
      "Epoch: 25, Train_Loss: 0.16510674357414246, Test_Loss: 0.17523273825645447 *\n",
      "Epoch: 25, Train_Loss: 0.15969166159629822, Test_Loss: 0.19792932271957397\n",
      "Epoch: 25, Train_Loss: 0.15435390174388885, Test_Loss: 0.23319178819656372\n",
      "Epoch: 25, Train_Loss: 0.16447919607162476, Test_Loss: 0.159052312374115 *\n",
      "Epoch: 25, Train_Loss: 0.180876225233078, Test_Loss: 0.17230643332004547\n",
      "Epoch: 25, Train_Loss: 0.16519951820373535, Test_Loss: 0.17807519435882568\n",
      "Epoch: 25, Train_Loss: 0.16068746149539948, Test_Loss: 0.18622078001499176\n",
      "Epoch: 25, Train_Loss: 0.15009447932243347, Test_Loss: 0.15847718715667725 *\n",
      "Epoch: 25, Train_Loss: 0.16386809945106506, Test_Loss: 0.213578999042511\n",
      "Epoch: 25, Train_Loss: 0.15591023862361908, Test_Loss: 0.2225409746170044\n",
      "Epoch: 25, Train_Loss: 0.16324713826179504, Test_Loss: 0.19913572072982788 *\n",
      "Epoch: 25, Train_Loss: 0.1540205478668213, Test_Loss: 0.164078027009964 *\n",
      "Epoch: 25, Train_Loss: 0.15003612637519836, Test_Loss: 0.22744742035865784\n",
      "Epoch: 25, Train_Loss: 0.14901088178157806, Test_Loss: 0.20438744127750397 *\n",
      "Epoch: 25, Train_Loss: 0.15517301857471466, Test_Loss: 0.21680018305778503\n",
      "Epoch: 25, Train_Loss: 0.15215076506137848, Test_Loss: 0.20032764971256256 *\n",
      "Epoch: 25, Train_Loss: 0.15240347385406494, Test_Loss: 0.1817505955696106 *\n",
      "Epoch: 25, Train_Loss: 0.1553259640932083, Test_Loss: 0.2136334776878357\n",
      "Epoch: 25, Train_Loss: 0.15181320905685425, Test_Loss: 0.1880837380886078 *\n",
      "Epoch: 25, Train_Loss: 0.1487465649843216, Test_Loss: 0.15981163084506989 *\n",
      "Epoch: 25, Train_Loss: 0.15415234863758087, Test_Loss: 0.19304159283638\n",
      "Epoch: 25, Train_Loss: 0.15713584423065186, Test_Loss: 0.3215303421020508\n",
      "Epoch: 25, Train_Loss: 0.16204872727394104, Test_Loss: 0.16026510298252106 *\n",
      "Epoch: 25, Train_Loss: 0.15780042111873627, Test_Loss: 0.1567225307226181 *\n",
      "Epoch: 25, Train_Loss: 0.17463012039661407, Test_Loss: 0.17774255573749542\n",
      "Epoch: 25, Train_Loss: 0.17564761638641357, Test_Loss: 0.23782141506671906\n",
      "Epoch: 25, Train_Loss: 0.1780606359243393, Test_Loss: 0.273250550031662\n",
      "Epoch: 25, Train_Loss: 0.1606181114912033, Test_Loss: 0.42484796047210693\n",
      "Epoch: 25, Train_Loss: 0.15520575642585754, Test_Loss: 0.4404298663139343\n",
      "Epoch: 25, Train_Loss: 0.1769389510154724, Test_Loss: 0.21597442030906677 *\n",
      "Epoch: 25, Train_Loss: 0.16969530284404755, Test_Loss: 0.18273861706256866 *\n",
      "Epoch: 25, Train_Loss: 0.15698222815990448, Test_Loss: 0.1759738177061081 *\n",
      "Epoch: 25, Train_Loss: 0.15989604592323303, Test_Loss: 0.17222021520137787 *\n",
      "Epoch: 26, Train_Loss: 0.16482947766780853, Test_Loss: 0.34703147411346436 *\n",
      "Epoch: 26, Train_Loss: 0.16393819451332092, Test_Loss: 0.287803590297699 *\n",
      "Epoch: 26, Train_Loss: 0.1906183362007141, Test_Loss: 0.4855802655220032\n",
      "Epoch: 26, Train_Loss: 0.17961376905441284, Test_Loss: 0.2577481269836426 *\n",
      "Epoch: 26, Train_Loss: 0.14989709854125977, Test_Loss: 0.1913014054298401 *\n",
      "Epoch: 26, Train_Loss: 0.17366927862167358, Test_Loss: 0.15649808943271637 *\n",
      "Epoch: 26, Train_Loss: 0.19357185065746307, Test_Loss: 0.14924471080303192 *\n",
      "Epoch: 26, Train_Loss: 0.15967893600463867, Test_Loss: 0.171886146068573\n",
      "Epoch: 26, Train_Loss: 0.1556244045495987, Test_Loss: 0.1534060388803482 *\n",
      "Epoch: 26, Train_Loss: 0.19339300692081451, Test_Loss: 0.17038296163082123\n",
      "Epoch: 26, Train_Loss: 0.23305034637451172, Test_Loss: 0.1551283597946167 *\n",
      "Epoch: 26, Train_Loss: 0.18486268818378448, Test_Loss: 0.23863595724105835\n",
      "Epoch: 26, Train_Loss: 0.17386586964130402, Test_Loss: 0.3126601576805115\n",
      "Epoch: 26, Train_Loss: 0.17398184537887573, Test_Loss: 0.4414769411087036\n",
      "Epoch: 26, Train_Loss: 0.15375542640686035, Test_Loss: 0.37090182304382324 *\n",
      "Epoch: 26, Train_Loss: 0.1718035638332367, Test_Loss: 0.16203558444976807 *\n",
      "Epoch: 26, Train_Loss: 0.14989042282104492, Test_Loss: 0.15836289525032043 *\n",
      "Epoch: 26, Train_Loss: 0.15603120625019073, Test_Loss: 0.1579747498035431 *\n",
      "Epoch: 26, Train_Loss: 0.15574410557746887, Test_Loss: 0.15791712701320648 *\n",
      "Epoch: 26, Train_Loss: 0.16650885343551636, Test_Loss: 0.1877746284008026\n",
      "Epoch: 26, Train_Loss: 0.1995094120502472, Test_Loss: 2.3381295204162598\n",
      "Epoch: 26, Train_Loss: 0.1661134660243988, Test_Loss: 3.0129642486572266\n",
      "Epoch: 26, Train_Loss: 0.19116419553756714, Test_Loss: 0.16255520284175873 *\n",
      "Epoch: 26, Train_Loss: 0.15742254257202148, Test_Loss: 0.1598215401172638 *\n",
      "Epoch: 26, Train_Loss: 0.192564457654953, Test_Loss: 0.15693573653697968 *\n",
      "Epoch: 26, Train_Loss: 0.1633344441652298, Test_Loss: 0.15416698157787323 *\n",
      "Epoch: 26, Train_Loss: 0.43181174993515015, Test_Loss: 0.15257662534713745 *\n",
      "Epoch: 26, Train_Loss: 0.19935885071754456, Test_Loss: 0.16373561322689056\n",
      "Epoch: 26, Train_Loss: 0.16227373480796814, Test_Loss: 0.15235033631324768 *\n",
      "Epoch: 26, Train_Loss: 0.1640629768371582, Test_Loss: 0.1507134586572647 *\n",
      "Epoch: 26, Train_Loss: 0.14981497824192047, Test_Loss: 0.15178295969963074\n",
      "Epoch: 26, Train_Loss: 0.15421627461910248, Test_Loss: 0.1509559154510498 *\n",
      "Epoch: 26, Train_Loss: 0.15287458896636963, Test_Loss: 0.1586109697818756\n",
      "Epoch: 26, Train_Loss: 0.1557275503873825, Test_Loss: 0.18027301132678986\n",
      "Epoch: 26, Train_Loss: 0.149624302983284, Test_Loss: 0.20320574939250946\n",
      "Epoch: 26, Train_Loss: 0.16639725863933563, Test_Loss: 0.15410147607326508 *\n",
      "Epoch: 26, Train_Loss: 0.15301768481731415, Test_Loss: 0.15050435066223145 *\n",
      "Epoch: 26, Train_Loss: 0.15418210625648499, Test_Loss: 0.15144965052604675\n",
      "Epoch: 26, Train_Loss: 0.15991026163101196, Test_Loss: 0.15459664165973663\n",
      "Epoch: 26, Train_Loss: 0.15031005442142487, Test_Loss: 0.15289980173110962 *\n",
      "Epoch: 26, Train_Loss: 0.14826086163520813, Test_Loss: 0.15048587322235107 *\n",
      "Epoch: 26, Train_Loss: 0.1636657565832138, Test_Loss: 0.1556922197341919\n",
      "Epoch: 26, Train_Loss: 0.15510785579681396, Test_Loss: 0.14835497736930847 *\n",
      "Epoch: 26, Train_Loss: 0.15810620784759521, Test_Loss: 0.15071165561676025\n",
      "Epoch: 26, Train_Loss: 0.15592464804649353, Test_Loss: 0.1525290608406067\n",
      "Epoch: 26, Train_Loss: 0.16103781759738922, Test_Loss: 0.15051282942295074 *\n",
      "Epoch: 26, Train_Loss: 0.15392771363258362, Test_Loss: 0.14936548471450806 *\n",
      "Epoch: 26, Train_Loss: 0.17010509967803955, Test_Loss: 0.15685990452766418\n",
      "Epoch: 26, Train_Loss: 0.14967067539691925, Test_Loss: 0.1530373990535736 *\n",
      "Epoch: 26, Train_Loss: 0.16510054469108582, Test_Loss: 0.14813031256198883 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.15938054025173187, Test_Loss: 0.20285248756408691\n",
      "Epoch: 26, Train_Loss: 0.15991133451461792, Test_Loss: 0.20653916895389557\n",
      "Epoch: 26, Train_Loss: 0.14985990524291992, Test_Loss: 5.0186052322387695\n",
      "Epoch: 26, Train_Loss: 0.16654519736766815, Test_Loss: 1.4841126203536987 *\n",
      "Epoch: 26, Train_Loss: 0.21002373099327087, Test_Loss: 0.16051596403121948 *\n",
      "Epoch: 26, Train_Loss: 2.663931131362915, Test_Loss: 0.17696397006511688\n",
      "Epoch: 26, Train_Loss: 3.113374948501587, Test_Loss: 0.16139060258865356 *\n",
      "Epoch: 26, Train_Loss: 0.1665436029434204, Test_Loss: 0.1587320864200592 *\n",
      "Epoch: 26, Train_Loss: 0.1498786211013794, Test_Loss: 0.16031350195407867\n",
      "Epoch: 26, Train_Loss: 0.17613102495670319, Test_Loss: 0.2514371871948242\n",
      "Epoch: 26, Train_Loss: 0.22288411855697632, Test_Loss: 0.25120776891708374 *\n",
      "Epoch: 26, Train_Loss: 0.16840225458145142, Test_Loss: 0.14984872937202454 *\n",
      "Epoch: 26, Train_Loss: 0.15069307386875153, Test_Loss: 0.17703258991241455\n",
      "Epoch: 26, Train_Loss: 0.17751207947731018, Test_Loss: 0.16002832353115082 *\n",
      "Epoch: 26, Train_Loss: 0.20841661095619202, Test_Loss: 0.15968675911426544 *\n",
      "Epoch: 26, Train_Loss: 0.16477704048156738, Test_Loss: 0.16395749151706696\n",
      "Epoch: 26, Train_Loss: 0.1863185465335846, Test_Loss: 0.17515681684017181\n",
      "Epoch: 26, Train_Loss: 0.48048025369644165, Test_Loss: 0.1933208554983139\n",
      "Epoch: 26, Train_Loss: 0.6179265975952148, Test_Loss: 0.23200355470180511\n",
      "Epoch: 26, Train_Loss: 0.3192421495914459, Test_Loss: 0.177517831325531 *\n",
      "Epoch: 26, Train_Loss: 0.2760746479034424, Test_Loss: 0.18911780416965485\n",
      "Epoch: 26, Train_Loss: 0.6899619102478027, Test_Loss: 0.1737050712108612 *\n",
      "Epoch: 26, Train_Loss: 0.4504907727241516, Test_Loss: 0.22833535075187683\n",
      "Epoch: 26, Train_Loss: 0.1588279753923416, Test_Loss: 0.21202895045280457 *\n",
      "Epoch: 26, Train_Loss: 0.14920084178447723, Test_Loss: 0.25888198614120483\n",
      "Epoch: 26, Train_Loss: 0.4432646632194519, Test_Loss: 0.1935059130191803 *\n",
      "Epoch: 26, Train_Loss: 0.4608820676803589, Test_Loss: 0.2151048183441162\n",
      "Epoch: 26, Train_Loss: 0.589290201663971, Test_Loss: 0.2909189462661743\n",
      "Epoch: 26, Train_Loss: 0.15541118383407593, Test_Loss: 0.29027795791625977 *\n",
      "Epoch: 26, Train_Loss: 0.16885429620742798, Test_Loss: 0.33762866258621216\n",
      "Epoch: 26, Train_Loss: 0.21329522132873535, Test_Loss: 0.1690974086523056 *\n",
      "Epoch: 26, Train_Loss: 0.29684728384017944, Test_Loss: 0.16334189474582672 *\n",
      "Epoch: 26, Train_Loss: 0.1599796712398529, Test_Loss: 0.1613861620426178 *\n",
      "Epoch: 26, Train_Loss: 0.2151479721069336, Test_Loss: 0.1900579184293747\n",
      "Epoch: 26, Train_Loss: 0.1746048480272293, Test_Loss: 0.342960387468338\n",
      "Epoch: 26, Train_Loss: 0.22107020020484924, Test_Loss: 0.23993852734565735 *\n",
      "Epoch: 26, Train_Loss: 0.28648805618286133, Test_Loss: 0.30529093742370605\n",
      "Epoch: 26, Train_Loss: 0.25995075702667236, Test_Loss: 0.2058933973312378 *\n",
      "Epoch: 26, Train_Loss: 0.18436762690544128, Test_Loss: 0.20837914943695068\n",
      "Epoch: 26, Train_Loss: 0.18554633855819702, Test_Loss: 0.16773726046085358 *\n",
      "Epoch: 26, Train_Loss: 0.2633504271507263, Test_Loss: 0.17257988452911377\n",
      "Epoch: 26, Train_Loss: 0.1804293692111969, Test_Loss: 0.518959641456604\n",
      "Epoch: 26, Train_Loss: 0.18109862506389618, Test_Loss: 0.19899970293045044 *\n",
      "Epoch: 26, Train_Loss: 0.27889350056648254, Test_Loss: 0.49368569254875183\n",
      "Epoch: 26, Train_Loss: 0.18705344200134277, Test_Loss: 0.22104588150978088 *\n",
      "Epoch: 26, Train_Loss: 0.18544644117355347, Test_Loss: 0.1778343766927719 *\n",
      "Epoch: 26, Train_Loss: 0.2101231813430786, Test_Loss: 0.16055253148078918 *\n",
      "Epoch: 26, Train_Loss: 0.16608943045139313, Test_Loss: 0.1515260487794876 *\n",
      "Epoch: 26, Train_Loss: 0.15710283815860748, Test_Loss: 0.2448175549507141\n",
      "Epoch: 26, Train_Loss: 0.14800173044204712, Test_Loss: 0.16264285147190094 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.15144187211990356, Test_Loss: 0.17249594628810883\n",
      "Epoch: 26, Train_Loss: 0.1512402445077896, Test_Loss: 0.16775009036064148 *\n",
      "Epoch: 26, Train_Loss: 0.16068488359451294, Test_Loss: 0.3380255103111267\n",
      "Epoch: 26, Train_Loss: 0.1578916609287262, Test_Loss: 0.4946541488170624\n",
      "Epoch: 26, Train_Loss: 0.1561749279499054, Test_Loss: 0.26015985012054443 *\n",
      "Epoch: 26, Train_Loss: 0.15665403008460999, Test_Loss: 0.4220693111419678\n",
      "Epoch: 26, Train_Loss: 0.37064677476882935, Test_Loss: 0.16738757491111755 *\n",
      "Epoch: 26, Train_Loss: 0.4172307550907135, Test_Loss: 0.16477909684181213 *\n",
      "Epoch: 26, Train_Loss: 0.15941911935806274, Test_Loss: 0.16320979595184326 *\n",
      "Epoch: 26, Train_Loss: 0.20454144477844238, Test_Loss: 0.16146916151046753 *\n",
      "Epoch: 26, Train_Loss: 0.19008639454841614, Test_Loss: 0.2392987757921219\n",
      "Epoch: 26, Train_Loss: 0.18840602040290833, Test_Loss: 4.23225736618042\n",
      "Epoch: 26, Train_Loss: 0.3496370315551758, Test_Loss: 1.6633665561676025 *\n",
      "Epoch: 26, Train_Loss: 0.1906375288963318, Test_Loss: 0.1548122763633728 *\n",
      "Epoch: 26, Train_Loss: 0.5077780485153198, Test_Loss: 0.16286572813987732\n",
      "Epoch: 26, Train_Loss: 0.21110354363918304, Test_Loss: 0.1561376303434372 *\n",
      "Epoch: 26, Train_Loss: 0.26775434613227844, Test_Loss: 0.15111030638217926 *\n",
      "Epoch: 26, Train_Loss: 0.15941649675369263, Test_Loss: 0.1631954312324524\n",
      "Epoch: 26, Train_Loss: 0.15844713151454926, Test_Loss: 0.1778002232313156\n",
      "Epoch: 26, Train_Loss: 0.28418007493019104, Test_Loss: 0.16251839697360992 *\n",
      "Epoch: 26, Train_Loss: 0.6104403734207153, Test_Loss: 0.15071846544742584 *\n",
      "Epoch: 26, Train_Loss: 0.5569360852241516, Test_Loss: 0.1559080332517624\n",
      "Epoch: 26, Train_Loss: 0.16223669052124023, Test_Loss: 0.15712913870811462\n",
      "Epoch: 26, Train_Loss: 0.16862992942333221, Test_Loss: 0.18917778134346008\n",
      "Epoch: 26, Train_Loss: 0.15475107729434967, Test_Loss: 0.16666123270988464 *\n",
      "Epoch: 26, Train_Loss: 0.33401691913604736, Test_Loss: 0.1828664392232895\n",
      "Epoch: 26, Train_Loss: 0.4545219838619232, Test_Loss: 0.15166352689266205 *\n",
      "Epoch: 26, Train_Loss: 0.154889777302742, Test_Loss: 0.15232141315937042\n",
      "Epoch: 26, Train_Loss: 0.3230177164077759, Test_Loss: 0.1652737259864807\n",
      "Epoch: 26, Train_Loss: 0.1658777892589569, Test_Loss: 0.16468775272369385 *\n",
      "Epoch: 26, Train_Loss: 0.1720823347568512, Test_Loss: 0.14924149215221405 *\n",
      "Epoch: 26, Train_Loss: 0.20576460659503937, Test_Loss: 0.15346230566501617\n",
      "Epoch: 26, Train_Loss: 0.25706347823143005, Test_Loss: 0.1564817875623703\n",
      "Epoch: 26, Train_Loss: 0.2286836802959442, Test_Loss: 0.1512746661901474 *\n",
      "Epoch: 26, Train_Loss: 0.2102535367012024, Test_Loss: 0.15063589811325073 *\n",
      "Epoch: 26, Train_Loss: 0.1674070954322815, Test_Loss: 0.1624302715063095\n",
      "Epoch: 26, Train_Loss: 0.2238846719264984, Test_Loss: 0.15193593502044678 *\n",
      "Epoch: 26, Train_Loss: 0.19073733687400818, Test_Loss: 0.15031911432743073 *\n",
      "Epoch: 26, Train_Loss: 0.1731434464454651, Test_Loss: 0.17948104441165924\n",
      "Epoch: 26, Train_Loss: 0.15738822519779205, Test_Loss: 0.17292293906211853 *\n",
      "Epoch: 26, Train_Loss: 0.16212224960327148, Test_Loss: 0.14857229590415955 *\n",
      "Epoch: 26, Train_Loss: 0.2124926596879959, Test_Loss: 0.22618326544761658\n",
      "Epoch: 26, Train_Loss: 0.2957965135574341, Test_Loss: 0.254242479801178\n",
      "Epoch: 26, Train_Loss: 0.33785516023635864, Test_Loss: 5.136780261993408\n",
      "Epoch: 26, Train_Loss: 0.6115305423736572, Test_Loss: 0.3116586208343506 *\n",
      "Epoch: 26, Train_Loss: 0.49263542890548706, Test_Loss: 0.1570909023284912 *\n",
      "Epoch: 26, Train_Loss: 0.3453757166862488, Test_Loss: 0.18201835453510284\n",
      "Epoch: 26, Train_Loss: 0.2064010500907898, Test_Loss: 0.16191256046295166 *\n",
      "Epoch: 26, Train_Loss: 0.17187397181987762, Test_Loss: 0.15472963452339172 *\n",
      "Epoch: 26, Train_Loss: 0.15257595479488373, Test_Loss: 0.1606895476579666\n",
      "Epoch: 26, Train_Loss: 0.15762285888195038, Test_Loss: 0.19286100566387177\n",
      "Epoch: 26, Train_Loss: 0.26266103982925415, Test_Loss: 0.20096875727176666\n",
      "Epoch: 26, Train_Loss: 0.4695596396923065, Test_Loss: 0.14780569076538086 *\n",
      "Epoch: 26, Train_Loss: 0.5412391424179077, Test_Loss: 0.1736045479774475\n",
      "Epoch: 26, Train_Loss: 0.9587137699127197, Test_Loss: 0.17236104607582092 *\n",
      "Epoch: 26, Train_Loss: 1.0742805004119873, Test_Loss: 0.17695637047290802\n",
      "Epoch: 26, Train_Loss: 0.2820410132408142, Test_Loss: 0.19570286571979523\n",
      "Epoch: 26, Train_Loss: 0.3340472877025604, Test_Loss: 0.17756441235542297 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.15356487035751343, Test_Loss: 0.21839606761932373\n",
      "Epoch: 26, Train_Loss: 0.2142476737499237, Test_Loss: 0.2004595696926117 *\n",
      "Epoch: 26, Train_Loss: 0.3551119565963745, Test_Loss: 0.20326419174671173\n",
      "Epoch: 26, Train_Loss: 0.7356753945350647, Test_Loss: 0.24921508133411407\n",
      "Epoch: 26, Train_Loss: 0.18195262551307678, Test_Loss: 0.1702442318201065 *\n",
      "Epoch: 26, Train_Loss: 0.1841469556093216, Test_Loss: 0.28209012746810913\n",
      "Epoch: 26, Train_Loss: 0.1831609606742859, Test_Loss: 0.28355520963668823\n",
      "Epoch: 26, Train_Loss: 0.3481771647930145, Test_Loss: 0.328728049993515\n",
      "Epoch: 26, Train_Loss: 0.31375768780708313, Test_Loss: 0.2856605052947998 *\n",
      "Epoch: 26, Train_Loss: 0.43257617950439453, Test_Loss: 0.2673216462135315 *\n",
      "Epoch: 26, Train_Loss: 0.3019261658191681, Test_Loss: 0.32380086183547974\n",
      "Epoch: 26, Train_Loss: 0.353118360042572, Test_Loss: 0.2526654005050659 *\n",
      "Epoch: 26, Train_Loss: 0.15424659848213196, Test_Loss: 0.20317897200584412 *\n",
      "Epoch: 26, Train_Loss: 0.16400785744190216, Test_Loss: 0.19609886407852173 *\n",
      "Epoch: 26, Train_Loss: 0.1497650295495987, Test_Loss: 0.16667869687080383 *\n",
      "Epoch: 26, Train_Loss: 0.1764746457338333, Test_Loss: 0.1530577689409256 *\n",
      "Epoch: 26, Train_Loss: 0.17526717483997345, Test_Loss: 0.17065805196762085\n",
      "Epoch: 26, Train_Loss: 0.2451213002204895, Test_Loss: 0.31649065017700195\n",
      "Epoch: 26, Train_Loss: 14.908635139465332, Test_Loss: 0.2282244712114334 *\n",
      "Epoch: 26, Train_Loss: 0.31304532289505005, Test_Loss: 0.22869527339935303\n",
      "Epoch: 26, Train_Loss: 1.1542551517486572, Test_Loss: 0.18998250365257263 *\n",
      "Epoch: 26, Train_Loss: 1.0079236030578613, Test_Loss: 0.19049116969108582\n",
      "Epoch: 26, Train_Loss: 0.19619593024253845, Test_Loss: 0.1589997559785843 *\n",
      "Epoch: 26, Train_Loss: 0.2545316815376282, Test_Loss: 0.19800786674022675\n",
      "Epoch: 26, Train_Loss: 2.0895304679870605, Test_Loss: 0.3794323205947876\n",
      "Epoch: 26, Train_Loss: 4.404035568237305, Test_Loss: 0.2929409146308899 *\n",
      "Epoch: 26, Train_Loss: 0.23643186688423157, Test_Loss: 0.33352282643318176\n",
      "Epoch: 26, Train_Loss: 0.37793171405792236, Test_Loss: 0.19158795475959778 *\n",
      "Epoch: 26, Train_Loss: 4.076794147491455, Test_Loss: 0.17949455976486206 *\n",
      "Epoch: 26, Train_Loss: 0.3235676884651184, Test_Loss: 0.22305262088775635\n",
      "Epoch: 26, Train_Loss: 0.25721558928489685, Test_Loss: 0.19237133860588074 *\n",
      "Epoch: 26, Train_Loss: 0.1590886265039444, Test_Loss: 0.25952088832855225\n",
      "Epoch: 26, Train_Loss: 0.19313174486160278, Test_Loss: 0.16761550307273865 *\n",
      "Epoch: 26, Train_Loss: 0.1980689913034439, Test_Loss: 0.1773950457572937\n",
      "Epoch: 26, Train_Loss: 0.14799141883850098, Test_Loss: 0.19091874361038208\n",
      "Epoch: 26, Train_Loss: 0.16662733256816864, Test_Loss: 0.30201074481010437\n",
      "Epoch: 26, Train_Loss: 0.14680278301239014, Test_Loss: 1.0088934898376465\n",
      "Epoch: 26, Train_Loss: 0.14742939174175262, Test_Loss: 0.2631090581417084 *\n",
      "Epoch: 26, Train_Loss: 0.16458182036876678, Test_Loss: 0.3142995536327362\n",
      "Epoch: 26, Train_Loss: 0.1610887348651886, Test_Loss: 0.15484845638275146 *\n",
      "Epoch: 26, Train_Loss: 0.17332829535007477, Test_Loss: 0.154841348528862 *\n",
      "Epoch: 26, Train_Loss: 0.2010461986064911, Test_Loss: 0.1550874263048172\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.15750843286514282, Test_Loss: 0.15540990233421326\n",
      "Epoch: 26, Train_Loss: 0.1590777039527893, Test_Loss: 0.19870969653129578\n",
      "Epoch: 26, Train_Loss: 0.16444925963878632, Test_Loss: 6.260705947875977\n",
      "Epoch: 26, Train_Loss: 0.15155859291553497, Test_Loss: 0.8704770803451538 *\n",
      "Epoch: 26, Train_Loss: 0.15886470675468445, Test_Loss: 0.4270462393760681 *\n",
      "Epoch: 26, Train_Loss: 0.1491418033838272, Test_Loss: 0.4508329927921295\n",
      "Epoch: 26, Train_Loss: 0.14649811387062073, Test_Loss: 0.5549936890602112\n",
      "Epoch: 26, Train_Loss: 0.14642804861068726, Test_Loss: 0.35339540243148804 *\n",
      "Epoch: 26, Train_Loss: 0.14708499610424042, Test_Loss: 0.8255711793899536\n",
      "Epoch: 26, Train_Loss: 0.146648570895195, Test_Loss: 0.8083525896072388 *\n",
      "Epoch: 26, Train_Loss: 0.14628954231739044, Test_Loss: 0.7049155235290527 *\n",
      "Epoch: 26, Train_Loss: 0.14618952572345734, Test_Loss: 0.6469174027442932 *\n",
      "Epoch: 26, Train_Loss: 0.153323695063591, Test_Loss: 0.513538658618927 *\n",
      "Epoch: 26, Train_Loss: 0.17440830171108246, Test_Loss: 0.8211418986320496\n",
      "Epoch: 26, Train_Loss: 0.1779206246137619, Test_Loss: 1.0606188774108887\n",
      "Epoch: 26, Train_Loss: 0.21235918998718262, Test_Loss: 0.5011155605316162 *\n",
      "Epoch: 26, Train_Loss: 0.26559823751449585, Test_Loss: 0.45416539907455444 *\n",
      "Epoch: 26, Train_Loss: 1.3539915084838867, Test_Loss: 0.1599092036485672 *\n",
      "Epoch: 26, Train_Loss: 4.584869861602783, Test_Loss: 0.16158810257911682\n",
      "Epoch: 26, Train_Loss: 0.2177332043647766, Test_Loss: 0.1599770337343216 *\n",
      "Epoch: 26, Train_Loss: 0.22122032940387726, Test_Loss: 0.34452325105667114\n",
      "Epoch: 26, Train_Loss: 0.2055816352367401, Test_Loss: 0.23647046089172363 *\n",
      "Epoch: 26, Train_Loss: 0.3378528356552124, Test_Loss: 0.20482978224754333 *\n",
      "Epoch: 26, Train_Loss: 0.2550352215766907, Test_Loss: 0.28569933772087097\n",
      "Epoch: 26, Train_Loss: 0.21581462025642395, Test_Loss: 0.26011592149734497 *\n",
      "Epoch: 26, Train_Loss: 0.19481325149536133, Test_Loss: 0.21036577224731445 *\n",
      "Epoch: 26, Train_Loss: 0.22370798885822296, Test_Loss: 0.2964093089103699\n",
      "Epoch: 26, Train_Loss: 0.23746134340763092, Test_Loss: 0.16081614792346954 *\n",
      "Epoch: 26, Train_Loss: 0.17767097055912018, Test_Loss: 0.15822559595108032 *\n",
      "Epoch: 26, Train_Loss: 0.15779289603233337, Test_Loss: 0.17840665578842163\n",
      "Epoch: 26, Train_Loss: 0.17155596613883972, Test_Loss: 0.15894095599651337 *\n",
      "Epoch: 26, Train_Loss: 0.15557166934013367, Test_Loss: 0.1511811763048172 *\n",
      "Epoch: 26, Train_Loss: 0.16688589751720428, Test_Loss: 0.3531627655029297\n",
      "Epoch: 26, Train_Loss: 0.15627267956733704, Test_Loss: 0.8563416004180908\n",
      "Epoch: 26, Train_Loss: 0.19615457952022552, Test_Loss: 6.231749534606934\n",
      "Epoch: 26, Train_Loss: 0.15467698872089386, Test_Loss: 0.33511942625045776 *\n",
      "Epoch: 26, Train_Loss: 0.1771044135093689, Test_Loss: 0.15260128676891327 *\n",
      "Epoch: 26, Train_Loss: 0.22928479313850403, Test_Loss: 0.2036711573600769\n",
      "Epoch: 26, Train_Loss: 0.21219104528427124, Test_Loss: 0.19698071479797363 *\n",
      "Epoch: 26, Train_Loss: 0.15504597127437592, Test_Loss: 0.1845315396785736 *\n",
      "Epoch: 26, Train_Loss: 0.14714141190052032, Test_Loss: 0.16733139753341675 *\n",
      "Epoch: 26, Train_Loss: 0.15376143157482147, Test_Loss: 0.36003002524375916\n",
      "Epoch: 26, Train_Loss: 1.1774622201919556, Test_Loss: 0.17730289697647095 *\n",
      "Epoch: 26, Train_Loss: 3.8986399173736572, Test_Loss: 0.15428844094276428 *\n",
      "Epoch: 26, Train_Loss: 0.15554305911064148, Test_Loss: 0.16649124026298523\n",
      "Epoch: 26, Train_Loss: 0.15832385420799255, Test_Loss: 0.19794033467769623\n",
      "Epoch: 26, Train_Loss: 0.15464690327644348, Test_Loss: 0.15319308638572693 *\n",
      "Epoch: 26, Train_Loss: 0.14867869019508362, Test_Loss: 0.21642547845840454\n",
      "Epoch: 26, Train_Loss: 0.14791186153888702, Test_Loss: 0.17596550285816193 *\n",
      "Epoch: 26, Train_Loss: 0.15064875781536102, Test_Loss: 0.25078803300857544\n",
      "Epoch: 26, Train_Loss: 0.16190265119075775, Test_Loss: 0.17647399008274078 *\n",
      "Epoch: 26, Train_Loss: 0.15850278735160828, Test_Loss: 0.19059252738952637\n",
      "Epoch: 26, Train_Loss: 0.1747845709323883, Test_Loss: 0.2293243110179901\n",
      "Epoch: 26, Train_Loss: 0.14746859669685364, Test_Loss: 0.25635480880737305\n",
      "Epoch: 26, Train_Loss: 0.14694923162460327, Test_Loss: 0.22658102214336395 *\n",
      "Epoch: 26, Train_Loss: 0.1492013931274414, Test_Loss: 0.17965617775917053 *\n",
      "Epoch: 26, Train_Loss: 0.1638135462999344, Test_Loss: 0.1750849336385727 *\n",
      "Epoch: 26, Train_Loss: 0.1483176052570343, Test_Loss: 0.1826508492231369\n",
      "Epoch: 26, Train_Loss: 0.14878034591674805, Test_Loss: 0.17869193851947784 *\n",
      "Epoch: 26, Train_Loss: 0.1629600077867508, Test_Loss: 0.21552813053131104\n",
      "Epoch: 26, Train_Loss: 0.1638214886188507, Test_Loss: 0.3293493986129761\n",
      "Epoch: 26, Train_Loss: 0.15004193782806396, Test_Loss: 0.20752353966236115 *\n",
      "Epoch: 26, Train_Loss: 0.14653997123241425, Test_Loss: 0.18371090292930603 *\n",
      "Epoch: 26, Train_Loss: 0.17290601134300232, Test_Loss: 0.15085621178150177 *\n",
      "Epoch: 26, Train_Loss: 0.19868265092372894, Test_Loss: 0.16353186964988708\n",
      "Epoch: 26, Train_Loss: 0.18160846829414368, Test_Loss: 0.19942615926265717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.1723458170890808, Test_Loss: 0.3407529592514038\n",
      "Epoch: 26, Train_Loss: 0.24207323789596558, Test_Loss: 0.26983195543289185 *\n",
      "Epoch: 26, Train_Loss: 0.20010800659656525, Test_Loss: 0.19360662996768951 *\n",
      "Epoch: 26, Train_Loss: 0.16932427883148193, Test_Loss: 0.23003673553466797\n",
      "Epoch: 26, Train_Loss: 0.19202955067157745, Test_Loss: 0.17144621908664703 *\n",
      "Epoch: 26, Train_Loss: 0.16300499439239502, Test_Loss: 0.17045974731445312 *\n",
      "Epoch: 26, Train_Loss: 0.2934619188308716, Test_Loss: 0.24903680384159088\n",
      "Epoch: 26, Train_Loss: 0.16927042603492737, Test_Loss: 0.5370999574661255\n",
      "Epoch: 26, Train_Loss: 0.14710113406181335, Test_Loss: 0.7102111577987671\n",
      "Epoch: 26, Train_Loss: 0.146137997508049, Test_Loss: 0.21716022491455078 *\n",
      "Epoch: 26, Train_Loss: 0.14568819105625153, Test_Loss: 0.22067725658416748\n",
      "Epoch: 26, Train_Loss: 0.14556320011615753, Test_Loss: 0.1504601091146469 *\n",
      "Epoch: 26, Train_Loss: 0.14648611843585968, Test_Loss: 0.15849286317825317\n",
      "Epoch: 26, Train_Loss: 1.8489190340042114, Test_Loss: 0.1571255475282669 *\n",
      "Epoch: 26, Train_Loss: 2.783010482788086, Test_Loss: 0.15589454770088196 *\n",
      "Epoch: 26, Train_Loss: 0.15548568964004517, Test_Loss: 0.1675945520401001\n",
      "Epoch: 26, Train_Loss: 0.1498575657606125, Test_Loss: 0.16360022127628326 *\n",
      "Epoch: 26, Train_Loss: 0.15262481570243835, Test_Loss: 0.1534925252199173 *\n",
      "Epoch: 26, Train_Loss: 0.14748021960258484, Test_Loss: 0.2640482783317566\n",
      "Epoch: 26, Train_Loss: 0.14983101189136505, Test_Loss: 0.5172868967056274\n",
      "Epoch: 26, Train_Loss: 0.1522444188594818, Test_Loss: 0.3053273558616638 *\n",
      "Epoch: 26, Train_Loss: 0.14835424721240997, Test_Loss: 0.2465551197528839 *\n",
      "Epoch: 26, Train_Loss: 0.14950378239154816, Test_Loss: 0.16371196508407593 *\n",
      "Epoch: 26, Train_Loss: 0.1549510657787323, Test_Loss: 0.16419318318367004\n",
      "Epoch: 26, Train_Loss: 0.1629180908203125, Test_Loss: 0.16507330536842346\n",
      "Epoch: 26, Train_Loss: 0.1585422307252884, Test_Loss: 0.17504964768886566\n",
      "Epoch: 26, Train_Loss: 0.17356270551681519, Test_Loss: 0.25659048557281494\n",
      "Epoch: 26, Train_Loss: 0.17407917976379395, Test_Loss: 5.425703048706055\n",
      "Epoch: 26, Train_Loss: 0.14995357394218445, Test_Loss: 0.2966865599155426 *\n",
      "Epoch: 26, Train_Loss: 0.32221752405166626, Test_Loss: 0.16359999775886536 *\n",
      "Epoch: 26, Train_Loss: 0.21863481402397156, Test_Loss: 0.16192272305488586 *\n",
      "Epoch: 26, Train_Loss: 0.21330025792121887, Test_Loss: 0.15368962287902832 *\n",
      "Epoch: 26, Train_Loss: 0.2479950189590454, Test_Loss: 0.15275612473487854 *\n",
      "Epoch: 26, Train_Loss: 0.151508629322052, Test_Loss: 0.1708148717880249\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.14653344452381134, Test_Loss: 0.17065787315368652 *\n",
      "Epoch: 26, Train_Loss: 0.14988583326339722, Test_Loss: 0.158893883228302 *\n",
      "Epoch: 26, Train_Loss: 0.14768588542938232, Test_Loss: 0.15044574439525604 *\n",
      "Epoch: 26, Train_Loss: 0.14836464822292328, Test_Loss: 0.1649257242679596\n",
      "Epoch: 26, Train_Loss: 0.15088559687137604, Test_Loss: 0.27148371934890747\n",
      "Epoch: 26, Train_Loss: 0.14646390080451965, Test_Loss: 0.17274588346481323 *\n",
      "Epoch: 26, Train_Loss: 0.14647583663463593, Test_Loss: 0.15666836500167847 *\n",
      "Epoch: 26, Train_Loss: 0.15225115418434143, Test_Loss: 0.17473754286766052\n",
      "Epoch: 26, Train_Loss: 0.19065365195274353, Test_Loss: 0.1483631432056427 *\n",
      "Epoch: 26, Train_Loss: 0.20100486278533936, Test_Loss: 0.15104088187217712\n",
      "Epoch: 26, Train_Loss: 0.1609821915626526, Test_Loss: 0.15379689633846283\n",
      "Epoch: 26, Train_Loss: 0.2734307050704956, Test_Loss: 0.17128263413906097\n",
      "Epoch: 26, Train_Loss: 0.21866267919540405, Test_Loss: 0.15004763007164001 *\n",
      "Epoch: 26, Train_Loss: 0.23191386461257935, Test_Loss: 0.18543316423892975\n",
      "Epoch: 26, Train_Loss: 0.1992570161819458, Test_Loss: 0.14674344658851624 *\n",
      "Epoch: 26, Train_Loss: 0.23043054342269897, Test_Loss: 0.16748470067977905\n",
      "Epoch: 26, Train_Loss: 0.18935218453407288, Test_Loss: 0.16349902749061584 *\n",
      "Epoch: 26, Train_Loss: 0.38283976912498474, Test_Loss: 0.17515458166599274\n",
      "Epoch: 26, Train_Loss: 0.15955060720443726, Test_Loss: 0.14989978075027466 *\n",
      "Epoch: 26, Train_Loss: 0.19392512738704681, Test_Loss: 0.15115197002887726\n",
      "Epoch: 26, Train_Loss: 2.56124210357666, Test_Loss: 0.15787909924983978\n",
      "Epoch: 26, Train_Loss: 0.2948416471481323, Test_Loss: 0.1517016589641571 *\n",
      "Epoch: 26, Train_Loss: 0.18682979047298431, Test_Loss: 0.1513146609067917 *\n",
      "Epoch: 26, Train_Loss: 0.16857774555683136, Test_Loss: 0.26797792315483093\n",
      "Epoch: 26, Train_Loss: 0.15305198729038239, Test_Loss: 2.0579094886779785\n",
      "Epoch: 26, Train_Loss: 0.15733617544174194, Test_Loss: 4.19331169128418\n",
      "Epoch: 26, Train_Loss: 0.16196803748607635, Test_Loss: 0.22734230756759644 *\n",
      "Epoch: 26, Train_Loss: 0.18591897189617157, Test_Loss: 0.15076003968715668 *\n",
      "Epoch: 26, Train_Loss: 0.21551260352134705, Test_Loss: 0.1769036054611206\n",
      "Epoch: 26, Train_Loss: 0.17401564121246338, Test_Loss: 0.1820618361234665\n",
      "Epoch: 26, Train_Loss: 0.16330263018608093, Test_Loss: 0.16830943524837494 *\n",
      "Epoch: 26, Train_Loss: 0.1547541618347168, Test_Loss: 0.1717732548713684\n",
      "Epoch: 26, Train_Loss: 0.15164929628372192, Test_Loss: 0.24119700491428375\n",
      "Epoch: 26, Train_Loss: 0.15258879959583282, Test_Loss: 0.16062237322330475 *\n",
      "Epoch: 26, Train_Loss: 0.15703000128269196, Test_Loss: 0.1722647249698639\n",
      "Epoch: 26, Train_Loss: 0.17014296352863312, Test_Loss: 0.18643327057361603\n",
      "Epoch: 26, Train_Loss: 0.15716920793056488, Test_Loss: 0.2224799394607544\n",
      "Epoch: 26, Train_Loss: 0.14564299583435059, Test_Loss: 0.17824968695640564 *\n",
      "Epoch: 26, Train_Loss: 0.15802861750125885, Test_Loss: 0.24373146891593933\n",
      "Epoch: 26, Train_Loss: 0.1549934446811676, Test_Loss: 0.20840314030647278 *\n",
      "Epoch: 26, Train_Loss: 0.1549200862646103, Test_Loss: 0.19919386506080627 *\n",
      "Epoch: 26, Train_Loss: 0.1486300379037857, Test_Loss: 0.16732260584831238 *\n",
      "Epoch: 26, Train_Loss: 0.14727482199668884, Test_Loss: 0.25937703251838684\n",
      "Epoch: 26, Train_Loss: 0.14537860453128815, Test_Loss: 0.25766316056251526 *\n",
      "Epoch: 26, Train_Loss: 0.14996249973773956, Test_Loss: 0.2377324402332306 *\n",
      "Epoch: 26, Train_Loss: 0.15008336305618286, Test_Loss: 0.24474766850471497\n",
      "Epoch: 26, Train_Loss: 0.14725075662136078, Test_Loss: 0.23703867197036743 *\n",
      "Epoch: 26, Train_Loss: 0.14960405230522156, Test_Loss: 0.19975796341896057 *\n",
      "Epoch: 26, Train_Loss: 0.14895784854888916, Test_Loss: 0.20400623977184296\n",
      "Epoch: 26, Train_Loss: 0.14562588930130005, Test_Loss: 0.22110497951507568\n",
      "Epoch: 26, Train_Loss: 0.14956597983837128, Test_Loss: 0.2189592719078064 *\n",
      "Epoch: 26, Train_Loss: 0.15067148208618164, Test_Loss: 0.34334003925323486\n",
      "Epoch: 26, Train_Loss: 0.15368902683258057, Test_Loss: 0.17394837737083435 *\n",
      "Epoch: 26, Train_Loss: 0.15639469027519226, Test_Loss: 0.15480631589889526 *\n",
      "Epoch: 26, Train_Loss: 0.15550430119037628, Test_Loss: 0.16249141097068787\n",
      "Epoch: 26, Train_Loss: 0.18431499600410461, Test_Loss: 0.2184867262840271\n",
      "Epoch: 26, Train_Loss: 0.19552545249462128, Test_Loss: 0.18070341646671295 *\n",
      "Epoch: 26, Train_Loss: 0.16610664129257202, Test_Loss: 0.4697762131690979\n",
      "Epoch: 26, Train_Loss: 0.15640468895435333, Test_Loss: 0.36555173993110657 *\n",
      "Epoch: 26, Train_Loss: 0.17089256644248962, Test_Loss: 0.18573692440986633 *\n",
      "Epoch: 26, Train_Loss: 0.1781105399131775, Test_Loss: 0.20934268832206726\n",
      "Epoch: 26, Train_Loss: 0.153146892786026, Test_Loss: 0.1717831790447235 *\n",
      "Epoch: 26, Train_Loss: 0.16397704184055328, Test_Loss: 0.1691122055053711 *\n",
      "Epoch: 26, Train_Loss: 0.16325776278972626, Test_Loss: 0.22609493136405945\n",
      "Epoch: 26, Train_Loss: 0.16365264356136322, Test_Loss: 0.2091943472623825 *\n",
      "Epoch: 26, Train_Loss: 0.20475688576698303, Test_Loss: 0.4062000513076782\n",
      "Epoch: 26, Train_Loss: 0.16991673409938812, Test_Loss: 0.21050260961055756 *\n",
      "Epoch: 26, Train_Loss: 0.1543513536453247, Test_Loss: 0.2516902685165405\n",
      "Epoch: 26, Train_Loss: 0.15268711745738983, Test_Loss: 0.1515439748764038 *\n",
      "Epoch: 26, Train_Loss: 0.1823807656764984, Test_Loss: 0.14743945002555847 *\n",
      "Epoch: 26, Train_Loss: 0.16590310633182526, Test_Loss: 0.15544553101062775\n",
      "Epoch: 26, Train_Loss: 0.15665772557258606, Test_Loss: 0.15554842352867126\n",
      "Epoch: 26, Train_Loss: 0.15764646232128143, Test_Loss: 0.1657470166683197\n",
      "Epoch: 26, Train_Loss: 0.18079426884651184, Test_Loss: 0.15634898841381073 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train_Loss: 0.20605555176734924, Test_Loss: 0.1695956587791443\n",
      "Epoch: 26, Train_Loss: 0.16702128946781158, Test_Loss: 0.2730616331100464\n",
      "Epoch: 26, Train_Loss: 0.16791826486587524, Test_Loss: 0.4970737099647522\n",
      "Epoch: 26, Train_Loss: 0.16016069054603577, Test_Loss: 0.3646506071090698 *\n",
      "Epoch: 26, Train_Loss: 0.17645443975925446, Test_Loss: 0.15941786766052246 *\n",
      "Epoch: 26, Train_Loss: 0.14834332466125488, Test_Loss: 0.14997966587543488 *\n",
      "Epoch: 26, Train_Loss: 0.15060323476791382, Test_Loss: 0.14984455704689026 *\n",
      "Epoch: 26, Train_Loss: 0.17298519611358643, Test_Loss: 0.15003252029418945\n",
      "Epoch: 26, Train_Loss: 0.15317420661449432, Test_Loss: 0.17048025131225586\n",
      "Epoch: 26, Train_Loss: 0.17239098250865936, Test_Loss: 0.6722224354743958\n",
      "Epoch: 26, Train_Loss: 0.20732936263084412, Test_Loss: 4.915355205535889\n",
      "Epoch: 26, Train_Loss: 0.1673993468284607, Test_Loss: 0.19283917546272278 *\n",
      "Epoch: 26, Train_Loss: 0.17459990084171295, Test_Loss: 0.1575024276971817 *\n",
      "Epoch: 26, Train_Loss: 0.1662266105413437, Test_Loss: 0.15733011066913605 *\n",
      "Epoch: 26, Train_Loss: 0.15425392985343933, Test_Loss: 0.15034885704517365 *\n",
      "Epoch: 26, Train_Loss: 0.29062604904174805, Test_Loss: 0.15014758706092834 *\n",
      "Epoch: 26, Train_Loss: 0.2862294316291809, Test_Loss: 0.1633337289094925\n",
      "Epoch: 26, Train_Loss: 0.14761272072792053, Test_Loss: 0.1482238620519638 *\n",
      "Epoch: 26, Train_Loss: 0.1819545030593872, Test_Loss: 0.15524092316627502\n",
      "Epoch: 26, Train_Loss: 0.14593954384326935, Test_Loss: 0.1509976089000702 *\n",
      "Epoch: 26, Train_Loss: 0.14801304042339325, Test_Loss: 0.1487763375043869 *\n",
      "Epoch: 26, Train_Loss: 0.14845499396324158, Test_Loss: 0.1598636358976364\n",
      "Epoch: 26, Train_Loss: 0.15122786164283752, Test_Loss: 0.18703651428222656\n",
      "Epoch: 26, Train_Loss: 0.14558447897434235, Test_Loss: 0.20864060521125793\n",
      "Epoch: 26, Train_Loss: 0.15962083637714386, Test_Loss: 0.17994707822799683 *\n",
      "Epoch: 26, Train_Loss: 0.15023761987686157, Test_Loss: 0.14766691625118256 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 26\n",
      "Epoch: 26, Train_Loss: 0.15224049985408783, Test_Loss: 0.14887546002864838\n",
      "Epoch: 26, Train_Loss: 0.15499679744243622, Test_Loss: 0.14949534833431244\n",
      "Epoch: 26, Train_Loss: 0.14527010917663574, Test_Loss: 0.1501796394586563\n",
      "Epoch: 26, Train_Loss: 0.146126389503479, Test_Loss: 0.1494714319705963 *\n",
      "Epoch: 26, Train_Loss: 0.14949391782283783, Test_Loss: 0.1662423014640808\n",
      "Epoch: 26, Train_Loss: 0.1514832079410553, Test_Loss: 0.14529262483119965 *\n",
      "Epoch: 26, Train_Loss: 0.14965935051441193, Test_Loss: 0.148956298828125\n",
      "Epoch: 26, Train_Loss: 0.16269396245479584, Test_Loss: 0.1494203805923462\n",
      "Epoch: 26, Train_Loss: 0.16016080975532532, Test_Loss: 0.14715337753295898 *\n",
      "Epoch: 26, Train_Loss: 0.15277965366840363, Test_Loss: 0.14600393176078796 *\n",
      "Epoch: 26, Train_Loss: 0.15957282483577728, Test_Loss: 0.15221509337425232\n",
      "Epoch: 26, Train_Loss: 0.159497931599617, Test_Loss: 0.15167011320590973 *\n",
      "Epoch: 26, Train_Loss: 0.15470291674137115, Test_Loss: 0.14719665050506592 *\n",
      "Epoch: 26, Train_Loss: 0.16877174377441406, Test_Loss: 0.1579435020685196\n",
      "Epoch: 26, Train_Loss: 0.1497470587491989, Test_Loss: 0.23969073593616486\n",
      "Epoch: 26, Train_Loss: 0.15313777327537537, Test_Loss: 3.1771414279937744\n",
      "Epoch: 26, Train_Loss: 0.16601793467998505, Test_Loss: 3.6667354106903076\n",
      "Epoch: 26, Train_Loss: 0.18548794090747833, Test_Loss: 0.2019059956073761 *\n",
      "Epoch: 26, Train_Loss: 2.362060070037842, Test_Loss: 0.15273623168468475 *\n",
      "Epoch: 26, Train_Loss: 2.7887954711914062, Test_Loss: 0.1499614715576172 *\n",
      "Epoch: 26, Train_Loss: 0.16862256824970245, Test_Loss: 0.1577834188938141\n",
      "Epoch: 26, Train_Loss: 0.15332157909870148, Test_Loss: 0.1684635579586029\n",
      "Epoch: 26, Train_Loss: 0.17734025418758392, Test_Loss: 0.21357902884483337\n",
      "Epoch: 26, Train_Loss: 0.2139303982257843, Test_Loss: 0.32836979627609253\n",
      "Epoch: 26, Train_Loss: 0.16415594518184662, Test_Loss: 0.148731529712677 *\n",
      "Epoch: 26, Train_Loss: 0.15013180673122406, Test_Loss: 0.1688404232263565\n",
      "Epoch: 26, Train_Loss: 0.1517094224691391, Test_Loss: 0.17088943719863892\n",
      "Epoch: 26, Train_Loss: 0.22094199061393738, Test_Loss: 0.17220178246498108\n",
      "Epoch: 26, Train_Loss: 0.16660374402999878, Test_Loss: 0.15438181161880493 *\n",
      "Epoch: 26, Train_Loss: 0.16087643802165985, Test_Loss: 0.21431434154510498\n",
      "Epoch: 26, Train_Loss: 0.5239242911338806, Test_Loss: 0.21474355459213257\n",
      "Epoch: 26, Train_Loss: 0.3601323366165161, Test_Loss: 0.21297158300876617 *\n",
      "Epoch: 26, Train_Loss: 0.5794137716293335, Test_Loss: 0.15401482582092285 *\n",
      "Epoch: 26, Train_Loss: 0.22658804059028625, Test_Loss: 0.1936349868774414\n",
      "Epoch: 26, Train_Loss: 0.6272433996200562, Test_Loss: 0.17484349012374878 *\n",
      "Epoch: 26, Train_Loss: 0.4125048518180847, Test_Loss: 0.23996298015117645\n",
      "Epoch: 26, Train_Loss: 0.4233948886394501, Test_Loss: 0.32143938541412354\n",
      "Epoch: 26, Train_Loss: 0.1448211818933487, Test_Loss: 0.35771340131759644\n",
      "Epoch: 26, Train_Loss: 0.19380851089954376, Test_Loss: 0.25352251529693604 *\n",
      "Epoch: 26, Train_Loss: 0.4196361303329468, Test_Loss: 0.2887376546859741\n",
      "Epoch: 26, Train_Loss: 0.33959531784057617, Test_Loss: 0.34767067432403564\n",
      "Epoch: 26, Train_Loss: 0.22013546526432037, Test_Loss: 0.2953532338142395 *\n",
      "Epoch: 26, Train_Loss: 0.1538163721561432, Test_Loss: 0.3641839623451233\n",
      "Epoch: 26, Train_Loss: 0.1500132828950882, Test_Loss: 0.16101408004760742 *\n",
      "Epoch: 26, Train_Loss: 0.32151347398757935, Test_Loss: 0.156277135014534 *\n",
      "Epoch: 26, Train_Loss: 0.2322404682636261, Test_Loss: 0.15664395689964294\n",
      "Epoch: 26, Train_Loss: 0.2160586565732956, Test_Loss: 0.2109588384628296\n",
      "Epoch: 26, Train_Loss: 0.172602578997612, Test_Loss: 0.21535691618919373\n",
      "Epoch: 26, Train_Loss: 0.22086971998214722, Test_Loss: 0.28597766160964966\n",
      "Epoch: 26, Train_Loss: 0.282895028591156, Test_Loss: 0.3424845337867737\n",
      "Epoch: 26, Train_Loss: 0.3008711338043213, Test_Loss: 0.20538678765296936 *\n",
      "Epoch: 26, Train_Loss: 0.16411982476711273, Test_Loss: 0.20900849997997284\n",
      "Epoch: 26, Train_Loss: 0.18013553321361542, Test_Loss: 0.18255363404750824 *\n",
      "Epoch: 26, Train_Loss: 0.26655110716819763, Test_Loss: 0.15734729170799255 *\n",
      "Epoch: 27, Train_Loss: 0.1812920868396759, Test_Loss: 0.34873098134994507 *\n",
      "Epoch: 27, Train_Loss: 0.1753709763288498, Test_Loss: 0.22020146250724792 *\n",
      "Epoch: 27, Train_Loss: 0.2002081274986267, Test_Loss: 0.548255980014801\n",
      "Epoch: 27, Train_Loss: 0.23505786061286926, Test_Loss: 0.22804142534732819 *\n",
      "Epoch: 27, Train_Loss: 0.16959451138973236, Test_Loss: 0.20587347447872162 *\n",
      "Epoch: 27, Train_Loss: 0.18721088767051697, Test_Loss: 0.158444344997406 *\n",
      "Epoch: 27, Train_Loss: 0.1721166968345642, Test_Loss: 0.1451180875301361 *\n",
      "Epoch: 27, Train_Loss: 0.1527727246284485, Test_Loss: 0.16129019856452942\n",
      "Epoch: 27, Train_Loss: 0.14446480572223663, Test_Loss: 0.17896388471126556\n",
      "Epoch: 27, Train_Loss: 0.145018070936203, Test_Loss: 0.156954824924469 *\n",
      "Epoch: 27, Train_Loss: 0.14560888707637787, Test_Loss: 0.18347477912902832\n",
      "Epoch: 27, Train_Loss: 0.15344743430614471, Test_Loss: 0.25521719455718994\n",
      "Epoch: 27, Train_Loss: 0.15998969972133636, Test_Loss: 0.38132017850875854\n",
      "Epoch: 27, Train_Loss: 0.1553383767604828, Test_Loss: 0.4078071117401123\n",
      "Epoch: 27, Train_Loss: 0.154507577419281, Test_Loss: 0.4511057436466217\n",
      "Epoch: 27, Train_Loss: 0.3492196202278137, Test_Loss: 0.17677778005599976 *\n",
      "Epoch: 27, Train_Loss: 0.28631412982940674, Test_Loss: 0.16030511260032654 *\n",
      "Epoch: 27, Train_Loss: 0.15569186210632324, Test_Loss: 0.15857665240764618 *\n",
      "Epoch: 27, Train_Loss: 0.17817765474319458, Test_Loss: 0.15803276002407074 *\n",
      "Epoch: 27, Train_Loss: 0.19088239967823029, Test_Loss: 0.22504472732543945\n",
      "Epoch: 27, Train_Loss: 0.19961054623126984, Test_Loss: 1.9494922161102295\n",
      "Epoch: 27, Train_Loss: 0.375949501991272, Test_Loss: 3.638666868209839\n",
      "Epoch: 27, Train_Loss: 0.16711990535259247, Test_Loss: 0.15879939496517181 *\n",
      "Epoch: 27, Train_Loss: 0.3748220205307007, Test_Loss: 0.1572808176279068 *\n",
      "Epoch: 27, Train_Loss: 0.19478219747543335, Test_Loss: 0.15218372642993927 *\n",
      "Epoch: 27, Train_Loss: 0.20661789178848267, Test_Loss: 0.14698587357997894 *\n",
      "Epoch: 27, Train_Loss: 0.18521402776241302, Test_Loss: 0.1599641889333725\n",
      "Epoch: 27, Train_Loss: 0.15430134534835815, Test_Loss: 0.18402324616909027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.17281801998615265, Test_Loss: 0.15903440117835999 *\n",
      "Epoch: 27, Train_Loss: 0.542631208896637, Test_Loss: 0.15067371726036072 *\n",
      "Epoch: 27, Train_Loss: 0.5361944437026978, Test_Loss: 0.1501585692167282 *\n",
      "Epoch: 27, Train_Loss: 0.15122559666633606, Test_Loss: 0.1482847034931183 *\n",
      "Epoch: 27, Train_Loss: 0.17216870188713074, Test_Loss: 0.2135431170463562\n",
      "Epoch: 27, Train_Loss: 0.14784811437129974, Test_Loss: 0.15707631409168243 *\n",
      "Epoch: 27, Train_Loss: 0.20058676600456238, Test_Loss: 0.15851125121116638\n",
      "Epoch: 27, Train_Loss: 0.43235358595848083, Test_Loss: 0.1660463809967041\n",
      "Epoch: 27, Train_Loss: 0.14879760146141052, Test_Loss: 0.1512736827135086 *\n",
      "Epoch: 27, Train_Loss: 0.27533966302871704, Test_Loss: 0.1607968807220459\n",
      "Epoch: 27, Train_Loss: 0.20289236307144165, Test_Loss: 0.17333771288394928\n",
      "Epoch: 27, Train_Loss: 0.16872763633728027, Test_Loss: 0.15182898938655853 *\n",
      "Epoch: 27, Train_Loss: 0.20122110843658447, Test_Loss: 0.15077005326747894 *\n",
      "Epoch: 27, Train_Loss: 0.25889405608177185, Test_Loss: 0.1722777932882309\n",
      "Epoch: 27, Train_Loss: 0.26926445960998535, Test_Loss: 0.14509643614292145 *\n",
      "Epoch: 27, Train_Loss: 0.1675514578819275, Test_Loss: 0.16234493255615234\n",
      "Epoch: 27, Train_Loss: 0.1747010499238968, Test_Loss: 0.15968967974185944 *\n",
      "Epoch: 27, Train_Loss: 0.18460501730442047, Test_Loss: 0.14826230704784393 *\n",
      "Epoch: 27, Train_Loss: 0.20489180088043213, Test_Loss: 0.14778535068035126 *\n",
      "Epoch: 27, Train_Loss: 0.1746157705783844, Test_Loss: 0.17525151371955872\n",
      "Epoch: 27, Train_Loss: 0.1558782458305359, Test_Loss: 0.16970707476139069 *\n",
      "Epoch: 27, Train_Loss: 0.1557420790195465, Test_Loss: 0.14968617260456085 *\n",
      "Epoch: 27, Train_Loss: 0.1747928112745285, Test_Loss: 0.19499088823795319\n",
      "Epoch: 27, Train_Loss: 0.3423583507537842, Test_Loss: 0.287509560585022\n",
      "Epoch: 27, Train_Loss: 0.45447689294815063, Test_Loss: 3.8995883464813232\n",
      "Epoch: 27, Train_Loss: 0.6155661940574646, Test_Loss: 1.7207740545272827 *\n",
      "Epoch: 27, Train_Loss: 0.3831649720668793, Test_Loss: 0.17625628411769867 *\n",
      "Epoch: 27, Train_Loss: 0.386587917804718, Test_Loss: 0.16590677201747894 *\n",
      "Epoch: 27, Train_Loss: 0.20426185429096222, Test_Loss: 0.15711680054664612 *\n",
      "Epoch: 27, Train_Loss: 0.19407427310943604, Test_Loss: 0.16066034138202667\n",
      "Epoch: 27, Train_Loss: 0.1574046015739441, Test_Loss: 0.15779303014278412 *\n",
      "Epoch: 27, Train_Loss: 0.15243442356586456, Test_Loss: 0.20282214879989624\n",
      "Epoch: 27, Train_Loss: 0.22879725694656372, Test_Loss: 0.20292071998119354\n",
      "Epoch: 27, Train_Loss: 0.4263959527015686, Test_Loss: 0.14550238847732544 *\n",
      "Epoch: 27, Train_Loss: 0.5053493976593018, Test_Loss: 0.1842547208070755\n",
      "Epoch: 27, Train_Loss: 0.7832127809524536, Test_Loss: 0.1703559011220932 *\n",
      "Epoch: 27, Train_Loss: 0.9624736905097961, Test_Loss: 0.18178509175777435\n",
      "Epoch: 27, Train_Loss: 0.33799341320991516, Test_Loss: 0.1562710851430893 *\n",
      "Epoch: 27, Train_Loss: 0.3983703851699829, Test_Loss: 0.23509331047534943\n",
      "Epoch: 27, Train_Loss: 0.14678846299648285, Test_Loss: 0.2259427160024643 *\n",
      "Epoch: 27, Train_Loss: 0.16434180736541748, Test_Loss: 0.22669996321201324\n",
      "Epoch: 27, Train_Loss: 0.3051210641860962, Test_Loss: 0.1773872971534729 *\n",
      "Epoch: 27, Train_Loss: 0.5293298959732056, Test_Loss: 0.20366349816322327\n",
      "Epoch: 27, Train_Loss: 0.20772115886211395, Test_Loss: 0.17989644408226013 *\n",
      "Epoch: 27, Train_Loss: 0.1834137737751007, Test_Loss: 0.318217009305954\n",
      "Epoch: 27, Train_Loss: 0.1504291146993637, Test_Loss: 0.35024112462997437\n",
      "Epoch: 27, Train_Loss: 0.24305491149425507, Test_Loss: 0.4047201871871948\n",
      "Epoch: 27, Train_Loss: 0.4450794756412506, Test_Loss: 0.3361029028892517 *\n",
      "Epoch: 27, Train_Loss: 0.4335139989852905, Test_Loss: 0.34302085638046265\n",
      "Epoch: 27, Train_Loss: 0.2791846692562103, Test_Loss: 0.43471652269363403\n",
      "Epoch: 27, Train_Loss: 0.40690550208091736, Test_Loss: 0.3340599536895752 *\n",
      "Epoch: 27, Train_Loss: 0.14936643838882446, Test_Loss: 0.2979263663291931 *\n",
      "Epoch: 27, Train_Loss: 0.15436100959777832, Test_Loss: 0.1852516531944275 *\n",
      "Epoch: 27, Train_Loss: 0.15333877503871918, Test_Loss: 0.1726972758769989 *\n",
      "Epoch: 27, Train_Loss: 0.18447333574295044, Test_Loss: 0.1470816284418106 *\n",
      "Epoch: 27, Train_Loss: 0.17059192061424255, Test_Loss: 0.1658422201871872\n",
      "Epoch: 27, Train_Loss: 0.20480361580848694, Test_Loss: 0.250727117061615\n",
      "Epoch: 27, Train_Loss: 14.202385902404785, Test_Loss: 0.20636072754859924 *\n",
      "Epoch: 27, Train_Loss: 0.6178643107414246, Test_Loss: 0.2537866234779358\n",
      "Epoch: 27, Train_Loss: 1.136248230934143, Test_Loss: 0.1830287128686905 *\n",
      "Epoch: 27, Train_Loss: 0.6908714771270752, Test_Loss: 0.23570725321769714\n",
      "Epoch: 27, Train_Loss: 0.19702385365962982, Test_Loss: 0.17543767392635345 *\n",
      "Epoch: 27, Train_Loss: 0.19539161026477814, Test_Loss: 0.17429420351982117 *\n",
      "Epoch: 27, Train_Loss: 1.5462758541107178, Test_Loss: 0.36044424772262573\n",
      "Epoch: 27, Train_Loss: 3.916027307510376, Test_Loss: 0.2591409385204315 *\n",
      "Epoch: 27, Train_Loss: 0.39442530274391174, Test_Loss: 0.5300639271736145\n",
      "Epoch: 27, Train_Loss: 0.2583200931549072, Test_Loss: 0.21662646532058716 *\n",
      "Epoch: 27, Train_Loss: 3.6843576431274414, Test_Loss: 0.21861155331134796\n",
      "Epoch: 27, Train_Loss: 0.634642481803894, Test_Loss: 0.27317899465560913\n",
      "Epoch: 27, Train_Loss: 0.3958376348018646, Test_Loss: 0.2312692254781723 *\n",
      "Epoch: 27, Train_Loss: 0.15517544746398926, Test_Loss: 0.33153215050697327\n",
      "Epoch: 27, Train_Loss: 0.1549758017063141, Test_Loss: 0.18130230903625488 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.19500228762626648, Test_Loss: 0.19284825026988983\n",
      "Epoch: 27, Train_Loss: 0.14999784529209137, Test_Loss: 0.16621391475200653 *\n",
      "Epoch: 27, Train_Loss: 0.1661825329065323, Test_Loss: 0.426305890083313\n",
      "Epoch: 27, Train_Loss: 0.143529012799263, Test_Loss: 1.6091203689575195\n",
      "Epoch: 27, Train_Loss: 0.14391249418258667, Test_Loss: 0.7467257976531982 *\n",
      "Epoch: 27, Train_Loss: 0.14922013878822327, Test_Loss: 0.3348751962184906 *\n",
      "Epoch: 27, Train_Loss: 0.15298479795455933, Test_Loss: 0.14953994750976562 *\n",
      "Epoch: 27, Train_Loss: 0.16601762175559998, Test_Loss: 0.14810901880264282 *\n",
      "Epoch: 27, Train_Loss: 0.18041369318962097, Test_Loss: 0.14879532158374786\n",
      "Epoch: 27, Train_Loss: 0.18984642624855042, Test_Loss: 0.14693592488765717 *\n",
      "Epoch: 27, Train_Loss: 0.16406239569187164, Test_Loss: 0.18062154948711395\n",
      "Epoch: 27, Train_Loss: 0.17692744731903076, Test_Loss: 5.028284549713135\n",
      "Epoch: 27, Train_Loss: 0.14691540598869324, Test_Loss: 2.8527944087982178 *\n",
      "Epoch: 27, Train_Loss: 0.1653723567724228, Test_Loss: 0.5337626934051514 *\n",
      "Epoch: 27, Train_Loss: 0.14514628052711487, Test_Loss: 0.6821372509002686\n",
      "Epoch: 27, Train_Loss: 0.1435806304216385, Test_Loss: 0.8188682794570923\n",
      "Epoch: 27, Train_Loss: 0.1431245058774948, Test_Loss: 0.361288845539093 *\n",
      "Epoch: 27, Train_Loss: 0.14393095672130585, Test_Loss: 0.6860073208808899\n",
      "Epoch: 27, Train_Loss: 0.14365874230861664, Test_Loss: 1.305929183959961\n",
      "Epoch: 27, Train_Loss: 0.14336827397346497, Test_Loss: 1.1118614673614502 *\n",
      "Epoch: 27, Train_Loss: 0.14324350655078888, Test_Loss: 0.6351138949394226 *\n",
      "Epoch: 27, Train_Loss: 0.14639493823051453, Test_Loss: 0.8852635622024536\n",
      "Epoch: 27, Train_Loss: 0.16504420340061188, Test_Loss: 0.5763332843780518 *\n",
      "Epoch: 27, Train_Loss: 0.16763734817504883, Test_Loss: 1.7524558305740356\n",
      "Epoch: 27, Train_Loss: 0.21796593070030212, Test_Loss: 0.8067764639854431 *\n",
      "Epoch: 27, Train_Loss: 0.15269549190998077, Test_Loss: 0.9846352338790894\n",
      "Epoch: 27, Train_Loss: 0.4222932755947113, Test_Loss: 0.3466722369194031 *\n",
      "Epoch: 27, Train_Loss: 4.933194160461426, Test_Loss: 0.15309670567512512 *\n",
      "Epoch: 27, Train_Loss: 0.23626255989074707, Test_Loss: 0.15055875480175018 *\n",
      "Epoch: 27, Train_Loss: 0.1653168797492981, Test_Loss: 0.28126001358032227\n",
      "Epoch: 27, Train_Loss: 0.18203571438789368, Test_Loss: 0.28546667098999023\n",
      "Epoch: 27, Train_Loss: 0.2912864089012146, Test_Loss: 0.20111577212810516 *\n",
      "Epoch: 27, Train_Loss: 0.22520115971565247, Test_Loss: 0.5076613426208496\n",
      "Epoch: 27, Train_Loss: 0.215030699968338, Test_Loss: 0.1812017410993576 *\n",
      "Epoch: 27, Train_Loss: 0.23605793714523315, Test_Loss: 0.48970746994018555\n",
      "Epoch: 27, Train_Loss: 0.24394741654396057, Test_Loss: 0.43704724311828613 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.23899199068546295, Test_Loss: 0.1662851870059967 *\n",
      "Epoch: 27, Train_Loss: 0.17917673289775848, Test_Loss: 0.1537550985813141 *\n",
      "Epoch: 27, Train_Loss: 0.14911291003227234, Test_Loss: 0.26100096106529236\n",
      "Epoch: 27, Train_Loss: 0.17401519417762756, Test_Loss: 0.17979001998901367 *\n",
      "Epoch: 27, Train_Loss: 0.14946980774402618, Test_Loss: 0.14524617791175842 *\n",
      "Epoch: 27, Train_Loss: 0.25823476910591125, Test_Loss: 0.3461652398109436\n",
      "Epoch: 27, Train_Loss: 0.14791597425937653, Test_Loss: 0.27258381247520447 *\n",
      "Epoch: 27, Train_Loss: 0.16249015927314758, Test_Loss: 6.216113090515137\n",
      "Epoch: 27, Train_Loss: 0.16258376836776733, Test_Loss: 0.9655871391296387 *\n",
      "Epoch: 27, Train_Loss: 0.17436723411083221, Test_Loss: 0.1481650322675705 *\n",
      "Epoch: 27, Train_Loss: 0.2221185564994812, Test_Loss: 0.20950236916542053\n",
      "Epoch: 27, Train_Loss: 0.23013654351234436, Test_Loss: 0.2238454520702362\n",
      "Epoch: 27, Train_Loss: 0.15202905237674713, Test_Loss: 0.16376882791519165 *\n",
      "Epoch: 27, Train_Loss: 0.14361414313316345, Test_Loss: 0.15660686790943146 *\n",
      "Epoch: 27, Train_Loss: 0.14548733830451965, Test_Loss: 0.20507781207561493\n",
      "Epoch: 27, Train_Loss: 0.46439892053604126, Test_Loss: 0.1919422596693039 *\n",
      "Epoch: 27, Train_Loss: 4.00718879699707, Test_Loss: 0.14585353434085846 *\n",
      "Epoch: 27, Train_Loss: 0.1492711752653122, Test_Loss: 0.16559453308582306\n",
      "Epoch: 27, Train_Loss: 0.16074278950691223, Test_Loss: 0.16896486282348633\n",
      "Epoch: 27, Train_Loss: 0.15579570829868317, Test_Loss: 0.15380427241325378 *\n",
      "Epoch: 27, Train_Loss: 0.14729610085487366, Test_Loss: 0.21343602240085602\n",
      "Epoch: 27, Train_Loss: 0.14481855928897858, Test_Loss: 0.24871660768985748\n",
      "Epoch: 27, Train_Loss: 0.14423762261867523, Test_Loss: 0.20803239941596985 *\n",
      "Epoch: 27, Train_Loss: 0.15181218087673187, Test_Loss: 0.2155173420906067\n",
      "Epoch: 27, Train_Loss: 0.1887042075395584, Test_Loss: 0.17377465963363647 *\n",
      "Epoch: 27, Train_Loss: 0.19743314385414124, Test_Loss: 0.19366367161273956\n",
      "Epoch: 27, Train_Loss: 0.14916832745075226, Test_Loss: 0.2294226586818695\n",
      "Epoch: 27, Train_Loss: 0.14443440735340118, Test_Loss: 0.3350769281387329\n",
      "Epoch: 27, Train_Loss: 0.1498509794473648, Test_Loss: 0.37838518619537354\n",
      "Epoch: 27, Train_Loss: 0.16135796904563904, Test_Loss: 0.3806314468383789\n",
      "Epoch: 27, Train_Loss: 0.1479056179523468, Test_Loss: 0.31544673442840576 *\n",
      "Epoch: 27, Train_Loss: 0.1469409465789795, Test_Loss: 0.3005591034889221 *\n",
      "Epoch: 27, Train_Loss: 0.16014358401298523, Test_Loss: 0.38674402236938477\n",
      "Epoch: 27, Train_Loss: 0.1582302451133728, Test_Loss: 0.37427353858947754 *\n",
      "Epoch: 27, Train_Loss: 0.1531839668750763, Test_Loss: 0.29290539026260376 *\n",
      "Epoch: 27, Train_Loss: 0.14550329744815826, Test_Loss: 0.17455463111400604 *\n",
      "Epoch: 27, Train_Loss: 0.148016095161438, Test_Loss: 0.1578546166419983 *\n",
      "Epoch: 27, Train_Loss: 0.3005061745643616, Test_Loss: 0.16332170367240906\n",
      "Epoch: 27, Train_Loss: 0.22893670201301575, Test_Loss: 0.16481880843639374\n",
      "Epoch: 27, Train_Loss: 0.1663384884595871, Test_Loss: 0.3398379981517792\n",
      "Epoch: 27, Train_Loss: 0.23030388355255127, Test_Loss: 0.21561011672019958 *\n",
      "Epoch: 27, Train_Loss: 0.161198228597641, Test_Loss: 0.25056517124176025\n",
      "Epoch: 27, Train_Loss: 0.16854873299598694, Test_Loss: 0.19719275832176208 *\n",
      "Epoch: 27, Train_Loss: 0.17548444867134094, Test_Loss: 0.19091260433197021 *\n",
      "Epoch: 27, Train_Loss: 0.15150675177574158, Test_Loss: 0.155279740691185 *\n",
      "Epoch: 27, Train_Loss: 0.2703852653503418, Test_Loss: 0.21078619360923767\n",
      "Epoch: 27, Train_Loss: 0.16445887088775635, Test_Loss: 0.43019574880599976\n",
      "Epoch: 27, Train_Loss: 0.1482781171798706, Test_Loss: 0.37821662425994873 *\n",
      "Epoch: 27, Train_Loss: 0.14386123418807983, Test_Loss: 0.37224525213241577 *\n",
      "Epoch: 27, Train_Loss: 0.14321571588516235, Test_Loss: 0.1981741487979889 *\n",
      "Epoch: 27, Train_Loss: 0.14309726655483246, Test_Loss: 0.16475814580917358 *\n",
      "Epoch: 27, Train_Loss: 0.1430979073047638, Test_Loss: 0.15049204230308533 *\n",
      "Epoch: 27, Train_Loss: 0.5098042488098145, Test_Loss: 0.14606153964996338 *\n",
      "Epoch: 27, Train_Loss: 3.607051134109497, Test_Loss: 0.16192983090877533\n",
      "Epoch: 27, Train_Loss: 0.1634495109319687, Test_Loss: 0.1536896675825119 *\n",
      "Epoch: 27, Train_Loss: 0.14818355441093445, Test_Loss: 0.16891396045684814\n",
      "Epoch: 27, Train_Loss: 0.14926405251026154, Test_Loss: 0.14889459311962128 *\n",
      "Epoch: 27, Train_Loss: 0.1442665457725525, Test_Loss: 0.2683570086956024\n",
      "Epoch: 27, Train_Loss: 0.15005367994308472, Test_Loss: 0.5120946764945984\n",
      "Epoch: 27, Train_Loss: 0.15160760283470154, Test_Loss: 0.22671644389629364 *\n",
      "Epoch: 27, Train_Loss: 0.14731062948703766, Test_Loss: 0.3573853373527527\n",
      "Epoch: 27, Train_Loss: 0.1489582657814026, Test_Loss: 0.16668646037578583 *\n",
      "Epoch: 27, Train_Loss: 0.14730870723724365, Test_Loss: 0.16880272328853607\n",
      "Epoch: 27, Train_Loss: 0.15723691880702972, Test_Loss: 0.169236958026886\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.15013769268989563, Test_Loss: 0.16843411326408386 *\n",
      "Epoch: 27, Train_Loss: 0.1562248319387436, Test_Loss: 0.1926456093788147\n",
      "Epoch: 27, Train_Loss: 0.1657116413116455, Test_Loss: 4.5811381340026855\n",
      "Epoch: 27, Train_Loss: 0.15785767138004303, Test_Loss: 1.2359122037887573 *\n",
      "Epoch: 27, Train_Loss: 0.3562905490398407, Test_Loss: 0.16772614419460297 *\n",
      "Epoch: 27, Train_Loss: 0.2943574786186218, Test_Loss: 0.1701239049434662\n",
      "Epoch: 27, Train_Loss: 0.2917768359184265, Test_Loss: 0.16056174039840698 *\n",
      "Epoch: 27, Train_Loss: 0.27220362424850464, Test_Loss: 0.1559249460697174 *\n",
      "Epoch: 27, Train_Loss: 0.1466023176908493, Test_Loss: 0.15556220710277557 *\n",
      "Epoch: 27, Train_Loss: 0.14396053552627563, Test_Loss: 0.1540893316268921 *\n",
      "Epoch: 27, Train_Loss: 0.14581513404846191, Test_Loss: 0.15484419465065002\n",
      "Epoch: 27, Train_Loss: 0.14519095420837402, Test_Loss: 0.1468009501695633 *\n",
      "Epoch: 27, Train_Loss: 0.1490531712770462, Test_Loss: 0.14976978302001953\n",
      "Epoch: 27, Train_Loss: 0.15438105165958405, Test_Loss: 0.15601027011871338\n",
      "Epoch: 27, Train_Loss: 0.14724460244178772, Test_Loss: 0.18264560401439667\n",
      "Epoch: 27, Train_Loss: 0.1447048783302307, Test_Loss: 0.1629941761493683 *\n",
      "Epoch: 27, Train_Loss: 0.1545066088438034, Test_Loss: 0.16204458475112915 *\n",
      "Epoch: 27, Train_Loss: 0.16867022216320038, Test_Loss: 0.1456112265586853 *\n",
      "Epoch: 27, Train_Loss: 0.2400393784046173, Test_Loss: 0.14842097461223602\n",
      "Epoch: 27, Train_Loss: 0.17289693653583527, Test_Loss: 0.14709731936454773 *\n",
      "Epoch: 27, Train_Loss: 0.18078303337097168, Test_Loss: 0.16843625903129578\n",
      "Epoch: 27, Train_Loss: 0.18723413348197937, Test_Loss: 0.15063385665416718 *\n",
      "Epoch: 27, Train_Loss: 0.25518396496772766, Test_Loss: 0.14573726058006287 *\n",
      "Epoch: 27, Train_Loss: 0.23196986317634583, Test_Loss: 0.14758819341659546\n",
      "Epoch: 27, Train_Loss: 0.22214190661907196, Test_Loss: 0.1613921970129013\n",
      "Epoch: 27, Train_Loss: 0.21057310700416565, Test_Loss: 0.1587565690279007 *\n",
      "Epoch: 27, Train_Loss: 0.33585596084594727, Test_Loss: 0.2047692835330963\n",
      "Epoch: 27, Train_Loss: 0.156233012676239, Test_Loss: 0.15134049952030182 *\n",
      "Epoch: 27, Train_Loss: 0.1676844209432602, Test_Loss: 0.14643847942352295 *\n",
      "Epoch: 27, Train_Loss: 2.5363481044769287, Test_Loss: 0.1598953753709793\n",
      "Epoch: 27, Train_Loss: 0.5240857601165771, Test_Loss: 0.1513175517320633 *\n",
      "Epoch: 27, Train_Loss: 0.16741430759429932, Test_Loss: 0.144232377409935 *\n",
      "Epoch: 27, Train_Loss: 0.15969876945018768, Test_Loss: 0.25757813453674316\n",
      "Epoch: 27, Train_Loss: 0.1463179737329483, Test_Loss: 0.2677396535873413\n",
      "Epoch: 27, Train_Loss: 0.17083117365837097, Test_Loss: 5.048585414886475\n",
      "Epoch: 27, Train_Loss: 0.15570031106472015, Test_Loss: 0.20704764127731323 *\n",
      "Epoch: 27, Train_Loss: 0.18395498394966125, Test_Loss: 0.1504049003124237 *\n",
      "Epoch: 27, Train_Loss: 0.22459349036216736, Test_Loss: 0.17605751752853394\n",
      "Epoch: 27, Train_Loss: 0.18196099996566772, Test_Loss: 0.2129201889038086\n",
      "Epoch: 27, Train_Loss: 0.16317841410636902, Test_Loss: 0.15977779030799866 *\n",
      "Epoch: 27, Train_Loss: 0.152756005525589, Test_Loss: 0.14892800152301788 *\n",
      "Epoch: 27, Train_Loss: 0.15676060318946838, Test_Loss: 0.20369699597358704\n",
      "Epoch: 27, Train_Loss: 0.14586426317691803, Test_Loss: 0.18121545016765594 *\n",
      "Epoch: 27, Train_Loss: 0.15398505330085754, Test_Loss: 0.14875873923301697 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.18263399600982666, Test_Loss: 0.16119082272052765\n",
      "Epoch: 27, Train_Loss: 0.15602420270442963, Test_Loss: 0.18752162158489227\n",
      "Epoch: 27, Train_Loss: 0.14454960823059082, Test_Loss: 0.15301470458507538 *\n",
      "Epoch: 27, Train_Loss: 0.15044699609279633, Test_Loss: 0.193879634141922\n",
      "Epoch: 27, Train_Loss: 0.1575239598751068, Test_Loss: 0.174851655960083 *\n",
      "Epoch: 27, Train_Loss: 0.15650621056556702, Test_Loss: 0.1903424859046936\n",
      "Epoch: 27, Train_Loss: 0.14798752963542938, Test_Loss: 0.1615312695503235 *\n",
      "Epoch: 27, Train_Loss: 0.1438818722963333, Test_Loss: 0.22706615924835205\n",
      "Epoch: 27, Train_Loss: 0.1431088149547577, Test_Loss: 0.22794301807880402\n",
      "Epoch: 27, Train_Loss: 0.143443301320076, Test_Loss: 0.16297608613967896 *\n",
      "Epoch: 27, Train_Loss: 0.15138483047485352, Test_Loss: 0.21008887887001038\n",
      "Epoch: 27, Train_Loss: 0.14429636299610138, Test_Loss: 0.20884846150875092 *\n",
      "Epoch: 27, Train_Loss: 0.14517255127429962, Test_Loss: 0.21055549383163452\n",
      "Epoch: 27, Train_Loss: 0.1467006504535675, Test_Loss: 0.18585920333862305 *\n",
      "Epoch: 27, Train_Loss: 0.1433071345090866, Test_Loss: 0.19455328583717346\n",
      "Epoch: 27, Train_Loss: 0.14471517503261566, Test_Loss: 0.20222216844558716\n",
      "Epoch: 27, Train_Loss: 0.14550571143627167, Test_Loss: 0.1994779109954834 *\n",
      "Epoch: 27, Train_Loss: 0.15564189851284027, Test_Loss: 0.1820535510778427 *\n",
      "Epoch: 27, Train_Loss: 0.15291832387447357, Test_Loss: 0.1526005119085312 *\n",
      "Epoch: 27, Train_Loss: 0.14938688278198242, Test_Loss: 0.15953369438648224\n",
      "Epoch: 27, Train_Loss: 0.18080534040927887, Test_Loss: 0.2054598033428192\n",
      "Epoch: 27, Train_Loss: 0.17969664931297302, Test_Loss: 0.1884087324142456 *\n",
      "Epoch: 27, Train_Loss: 0.15528759360313416, Test_Loss: 0.42609286308288574\n",
      "Epoch: 27, Train_Loss: 0.15498292446136475, Test_Loss: 0.3003475069999695 *\n",
      "Epoch: 27, Train_Loss: 0.16165386140346527, Test_Loss: 0.22866013646125793 *\n",
      "Epoch: 27, Train_Loss: 0.17446565628051758, Test_Loss: 0.18001268804073334 *\n",
      "Epoch: 27, Train_Loss: 0.148426353931427, Test_Loss: 0.1565271019935608 *\n",
      "Epoch: 27, Train_Loss: 0.1576041430234909, Test_Loss: 0.15022161602973938 *\n",
      "Epoch: 27, Train_Loss: 0.15280848741531372, Test_Loss: 0.18514218926429749\n",
      "Epoch: 27, Train_Loss: 0.15567085146903992, Test_Loss: 0.3249068856239319\n",
      "Epoch: 27, Train_Loss: 0.21127811074256897, Test_Loss: 0.28526830673217773 *\n",
      "Epoch: 27, Train_Loss: 0.1828506886959076, Test_Loss: 0.2548287808895111 *\n",
      "Epoch: 27, Train_Loss: 0.15539759397506714, Test_Loss: 0.25115767121315 *\n",
      "Epoch: 27, Train_Loss: 0.1432243138551712, Test_Loss: 0.1496758759021759 *\n",
      "Epoch: 27, Train_Loss: 0.17660509049892426, Test_Loss: 0.14602290093898773 *\n",
      "Epoch: 27, Train_Loss: 0.16287967562675476, Test_Loss: 0.15100063383579254\n",
      "Epoch: 27, Train_Loss: 0.15022455155849457, Test_Loss: 0.1626226305961609\n",
      "Epoch: 27, Train_Loss: 0.15149962902069092, Test_Loss: 0.15287192165851593 *\n",
      "Epoch: 27, Train_Loss: 0.16437160968780518, Test_Loss: 0.16419591009616852\n",
      "Epoch: 27, Train_Loss: 0.2220698893070221, Test_Loss: 0.1506083905696869 *\n",
      "Epoch: 27, Train_Loss: 0.1703464388847351, Test_Loss: 0.22418934106826782\n",
      "Epoch: 27, Train_Loss: 0.16488635540008545, Test_Loss: 0.5055457353591919\n",
      "Epoch: 27, Train_Loss: 0.15502341091632843, Test_Loss: 0.20041000843048096 *\n",
      "Epoch: 27, Train_Loss: 0.16274063289165497, Test_Loss: 0.28849637508392334\n",
      "Epoch: 27, Train_Loss: 0.15321511030197144, Test_Loss: 0.15075336396694183 *\n",
      "Epoch: 27, Train_Loss: 0.14648863673210144, Test_Loss: 0.1511387676000595\n",
      "Epoch: 27, Train_Loss: 0.16188395023345947, Test_Loss: 0.1510060876607895 *\n",
      "Epoch: 27, Train_Loss: 0.14981016516685486, Test_Loss: 0.15308809280395508\n",
      "Epoch: 27, Train_Loss: 0.15490135550498962, Test_Loss: 0.17588382959365845\n",
      "Epoch: 27, Train_Loss: 0.22196437418460846, Test_Loss: 5.439709663391113\n",
      "Epoch: 27, Train_Loss: 0.1485997438430786, Test_Loss: 0.4483264684677124 *\n",
      "Epoch: 27, Train_Loss: 0.19200599193572998, Test_Loss: 0.15637235343456268 *\n",
      "Epoch: 27, Train_Loss: 0.16081851720809937, Test_Loss: 0.15381690859794617 *\n",
      "Epoch: 27, Train_Loss: 0.1683104932308197, Test_Loss: 0.14766380190849304 *\n",
      "Epoch: 27, Train_Loss: 0.24260717630386353, Test_Loss: 0.1488085240125656\n",
      "Epoch: 27, Train_Loss: 0.2871881127357483, Test_Loss: 0.15146762132644653\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.14640316367149353, Test_Loss: 0.14477314054965973 *\n",
      "Epoch: 27, Train_Loss: 0.1740904152393341, Test_Loss: 0.15200185775756836\n",
      "Epoch: 27, Train_Loss: 0.14351703226566315, Test_Loss: 0.14657455682754517 *\n",
      "Epoch: 27, Train_Loss: 0.14379729330539703, Test_Loss: 0.14720270037651062\n",
      "Epoch: 27, Train_Loss: 0.1473027914762497, Test_Loss: 0.16562004387378693\n",
      "Epoch: 27, Train_Loss: 0.14597639441490173, Test_Loss: 0.15337173640727997 *\n",
      "Epoch: 27, Train_Loss: 0.14405614137649536, Test_Loss: 0.17004016041755676\n",
      "Epoch: 27, Train_Loss: 0.15097904205322266, Test_Loss: 0.17342177033424377\n",
      "Epoch: 27, Train_Loss: 0.14998918771743774, Test_Loss: 0.14415891468524933 *\n",
      "Epoch: 27, Train_Loss: 0.14678457379341125, Test_Loss: 0.14741261303424835\n",
      "Epoch: 27, Train_Loss: 0.14872518181800842, Test_Loss: 0.1447145640850067 *\n",
      "Epoch: 27, Train_Loss: 0.14750176668167114, Test_Loss: 0.15192118287086487\n",
      "Epoch: 27, Train_Loss: 0.1439378261566162, Test_Loss: 0.14512614905834198 *\n",
      "Epoch: 27, Train_Loss: 0.14261549711227417, Test_Loss: 0.15134623646736145\n",
      "Epoch: 27, Train_Loss: 0.15488868951797485, Test_Loss: 0.145420104265213 *\n",
      "Epoch: 27, Train_Loss: 0.1473298966884613, Test_Loss: 0.14658987522125244\n",
      "Epoch: 27, Train_Loss: 0.16637535393238068, Test_Loss: 0.14752651751041412\n",
      "Epoch: 27, Train_Loss: 0.15065118670463562, Test_Loss: 0.14961199462413788\n",
      "Epoch: 27, Train_Loss: 0.14760923385620117, Test_Loss: 0.14504221081733704 *\n",
      "Epoch: 27, Train_Loss: 0.15179792046546936, Test_Loss: 0.14653925597667694\n",
      "Epoch: 27, Train_Loss: 0.1654852330684662, Test_Loss: 0.15215657651424408\n",
      "Epoch: 27, Train_Loss: 0.15082091093063354, Test_Loss: 0.14431379735469818 *\n",
      "Epoch: 27, Train_Loss: 0.16837051510810852, Test_Loss: 0.14525792002677917\n",
      "Epoch: 27, Train_Loss: 0.1430765688419342, Test_Loss: 0.23838815093040466\n",
      "Epoch: 27, Train_Loss: 0.15720394253730774, Test_Loss: 1.150862216949463\n",
      "Epoch: 27, Train_Loss: 0.16033825278282166, Test_Loss: 4.833658218383789\n",
      "Epoch: 27, Train_Loss: 0.16617551445960999, Test_Loss: 0.21131443977355957 *\n",
      "Epoch: 27, Train_Loss: 1.9052892923355103, Test_Loss: 0.1440984159708023 *\n",
      "Epoch: 27, Train_Loss: 3.62795090675354, Test_Loss: 0.1675148904323578\n",
      "Epoch: 27, Train_Loss: 0.17887353897094727, Test_Loss: 0.16170750558376312 *\n",
      "Epoch: 27, Train_Loss: 0.15815700590610504, Test_Loss: 0.15580809116363525 *\n",
      "Epoch: 27, Train_Loss: 0.16048866510391235, Test_Loss: 0.16317345201969147\n",
      "Epoch: 27, Train_Loss: 0.19170817732810974, Test_Loss: 0.24274413287639618\n",
      "Epoch: 27, Train_Loss: 0.17256544530391693, Test_Loss: 0.16515962779521942 *\n",
      "Epoch: 27, Train_Loss: 0.1520088165998459, Test_Loss: 0.1486274003982544 *\n",
      "Epoch: 27, Train_Loss: 0.14203231036663055, Test_Loss: 0.17265881597995758\n",
      "Epoch: 27, Train_Loss: 0.20916962623596191, Test_Loss: 0.1549619436264038 *\n",
      "Epoch: 27, Train_Loss: 0.17099127173423767, Test_Loss: 0.15200693905353546 *\n",
      "Epoch: 27, Train_Loss: 0.15401524305343628, Test_Loss: 0.19162222743034363\n",
      "Epoch: 27, Train_Loss: 0.5110518932342529, Test_Loss: 0.1776600033044815 *\n",
      "Epoch: 27, Train_Loss: 0.318008691072464, Test_Loss: 0.21970152854919434\n",
      "Epoch: 27, Train_Loss: 0.7304208278656006, Test_Loss: 0.16923271119594574 *\n",
      "Epoch: 27, Train_Loss: 0.19538141787052155, Test_Loss: 0.18012015521526337\n",
      "Epoch: 27, Train_Loss: 0.41455864906311035, Test_Loss: 0.1665325164794922 *\n",
      "Epoch: 27, Train_Loss: 0.5828466415405273, Test_Loss: 0.16312965750694275 *\n",
      "Epoch: 27, Train_Loss: 0.6554170250892639, Test_Loss: 0.24861648678779602\n",
      "Epoch: 27, Train_Loss: 0.14472071826457977, Test_Loss: 0.23378834128379822 *\n",
      "Epoch: 27, Train_Loss: 0.15622228384017944, Test_Loss: 0.24764183163642883\n",
      "Epoch: 27, Train_Loss: 0.5918710231781006, Test_Loss: 0.24289648234844208 *\n",
      "Epoch: 27, Train_Loss: 0.35730576515197754, Test_Loss: 0.20635540783405304 *\n",
      "Epoch: 27, Train_Loss: 0.559100866317749, Test_Loss: 0.21899822354316711\n",
      "Epoch: 27, Train_Loss: 0.14815954864025116, Test_Loss: 0.23246848583221436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train_Loss: 0.15287430584430695, Test_Loss: 0.1551138460636139 *\n",
      "Epoch: 27, Train_Loss: 0.39968663454055786, Test_Loss: 0.15713909268379211\n",
      "Epoch: 27, Train_Loss: 0.3620055019855499, Test_Loss: 0.1497156322002411 *\n",
      "Epoch: 27, Train_Loss: 0.23523978888988495, Test_Loss: 0.18500708043575287\n",
      "Epoch: 27, Train_Loss: 0.20478931069374084, Test_Loss: 0.17261746525764465 *\n",
      "Epoch: 27, Train_Loss: 0.18213225901126862, Test_Loss: 0.396968275308609\n",
      "Epoch: 27, Train_Loss: 0.20388218760490417, Test_Loss: 0.296392560005188 *\n",
      "Epoch: 27, Train_Loss: 0.26242440938949585, Test_Loss: 0.19565510749816895 *\n",
      "Epoch: 27, Train_Loss: 0.24370063841342926, Test_Loss: 0.18059326708316803 *\n",
      "Epoch: 27, Train_Loss: 0.17411664128303528, Test_Loss: 0.16974972188472748 *\n",
      "Epoch: 27, Train_Loss: 0.2593402862548828, Test_Loss: 0.1483624279499054 *\n",
      "Epoch: 27, Train_Loss: 0.23777469992637634, Test_Loss: 0.17710380256175995\n",
      "Epoch: 27, Train_Loss: 0.23368333280086517, Test_Loss: 0.2557764947414398\n",
      "Epoch: 27, Train_Loss: 0.23910130560398102, Test_Loss: 0.34431564807891846\n",
      "Epoch: 27, Train_Loss: 0.32505732774734497, Test_Loss: 0.20413850247859955 *\n",
      "Epoch: 27, Train_Loss: 0.1677306443452835, Test_Loss: 0.2377781867980957\n",
      "Epoch: 27, Train_Loss: 0.18347840011119843, Test_Loss: 0.15384933352470398 *\n",
      "Epoch: 27, Train_Loss: 0.18243445456027985, Test_Loss: 0.1458340585231781 *\n",
      "Epoch: 27, Train_Loss: 0.15694426000118256, Test_Loss: 0.1533585786819458\n",
      "Epoch: 27, Train_Loss: 0.1426098495721817, Test_Loss: 0.1885010302066803\n",
      "Epoch: 27, Train_Loss: 0.1447262465953827, Test_Loss: 0.14873678982257843 *\n",
      "Epoch: 27, Train_Loss: 0.14649827778339386, Test_Loss: 0.17800119519233704\n",
      "Epoch: 27, Train_Loss: 0.14906418323516846, Test_Loss: 0.16118288040161133 *\n",
      "Epoch: 27, Train_Loss: 0.1573142409324646, Test_Loss: 0.2868753671646118\n",
      "Epoch: 27, Train_Loss: 0.15561449527740479, Test_Loss: 0.47888341546058655\n",
      "Epoch: 27, Train_Loss: 0.15094077587127686, Test_Loss: 0.413216233253479 *\n",
      "Epoch: 27, Train_Loss: 0.2086118906736374, Test_Loss: 0.24335607886314392 *\n",
      "Epoch: 27, Train_Loss: 0.28384727239608765, Test_Loss: 0.1970670521259308 *\n",
      "Epoch: 27, Train_Loss: 0.21827539801597595, Test_Loss: 0.1948188990354538 *\n",
      "Epoch: 27, Train_Loss: 0.1600230634212494, Test_Loss: 0.1959574818611145\n",
      "Epoch: 27, Train_Loss: 0.1747162938117981, Test_Loss: 0.25072988867759705\n",
      "Epoch: 27, Train_Loss: 0.20432472229003906, Test_Loss: 0.3840216398239136\n",
      "Epoch: 27, Train_Loss: 0.29395240545272827, Test_Loss: 5.935091495513916\n",
      "Epoch: 27, Train_Loss: 0.17111612856388092, Test_Loss: 0.23731333017349243 *\n",
      "Epoch: 27, Train_Loss: 0.18852099776268005, Test_Loss: 0.1538710594177246 *\n",
      "Epoch: 27, Train_Loss: 0.2011139690876007, Test_Loss: 0.16116419434547424\n",
      "Epoch: 27, Train_Loss: 0.19058983027935028, Test_Loss: 0.14607076346874237 *\n",
      "Epoch: 27, Train_Loss: 0.2340981662273407, Test_Loss: 0.15550772845745087\n",
      "Epoch: 27, Train_Loss: 0.15549404919147491, Test_Loss: 0.17834240198135376\n",
      "Epoch: 27, Train_Loss: 0.19247041642665863, Test_Loss: 0.16326910257339478 *\n",
      "Epoch: 27, Train_Loss: 0.5146636366844177, Test_Loss: 0.1566440761089325 *\n",
      "Epoch: 27, Train_Loss: 0.4202576279640198, Test_Loss: 0.14933304488658905 *\n",
      "Epoch: 27, Train_Loss: 0.2090306580066681, Test_Loss: 0.15690766274929047\n",
      "Epoch: 27, Train_Loss: 0.1797265112400055, Test_Loss: 0.2549605071544647\n",
      "Epoch: 27, Train_Loss: 0.15762178599834442, Test_Loss: 0.17277635633945465 *\n",
      "Epoch: 27, Train_Loss: 0.156111940741539, Test_Loss: 0.15807507932186127 *\n",
      "Epoch: 27, Train_Loss: 0.3613550066947937, Test_Loss: 0.19190622866153717\n",
      "Epoch: 27, Train_Loss: 0.1706700623035431, Test_Loss: 0.15767474472522736 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 27\n",
      "Epoch: 27, Train_Loss: 0.2054862380027771, Test_Loss: 0.1518782079219818 *\n",
      "Epoch: 27, Train_Loss: 0.2771494388580322, Test_Loss: 0.17288249731063843\n",
      "Epoch: 27, Train_Loss: 0.16855566203594208, Test_Loss: 0.15835992991924286 *\n",
      "Epoch: 27, Train_Loss: 0.17477883398532867, Test_Loss: 0.14604467153549194 *\n",
      "Epoch: 27, Train_Loss: 0.22975127398967743, Test_Loss: 0.18549887835979462\n",
      "Epoch: 27, Train_Loss: 0.28155189752578735, Test_Loss: 0.1460350602865219 *\n",
      "Epoch: 27, Train_Loss: 0.1673808991909027, Test_Loss: 0.16374057531356812\n",
      "Epoch: 27, Train_Loss: 0.20659276843070984, Test_Loss: 0.15305545926094055 *\n",
      "Epoch: 27, Train_Loss: 0.1732528805732727, Test_Loss: 0.15504874289035797\n",
      "Epoch: 27, Train_Loss: 0.2120307981967926, Test_Loss: 0.14719700813293457 *\n",
      "Epoch: 27, Train_Loss: 0.19264525175094604, Test_Loss: 0.1534956395626068\n",
      "Epoch: 27, Train_Loss: 0.16038784384727478, Test_Loss: 0.1700962781906128\n",
      "Epoch: 27, Train_Loss: 0.147075355052948, Test_Loss: 0.15250426530838013 *\n",
      "Epoch: 27, Train_Loss: 0.16935601830482483, Test_Loss: 0.1525312066078186\n",
      "Epoch: 27, Train_Loss: 0.4334666132926941, Test_Loss: 0.24618521332740784\n",
      "Epoch: 27, Train_Loss: 0.44669678807258606, Test_Loss: 2.25789213180542\n",
      "Epoch: 27, Train_Loss: 0.5235190987586975, Test_Loss: 2.930633544921875\n",
      "Epoch: 27, Train_Loss: 0.4311359226703644, Test_Loss: 0.17241886258125305 *\n",
      "Epoch: 27, Train_Loss: 0.3998834192752838, Test_Loss: 0.152443990111351 *\n",
      "Epoch: 27, Train_Loss: 0.2129301130771637, Test_Loss: 0.16404573619365692\n",
      "Epoch: 27, Train_Loss: 0.2044181227684021, Test_Loss: 0.1627092808485031 *\n",
      "Epoch: 27, Train_Loss: 0.1543901115655899, Test_Loss: 0.15976686775684357 *\n",
      "Epoch: 27, Train_Loss: 0.15031114220619202, Test_Loss: 0.1882609724998474\n",
      "Epoch: 27, Train_Loss: 0.18018700182437897, Test_Loss: 0.1850079745054245 *\n",
      "Epoch: 27, Train_Loss: 0.38515520095825195, Test_Loss: 0.14717398583889008 *\n",
      "Epoch: 27, Train_Loss: 0.40781253576278687, Test_Loss: 0.17044579982757568\n",
      "Epoch: 27, Train_Loss: 0.5575830936431885, Test_Loss: 0.17213281989097595\n",
      "Epoch: 27, Train_Loss: 0.761600136756897, Test_Loss: 0.18714848160743713\n",
      "Epoch: 27, Train_Loss: 0.49790841341018677, Test_Loss: 0.1522388458251953 *\n",
      "Epoch: 27, Train_Loss: 0.38449305295944214, Test_Loss: 0.2580704391002655\n",
      "Epoch: 27, Train_Loss: 0.1539042741060257, Test_Loss: 0.21607695519924164 *\n",
      "Epoch: 27, Train_Loss: 0.15070785582065582, Test_Loss: 0.21841977536678314\n",
      "Epoch: 27, Train_Loss: 0.3566124439239502, Test_Loss: 0.15932127833366394 *\n",
      "Epoch: 27, Train_Loss: 0.3233462870121002, Test_Loss: 0.1990806609392166\n",
      "Epoch: 27, Train_Loss: 0.38490310311317444, Test_Loss: 0.17520269751548767 *\n",
      "Epoch: 27, Train_Loss: 0.1838730126619339, Test_Loss: 0.2429260015487671\n",
      "Epoch: 27, Train_Loss: 0.1524311602115631, Test_Loss: 0.38378655910491943\n",
      "Epoch: 27, Train_Loss: 0.21555685997009277, Test_Loss: 0.4312717020511627\n",
      "Epoch: 27, Train_Loss: 0.4114920198917389, Test_Loss: 0.30106571316719055 *\n",
      "Epoch: 27, Train_Loss: 0.2990729808807373, Test_Loss: 0.3564543128013611\n",
      "Epoch: 27, Train_Loss: 0.20956727862358093, Test_Loss: 0.3992585241794586\n",
      "Epoch: 27, Train_Loss: 0.31984782218933105, Test_Loss: 0.34332913160324097 *\n",
      "Epoch: 27, Train_Loss: 0.15002253651618958, Test_Loss: 0.2890385687351227 *\n",
      "Epoch: 27, Train_Loss: 0.14856183528900146, Test_Loss: 0.15682412683963776 *\n",
      "Epoch: 27, Train_Loss: 0.160992830991745, Test_Loss: 0.17149466276168823\n",
      "Epoch: 27, Train_Loss: 0.14817160367965698, Test_Loss: 0.14253585040569305 *\n",
      "Epoch: 27, Train_Loss: 0.16569718718528748, Test_Loss: 0.15876498818397522\n",
      "Epoch: 27, Train_Loss: 0.21477992832660675, Test_Loss: 0.18052658438682556\n",
      "Epoch: 27, Train_Loss: 6.550527572631836, Test_Loss: 0.24276861548423767\n",
      "Epoch: 27, Train_Loss: 9.199479103088379, Test_Loss: 0.23666167259216309 *\n",
      "Epoch: 27, Train_Loss: 0.6839389204978943, Test_Loss: 0.16250118613243103 *\n",
      "Epoch: 27, Train_Loss: 0.5046705603599548, Test_Loss: 0.2450939565896988\n",
      "Epoch: 27, Train_Loss: 0.4660552740097046, Test_Loss: 0.2077048420906067 *\n",
      "Epoch: 27, Train_Loss: 0.1888679414987564, Test_Loss: 0.15334489941596985 *\n",
      "Epoch: 28, Train_Loss: 0.50370854139328, Test_Loss: 0.24540835618972778 *\n",
      "Epoch: 28, Train_Loss: 3.3178586959838867, Test_Loss: 0.27249449491500854\n",
      "Epoch: 28, Train_Loss: 1.4289542436599731, Test_Loss: 0.4642801880836487\n",
      "Epoch: 28, Train_Loss: 0.25434479117393494, Test_Loss: 0.21520692110061646 *\n",
      "Epoch: 28, Train_Loss: 2.292079448699951, Test_Loss: 0.21602007746696472\n",
      "Epoch: 28, Train_Loss: 2.718966245651245, Test_Loss: 0.19904382526874542 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.3475654721260071, Test_Loss: 0.20700082182884216\n",
      "Epoch: 28, Train_Loss: 0.15034528076648712, Test_Loss: 0.22038094699382782\n",
      "Epoch: 28, Train_Loss: 0.16067270934581757, Test_Loss: 0.16340479254722595 *\n",
      "Epoch: 28, Train_Loss: 0.21008405089378357, Test_Loss: 0.19450265169143677\n",
      "Epoch: 28, Train_Loss: 0.15802061557769775, Test_Loss: 0.15683823823928833 *\n",
      "Epoch: 28, Train_Loss: 0.15144120156764984, Test_Loss: 0.2653520703315735\n",
      "Epoch: 28, Train_Loss: 0.1411527842283249, Test_Loss: 0.725420355796814\n",
      "Epoch: 28, Train_Loss: 0.1413029432296753, Test_Loss: 0.5465752482414246 *\n",
      "Epoch: 28, Train_Loss: 0.1434529721736908, Test_Loss: 0.2586109936237335 *\n",
      "Epoch: 28, Train_Loss: 0.15722395479679108, Test_Loss: 0.15385185182094574 *\n",
      "Epoch: 28, Train_Loss: 0.1597428023815155, Test_Loss: 0.15670587122440338\n",
      "Epoch: 28, Train_Loss: 0.198618084192276, Test_Loss: 0.15736564993858337\n",
      "Epoch: 28, Train_Loss: 0.19085195660591125, Test_Loss: 0.15372566878795624 *\n",
      "Epoch: 28, Train_Loss: 0.16447922587394714, Test_Loss: 0.15701541304588318\n",
      "Epoch: 28, Train_Loss: 0.157339945435524, Test_Loss: 1.8202153444290161\n",
      "Epoch: 28, Train_Loss: 0.16045789420604706, Test_Loss: 6.444559574127197\n",
      "Epoch: 28, Train_Loss: 0.19477222859859467, Test_Loss: 0.22529223561286926 *\n",
      "Epoch: 28, Train_Loss: 0.14313611388206482, Test_Loss: 0.43458425998687744\n",
      "Epoch: 28, Train_Loss: 0.14675916731357574, Test_Loss: 0.545732855796814\n",
      "Epoch: 28, Train_Loss: 0.1410752683877945, Test_Loss: 0.3459753692150116 *\n",
      "Epoch: 28, Train_Loss: 0.1418427675962448, Test_Loss: 0.41987931728363037\n",
      "Epoch: 28, Train_Loss: 0.14194881916046143, Test_Loss: 0.8690105676651001\n",
      "Epoch: 28, Train_Loss: 0.14128819108009338, Test_Loss: 0.8222616910934448 *\n",
      "Epoch: 28, Train_Loss: 0.14092238247394562, Test_Loss: 0.6652127504348755 *\n",
      "Epoch: 28, Train_Loss: 0.14767517149448395, Test_Loss: 0.966029167175293\n",
      "Epoch: 28, Train_Loss: 0.16176097095012665, Test_Loss: 0.5364401936531067 *\n",
      "Epoch: 28, Train_Loss: 0.16515026986598969, Test_Loss: 1.3690259456634521\n",
      "Epoch: 28, Train_Loss: 0.2436220943927765, Test_Loss: 0.563800036907196 *\n",
      "Epoch: 28, Train_Loss: 0.16905361413955688, Test_Loss: 0.5953800082206726\n",
      "Epoch: 28, Train_Loss: 0.279772013425827, Test_Loss: 0.4416896402835846 *\n",
      "Epoch: 28, Train_Loss: 5.153905391693115, Test_Loss: 0.1611618548631668 *\n",
      "Epoch: 28, Train_Loss: 0.6605756878852844, Test_Loss: 0.15277110040187836 *\n",
      "Epoch: 28, Train_Loss: 0.15203742682933807, Test_Loss: 0.14443188905715942 *\n",
      "Epoch: 28, Train_Loss: 0.16152654588222504, Test_Loss: 0.3798069953918457\n",
      "Epoch: 28, Train_Loss: 0.19350910186767578, Test_Loss: 0.15714463591575623 *\n",
      "Epoch: 28, Train_Loss: 0.15268287062644958, Test_Loss: 0.3959220349788666\n",
      "Epoch: 28, Train_Loss: 0.16109593212604523, Test_Loss: 0.15747426450252533 *\n",
      "Epoch: 28, Train_Loss: 0.18073448538780212, Test_Loss: 0.3103606104850769\n",
      "Epoch: 28, Train_Loss: 0.26337960362434387, Test_Loss: 0.3447102904319763\n",
      "Epoch: 28, Train_Loss: 0.25514906644821167, Test_Loss: 0.21432054042816162 *\n",
      "Epoch: 28, Train_Loss: 0.21712630987167358, Test_Loss: 0.161612868309021 *\n",
      "Epoch: 28, Train_Loss: 0.14812636375427246, Test_Loss: 0.16370661556720734\n",
      "Epoch: 28, Train_Loss: 0.17616313695907593, Test_Loss: 0.18106713891029358\n",
      "Epoch: 28, Train_Loss: 0.14558178186416626, Test_Loss: 0.14903736114501953 *\n",
      "Epoch: 28, Train_Loss: 0.25928008556365967, Test_Loss: 0.21887265145778656\n",
      "Epoch: 28, Train_Loss: 0.14711226522922516, Test_Loss: 0.3371807932853699\n",
      "Epoch: 28, Train_Loss: 0.16275256872177124, Test_Loss: 3.6165688037872314\n",
      "Epoch: 28, Train_Loss: 0.2218685895204544, Test_Loss: 2.8327157497406006 *\n",
      "Epoch: 28, Train_Loss: 0.18867692351341248, Test_Loss: 0.1730498969554901 *\n",
      "Epoch: 28, Train_Loss: 0.1957779973745346, Test_Loss: 0.24380463361740112\n",
      "Epoch: 28, Train_Loss: 0.23644568026065826, Test_Loss: 0.3258267641067505\n",
      "Epoch: 28, Train_Loss: 0.1589626967906952, Test_Loss: 0.32081449031829834 *\n",
      "Epoch: 28, Train_Loss: 0.154054194688797, Test_Loss: 0.16966308653354645 *\n",
      "Epoch: 28, Train_Loss: 0.1462472826242447, Test_Loss: 0.2324800044298172\n",
      "Epoch: 28, Train_Loss: 0.268609881401062, Test_Loss: 0.25028443336486816\n",
      "Epoch: 28, Train_Loss: 3.0316622257232666, Test_Loss: 0.14835327863693237 *\n",
      "Epoch: 28, Train_Loss: 0.1896933913230896, Test_Loss: 0.1750279814004898\n",
      "Epoch: 28, Train_Loss: 0.17495471239089966, Test_Loss: 0.18188375234603882\n",
      "Epoch: 28, Train_Loss: 0.15500937402248383, Test_Loss: 0.16763071715831757 *\n",
      "Epoch: 28, Train_Loss: 0.1469733715057373, Test_Loss: 0.16691823303699493 *\n",
      "Epoch: 28, Train_Loss: 0.14207156002521515, Test_Loss: 0.3324463963508606\n",
      "Epoch: 28, Train_Loss: 0.15607759356498718, Test_Loss: 0.25915825366973877 *\n",
      "Epoch: 28, Train_Loss: 0.14455460011959076, Test_Loss: 0.2698262333869934\n",
      "Epoch: 28, Train_Loss: 0.1626996546983719, Test_Loss: 0.18787699937820435 *\n",
      "Epoch: 28, Train_Loss: 0.1555519551038742, Test_Loss: 0.17676374316215515 *\n",
      "Epoch: 28, Train_Loss: 0.1484440565109253, Test_Loss: 0.2172345668077469\n",
      "Epoch: 28, Train_Loss: 0.14157435297966003, Test_Loss: 0.3842947483062744\n",
      "Epoch: 28, Train_Loss: 0.14797967672348022, Test_Loss: 0.4149937033653259\n",
      "Epoch: 28, Train_Loss: 0.15429124236106873, Test_Loss: 0.40510353446006775 *\n",
      "Epoch: 28, Train_Loss: 0.15635116398334503, Test_Loss: 0.3604047894477844 *\n",
      "Epoch: 28, Train_Loss: 0.14634016156196594, Test_Loss: 0.36987996101379395\n",
      "Epoch: 28, Train_Loss: 0.1558786928653717, Test_Loss: 0.4351367950439453\n",
      "Epoch: 28, Train_Loss: 0.15451498329639435, Test_Loss: 0.3648315370082855 *\n",
      "Epoch: 28, Train_Loss: 0.15497800707817078, Test_Loss: 0.3073124885559082 *\n",
      "Epoch: 28, Train_Loss: 0.1452772617340088, Test_Loss: 0.16232937574386597 *\n",
      "Epoch: 28, Train_Loss: 0.1418767273426056, Test_Loss: 0.16190621256828308 *\n",
      "Epoch: 28, Train_Loss: 0.20739507675170898, Test_Loss: 0.14909683167934418 *\n",
      "Epoch: 28, Train_Loss: 0.16708993911743164, Test_Loss: 0.1673285961151123\n",
      "Epoch: 28, Train_Loss: 0.1594998985528946, Test_Loss: 0.21419960260391235\n",
      "Epoch: 28, Train_Loss: 0.18210172653198242, Test_Loss: 0.22889503836631775\n",
      "Epoch: 28, Train_Loss: 0.1655278503894806, Test_Loss: 0.3043377995491028\n",
      "Epoch: 28, Train_Loss: 0.17277675867080688, Test_Loss: 0.18433751165866852 *\n",
      "Epoch: 28, Train_Loss: 0.161431685090065, Test_Loss: 0.22473904490470886\n",
      "Epoch: 28, Train_Loss: 0.17622973024845123, Test_Loss: 0.18397580087184906 *\n",
      "Epoch: 28, Train_Loss: 0.2553560435771942, Test_Loss: 0.17763373255729675 *\n",
      "Epoch: 28, Train_Loss: 0.18432337045669556, Test_Loss: 0.3674101233482361\n",
      "Epoch: 28, Train_Loss: 0.17310909926891327, Test_Loss: 0.3578944206237793 *\n",
      "Epoch: 28, Train_Loss: 0.14603707194328308, Test_Loss: 0.5375354886054993\n",
      "Epoch: 28, Train_Loss: 0.14089323580265045, Test_Loss: 0.3169531524181366 *\n",
      "Epoch: 28, Train_Loss: 0.14054615795612335, Test_Loss: 0.19314005970954895 *\n",
      "Epoch: 28, Train_Loss: 0.1404508650302887, Test_Loss: 0.15925218164920807 *\n",
      "Epoch: 28, Train_Loss: 0.17356687784194946, Test_Loss: 0.15074658393859863 *\n",
      "Epoch: 28, Train_Loss: 3.7354960441589355, Test_Loss: 0.15693001449108124\n",
      "Epoch: 28, Train_Loss: 0.38900327682495117, Test_Loss: 0.15160788595676422 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.14482952654361725, Test_Loss: 0.1712159812450409\n",
      "Epoch: 28, Train_Loss: 0.14874155819416046, Test_Loss: 0.14527466893196106 *\n",
      "Epoch: 28, Train_Loss: 0.14375577867031097, Test_Loss: 0.20655634999275208\n",
      "Epoch: 28, Train_Loss: 0.14554250240325928, Test_Loss: 0.3057520389556885\n",
      "Epoch: 28, Train_Loss: 0.14640973508358002, Test_Loss: 0.3627031445503235\n",
      "Epoch: 28, Train_Loss: 0.14592359960079193, Test_Loss: 0.3939840793609619\n",
      "Epoch: 28, Train_Loss: 0.14783033728599548, Test_Loss: 0.20348384976387024 *\n",
      "Epoch: 28, Train_Loss: 0.1444903016090393, Test_Loss: 0.2021854817867279 *\n",
      "Epoch: 28, Train_Loss: 0.1659516543149948, Test_Loss: 0.20144915580749512 *\n",
      "Epoch: 28, Train_Loss: 0.15365727245807648, Test_Loss: 0.20308159291744232\n",
      "Epoch: 28, Train_Loss: 0.16141390800476074, Test_Loss: 0.20681506395339966\n",
      "Epoch: 28, Train_Loss: 0.1536744087934494, Test_Loss: 2.097144365310669\n",
      "Epoch: 28, Train_Loss: 0.15057805180549622, Test_Loss: 2.9272549152374268\n",
      "Epoch: 28, Train_Loss: 0.23946763575077057, Test_Loss: 0.1745680570602417 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.2617618441581726, Test_Loss: 0.17800702154636383\n",
      "Epoch: 28, Train_Loss: 0.2880062460899353, Test_Loss: 0.16588447988033295 *\n",
      "Epoch: 28, Train_Loss: 0.26458024978637695, Test_Loss: 0.15029603242874146 *\n",
      "Epoch: 28, Train_Loss: 0.15113374590873718, Test_Loss: 0.15083010494709015\n",
      "Epoch: 28, Train_Loss: 0.14482218027114868, Test_Loss: 0.1627640426158905\n",
      "Epoch: 28, Train_Loss: 0.1462675780057907, Test_Loss: 0.1592538058757782 *\n",
      "Epoch: 28, Train_Loss: 0.145349383354187, Test_Loss: 0.1495354026556015 *\n",
      "Epoch: 28, Train_Loss: 0.1438174694776535, Test_Loss: 0.14787468314170837 *\n",
      "Epoch: 28, Train_Loss: 0.14918971061706543, Test_Loss: 0.15239199995994568\n",
      "Epoch: 28, Train_Loss: 0.1502811759710312, Test_Loss: 0.2038324624300003\n",
      "Epoch: 28, Train_Loss: 0.14299878478050232, Test_Loss: 0.1530512273311615 *\n",
      "Epoch: 28, Train_Loss: 0.15224428474903107, Test_Loss: 0.16255797445774078\n",
      "Epoch: 28, Train_Loss: 0.15032772719860077, Test_Loss: 0.1559089869260788 *\n",
      "Epoch: 28, Train_Loss: 0.21169829368591309, Test_Loss: 0.15499600768089294 *\n",
      "Epoch: 28, Train_Loss: 0.19891774654388428, Test_Loss: 0.14579543471336365 *\n",
      "Epoch: 28, Train_Loss: 0.17136411368846893, Test_Loss: 0.15316390991210938\n",
      "Epoch: 28, Train_Loss: 0.20500674843788147, Test_Loss: 0.18912050127983093\n",
      "Epoch: 28, Train_Loss: 0.2906803786754608, Test_Loss: 0.15027140080928802 *\n",
      "Epoch: 28, Train_Loss: 0.2671055793762207, Test_Loss: 0.15297752618789673\n",
      "Epoch: 28, Train_Loss: 0.20047637820243835, Test_Loss: 0.1449384093284607 *\n",
      "Epoch: 28, Train_Loss: 0.23796164989471436, Test_Loss: 0.16470198333263397\n",
      "Epoch: 28, Train_Loss: 0.1863318681716919, Test_Loss: 0.1998864710330963\n",
      "Epoch: 28, Train_Loss: 0.34274810552597046, Test_Loss: 0.16216802597045898 *\n",
      "Epoch: 28, Train_Loss: 0.15777505934238434, Test_Loss: 0.15376242995262146 *\n",
      "Epoch: 28, Train_Loss: 1.5529826879501343, Test_Loss: 0.15511581301689148\n",
      "Epoch: 28, Train_Loss: 1.1748762130737305, Test_Loss: 0.14803816378116608 *\n",
      "Epoch: 28, Train_Loss: 0.21077492833137512, Test_Loss: 0.14369913935661316 *\n",
      "Epoch: 28, Train_Loss: 0.14572501182556152, Test_Loss: 0.2304200530052185\n",
      "Epoch: 28, Train_Loss: 0.15442878007888794, Test_Loss: 0.20569264888763428 *\n",
      "Epoch: 28, Train_Loss: 0.16747316718101501, Test_Loss: 3.892692804336548\n",
      "Epoch: 28, Train_Loss: 0.14714324474334717, Test_Loss: 1.1641228199005127 *\n",
      "Epoch: 28, Train_Loss: 0.16184577345848083, Test_Loss: 0.15874092280864716 *\n",
      "Epoch: 28, Train_Loss: 0.20625679194927216, Test_Loss: 0.1840355098247528\n",
      "Epoch: 28, Train_Loss: 0.17000560462474823, Test_Loss: 0.17242194712162018 *\n",
      "Epoch: 28, Train_Loss: 0.1524622142314911, Test_Loss: 0.16722433269023895 *\n",
      "Epoch: 28, Train_Loss: 0.177954763174057, Test_Loss: 0.15574511885643005 *\n",
      "Epoch: 28, Train_Loss: 0.17300257086753845, Test_Loss: 0.2002321183681488\n",
      "Epoch: 28, Train_Loss: 0.14978203177452087, Test_Loss: 0.1768818199634552 *\n",
      "Epoch: 28, Train_Loss: 0.1623278558254242, Test_Loss: 0.15807795524597168 *\n",
      "Epoch: 28, Train_Loss: 0.2006295621395111, Test_Loss: 0.1712392270565033\n",
      "Epoch: 28, Train_Loss: 0.14785027503967285, Test_Loss: 0.20911164581775665\n",
      "Epoch: 28, Train_Loss: 0.1518077701330185, Test_Loss: 0.22041577100753784\n",
      "Epoch: 28, Train_Loss: 0.14338091015815735, Test_Loss: 0.18322232365608215 *\n",
      "Epoch: 28, Train_Loss: 0.15338721871376038, Test_Loss: 0.18538683652877808\n",
      "Epoch: 28, Train_Loss: 0.1491844356060028, Test_Loss: 0.18671992421150208\n",
      "Epoch: 28, Train_Loss: 0.1555081605911255, Test_Loss: 0.18411274254322052 *\n",
      "Epoch: 28, Train_Loss: 0.14196087419986725, Test_Loss: 0.17414812743663788 *\n",
      "Epoch: 28, Train_Loss: 0.14119940996170044, Test_Loss: 0.30937182903289795\n",
      "Epoch: 28, Train_Loss: 0.14139993488788605, Test_Loss: 0.20194660127162933 *\n",
      "Epoch: 28, Train_Loss: 0.15032735466957092, Test_Loss: 0.16814075410366058 *\n",
      "Epoch: 28, Train_Loss: 0.1414327770471573, Test_Loss: 0.21275129914283752\n",
      "Epoch: 28, Train_Loss: 0.1430085003376007, Test_Loss: 0.18468835949897766 *\n",
      "Epoch: 28, Train_Loss: 0.14355862140655518, Test_Loss: 0.167495995759964 *\n",
      "Epoch: 28, Train_Loss: 0.14494211971759796, Test_Loss: 0.17397937178611755\n",
      "Epoch: 28, Train_Loss: 0.14061085879802704, Test_Loss: 0.19158869981765747\n",
      "Epoch: 28, Train_Loss: 0.14461350440979004, Test_Loss: 0.17804507911205292 *\n",
      "Epoch: 28, Train_Loss: 0.1500527411699295, Test_Loss: 0.2008560299873352\n",
      "Epoch: 28, Train_Loss: 0.15168914198875427, Test_Loss: 0.15293550491333008 *\n",
      "Epoch: 28, Train_Loss: 0.1455707997083664, Test_Loss: 0.15255849063396454 *\n",
      "Epoch: 28, Train_Loss: 0.16869471967220306, Test_Loss: 0.16733311116695404\n",
      "Epoch: 28, Train_Loss: 0.16048908233642578, Test_Loss: 0.22439253330230713\n",
      "Epoch: 28, Train_Loss: 0.15347924828529358, Test_Loss: 0.3962952792644501\n",
      "Epoch: 28, Train_Loss: 0.1504822075366974, Test_Loss: 0.2711711525917053 *\n",
      "Epoch: 28, Train_Loss: 0.15226122736930847, Test_Loss: 0.2912324070930481\n",
      "Epoch: 28, Train_Loss: 0.17385199666023254, Test_Loss: 0.18862313032150269 *\n",
      "Epoch: 28, Train_Loss: 0.1521020084619522, Test_Loss: 0.1521548628807068 *\n",
      "Epoch: 28, Train_Loss: 0.14950035512447357, Test_Loss: 0.16045786440372467\n",
      "Epoch: 28, Train_Loss: 0.14487935602664948, Test_Loss: 0.17067250609397888\n",
      "Epoch: 28, Train_Loss: 0.15203645825386047, Test_Loss: 0.38860857486724854\n",
      "Epoch: 28, Train_Loss: 0.18113890290260315, Test_Loss: 0.20244669914245605 *\n",
      "Epoch: 28, Train_Loss: 0.18794098496437073, Test_Loss: 0.471565306186676\n",
      "Epoch: 28, Train_Loss: 0.15698514878749847, Test_Loss: 0.22088836133480072 *\n",
      "Epoch: 28, Train_Loss: 0.14089706540107727, Test_Loss: 0.17625468969345093 *\n",
      "Epoch: 28, Train_Loss: 0.17412127554416656, Test_Loss: 0.14755849540233612 *\n",
      "Epoch: 28, Train_Loss: 0.17513278126716614, Test_Loss: 0.1417122781276703 *\n",
      "Epoch: 28, Train_Loss: 0.146621435880661, Test_Loss: 0.1817561537027359\n",
      "Epoch: 28, Train_Loss: 0.14645986258983612, Test_Loss: 0.1480426788330078 *\n",
      "Epoch: 28, Train_Loss: 0.2005433440208435, Test_Loss: 0.16451002657413483\n",
      "Epoch: 28, Train_Loss: 0.22009828686714172, Test_Loss: 0.14546993374824524 *\n",
      "Epoch: 28, Train_Loss: 0.16794970631599426, Test_Loss: 0.22291606664657593\n",
      "Epoch: 28, Train_Loss: 0.15832597017288208, Test_Loss: 0.4525517225265503\n",
      "Epoch: 28, Train_Loss: 0.15765835344791412, Test_Loss: 0.20701639354228973 *\n",
      "Epoch: 28, Train_Loss: 0.14596529304981232, Test_Loss: 0.3513171076774597\n",
      "Epoch: 28, Train_Loss: 0.15419164299964905, Test_Loss: 0.16220569610595703 *\n",
      "Epoch: 28, Train_Loss: 0.14304004609584808, Test_Loss: 0.16174568235874176 *\n",
      "Epoch: 28, Train_Loss: 0.15863339602947235, Test_Loss: 0.16030898690223694 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.14841328561306, Test_Loss: 0.15847724676132202 *\n",
      "Epoch: 28, Train_Loss: 0.1504824012517929, Test_Loss: 0.17553654313087463\n",
      "Epoch: 28, Train_Loss: 0.21183033287525177, Test_Loss: 3.5594465732574463\n",
      "Epoch: 28, Train_Loss: 0.14276519417762756, Test_Loss: 1.8944555521011353 *\n",
      "Epoch: 28, Train_Loss: 0.18647657334804535, Test_Loss: 0.1535130739212036 *\n",
      "Epoch: 28, Train_Loss: 0.15811936557292938, Test_Loss: 0.15592564642429352\n",
      "Epoch: 28, Train_Loss: 0.19053304195404053, Test_Loss: 0.14693784713745117 *\n",
      "Epoch: 28, Train_Loss: 0.19863784313201904, Test_Loss: 0.14291192591190338 *\n",
      "Epoch: 28, Train_Loss: 0.39674365520477295, Test_Loss: 0.1501026600599289\n",
      "Epoch: 28, Train_Loss: 0.14446811378002167, Test_Loss: 0.15186692774295807\n",
      "Epoch: 28, Train_Loss: 0.16465578973293304, Test_Loss: 0.1550285369157791\n",
      "Epoch: 28, Train_Loss: 0.1409718096256256, Test_Loss: 0.1452340930700302 *\n",
      "Epoch: 28, Train_Loss: 0.14114049077033997, Test_Loss: 0.14719818532466888\n",
      "Epoch: 28, Train_Loss: 0.14418469369411469, Test_Loss: 0.14845731854438782\n",
      "Epoch: 28, Train_Loss: 0.14214378595352173, Test_Loss: 0.15957026183605194\n",
      "Epoch: 28, Train_Loss: 0.1454506516456604, Test_Loss: 0.16910526156425476\n",
      "Epoch: 28, Train_Loss: 0.14403468370437622, Test_Loss: 0.23058968782424927\n",
      "Epoch: 28, Train_Loss: 0.15400464832782745, Test_Loss: 0.1436527818441391 *\n",
      "Epoch: 28, Train_Loss: 0.14506465196609497, Test_Loss: 0.14593923091888428\n",
      "Epoch: 28, Train_Loss: 0.14561137557029724, Test_Loss: 0.14245633780956268 *\n",
      "Epoch: 28, Train_Loss: 0.15089748799800873, Test_Loss: 0.1537436693906784\n",
      "Epoch: 28, Train_Loss: 0.14209850132465363, Test_Loss: 0.1461443305015564 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.14037376642227173, Test_Loss: 0.14749643206596375\n",
      "Epoch: 28, Train_Loss: 0.15926125645637512, Test_Loss: 0.14819446206092834\n",
      "Epoch: 28, Train_Loss: 0.1445973515510559, Test_Loss: 0.1409081369638443 *\n",
      "Epoch: 28, Train_Loss: 0.16356433928012848, Test_Loss: 0.14469359815120697\n",
      "Epoch: 28, Train_Loss: 0.14241304993629456, Test_Loss: 0.15150362253189087\n",
      "Epoch: 28, Train_Loss: 0.14921659231185913, Test_Loss: 0.14347927272319794 *\n",
      "Epoch: 28, Train_Loss: 0.1557241529226303, Test_Loss: 0.14214886724948883 *\n",
      "Epoch: 28, Train_Loss: 0.16614067554473877, Test_Loss: 0.15767474472522736\n",
      "Epoch: 28, Train_Loss: 0.14400775730609894, Test_Loss: 0.1481872946023941 *\n",
      "Epoch: 28, Train_Loss: 0.15940527617931366, Test_Loss: 0.14065417647361755 *\n",
      "Epoch: 28, Train_Loss: 0.14146170020103455, Test_Loss: 0.2140495479106903\n",
      "Epoch: 28, Train_Loss: 0.1559588462114334, Test_Loss: 0.1949988603591919 *\n",
      "Epoch: 28, Train_Loss: 0.14805439114570618, Test_Loss: 5.417869567871094\n",
      "Epoch: 28, Train_Loss: 0.15487274527549744, Test_Loss: 0.3978629410266876 *\n",
      "Epoch: 28, Train_Loss: 0.5382830500602722, Test_Loss: 0.1427956074476242 *\n",
      "Epoch: 28, Train_Loss: 4.0569634437561035, Test_Loss: 0.1746435910463333\n",
      "Epoch: 28, Train_Loss: 1.4006402492523193, Test_Loss: 0.15861834585666656 *\n",
      "Epoch: 28, Train_Loss: 0.1562715321779251, Test_Loss: 0.1485564410686493 *\n",
      "Epoch: 28, Train_Loss: 0.14787298440933228, Test_Loss: 0.15117746591567993\n",
      "Epoch: 28, Train_Loss: 0.1730523407459259, Test_Loss: 0.2782382369041443\n",
      "Epoch: 28, Train_Loss: 0.20027589797973633, Test_Loss: 0.19943955540657043 *\n",
      "Epoch: 28, Train_Loss: 0.1527828723192215, Test_Loss: 0.14172834157943726 *\n",
      "Epoch: 28, Train_Loss: 0.1399824321269989, Test_Loss: 0.16485029458999634\n",
      "Epoch: 28, Train_Loss: 0.1907673329114914, Test_Loss: 0.15188297629356384 *\n",
      "Epoch: 28, Train_Loss: 0.1796458512544632, Test_Loss: 0.14573118090629578 *\n",
      "Epoch: 28, Train_Loss: 0.14747196435928345, Test_Loss: 0.16352589428424835\n",
      "Epoch: 28, Train_Loss: 0.32606661319732666, Test_Loss: 0.14728400111198425 *\n",
      "Epoch: 28, Train_Loss: 0.3518609404563904, Test_Loss: 0.19891351461410522\n",
      "Epoch: 28, Train_Loss: 0.7524508833885193, Test_Loss: 0.20788417756557465\n",
      "Epoch: 28, Train_Loss: 0.20608621835708618, Test_Loss: 0.17599134147167206 *\n",
      "Epoch: 28, Train_Loss: 0.27142882347106934, Test_Loss: 0.20547762513160706\n",
      "Epoch: 28, Train_Loss: 0.9963446855545044, Test_Loss: 0.15819081664085388 *\n",
      "Epoch: 28, Train_Loss: 0.688223659992218, Test_Loss: 0.20270708203315735\n",
      "Epoch: 28, Train_Loss: 0.14377672970294952, Test_Loss: 0.23479430377483368\n",
      "Epoch: 28, Train_Loss: 0.14744476974010468, Test_Loss: 0.26627594232559204\n",
      "Epoch: 28, Train_Loss: 0.5832287073135376, Test_Loss: 0.24977853894233704 *\n",
      "Epoch: 28, Train_Loss: 0.38712257146835327, Test_Loss: 0.24005557596683502 *\n",
      "Epoch: 28, Train_Loss: 0.8170536756515503, Test_Loss: 0.29190701246261597\n",
      "Epoch: 28, Train_Loss: 0.15971338748931885, Test_Loss: 0.21410605311393738 *\n",
      "Epoch: 28, Train_Loss: 0.16154952347278595, Test_Loss: 0.1867724359035492 *\n",
      "Epoch: 28, Train_Loss: 0.3286762237548828, Test_Loss: 0.15960298478603363 *\n",
      "Epoch: 28, Train_Loss: 0.37122613191604614, Test_Loss: 0.1499471515417099 *\n",
      "Epoch: 28, Train_Loss: 0.15971285104751587, Test_Loss: 0.160613015294075\n",
      "Epoch: 28, Train_Loss: 0.20867347717285156, Test_Loss: 0.1841653734445572\n",
      "Epoch: 28, Train_Loss: 0.18138928711414337, Test_Loss: 0.41638219356536865\n",
      "Epoch: 28, Train_Loss: 0.16955611109733582, Test_Loss: 0.20870670676231384 *\n",
      "Epoch: 28, Train_Loss: 0.26524603366851807, Test_Loss: 0.22896721959114075\n",
      "Epoch: 28, Train_Loss: 0.2181980311870575, Test_Loss: 0.18266835808753967 *\n",
      "Epoch: 28, Train_Loss: 0.18688078224658966, Test_Loss: 0.1573333442211151 *\n",
      "Epoch: 28, Train_Loss: 0.21114784479141235, Test_Loss: 0.1471160501241684 *\n",
      "Epoch: 28, Train_Loss: 0.22362908720970154, Test_Loss: 0.16712023317813873\n",
      "Epoch: 28, Train_Loss: 0.18181519210338593, Test_Loss: 0.41896259784698486\n",
      "Epoch: 28, Train_Loss: 0.23134097456932068, Test_Loss: 0.16278710961341858 *\n",
      "Epoch: 28, Train_Loss: 0.3320844769477844, Test_Loss: 0.3435523211956024\n",
      "Epoch: 28, Train_Loss: 0.17228657007217407, Test_Loss: 0.21191668510437012 *\n",
      "Epoch: 28, Train_Loss: 0.1958370804786682, Test_Loss: 0.18444086611270905 *\n",
      "Epoch: 28, Train_Loss: 0.1893019676208496, Test_Loss: 0.14786368608474731 *\n",
      "Epoch: 28, Train_Loss: 0.15691779553890228, Test_Loss: 0.14914818108081818\n",
      "Epoch: 28, Train_Loss: 0.14287060499191284, Test_Loss: 0.19944991171360016\n",
      "Epoch: 28, Train_Loss: 0.14057351648807526, Test_Loss: 0.1452629119157791 *\n",
      "Epoch: 28, Train_Loss: 0.1425849050283432, Test_Loss: 0.16486042737960815\n",
      "Epoch: 28, Train_Loss: 0.14458735287189484, Test_Loss: 0.14911942183971405 *\n",
      "Epoch: 28, Train_Loss: 0.14656057953834534, Test_Loss: 0.27079421281814575\n",
      "Epoch: 28, Train_Loss: 0.1594279259443283, Test_Loss: 0.5422449707984924\n",
      "Epoch: 28, Train_Loss: 0.1653347909450531, Test_Loss: 0.19134747982025146 *\n",
      "Epoch: 28, Train_Loss: 0.1604996919631958, Test_Loss: 0.48051589727401733\n",
      "Epoch: 28, Train_Loss: 0.33725255727767944, Test_Loss: 0.2500886619091034 *\n",
      "Epoch: 28, Train_Loss: 0.4315972328186035, Test_Loss: 0.2547050416469574\n",
      "Epoch: 28, Train_Loss: 0.1563103199005127, Test_Loss: 0.2527656853199005 *\n",
      "Epoch: 28, Train_Loss: 0.18475158512592316, Test_Loss: 0.2385772168636322 *\n",
      "Epoch: 28, Train_Loss: 0.1941271871328354, Test_Loss: 0.2521459460258484\n",
      "Epoch: 28, Train_Loss: 0.19148163497447968, Test_Loss: 4.740541458129883\n",
      "Epoch: 28, Train_Loss: 0.17779764533042908, Test_Loss: 0.8097611665725708 *\n",
      "Epoch: 28, Train_Loss: 0.1831098198890686, Test_Loss: 0.15061302483081818 *\n",
      "Epoch: 28, Train_Loss: 0.3356925845146179, Test_Loss: 0.15507422387599945\n",
      "Epoch: 28, Train_Loss: 0.2029246985912323, Test_Loss: 0.14507876336574554 *\n",
      "Epoch: 28, Train_Loss: 0.25056830048561096, Test_Loss: 0.1483151763677597\n",
      "Epoch: 28, Train_Loss: 0.15491613745689392, Test_Loss: 0.16361922025680542\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 0.16645854711532593, Test_Loss: 0.15327832102775574 *\n",
      "Epoch: 28, Train_Loss: 0.39587050676345825, Test_Loss: 0.16989994049072266\n",
      "Epoch: 28, Train_Loss: 0.5382657051086426, Test_Loss: 0.14473004639148712 *\n",
      "Epoch: 28, Train_Loss: 0.3821285367012024, Test_Loss: 0.14509256184101105\n",
      "Epoch: 28, Train_Loss: 0.19626347720623016, Test_Loss: 0.16781839728355408\n",
      "Epoch: 28, Train_Loss: 0.15893089771270752, Test_Loss: 0.17046687006950378\n",
      "Epoch: 28, Train_Loss: 0.14812755584716797, Test_Loss: 0.15778405964374542 *\n",
      "Epoch: 28, Train_Loss: 0.41816508769989014, Test_Loss: 0.1795808970928192\n",
      "Epoch: 28, Train_Loss: 0.24970632791519165, Test_Loss: 0.15114888548851013 *\n",
      "Epoch: 28, Train_Loss: 0.15603549778461456, Test_Loss: 0.14998212456703186 *\n",
      "Epoch: 28, Train_Loss: 0.32486462593078613, Test_Loss: 0.15690827369689941\n",
      "Epoch: 28, Train_Loss: 0.1600767821073532, Test_Loss: 0.15874925255775452\n",
      "Epoch: 28, Train_Loss: 0.16139322519302368, Test_Loss: 0.14214187860488892 *\n",
      "Epoch: 28, Train_Loss: 0.18729165196418762, Test_Loss: 0.1598796844482422\n",
      "Epoch: 28, Train_Loss: 0.25440388917922974, Test_Loss: 0.15810631215572357 *\n",
      "Epoch: 28, Train_Loss: 0.18256989121437073, Test_Loss: 0.1726410984992981\n",
      "Epoch: 28, Train_Loss: 0.24517452716827393, Test_Loss: 0.147071972489357 *\n",
      "Epoch: 28, Train_Loss: 0.15092015266418457, Test_Loss: 0.17097917199134827\n",
      "Epoch: 28, Train_Loss: 0.22790417075157166, Test_Loss: 0.14459867775440216 *\n",
      "Epoch: 28, Train_Loss: 0.18201103806495667, Test_Loss: 0.14852958917617798\n",
      "Epoch: 28, Train_Loss: 0.1614277958869934, Test_Loss: 0.16716283559799194\n",
      "Epoch: 28, Train_Loss: 0.14622902870178223, Test_Loss: 0.14644166827201843 *\n",
      "Epoch: 28, Train_Loss: 0.1720646768808365, Test_Loss: 0.14175212383270264 *\n",
      "Epoch: 28, Train_Loss: 0.3526154160499573, Test_Loss: 0.23395299911499023\n",
      "Epoch: 28, Train_Loss: 0.3507367968559265, Test_Loss: 0.4894142150878906\n",
      "Epoch: 28, Train_Loss: 0.390232652425766, Test_Loss: 4.691190719604492\n",
      "Epoch: 28, Train_Loss: 0.5447542667388916, Test_Loss: 0.17688316106796265 *\n",
      "Epoch: 28, Train_Loss: 0.4107392728328705, Test_Loss: 0.15412279963493347 *\n",
      "Epoch: 28, Train_Loss: 0.23529687523841858, Test_Loss: 0.19420096278190613\n",
      "Epoch: 28, Train_Loss: 0.23248955607414246, Test_Loss: 0.18653035163879395 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.15744251012802124, Test_Loss: 0.1474360078573227 *\n",
      "Epoch: 28, Train_Loss: 0.1460990607738495, Test_Loss: 0.1562669426202774\n",
      "Epoch: 28, Train_Loss: 0.1547967940568924, Test_Loss: 0.19384980201721191\n",
      "Epoch: 28, Train_Loss: 0.2983052134513855, Test_Loss: 0.17267858982086182 *\n",
      "Epoch: 28, Train_Loss: 0.3744257092475891, Test_Loss: 0.14395998418331146 *\n",
      "Epoch: 28, Train_Loss: 0.39502161741256714, Test_Loss: 0.1656605303287506\n",
      "Epoch: 28, Train_Loss: 0.7261675000190735, Test_Loss: 0.18054717779159546\n",
      "Epoch: 28, Train_Loss: 1.065253496170044, Test_Loss: 0.16030600666999817 *\n",
      "Epoch: 28, Train_Loss: 0.30920886993408203, Test_Loss: 0.24339857697486877\n",
      "Epoch: 28, Train_Loss: 0.2033165991306305, Test_Loss: 0.2436217963695526\n",
      "Epoch: 28, Train_Loss: 0.1463393121957779, Test_Loss: 0.2328178435564041 *\n",
      "Epoch: 28, Train_Loss: 0.29372310638427734, Test_Loss: 0.17627236247062683 *\n",
      "Epoch: 28, Train_Loss: 0.2964748740196228, Test_Loss: 0.21670667827129364\n",
      "Epoch: 28, Train_Loss: 0.7155471444129944, Test_Loss: 0.194337397813797 *\n",
      "Epoch: 28, Train_Loss: 0.16944992542266846, Test_Loss: 0.1890259087085724 *\n",
      "Epoch: 28, Train_Loss: 0.15916459262371063, Test_Loss: 0.4372398555278778\n",
      "Epoch: 28, Train_Loss: 0.21181908249855042, Test_Loss: 0.4001035690307617 *\n",
      "Epoch: 28, Train_Loss: 0.43903154134750366, Test_Loss: 0.43271416425704956\n",
      "Epoch: 28, Train_Loss: 0.2619026005268097, Test_Loss: 0.4018576741218567 *\n",
      "Epoch: 28, Train_Loss: 0.24655896425247192, Test_Loss: 0.3706132471561432 *\n",
      "Epoch: 28, Train_Loss: 0.2622331380844116, Test_Loss: 0.38108280301094055\n",
      "Epoch: 28, Train_Loss: 0.17665235698223114, Test_Loss: 0.29846733808517456 *\n",
      "Epoch: 28, Train_Loss: 0.14861895143985748, Test_Loss: 0.1652698665857315 *\n",
      "Epoch: 28, Train_Loss: 0.1583171784877777, Test_Loss: 0.17762549221515656\n",
      "Epoch: 28, Train_Loss: 0.14190562069416046, Test_Loss: 0.14557065069675446 *\n",
      "Epoch: 28, Train_Loss: 0.19046570360660553, Test_Loss: 0.15205061435699463\n",
      "Epoch: 28, Train_Loss: 0.22247688472270966, Test_Loss: 0.16399098932743073\n",
      "Epoch: 28, Train_Loss: 0.4475359320640564, Test_Loss: 0.3266821503639221\n",
      "Epoch: 28, Train_Loss: 15.297913551330566, Test_Loss: 0.2064918577671051 *\n",
      "Epoch: 28, Train_Loss: 0.23645278811454773, Test_Loss: 0.1751619428396225 *\n",
      "Epoch: 28, Train_Loss: 0.6864354610443115, Test_Loss: 0.1937282383441925\n",
      "Epoch: 28, Train_Loss: 0.8797612190246582, Test_Loss: 0.17002825438976288 *\n",
      "Epoch: 28, Train_Loss: 0.18971256911754608, Test_Loss: 0.14913256466388702 *\n",
      "Epoch: 28, Train_Loss: 0.3105866312980652, Test_Loss: 0.17645835876464844\n",
      "Epoch: 28, Train_Loss: 3.187584161758423, Test_Loss: 0.29462432861328125\n",
      "Epoch: 28, Train_Loss: 3.0620031356811523, Test_Loss: 0.3651460111141205\n",
      "Epoch: 28, Train_Loss: 0.23304688930511475, Test_Loss: 0.2557159662246704 *\n",
      "Epoch: 28, Train_Loss: 0.6354082226753235, Test_Loss: 0.21626943349838257 *\n",
      "Epoch: 28, Train_Loss: 4.232558250427246, Test_Loss: 0.1764928251504898 *\n",
      "Epoch: 28, Train_Loss: 0.33794814348220825, Test_Loss: 0.20565062761306763\n",
      "Epoch: 28, Train_Loss: 0.15077586472034454, Test_Loss: 0.21916940808296204\n",
      "Epoch: 28, Train_Loss: 0.14007484912872314, Test_Loss: 0.24096274375915527\n",
      "Epoch: 28, Train_Loss: 0.20441162586212158, Test_Loss: 0.1637733429670334 *\n",
      "Epoch: 28, Train_Loss: 0.1638195514678955, Test_Loss: 0.16074910759925842 *\n",
      "Epoch: 28, Train_Loss: 0.14066502451896667, Test_Loss: 0.1798498034477234\n",
      "Epoch: 28, Train_Loss: 0.14453841745853424, Test_Loss: 0.28202730417251587\n",
      "Epoch: 28, Train_Loss: 0.14013512432575226, Test_Loss: 0.7976150512695312\n",
      "Epoch: 28, Train_Loss: 0.1403346061706543, Test_Loss: 0.2225392460823059 *\n",
      "Epoch: 28, Train_Loss: 0.16091535985469818, Test_Loss: 0.22766733169555664\n",
      "Epoch: 28, Train_Loss: 0.14626096189022064, Test_Loss: 0.1444321572780609 *\n",
      "Epoch: 28, Train_Loss: 0.15891960263252258, Test_Loss: 0.1439860463142395 *\n",
      "Epoch: 28, Train_Loss: 0.1680198311805725, Test_Loss: 0.14278852939605713 *\n",
      "Epoch: 28, Train_Loss: 0.1868450790643692, Test_Loss: 0.1451607197523117\n",
      "Epoch: 28, Train_Loss: 0.14896754920482635, Test_Loss: 0.2523617148399353\n",
      "Epoch: 28, Train_Loss: 0.15017302334308624, Test_Loss: 8.136199951171875\n",
      "Epoch: 28, Train_Loss: 0.1718119978904724, Test_Loss: 0.38149285316467285 *\n",
      "Epoch: 28, Train_Loss: 0.14155207574367523, Test_Loss: 0.24477100372314453 *\n",
      "Epoch: 28, Train_Loss: 0.14444436132907867, Test_Loss: 0.33295387029647827\n",
      "Epoch: 28, Train_Loss: 0.13876540958881378, Test_Loss: 0.3334094285964966\n",
      "Epoch: 28, Train_Loss: 0.139005646109581, Test_Loss: 0.34885120391845703\n",
      "Epoch: 28, Train_Loss: 0.13963647186756134, Test_Loss: 0.5762646794319153\n",
      "Epoch: 28, Train_Loss: 0.13972437381744385, Test_Loss: 0.44661301374435425 *\n",
      "Epoch: 28, Train_Loss: 0.13881660997867584, Test_Loss: 0.24605420231819153 *\n",
      "Epoch: 28, Train_Loss: 0.13915051519870758, Test_Loss: 0.3273463845252991\n",
      "Epoch: 28, Train_Loss: 0.1475270539522171, Test_Loss: 0.34380483627319336\n",
      "Epoch: 28, Train_Loss: 0.16758964955806732, Test_Loss: 0.775101363658905\n",
      "Epoch: 28, Train_Loss: 0.19503122568130493, Test_Loss: 0.5681614875793457 *\n",
      "Epoch: 28, Train_Loss: 0.15900281071662903, Test_Loss: 0.46727001667022705 *\n",
      "Epoch: 28, Train_Loss: 0.30014801025390625, Test_Loss: 0.3534965515136719 *\n",
      "Epoch: 28, Train_Loss: 4.1139655113220215, Test_Loss: 0.1475156992673874 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 28\n",
      "Epoch: 28, Train_Loss: 2.526843309402466, Test_Loss: 0.17087092995643616\n",
      "Epoch: 28, Train_Loss: 0.1739678829908371, Test_Loss: 0.1429818719625473 *\n",
      "Epoch: 28, Train_Loss: 0.1807335615158081, Test_Loss: 0.36171162128448486\n",
      "Epoch: 28, Train_Loss: 0.18534773588180542, Test_Loss: 0.16792844235897064 *\n",
      "Epoch: 28, Train_Loss: 0.20210066437721252, Test_Loss: 0.31145578622817993\n",
      "Epoch: 28, Train_Loss: 0.18978345394134521, Test_Loss: 0.15985235571861267 *\n",
      "Epoch: 28, Train_Loss: 0.179446280002594, Test_Loss: 0.31221193075180054\n",
      "Epoch: 28, Train_Loss: 0.2141147255897522, Test_Loss: 0.2173246145248413 *\n",
      "Epoch: 28, Train_Loss: 0.2593461275100708, Test_Loss: 0.31628698110580444\n",
      "Epoch: 28, Train_Loss: 0.2914280891418457, Test_Loss: 0.15698257088661194 *\n",
      "Epoch: 28, Train_Loss: 0.15944017469882965, Test_Loss: 0.16319473087787628\n",
      "Epoch: 28, Train_Loss: 0.15757712721824646, Test_Loss: 0.17367660999298096\n",
      "Epoch: 28, Train_Loss: 0.15853093564510345, Test_Loss: 0.15508362650871277 *\n",
      "Epoch: 28, Train_Loss: 0.1578485369682312, Test_Loss: 0.15507149696350098 *\n",
      "Epoch: 28, Train_Loss: 0.14833170175552368, Test_Loss: 0.2981325089931488\n",
      "Epoch: 28, Train_Loss: 0.1610010713338852, Test_Loss: 2.0116803646087646\n",
      "Epoch: 28, Train_Loss: 0.1737518161535263, Test_Loss: 4.492805480957031\n",
      "Epoch: 28, Train_Loss: 0.1453935205936432, Test_Loss: 0.18800508975982666 *\n",
      "Epoch: 28, Train_Loss: 0.19396212697029114, Test_Loss: 0.16094085574150085 *\n",
      "Epoch: 28, Train_Loss: 0.17613287270069122, Test_Loss: 0.40562790632247925\n",
      "Epoch: 28, Train_Loss: 0.16389381885528564, Test_Loss: 0.3464868664741516 *\n",
      "Epoch: 28, Train_Loss: 0.1392611563205719, Test_Loss: 0.21276210248470306 *\n",
      "Epoch: 28, Train_Loss: 0.1387529820203781, Test_Loss: 0.1901801973581314 *\n",
      "Epoch: 28, Train_Loss: 0.1821909099817276, Test_Loss: 0.308321088552475\n",
      "Epoch: 28, Train_Loss: 2.5592432022094727, Test_Loss: 0.1538231521844864 *\n",
      "Epoch: 28, Train_Loss: 1.696622371673584, Test_Loss: 0.1524687111377716 *\n",
      "Epoch: 28, Train_Loss: 0.16627836227416992, Test_Loss: 0.18448783457279205\n",
      "Epoch: 28, Train_Loss: 0.1798810064792633, Test_Loss: 0.16657182574272156 *\n",
      "Epoch: 28, Train_Loss: 0.14647604525089264, Test_Loss: 0.15723934769630432 *\n",
      "Epoch: 28, Train_Loss: 0.13945206999778748, Test_Loss: 0.5223488211631775\n",
      "Epoch: 28, Train_Loss: 0.14976510405540466, Test_Loss: 0.3826620578765869 *\n",
      "Epoch: 28, Train_Loss: 0.14285115897655487, Test_Loss: 0.25214067101478577 *\n",
      "Epoch: 28, Train_Loss: 0.17161881923675537, Test_Loss: 0.21651406586170197 *\n",
      "Epoch: 28, Train_Loss: 0.155217245221138, Test_Loss: 0.1727725714445114 *\n",
      "Epoch: 28, Train_Loss: 0.15508189797401428, Test_Loss: 0.22712384164333344\n",
      "Epoch: 28, Train_Loss: 0.13987165689468384, Test_Loss: 0.648714542388916\n",
      "Epoch: 28, Train_Loss: 0.139852374792099, Test_Loss: 0.7859169244766235\n",
      "Epoch: 28, Train_Loss: 0.1496773660182953, Test_Loss: 0.6570436954498291 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train_Loss: 0.15080152451992035, Test_Loss: 0.5392731428146362 *\n",
      "Epoch: 28, Train_Loss: 0.140633687376976, Test_Loss: 0.6398099064826965\n",
      "Epoch: 28, Train_Loss: 0.14954355359077454, Test_Loss: 0.6309821605682373 *\n",
      "Epoch: 28, Train_Loss: 0.15504127740859985, Test_Loss: 0.6535932421684265\n",
      "Epoch: 28, Train_Loss: 0.1502745896577835, Test_Loss: 0.813443660736084\n",
      "Epoch: 28, Train_Loss: 0.14276258647441864, Test_Loss: 0.24840518832206726 *\n",
      "Epoch: 28, Train_Loss: 0.13906805217266083, Test_Loss: 0.16860178112983704 *\n",
      "Epoch: 28, Train_Loss: 0.1989080309867859, Test_Loss: 0.1408504843711853 *\n",
      "Epoch: 28, Train_Loss: 0.1671544313430786, Test_Loss: 0.15333813428878784\n",
      "Epoch: 28, Train_Loss: 0.1636107712984085, Test_Loss: 0.1924884170293808\n",
      "Epoch: 28, Train_Loss: 0.1921793818473816, Test_Loss: 0.27354684472084045\n",
      "Epoch: 28, Train_Loss: 0.19840237498283386, Test_Loss: 0.28357091546058655\n",
      "Epoch: 28, Train_Loss: 0.18004558980464935, Test_Loss: 0.1697942614555359 *\n",
      "Epoch: 28, Train_Loss: 0.15513327717781067, Test_Loss: 0.21793873608112335\n",
      "Epoch: 28, Train_Loss: 0.1860310584306717, Test_Loss: 0.1760953813791275 *\n",
      "Epoch: 28, Train_Loss: 0.21842679381370544, Test_Loss: 0.17701907455921173\n",
      "Epoch: 29, Train_Loss: 0.2051663100719452, Test_Loss: 0.2352205216884613 *\n",
      "Epoch: 29, Train_Loss: 0.19913017749786377, Test_Loss: 0.3856966197490692\n",
      "Epoch: 29, Train_Loss: 0.1438143104314804, Test_Loss: 0.5371142625808716\n",
      "Epoch: 29, Train_Loss: 0.13994990289211273, Test_Loss: 0.263960599899292 *\n",
      "Epoch: 29, Train_Loss: 0.13878320157527924, Test_Loss: 0.23570683598518372 *\n",
      "Epoch: 29, Train_Loss: 0.13809436559677124, Test_Loss: 0.1540200412273407 *\n",
      "Epoch: 29, Train_Loss: 0.142069011926651, Test_Loss: 0.15659911930561066\n",
      "Epoch: 29, Train_Loss: 3.5181167125701904, Test_Loss: 0.1534285694360733 *\n",
      "Epoch: 29, Train_Loss: 1.1149241924285889, Test_Loss: 0.14794214069843292 *\n",
      "Epoch: 29, Train_Loss: 0.14318279922008514, Test_Loss: 0.16591806709766388\n",
      "Epoch: 29, Train_Loss: 0.1428391933441162, Test_Loss: 0.14724946022033691 *\n",
      "Epoch: 29, Train_Loss: 0.14428789913654327, Test_Loss: 0.16401919722557068\n",
      "Epoch: 29, Train_Loss: 0.14069925248622894, Test_Loss: 0.24734441936016083\n",
      "Epoch: 29, Train_Loss: 0.14142417907714844, Test_Loss: 0.43874216079711914\n",
      "Epoch: 29, Train_Loss: 0.1433994472026825, Test_Loss: 0.31016606092453003 *\n",
      "Epoch: 29, Train_Loss: 0.1407594233751297, Test_Loss: 0.15341705083847046 *\n",
      "Epoch: 29, Train_Loss: 0.14083436131477356, Test_Loss: 0.14367201924324036 *\n",
      "Epoch: 29, Train_Loss: 0.15782760083675385, Test_Loss: 0.14378273487091064\n",
      "Epoch: 29, Train_Loss: 0.1503247320652008, Test_Loss: 0.14409124851226807\n",
      "Epoch: 29, Train_Loss: 0.1618378609418869, Test_Loss: 0.16149988770484924\n",
      "Epoch: 29, Train_Loss: 0.16771212220191956, Test_Loss: 0.6542167663574219\n",
      "Epoch: 29, Train_Loss: 0.15138618648052216, Test_Loss: 5.936283111572266\n",
      "Epoch: 29, Train_Loss: 0.17496202886104584, Test_Loss: 0.1948223114013672 *\n",
      "Epoch: 29, Train_Loss: 0.23094624280929565, Test_Loss: 0.15429595112800598 *\n",
      "Epoch: 29, Train_Loss: 0.17306794226169586, Test_Loss: 0.16157755255699158\n",
      "Epoch: 29, Train_Loss: 0.17402511835098267, Test_Loss: 0.14700712263584137 *\n",
      "Epoch: 29, Train_Loss: 0.19247278571128845, Test_Loss: 0.14461955428123474 *\n",
      "Epoch: 29, Train_Loss: 0.1402825117111206, Test_Loss: 0.1620446741580963\n",
      "Epoch: 29, Train_Loss: 0.14072953164577484, Test_Loss: 0.16622807085514069\n",
      "Epoch: 29, Train_Loss: 0.14750859141349792, Test_Loss: 0.14728288352489471 *\n",
      "Epoch: 29, Train_Loss: 0.1410640925168991, Test_Loss: 0.1461399793624878 *\n",
      "Epoch: 29, Train_Loss: 0.1453256607055664, Test_Loss: 0.17311209440231323\n",
      "Epoch: 29, Train_Loss: 0.15216976404190063, Test_Loss: 0.19580277800559998\n",
      "Epoch: 29, Train_Loss: 0.13867993652820587, Test_Loss: 0.15858986973762512 *\n",
      "Epoch: 29, Train_Loss: 0.1442490518093109, Test_Loss: 0.14801934361457825 *\n",
      "Epoch: 29, Train_Loss: 0.150717094540596, Test_Loss: 0.16579784452915192\n",
      "Epoch: 29, Train_Loss: 0.20108988881111145, Test_Loss: 0.14591114223003387 *\n",
      "Epoch: 29, Train_Loss: 0.21153050661087036, Test_Loss: 0.14343152940273285 *\n",
      "Epoch: 29, Train_Loss: 0.18420732021331787, Test_Loss: 0.1463342010974884\n",
      "Epoch: 29, Train_Loss: 0.2970033884048462, Test_Loss: 0.1833130568265915\n",
      "Epoch: 29, Train_Loss: 0.20433518290519714, Test_Loss: 0.14229939877986908 *\n",
      "Epoch: 29, Train_Loss: 0.21254803240299225, Test_Loss: 0.2098539024591446\n",
      "Epoch: 29, Train_Loss: 0.14148133993148804, Test_Loss: 0.13942956924438477 *\n",
      "Epoch: 29, Train_Loss: 0.2129688858985901, Test_Loss: 0.14692509174346924\n",
      "Epoch: 29, Train_Loss: 0.1486721634864807, Test_Loss: 0.15288525819778442\n",
      "Epoch: 29, Train_Loss: 0.41632336378097534, Test_Loss: 0.14281214773654938 *\n",
      "Epoch: 29, Train_Loss: 0.14917774498462677, Test_Loss: 0.1419609636068344 *\n",
      "Epoch: 29, Train_Loss: 0.745445191860199, Test_Loss: 0.14637883007526398\n",
      "Epoch: 29, Train_Loss: 1.7937901020050049, Test_Loss: 0.15453335642814636\n",
      "Epoch: 29, Train_Loss: 0.210636705160141, Test_Loss: 0.1501738429069519 *\n",
      "Epoch: 29, Train_Loss: 0.1869407743215561, Test_Loss: 0.14860957860946655 *\n",
      "Epoch: 29, Train_Loss: 0.15974971652030945, Test_Loss: 0.2663497030735016\n",
      "Epoch: 29, Train_Loss: 0.17162548005580902, Test_Loss: 3.290259838104248\n",
      "Epoch: 29, Train_Loss: 0.14042596518993378, Test_Loss: 3.8992221355438232\n",
      "Epoch: 29, Train_Loss: 0.14289715886116028, Test_Loss: 0.21152910590171814 *\n",
      "Epoch: 29, Train_Loss: 0.17308485507965088, Test_Loss: 0.19261664152145386 *\n",
      "Epoch: 29, Train_Loss: 0.18981647491455078, Test_Loss: 0.2504408061504364\n",
      "Epoch: 29, Train_Loss: 0.16513831913471222, Test_Loss: 0.3136032223701477\n",
      "Epoch: 29, Train_Loss: 0.15636558830738068, Test_Loss: 0.2069849967956543 *\n",
      "Epoch: 29, Train_Loss: 0.14834369719028473, Test_Loss: 0.21791189908981323\n",
      "Epoch: 29, Train_Loss: 0.14430230855941772, Test_Loss: 0.4986399710178375\n",
      "Epoch: 29, Train_Loss: 0.15454071760177612, Test_Loss: 0.1451054811477661 *\n",
      "Epoch: 29, Train_Loss: 0.15922962129116058, Test_Loss: 0.16583186388015747\n",
      "Epoch: 29, Train_Loss: 0.14513343572616577, Test_Loss: 0.20944488048553467\n",
      "Epoch: 29, Train_Loss: 0.14820101857185364, Test_Loss: 0.20434942841529846 *\n",
      "Epoch: 29, Train_Loss: 0.13876989483833313, Test_Loss: 0.15604491531848907 *\n",
      "Epoch: 29, Train_Loss: 0.1529679000377655, Test_Loss: 0.304335355758667\n",
      "Epoch: 29, Train_Loss: 0.14369365572929382, Test_Loss: 0.24485956132411957 *\n",
      "Epoch: 29, Train_Loss: 0.15367645025253296, Test_Loss: 0.22721517086029053 *\n",
      "Epoch: 29, Train_Loss: 0.1412987858057022, Test_Loss: 0.17179986834526062 *\n",
      "Epoch: 29, Train_Loss: 0.1386212855577469, Test_Loss: 0.2171400934457779\n",
      "Epoch: 29, Train_Loss: 0.13816656172275543, Test_Loss: 0.21956513822078705\n",
      "Epoch: 29, Train_Loss: 0.14232449233531952, Test_Loss: 0.4531514644622803\n",
      "Epoch: 29, Train_Loss: 0.14021846652030945, Test_Loss: 0.44009923934936523 *\n",
      "Epoch: 29, Train_Loss: 0.1395092010498047, Test_Loss: 0.4086758494377136 *\n",
      "Epoch: 29, Train_Loss: 0.14173072576522827, Test_Loss: 0.39925557374954224 *\n",
      "Epoch: 29, Train_Loss: 0.14082349836826324, Test_Loss: 0.4190235137939453\n",
      "Epoch: 29, Train_Loss: 0.13783803582191467, Test_Loss: 0.4174278974533081 *\n",
      "Epoch: 29, Train_Loss: 0.1421532779932022, Test_Loss: 0.36613062024116516 *\n",
      "Epoch: 29, Train_Loss: 0.14748463034629822, Test_Loss: 0.5305405259132385\n",
      "Epoch: 29, Train_Loss: 0.1509402096271515, Test_Loss: 0.20069573819637299 *\n",
      "Epoch: 29, Train_Loss: 0.14453428983688354, Test_Loss: 0.1560465693473816 *\n",
      "Epoch: 29, Train_Loss: 0.16141708195209503, Test_Loss: 0.1461096554994583 *\n",
      "Epoch: 29, Train_Loss: 0.1573828160762787, Test_Loss: 0.18747587502002716\n",
      "Epoch: 29, Train_Loss: 0.16393326222896576, Test_Loss: 0.1866631805896759 *\n",
      "Epoch: 29, Train_Loss: 0.14482128620147705, Test_Loss: 0.34027016162872314\n",
      "Epoch: 29, Train_Loss: 0.1421510875225067, Test_Loss: 0.3355317711830139 *\n",
      "Epoch: 29, Train_Loss: 0.16292724013328552, Test_Loss: 0.18001118302345276 *\n",
      "Epoch: 29, Train_Loss: 0.1571418046951294, Test_Loss: 0.17459432780742645 *\n",
      "Epoch: 29, Train_Loss: 0.14690053462982178, Test_Loss: 0.16556419432163239 *\n",
      "Epoch: 29, Train_Loss: 0.1459379643201828, Test_Loss: 0.17891553044319153\n",
      "Epoch: 29, Train_Loss: 0.15447580814361572, Test_Loss: 0.29227787256240845\n",
      "Epoch: 29, Train_Loss: 0.15245315432548523, Test_Loss: 0.1689731925725937 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.1741328090429306, Test_Loss: 0.4880049228668213\n",
      "Epoch: 29, Train_Loss: 0.15721601247787476, Test_Loss: 0.21138712763786316 *\n",
      "Epoch: 29, Train_Loss: 0.13875682651996613, Test_Loss: 0.21509557962417603\n",
      "Epoch: 29, Train_Loss: 0.1675659716129303, Test_Loss: 0.1474810689687729 *\n",
      "Epoch: 29, Train_Loss: 0.1730404943227768, Test_Loss: 0.14048561453819275 *\n",
      "Epoch: 29, Train_Loss: 0.16466793417930603, Test_Loss: 0.15433543920516968\n",
      "Epoch: 29, Train_Loss: 0.165870800614357, Test_Loss: 0.15474271774291992\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.18390867114067078, Test_Loss: 0.16201677918434143\n",
      "Epoch: 29, Train_Loss: 0.2330321967601776, Test_Loss: 0.14615491032600403 *\n",
      "Epoch: 29, Train_Loss: 0.18529857695102692, Test_Loss: 0.19065123796463013\n",
      "Epoch: 29, Train_Loss: 0.15988250076770782, Test_Loss: 0.27087700366973877\n",
      "Epoch: 29, Train_Loss: 0.18012498319149017, Test_Loss: 0.3675884008407593\n",
      "Epoch: 29, Train_Loss: 0.1428333818912506, Test_Loss: 0.28070682287216187 *\n",
      "Epoch: 29, Train_Loss: 0.1615293174982071, Test_Loss: 0.1405864655971527 *\n",
      "Epoch: 29, Train_Loss: 0.1393539309501648, Test_Loss: 0.13798928260803223 *\n",
      "Epoch: 29, Train_Loss: 0.14781489968299866, Test_Loss: 0.13814117014408112\n",
      "Epoch: 29, Train_Loss: 0.14523813128471375, Test_Loss: 0.1380673050880432 *\n",
      "Epoch: 29, Train_Loss: 0.14421223104000092, Test_Loss: 0.14940248429775238\n",
      "Epoch: 29, Train_Loss: 0.18153700232505798, Test_Loss: 1.816270112991333\n",
      "Epoch: 29, Train_Loss: 0.1480637639760971, Test_Loss: 4.056529521942139\n",
      "Epoch: 29, Train_Loss: 0.1785849928855896, Test_Loss: 0.153367817401886 *\n",
      "Epoch: 29, Train_Loss: 0.1484554409980774, Test_Loss: 0.14783132076263428 *\n",
      "Epoch: 29, Train_Loss: 0.1472851186990738, Test_Loss: 0.14409670233726501 *\n",
      "Epoch: 29, Train_Loss: 0.1541798710823059, Test_Loss: 0.1407560110092163 *\n",
      "Epoch: 29, Train_Loss: 0.45927730202674866, Test_Loss: 0.1453024297952652\n",
      "Epoch: 29, Train_Loss: 0.16447311639785767, Test_Loss: 0.15677379071712494\n",
      "Epoch: 29, Train_Loss: 0.15493759512901306, Test_Loss: 0.1551174819469452 *\n",
      "Epoch: 29, Train_Loss: 0.14800208806991577, Test_Loss: 0.1493813842535019 *\n",
      "Epoch: 29, Train_Loss: 0.13928627967834473, Test_Loss: 0.1486918181180954 *\n",
      "Epoch: 29, Train_Loss: 0.14412909746170044, Test_Loss: 0.15165480971336365\n",
      "Epoch: 29, Train_Loss: 0.14090564846992493, Test_Loss: 0.15868914127349854\n",
      "Epoch: 29, Train_Loss: 0.14513389766216278, Test_Loss: 0.16332930326461792\n",
      "Epoch: 29, Train_Loss: 0.13951769471168518, Test_Loss: 0.1822838932275772\n",
      "Epoch: 29, Train_Loss: 0.15215559303760529, Test_Loss: 0.15646247565746307 *\n",
      "Epoch: 29, Train_Loss: 0.14347584545612335, Test_Loss: 0.14169681072235107 *\n",
      "Epoch: 29, Train_Loss: 0.1442209929227829, Test_Loss: 0.14108026027679443 *\n",
      "Epoch: 29, Train_Loss: 0.1483226716518402, Test_Loss: 0.14564277231693268\n",
      "Epoch: 29, Train_Loss: 0.14009711146354675, Test_Loss: 0.15248596668243408\n",
      "Epoch: 29, Train_Loss: 0.13743789494037628, Test_Loss: 0.1411799192428589 *\n",
      "Epoch: 29, Train_Loss: 0.15355922281742096, Test_Loss: 0.16893810033798218\n",
      "Epoch: 29, Train_Loss: 0.14385218918323517, Test_Loss: 0.1377193033695221 *\n",
      "Epoch: 29, Train_Loss: 0.15027213096618652, Test_Loss: 0.1436363309621811\n",
      "Epoch: 29, Train_Loss: 0.14370518922805786, Test_Loss: 0.1444726139307022\n",
      "Epoch: 29, Train_Loss: 0.14946947991847992, Test_Loss: 0.13930116593837738 *\n",
      "Epoch: 29, Train_Loss: 0.14719155430793762, Test_Loss: 0.13976387679576874\n",
      "Epoch: 29, Train_Loss: 0.15737129747867584, Test_Loss: 0.14449740946292877\n",
      "Epoch: 29, Train_Loss: 0.13882213830947876, Test_Loss: 0.14709968864917755\n",
      "Epoch: 29, Train_Loss: 0.15355423092842102, Test_Loss: 0.140335351228714 *\n",
      "Epoch: 29, Train_Loss: 0.14637163281440735, Test_Loss: 0.17916785180568695\n",
      "Epoch: 29, Train_Loss: 0.1477670520544052, Test_Loss: 0.18874849379062653\n",
      "Epoch: 29, Train_Loss: 0.1391754299402237, Test_Loss: 4.127463340759277\n",
      "Epoch: 29, Train_Loss: 0.15369009971618652, Test_Loss: 2.451207160949707 *\n",
      "Epoch: 29, Train_Loss: 0.22331602871418, Test_Loss: 0.16568374633789062 *\n",
      "Epoch: 29, Train_Loss: 2.7383060455322266, Test_Loss: 0.16975025832653046\n",
      "Epoch: 29, Train_Loss: 2.760690212249756, Test_Loss: 0.17694143950939178\n",
      "Epoch: 29, Train_Loss: 0.1553223729133606, Test_Loss: 0.18920405209064484\n",
      "Epoch: 29, Train_Loss: 0.13867662847042084, Test_Loss: 0.15556661784648895 *\n",
      "Epoch: 29, Train_Loss: 0.16418160498142242, Test_Loss: 0.21876946091651917\n",
      "Epoch: 29, Train_Loss: 0.20494680106639862, Test_Loss: 0.2980040907859802\n",
      "Epoch: 29, Train_Loss: 0.15312016010284424, Test_Loss: 0.14033472537994385 *\n",
      "Epoch: 29, Train_Loss: 0.13844649493694305, Test_Loss: 0.16301722824573517\n",
      "Epoch: 29, Train_Loss: 0.16817529499530792, Test_Loss: 0.16040320694446564 *\n",
      "Epoch: 29, Train_Loss: 0.18351396918296814, Test_Loss: 0.16047589480876923\n",
      "Epoch: 29, Train_Loss: 0.14989307522773743, Test_Loss: 0.14956946671009064 *\n",
      "Epoch: 29, Train_Loss: 0.1990274339914322, Test_Loss: 0.19117139279842377\n",
      "Epoch: 29, Train_Loss: 0.3461933135986328, Test_Loss: 0.19315405189990997\n",
      "Epoch: 29, Train_Loss: 0.48245900869369507, Test_Loss: 0.227095827460289\n",
      "Epoch: 29, Train_Loss: 0.2680709958076477, Test_Loss: 0.15771077573299408 *\n",
      "Epoch: 29, Train_Loss: 0.2556956112384796, Test_Loss: 0.20352689921855927\n",
      "Epoch: 29, Train_Loss: 0.765763521194458, Test_Loss: 0.18932494521141052 *\n",
      "Epoch: 29, Train_Loss: 0.4695572257041931, Test_Loss: 0.298209011554718\n",
      "Epoch: 29, Train_Loss: 0.1442321240901947, Test_Loss: 0.36144012212753296\n",
      "Epoch: 29, Train_Loss: 0.1387307494878769, Test_Loss: 0.367658793926239\n",
      "Epoch: 29, Train_Loss: 0.5043990612030029, Test_Loss: 0.35317808389663696 *\n",
      "Epoch: 29, Train_Loss: 0.3660820722579956, Test_Loss: 0.3617064356803894\n",
      "Epoch: 29, Train_Loss: 0.6091362237930298, Test_Loss: 0.4083513617515564\n",
      "Epoch: 29, Train_Loss: 0.1535910964012146, Test_Loss: 0.3308485746383667 *\n",
      "Epoch: 29, Train_Loss: 0.1593431830406189, Test_Loss: 0.3904133439064026\n",
      "Epoch: 29, Train_Loss: 0.21519961953163147, Test_Loss: 0.16953232884407043 *\n",
      "Epoch: 29, Train_Loss: 0.32058408856391907, Test_Loss: 0.15221376717090607 *\n",
      "Epoch: 29, Train_Loss: 0.1461954414844513, Test_Loss: 0.1495492160320282 *\n",
      "Epoch: 29, Train_Loss: 0.19959037005901337, Test_Loss: 0.18281297385692596\n",
      "Epoch: 29, Train_Loss: 0.16437044739723206, Test_Loss: 0.27804678678512573\n",
      "Epoch: 29, Train_Loss: 0.19520145654678345, Test_Loss: 0.23486508429050446 *\n",
      "Epoch: 29, Train_Loss: 0.2820320129394531, Test_Loss: 0.30813896656036377\n",
      "Epoch: 29, Train_Loss: 0.2370726764202118, Test_Loss: 0.18816208839416504 *\n",
      "Epoch: 29, Train_Loss: 0.1792600452899933, Test_Loss: 0.164178267121315 *\n",
      "Epoch: 29, Train_Loss: 0.18626193702220917, Test_Loss: 0.16116048395633698 *\n",
      "Epoch: 29, Train_Loss: 0.2511046826839447, Test_Loss: 0.16125470399856567\n",
      "Epoch: 29, Train_Loss: 0.18268948793411255, Test_Loss: 0.4278375804424286\n",
      "Epoch: 29, Train_Loss: 0.21601849794387817, Test_Loss: 0.17595405876636505 *\n",
      "Epoch: 29, Train_Loss: 0.280224084854126, Test_Loss: 0.5754644274711609\n",
      "Epoch: 29, Train_Loss: 0.17163245379924774, Test_Loss: 0.22071027755737305 *\n",
      "Epoch: 29, Train_Loss: 0.2205340564250946, Test_Loss: 0.19833439588546753 *\n",
      "Epoch: 29, Train_Loss: 0.19848570227622986, Test_Loss: 0.14875058829784393 *\n",
      "Epoch: 29, Train_Loss: 0.15638691186904907, Test_Loss: 0.13813292980194092 *\n",
      "Epoch: 29, Train_Loss: 0.1458529680967331, Test_Loss: 0.20113605260849\n",
      "Epoch: 29, Train_Loss: 0.1385701447725296, Test_Loss: 0.15196773409843445 *\n",
      "Epoch: 29, Train_Loss: 0.15259262919425964, Test_Loss: 0.15490207076072693\n",
      "Epoch: 29, Train_Loss: 0.14132042229175568, Test_Loss: 0.16377709805965424\n",
      "Epoch: 29, Train_Loss: 0.1490463763475418, Test_Loss: 0.25025245547294617\n",
      "Epoch: 29, Train_Loss: 0.15344293415546417, Test_Loss: 0.42137688398361206\n",
      "Epoch: 29, Train_Loss: 0.1507536768913269, Test_Loss: 0.3156132400035858 *\n",
      "Epoch: 29, Train_Loss: 0.1546424776315689, Test_Loss: 0.4083118438720703\n",
      "Epoch: 29, Train_Loss: 0.408083438873291, Test_Loss: 0.16423086822032928 *\n",
      "Epoch: 29, Train_Loss: 0.42268750071525574, Test_Loss: 0.15738530457019806 *\n",
      "Epoch: 29, Train_Loss: 0.15401363372802734, Test_Loss: 0.1540171056985855 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.19464290142059326, Test_Loss: 0.1517886072397232 *\n",
      "Epoch: 29, Train_Loss: 0.19317439198493958, Test_Loss: 0.2180972844362259\n",
      "Epoch: 29, Train_Loss: 0.18630017340183258, Test_Loss: 3.5265040397644043\n",
      "Epoch: 29, Train_Loss: 0.33534252643585205, Test_Loss: 2.402492046356201 *\n",
      "Epoch: 29, Train_Loss: 0.18477807939052582, Test_Loss: 0.14786434173583984 *\n",
      "Epoch: 29, Train_Loss: 0.43116503953933716, Test_Loss: 0.1523347645998001\n",
      "Epoch: 29, Train_Loss: 0.20463284850120544, Test_Loss: 0.1444389820098877 *\n",
      "Epoch: 29, Train_Loss: 0.2442844957113266, Test_Loss: 0.13981786370277405 *\n",
      "Epoch: 29, Train_Loss: 0.15122339129447937, Test_Loss: 0.1498650163412094\n",
      "Epoch: 29, Train_Loss: 0.15471597015857697, Test_Loss: 0.1613992303609848\n",
      "Epoch: 29, Train_Loss: 0.2781408429145813, Test_Loss: 0.15496943891048431 *\n",
      "Epoch: 29, Train_Loss: 0.6383631825447083, Test_Loss: 0.14312584698200226 *\n",
      "Epoch: 29, Train_Loss: 0.5558124780654907, Test_Loss: 0.15302251279354095\n",
      "Epoch: 29, Train_Loss: 0.15275242924690247, Test_Loss: 0.15218113362789154 *\n",
      "Epoch: 29, Train_Loss: 0.1565532684326172, Test_Loss: 0.1836557686328888\n",
      "Epoch: 29, Train_Loss: 0.1435774564743042, Test_Loss: 0.15346334874629974 *\n",
      "Epoch: 29, Train_Loss: 0.34349656105041504, Test_Loss: 0.17147886753082275\n",
      "Epoch: 29, Train_Loss: 0.4422587454319, Test_Loss: 0.14349813759326935 *\n",
      "Epoch: 29, Train_Loss: 0.14441820979118347, Test_Loss: 0.14902088046073914\n",
      "Epoch: 29, Train_Loss: 0.3028756082057953, Test_Loss: 0.15313927829265594\n",
      "Epoch: 29, Train_Loss: 0.15116555988788605, Test_Loss: 0.16094306111335754\n",
      "Epoch: 29, Train_Loss: 0.1589515507221222, Test_Loss: 0.14477676153182983 *\n",
      "Epoch: 29, Train_Loss: 0.18716216087341309, Test_Loss: 0.14321903884410858 *\n",
      "Epoch: 29, Train_Loss: 0.23325428366661072, Test_Loss: 0.16944609582424164\n",
      "Epoch: 29, Train_Loss: 0.20972862839698792, Test_Loss: 0.13902126252651215 *\n",
      "Epoch: 29, Train_Loss: 0.21152949333190918, Test_Loss: 0.155318483710289\n",
      "Epoch: 29, Train_Loss: 0.1507381945848465, Test_Loss: 0.16017913818359375\n",
      "Epoch: 29, Train_Loss: 0.22697052359580994, Test_Loss: 0.14148569107055664 *\n",
      "Epoch: 29, Train_Loss: 0.16217240691184998, Test_Loss: 0.14257392287254333\n",
      "Epoch: 29, Train_Loss: 0.16080820560455322, Test_Loss: 0.1617438942193985\n",
      "Epoch: 29, Train_Loss: 0.14363691210746765, Test_Loss: 0.16206490993499756\n",
      "Epoch: 29, Train_Loss: 0.16102494299411774, Test_Loss: 0.1394994854927063 *\n",
      "Epoch: 29, Train_Loss: 0.21132412552833557, Test_Loss: 0.20440492033958435\n",
      "Epoch: 29, Train_Loss: 0.28703027963638306, Test_Loss: 0.3058171272277832\n",
      "Epoch: 29, Train_Loss: 0.3157098889350891, Test_Loss: 4.75031042098999\n",
      "Epoch: 29, Train_Loss: 0.5954009294509888, Test_Loss: 0.6681211590766907 *\n",
      "Epoch: 29, Train_Loss: 0.46456021070480347, Test_Loss: 0.15733392536640167 *\n",
      "Epoch: 29, Train_Loss: 0.32082265615463257, Test_Loss: 0.17722447216510773\n",
      "Epoch: 29, Train_Loss: 0.20160770416259766, Test_Loss: 0.1810779869556427\n",
      "Epoch: 29, Train_Loss: 0.15986646711826324, Test_Loss: 0.15385256707668304 *\n",
      "Epoch: 29, Train_Loss: 0.1426008939743042, Test_Loss: 0.15189965069293976 *\n",
      "Epoch: 29, Train_Loss: 0.15115435421466827, Test_Loss: 0.18888121843338013\n",
      "Epoch: 29, Train_Loss: 0.2452024221420288, Test_Loss: 0.188616544008255 *\n",
      "Epoch: 29, Train_Loss: 0.45502763986587524, Test_Loss: 0.13890230655670166 *\n",
      "Epoch: 29, Train_Loss: 0.48130103945732117, Test_Loss: 0.16775746643543243\n",
      "Epoch: 29, Train_Loss: 0.9693893194198608, Test_Loss: 0.17352211475372314\n",
      "Epoch: 29, Train_Loss: 1.093977928161621, Test_Loss: 0.19214263558387756\n",
      "Epoch: 29, Train_Loss: 0.23853687942028046, Test_Loss: 0.18031160533428192 *\n",
      "Epoch: 29, Train_Loss: 0.30079537630081177, Test_Loss: 0.1950954794883728\n",
      "Epoch: 29, Train_Loss: 0.14556092023849487, Test_Loss: 0.21368421614170074\n",
      "Epoch: 29, Train_Loss: 0.2265891283750534, Test_Loss: 0.21273648738861084 *\n",
      "Epoch: 29, Train_Loss: 0.3079674243927002, Test_Loss: 0.17956233024597168 *\n",
      "Epoch: 29, Train_Loss: 0.7834874987602234, Test_Loss: 0.26266276836395264\n",
      "Epoch: 29, Train_Loss: 0.1654212474822998, Test_Loss: 0.19504296779632568 *\n",
      "Epoch: 29, Train_Loss: 0.1707654893398285, Test_Loss: 0.3301094174385071\n",
      "Epoch: 29, Train_Loss: 0.1810530126094818, Test_Loss: 0.33890536427497864\n",
      "Epoch: 29, Train_Loss: 0.37339332699775696, Test_Loss: 0.4229164123535156\n",
      "Epoch: 29, Train_Loss: 0.2829282283782959, Test_Loss: 0.34834760427474976 *\n",
      "Epoch: 29, Train_Loss: 0.40597811341285706, Test_Loss: 0.332470178604126 *\n",
      "Epoch: 29, Train_Loss: 0.28961482644081116, Test_Loss: 0.42081791162490845\n",
      "Epoch: 29, Train_Loss: 0.3151710629463196, Test_Loss: 0.3044772148132324 *\n",
      "Epoch: 29, Train_Loss: 0.14509329199790955, Test_Loss: 0.23641248047351837 *\n",
      "Epoch: 29, Train_Loss: 0.150382861495018, Test_Loss: 0.1708609163761139 *\n",
      "Epoch: 29, Train_Loss: 0.1396019160747528, Test_Loss: 0.1596193015575409 *\n",
      "Epoch: 29, Train_Loss: 0.16911335289478302, Test_Loss: 0.14350764453411102 *\n",
      "Epoch: 29, Train_Loss: 0.18022598326206207, Test_Loss: 0.15510831773281097\n",
      "Epoch: 29, Train_Loss: 0.2370477020740509, Test_Loss: 0.304278701543808\n",
      "Epoch: 29, Train_Loss: 14.742664337158203, Test_Loss: 0.20780472457408905 *\n",
      "Epoch: 29, Train_Loss: 0.2561380863189697, Test_Loss: 0.23986664414405823\n",
      "Epoch: 29, Train_Loss: 1.0349527597427368, Test_Loss: 0.17158636450767517 *\n",
      "Epoch: 29, Train_Loss: 1.057760238647461, Test_Loss: 0.16452497243881226 *\n",
      "Epoch: 29, Train_Loss: 0.18936577439308167, Test_Loss: 0.15206851065158844 *\n",
      "Epoch: 29, Train_Loss: 0.5069745182991028, Test_Loss: 0.17127355933189392\n",
      "Epoch: 29, Train_Loss: 2.0800724029541016, Test_Loss: 0.37360498309135437\n",
      "Epoch: 29, Train_Loss: 4.2603535652160645, Test_Loss: 0.19011448323726654 *\n",
      "Epoch: 29, Train_Loss: 0.2735635042190552, Test_Loss: 0.40876293182373047\n",
      "Epoch: 29, Train_Loss: 0.2986602187156677, Test_Loss: 0.17836838960647583 *\n",
      "Epoch: 29, Train_Loss: 4.355465412139893, Test_Loss: 0.2070777416229248\n",
      "Epoch: 29, Train_Loss: 0.21517601609230042, Test_Loss: 0.23113122582435608\n",
      "Epoch: 29, Train_Loss: 0.162385493516922, Test_Loss: 0.18981383740901947 *\n",
      "Epoch: 29, Train_Loss: 0.14571191370487213, Test_Loss: 0.28525835275650024\n",
      "Epoch: 29, Train_Loss: 0.19536486268043518, Test_Loss: 0.15282630920410156 *\n",
      "Epoch: 29, Train_Loss: 0.20065994560718536, Test_Loss: 0.16618694365024567\n",
      "Epoch: 29, Train_Loss: 0.13720788061618805, Test_Loss: 0.17715036869049072\n",
      "Epoch: 29, Train_Loss: 0.15360420942306519, Test_Loss: 0.30058753490448\n",
      "Epoch: 29, Train_Loss: 0.13698793947696686, Test_Loss: 0.9997207522392273\n",
      "Epoch: 29, Train_Loss: 0.13789647817611694, Test_Loss: 0.4405549466609955 *\n",
      "Epoch: 29, Train_Loss: 0.16451865434646606, Test_Loss: 0.24895057082176208 *\n",
      "Epoch: 29, Train_Loss: 0.14552482962608337, Test_Loss: 0.165943443775177 *\n",
      "Epoch: 29, Train_Loss: 0.16789378225803375, Test_Loss: 0.16847364604473114\n",
      "Epoch: 29, Train_Loss: 0.17416539788246155, Test_Loss: 0.1693287193775177\n",
      "Epoch: 29, Train_Loss: 0.18302789330482483, Test_Loss: 0.16272109746932983 *\n",
      "Epoch: 29, Train_Loss: 0.17095321416854858, Test_Loss: 0.19949078559875488\n",
      "Epoch: 29, Train_Loss: 0.1772247701883316, Test_Loss: 5.228314399719238\n",
      "Epoch: 29, Train_Loss: 0.16293810307979584, Test_Loss: 1.5895402431488037 *\n",
      "Epoch: 29, Train_Loss: 0.1578345149755478, Test_Loss: 0.2526833117008209 *\n",
      "Epoch: 29, Train_Loss: 0.1426466554403305, Test_Loss: 0.2915512025356293\n",
      "Epoch: 29, Train_Loss: 0.13624928891658783, Test_Loss: 0.31972452998161316\n",
      "Epoch: 29, Train_Loss: 0.13607318699359894, Test_Loss: 0.23140466213226318 *\n",
      "Epoch: 29, Train_Loss: 0.13652350008487701, Test_Loss: 0.3345909118652344\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.1370341032743454, Test_Loss: 0.3327868580818176 *\n",
      "Epoch: 29, Train_Loss: 0.1360289752483368, Test_Loss: 0.2540576457977295 *\n",
      "Epoch: 29, Train_Loss: 0.13609233498573303, Test_Loss: 0.24816641211509705 *\n",
      "Epoch: 29, Train_Loss: 0.14259710907936096, Test_Loss: 0.19752220809459686 *\n",
      "Epoch: 29, Train_Loss: 0.16721634566783905, Test_Loss: 0.2061089277267456\n",
      "Epoch: 29, Train_Loss: 0.17407409846782684, Test_Loss: 0.510543167591095\n",
      "Epoch: 29, Train_Loss: 0.19412173330783844, Test_Loss: 0.21205806732177734 *\n",
      "Epoch: 29, Train_Loss: 0.18014384806156158, Test_Loss: 0.21359142661094666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 1.6217564344406128, Test_Loss: 0.1401231735944748 *\n",
      "Epoch: 29, Train_Loss: 4.728348731994629, Test_Loss: 0.1544562578201294\n",
      "Epoch: 29, Train_Loss: 0.17247621715068817, Test_Loss: 0.14542201161384583 *\n",
      "Epoch: 29, Train_Loss: 0.15184654295444489, Test_Loss: 0.27688950300216675\n",
      "Epoch: 29, Train_Loss: 0.1564202904701233, Test_Loss: 0.18895694613456726 *\n",
      "Epoch: 29, Train_Loss: 0.2130824476480484, Test_Loss: 0.141550213098526 *\n",
      "Epoch: 29, Train_Loss: 0.1779666244983673, Test_Loss: 0.24198368191719055\n",
      "Epoch: 29, Train_Loss: 0.1728280782699585, Test_Loss: 0.269359827041626\n",
      "Epoch: 29, Train_Loss: 0.16064783930778503, Test_Loss: 0.23949596285820007 *\n",
      "Epoch: 29, Train_Loss: 0.20212054252624512, Test_Loss: 0.45298197865486145\n",
      "Epoch: 29, Train_Loss: 0.20941142737865448, Test_Loss: 0.1571335345506668 *\n",
      "Epoch: 29, Train_Loss: 0.16270215809345245, Test_Loss: 0.16637641191482544\n",
      "Epoch: 29, Train_Loss: 0.14890174567699432, Test_Loss: 0.22521448135375977\n",
      "Epoch: 29, Train_Loss: 0.14965395629405975, Test_Loss: 0.18711325526237488 *\n",
      "Epoch: 29, Train_Loss: 0.14177988469600677, Test_Loss: 0.14205296337604523 *\n",
      "Epoch: 29, Train_Loss: 0.141931414604187, Test_Loss: 0.4289129078388214\n",
      "Epoch: 29, Train_Loss: 0.14712069928646088, Test_Loss: 0.31207168102264404 *\n",
      "Epoch: 29, Train_Loss: 0.1679653823375702, Test_Loss: 7.161290168762207\n",
      "Epoch: 29, Train_Loss: 0.14166459441184998, Test_Loss: 0.4344715476036072 *\n",
      "Epoch: 29, Train_Loss: 0.16834275424480438, Test_Loss: 0.17024564743041992 *\n",
      "Epoch: 29, Train_Loss: 0.19727231562137604, Test_Loss: 0.6766029596328735\n",
      "Epoch: 29, Train_Loss: 0.24284739792346954, Test_Loss: 0.9418337345123291\n",
      "Epoch: 29, Train_Loss: 0.15811623632907867, Test_Loss: 0.5597557425498962 *\n",
      "Epoch: 29, Train_Loss: 0.13745450973510742, Test_Loss: 0.19353103637695312 *\n",
      "Epoch: 29, Train_Loss: 0.15453891456127167, Test_Loss: 0.34118375182151794\n",
      "Epoch: 29, Train_Loss: 1.2217069864273071, Test_Loss: 0.2890404760837555 *\n",
      "Epoch: 29, Train_Loss: 3.752671718597412, Test_Loss: 0.14801736176013947 *\n",
      "Epoch: 29, Train_Loss: 0.14366157352924347, Test_Loss: 0.20001675188541412\n",
      "Epoch: 29, Train_Loss: 0.14219266176223755, Test_Loss: 0.23742952942848206\n",
      "Epoch: 29, Train_Loss: 0.14289234578609467, Test_Loss: 0.19765645265579224 *\n",
      "Epoch: 29, Train_Loss: 0.13808710873126984, Test_Loss: 0.6495295763015747\n",
      "Epoch: 29, Train_Loss: 0.1452856957912445, Test_Loss: 0.9629747867584229\n",
      "Epoch: 29, Train_Loss: 0.13906174898147583, Test_Loss: 0.34969812631607056 *\n",
      "Epoch: 29, Train_Loss: 0.1716681569814682, Test_Loss: 0.21547254920005798 *\n",
      "Epoch: 29, Train_Loss: 0.15442106127738953, Test_Loss: 0.2558569014072418\n",
      "Epoch: 29, Train_Loss: 0.16260699927806854, Test_Loss: 0.2508113980293274 *\n",
      "Epoch: 29, Train_Loss: 0.13719627261161804, Test_Loss: 0.4695034921169281\n",
      "Epoch: 29, Train_Loss: 0.13622644543647766, Test_Loss: 0.8487131595611572\n",
      "Epoch: 29, Train_Loss: 0.140871062874794, Test_Loss: 0.6954241991043091 *\n",
      "Epoch: 29, Train_Loss: 0.1526041328907013, Test_Loss: 0.5974716544151306 *\n",
      "Epoch: 29, Train_Loss: 0.13717718422412872, Test_Loss: 0.513504147529602 *\n",
      "Epoch: 29, Train_Loss: 0.14041569828987122, Test_Loss: 0.5797390937805176\n",
      "Epoch: 29, Train_Loss: 0.1554545909166336, Test_Loss: 0.7645939588546753\n",
      "Epoch: 29, Train_Loss: 0.1558855175971985, Test_Loss: 0.6394360065460205 *\n",
      "Epoch: 29, Train_Loss: 0.13925598561763763, Test_Loss: 0.3964136838912964 *\n",
      "Epoch: 29, Train_Loss: 0.13812817633152008, Test_Loss: 0.1946779191493988 *\n",
      "Epoch: 29, Train_Loss: 0.16595160961151123, Test_Loss: 0.1497250199317932 *\n",
      "Epoch: 29, Train_Loss: 0.1546061784029007, Test_Loss: 0.14734916388988495 *\n",
      "Epoch: 29, Train_Loss: 0.15367761254310608, Test_Loss: 0.17190992832183838\n",
      "Epoch: 29, Train_Loss: 0.1633606255054474, Test_Loss: 0.3210369348526001\n",
      "Epoch: 29, Train_Loss: 0.18340414762496948, Test_Loss: 0.19343571364879608 *\n",
      "Epoch: 29, Train_Loss: 0.17034339904785156, Test_Loss: 0.20507201552391052\n",
      "Epoch: 29, Train_Loss: 0.15517276525497437, Test_Loss: 0.20840555429458618\n",
      "Epoch: 29, Train_Loss: 0.17806020379066467, Test_Loss: 0.17256557941436768 *\n",
      "Epoch: 29, Train_Loss: 0.1483597755432129, Test_Loss: 0.14764663577079773 *\n",
      "Epoch: 29, Train_Loss: 0.2533189356327057, Test_Loss: 0.21929362416267395\n",
      "Epoch: 29, Train_Loss: 0.1558505892753601, Test_Loss: 0.5663275718688965\n",
      "Epoch: 29, Train_Loss: 0.13785263895988464, Test_Loss: 0.6708290576934814\n",
      "Epoch: 29, Train_Loss: 0.13624770939350128, Test_Loss: 0.2511284351348877 *\n",
      "Epoch: 29, Train_Loss: 0.1357882171869278, Test_Loss: 0.22776982188224792 *\n",
      "Epoch: 29, Train_Loss: 0.13542574644088745, Test_Loss: 0.16857071220874786 *\n",
      "Epoch: 29, Train_Loss: 0.13654731214046478, Test_Loss: 0.15424178540706635 *\n",
      "Epoch: 29, Train_Loss: 2.1049857139587402, Test_Loss: 0.14804522693157196 *\n",
      "Epoch: 29, Train_Loss: 2.4590721130371094, Test_Loss: 0.1563129723072052\n",
      "Epoch: 29, Train_Loss: 0.1416223645210266, Test_Loss: 0.1485232561826706 *\n",
      "Epoch: 29, Train_Loss: 0.1396859586238861, Test_Loss: 0.15715837478637695\n",
      "Epoch: 29, Train_Loss: 0.1412866711616516, Test_Loss: 0.1429676115512848 *\n",
      "Epoch: 29, Train_Loss: 0.13768558204174042, Test_Loss: 0.23294192552566528\n",
      "Epoch: 29, Train_Loss: 0.13823339343070984, Test_Loss: 0.5319086909294128\n",
      "Epoch: 29, Train_Loss: 0.14124439656734467, Test_Loss: 0.2034851610660553 *\n",
      "Epoch: 29, Train_Loss: 0.1374274045228958, Test_Loss: 0.2908705174922943\n",
      "Epoch: 29, Train_Loss: 0.13809330761432648, Test_Loss: 0.14208094775676727 *\n",
      "Epoch: 29, Train_Loss: 0.14624375104904175, Test_Loss: 0.14264735579490662\n",
      "Epoch: 29, Train_Loss: 0.1596701741218567, Test_Loss: 0.1426473706960678\n",
      "Epoch: 29, Train_Loss: 0.152322918176651, Test_Loss: 0.1477634310722351\n",
      "Epoch: 29, Train_Loss: 0.17921042442321777, Test_Loss: 0.18811076879501343\n",
      "Epoch: 29, Train_Loss: 0.16592684388160706, Test_Loss: 6.205544471740723\n",
      "Epoch: 29, Train_Loss: 0.138646200299263, Test_Loss: 0.5027886629104614 *\n",
      "Epoch: 29, Train_Loss: 0.21464209258556366, Test_Loss: 0.1540105640888214 *\n",
      "Epoch: 29, Train_Loss: 0.1709621548652649, Test_Loss: 0.1544698029756546\n",
      "Epoch: 29, Train_Loss: 0.14662015438079834, Test_Loss: 0.15575966238975525\n",
      "Epoch: 29, Train_Loss: 0.24238622188568115, Test_Loss: 0.14538316428661346 *\n",
      "Epoch: 29, Train_Loss: 0.1396133005619049, Test_Loss: 0.174978107213974\n",
      "Epoch: 29, Train_Loss: 0.13625627756118774, Test_Loss: 0.17149239778518677 *\n",
      "Epoch: 29, Train_Loss: 0.13999706506729126, Test_Loss: 0.16142672300338745 *\n",
      "Epoch: 29, Train_Loss: 0.1361943781375885, Test_Loss: 0.1442529559135437 *\n",
      "Epoch: 29, Train_Loss: 0.14381781220436096, Test_Loss: 0.1730579286813736\n",
      "Epoch: 29, Train_Loss: 0.14865164458751678, Test_Loss: 0.21597343683242798\n",
      "Epoch: 29, Train_Loss: 0.13660787045955658, Test_Loss: 0.17247074842453003 *\n",
      "Epoch: 29, Train_Loss: 0.13677051663398743, Test_Loss: 0.15959782898426056 *\n",
      "Epoch: 29, Train_Loss: 0.14690303802490234, Test_Loss: 0.17996162176132202\n",
      "Epoch: 29, Train_Loss: 0.192847341299057, Test_Loss: 0.1396811306476593 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 29\n",
      "Epoch: 29, Train_Loss: 0.24198229610919952, Test_Loss: 0.14248189330101013\n",
      "Epoch: 29, Train_Loss: 0.26339882612228394, Test_Loss: 0.14568153023719788\n",
      "Epoch: 29, Train_Loss: 0.24180996417999268, Test_Loss: 0.17338815331459045\n",
      "Epoch: 29, Train_Loss: 0.19384686648845673, Test_Loss: 0.14349265396595 *\n",
      "Epoch: 29, Train_Loss: 0.21100440621376038, Test_Loss: 0.18341176211833954\n",
      "Epoch: 29, Train_Loss: 0.16568730771541595, Test_Loss: 0.1566983461380005 *\n",
      "Epoch: 29, Train_Loss: 0.20328892767429352, Test_Loss: 0.16620656847953796\n",
      "Epoch: 29, Train_Loss: 0.15667779743671417, Test_Loss: 0.1464868038892746 *\n",
      "Epoch: 29, Train_Loss: 0.40411192178726196, Test_Loss: 0.1950773298740387\n",
      "Epoch: 29, Train_Loss: 0.15609343349933624, Test_Loss: 0.14055901765823364 *\n",
      "Epoch: 29, Train_Loss: 0.21761015057563782, Test_Loss: 0.13999399542808533 *\n",
      "Epoch: 29, Train_Loss: 2.4304585456848145, Test_Loss: 0.16185346245765686\n",
      "Epoch: 29, Train_Loss: 0.270370215177536, Test_Loss: 0.15077123045921326 *\n",
      "Epoch: 29, Train_Loss: 0.17968100309371948, Test_Loss: 0.1376086324453354 *\n",
      "Epoch: 29, Train_Loss: 0.16092267632484436, Test_Loss: 0.3137280344963074\n",
      "Epoch: 29, Train_Loss: 0.15684963762760162, Test_Loss: 1.2201581001281738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train_Loss: 0.14491693675518036, Test_Loss: 5.973352432250977\n",
      "Epoch: 29, Train_Loss: 0.16495999693870544, Test_Loss: 0.32534080743789673 *\n",
      "Epoch: 29, Train_Loss: 0.17353667318820953, Test_Loss: 0.1423742175102234 *\n",
      "Epoch: 29, Train_Loss: 0.20568853616714478, Test_Loss: 0.3394300937652588\n",
      "Epoch: 29, Train_Loss: 0.16414517164230347, Test_Loss: 0.4258913993835449\n",
      "Epoch: 29, Train_Loss: 0.15696018934249878, Test_Loss: 0.265539288520813 *\n",
      "Epoch: 29, Train_Loss: 0.14092928171157837, Test_Loss: 0.19136947393417358 *\n",
      "Epoch: 29, Train_Loss: 0.1413630098104477, Test_Loss: 0.3572980761528015\n",
      "Epoch: 29, Train_Loss: 0.14615343511104584, Test_Loss: 0.1608351320028305 *\n",
      "Epoch: 29, Train_Loss: 0.1459037959575653, Test_Loss: 0.1479141265153885 *\n",
      "Epoch: 29, Train_Loss: 0.1521967500448227, Test_Loss: 0.17090895771980286\n",
      "Epoch: 29, Train_Loss: 0.14732110500335693, Test_Loss: 0.18101239204406738\n",
      "Epoch: 29, Train_Loss: 0.13562601804733276, Test_Loss: 0.15100345015525818 *\n",
      "Epoch: 29, Train_Loss: 0.14862355589866638, Test_Loss: 0.3719237446784973\n",
      "Epoch: 29, Train_Loss: 0.14226751029491425, Test_Loss: 0.36487752199172974 *\n",
      "Epoch: 29, Train_Loss: 0.14472147822380066, Test_Loss: 0.2141648530960083 *\n",
      "Epoch: 29, Train_Loss: 0.13865871727466583, Test_Loss: 0.1502266228199005 *\n",
      "Epoch: 29, Train_Loss: 0.13669724762439728, Test_Loss: 0.20017346739768982\n",
      "Epoch: 29, Train_Loss: 0.13543950021266937, Test_Loss: 0.1654978096485138 *\n",
      "Epoch: 29, Train_Loss: 0.13857245445251465, Test_Loss: 0.3615196943283081\n",
      "Epoch: 29, Train_Loss: 0.14260384440422058, Test_Loss: 0.5467957258224487\n",
      "Epoch: 29, Train_Loss: 0.1361406147480011, Test_Loss: 0.4242740273475647 *\n",
      "Epoch: 29, Train_Loss: 0.1386582851409912, Test_Loss: 0.39948561787605286 *\n",
      "Epoch: 29, Train_Loss: 0.13920459151268005, Test_Loss: 0.4458443522453308\n",
      "Epoch: 29, Train_Loss: 0.1355343759059906, Test_Loss: 0.3854639530181885 *\n",
      "Epoch: 29, Train_Loss: 0.14071470499038696, Test_Loss: 0.40831804275512695\n",
      "Epoch: 29, Train_Loss: 0.1405850648880005, Test_Loss: 0.5173577070236206\n",
      "Epoch: 29, Train_Loss: 0.14071494340896606, Test_Loss: 0.26342666149139404 *\n",
      "Epoch: 29, Train_Loss: 0.14295294880867004, Test_Loss: 0.16118092834949493 *\n",
      "Epoch: 29, Train_Loss: 0.14762713015079498, Test_Loss: 0.13979610800743103 *\n",
      "Epoch: 29, Train_Loss: 0.16419456899166107, Test_Loss: 0.1611872911453247\n",
      "Epoch: 29, Train_Loss: 0.17405736446380615, Test_Loss: 0.17168527841567993\n",
      "Epoch: 29, Train_Loss: 0.15005779266357422, Test_Loss: 0.37596234679222107\n",
      "Epoch: 29, Train_Loss: 0.14302004873752594, Test_Loss: 0.2712773084640503 *\n",
      "Epoch: 29, Train_Loss: 0.15425877273082733, Test_Loss: 0.1933947205543518 *\n",
      "Epoch: 29, Train_Loss: 0.16543826460838318, Test_Loss: 0.18883267045021057 *\n",
      "Epoch: 29, Train_Loss: 0.1435529887676239, Test_Loss: 0.16005408763885498 *\n",
      "Epoch: 29, Train_Loss: 0.15223626792430878, Test_Loss: 0.14285632967948914 *\n",
      "Epoch: 30, Train_Loss: 0.1541462540626526, Test_Loss: 0.1791938692331314 *\n",
      "Epoch: 30, Train_Loss: 0.15470413863658905, Test_Loss: 0.24325509369373322\n",
      "Epoch: 30, Train_Loss: 0.19500204920768738, Test_Loss: 0.32219308614730835\n",
      "Epoch: 30, Train_Loss: 0.15588659048080444, Test_Loss: 0.19816988706588745 *\n",
      "Epoch: 30, Train_Loss: 0.14467480778694153, Test_Loss: 0.255806028842926\n",
      "Epoch: 30, Train_Loss: 0.14638172090053558, Test_Loss: 0.14349031448364258 *\n",
      "Epoch: 30, Train_Loss: 0.17185480892658234, Test_Loss: 0.1387031078338623 *\n",
      "Epoch: 30, Train_Loss: 0.17243865132331848, Test_Loss: 0.14533241093158722\n",
      "Epoch: 30, Train_Loss: 0.14685434103012085, Test_Loss: 0.14965736865997314\n",
      "Epoch: 30, Train_Loss: 0.15045428276062012, Test_Loss: 0.14790481328964233 *\n",
      "Epoch: 30, Train_Loss: 0.19395756721496582, Test_Loss: 0.1520063728094101\n",
      "Epoch: 30, Train_Loss: 0.19545374810695648, Test_Loss: 0.15134765207767487 *\n",
      "Epoch: 30, Train_Loss: 0.15479068458080292, Test_Loss: 0.2266899049282074\n",
      "Epoch: 30, Train_Loss: 0.16120362281799316, Test_Loss: 0.47293806076049805\n",
      "Epoch: 30, Train_Loss: 0.14926406741142273, Test_Loss: 0.28538233041763306 *\n",
      "Epoch: 30, Train_Loss: 0.17030122876167297, Test_Loss: 0.16406317055225372 *\n",
      "Epoch: 30, Train_Loss: 0.13754770159721375, Test_Loss: 0.13499297201633453 *\n",
      "Epoch: 30, Train_Loss: 0.14020171761512756, Test_Loss: 0.13516633212566376\n",
      "Epoch: 30, Train_Loss: 0.15512914955615997, Test_Loss: 0.13485202193260193 *\n",
      "Epoch: 30, Train_Loss: 0.14105169475078583, Test_Loss: 0.1477675437927246\n",
      "Epoch: 30, Train_Loss: 0.157783642411232, Test_Loss: 0.3265293836593628\n",
      "Epoch: 30, Train_Loss: 0.19341586530208588, Test_Loss: 6.021511077880859\n",
      "Epoch: 30, Train_Loss: 0.16128987073898315, Test_Loss: 0.2463875114917755 *\n",
      "Epoch: 30, Train_Loss: 0.16364619135856628, Test_Loss: 0.1473827064037323 *\n",
      "Epoch: 30, Train_Loss: 0.15244260430335999, Test_Loss: 0.14730913937091827 *\n",
      "Epoch: 30, Train_Loss: 0.1418541669845581, Test_Loss: 0.1390247642993927 *\n",
      "Epoch: 30, Train_Loss: 0.31573376059532166, Test_Loss: 0.142128124833107\n",
      "Epoch: 30, Train_Loss: 0.23872962594032288, Test_Loss: 0.15046830475330353\n",
      "Epoch: 30, Train_Loss: 0.1386604607105255, Test_Loss: 0.14906740188598633 *\n",
      "Epoch: 30, Train_Loss: 0.17320045828819275, Test_Loss: 0.1470521092414856 *\n",
      "Epoch: 30, Train_Loss: 0.13660849630832672, Test_Loss: 0.14638400077819824 *\n",
      "Epoch: 30, Train_Loss: 0.13906680047512054, Test_Loss: 0.14426672458648682 *\n",
      "Epoch: 30, Train_Loss: 0.1393078863620758, Test_Loss: 0.1648504137992859\n",
      "Epoch: 30, Train_Loss: 0.1435708999633789, Test_Loss: 0.15540193021297455 *\n",
      "Epoch: 30, Train_Loss: 0.13691087067127228, Test_Loss: 0.16443276405334473\n",
      "Epoch: 30, Train_Loss: 0.14576880633831024, Test_Loss: 0.17574462294578552\n",
      "Epoch: 30, Train_Loss: 0.13940325379371643, Test_Loss: 0.14038395881652832 *\n",
      "Epoch: 30, Train_Loss: 0.1410660743713379, Test_Loss: 0.13926531374454498 *\n",
      "Epoch: 30, Train_Loss: 0.1435532420873642, Test_Loss: 0.14032413065433502\n",
      "Epoch: 30, Train_Loss: 0.1355670839548111, Test_Loss: 0.15402574837207794\n",
      "Epoch: 30, Train_Loss: 0.13644367456436157, Test_Loss: 0.13924753665924072 *\n",
      "Epoch: 30, Train_Loss: 0.13930073380470276, Test_Loss: 0.18070246279239655\n",
      "Epoch: 30, Train_Loss: 0.1413542479276657, Test_Loss: 0.13671937584877014 *\n",
      "Epoch: 30, Train_Loss: 0.13927118480205536, Test_Loss: 0.14434567093849182\n",
      "Epoch: 30, Train_Loss: 0.15350650250911713, Test_Loss: 0.1416042149066925 *\n",
      "Epoch: 30, Train_Loss: 0.1501511037349701, Test_Loss: 0.14061763882637024 *\n",
      "Epoch: 30, Train_Loss: 0.14257584512233734, Test_Loss: 0.13742417097091675 *\n",
      "Epoch: 30, Train_Loss: 0.14670483767986298, Test_Loss: 0.14382606744766235\n",
      "Epoch: 30, Train_Loss: 0.14719785749912262, Test_Loss: 0.14879749715328217\n",
      "Epoch: 30, Train_Loss: 0.14657308161258698, Test_Loss: 0.1406477987766266 *\n",
      "Epoch: 30, Train_Loss: 0.1568422019481659, Test_Loss: 0.14102952182292938\n",
      "Epoch: 30, Train_Loss: 0.14036798477172852, Test_Loss: 0.23684771358966827\n",
      "Epoch: 30, Train_Loss: 0.1417136937379837, Test_Loss: 2.595032215118408\n",
      "Epoch: 30, Train_Loss: 0.15497949719429016, Test_Loss: 4.391768932342529\n",
      "Epoch: 30, Train_Loss: 0.18117183446884155, Test_Loss: 0.21025928854942322 *\n",
      "Epoch: 30, Train_Loss: 2.362185478210449, Test_Loss: 0.15252673625946045 *\n",
      "Epoch: 30, Train_Loss: 2.5765891075134277, Test_Loss: 0.21264462172985077\n",
      "Epoch: 30, Train_Loss: 0.15288850665092468, Test_Loss: 0.24952107667922974\n",
      "Epoch: 30, Train_Loss: 0.13857102394104004, Test_Loss: 0.1911623775959015 *\n",
      "Epoch: 30, Train_Loss: 0.16815431416034698, Test_Loss: 0.20888489484786987\n",
      "Epoch: 30, Train_Loss: 0.1942276954650879, Test_Loss: 0.29474931955337524\n",
      "Epoch: 30, Train_Loss: 0.15109260380268097, Test_Loss: 0.14180099964141846 *\n",
      "Epoch: 30, Train_Loss: 0.13903208076953888, Test_Loss: 0.15528550744056702\n",
      "Epoch: 30, Train_Loss: 0.14422960579395294, Test_Loss: 0.17999717593193054\n",
      "Epoch: 30, Train_Loss: 0.19445067644119263, Test_Loss: 0.16653242707252502 *\n",
      "Epoch: 30, Train_Loss: 0.15626336634159088, Test_Loss: 0.1458386927843094 *\n",
      "Epoch: 30, Train_Loss: 0.1526094377040863, Test_Loss: 0.3405560851097107\n",
      "Epoch: 30, Train_Loss: 0.4563266634941101, Test_Loss: 0.27723532915115356 *\n",
      "Epoch: 30, Train_Loss: 0.30907997488975525, Test_Loss: 0.21141934394836426 *\n",
      "Epoch: 30, Train_Loss: 0.5226699113845825, Test_Loss: 0.15826207399368286 *\n",
      "Epoch: 30, Train_Loss: 0.22580012679100037, Test_Loss: 0.17991358041763306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.6256042122840881, Test_Loss: 0.15373949706554413 *\n",
      "Epoch: 30, Train_Loss: 0.314971923828125, Test_Loss: 0.3096994161605835\n",
      "Epoch: 30, Train_Loss: 0.3895038962364197, Test_Loss: 0.4955165982246399\n",
      "Epoch: 30, Train_Loss: 0.13543249666690826, Test_Loss: 0.4506668746471405 *\n",
      "Epoch: 30, Train_Loss: 0.21329152584075928, Test_Loss: 0.3540506958961487 *\n",
      "Epoch: 30, Train_Loss: 0.38514649868011475, Test_Loss: 0.4207170605659485\n",
      "Epoch: 30, Train_Loss: 0.3315979242324829, Test_Loss: 0.41420602798461914 *\n",
      "Epoch: 30, Train_Loss: 0.18191123008728027, Test_Loss: 0.3201969265937805 *\n",
      "Epoch: 30, Train_Loss: 0.1431782841682434, Test_Loss: 0.3904276490211487\n",
      "Epoch: 30, Train_Loss: 0.14670144021511078, Test_Loss: 0.16510561108589172 *\n",
      "Epoch: 30, Train_Loss: 0.3027045726776123, Test_Loss: 0.1484074741601944 *\n",
      "Epoch: 30, Train_Loss: 0.19193139672279358, Test_Loss: 0.140167698264122 *\n",
      "Epoch: 30, Train_Loss: 0.2034606784582138, Test_Loss: 0.17396393418312073\n",
      "Epoch: 30, Train_Loss: 0.15682068467140198, Test_Loss: 0.17168259620666504 *\n",
      "Epoch: 30, Train_Loss: 0.18959954380989075, Test_Loss: 0.32910534739494324\n",
      "Epoch: 30, Train_Loss: 0.262697696685791, Test_Loss: 0.3012548089027405 *\n",
      "Epoch: 30, Train_Loss: 0.2823609709739685, Test_Loss: 0.18703767657279968 *\n",
      "Epoch: 30, Train_Loss: 0.163938969373703, Test_Loss: 0.16945244371891022 *\n",
      "Epoch: 30, Train_Loss: 0.17625615000724792, Test_Loss: 0.16854220628738403 *\n",
      "Epoch: 30, Train_Loss: 0.2560347616672516, Test_Loss: 0.14728349447250366 *\n",
      "Epoch: 30, Train_Loss: 0.17272008955478668, Test_Loss: 0.21920256316661835\n",
      "Epoch: 30, Train_Loss: 0.17632609605789185, Test_Loss: 0.23544210195541382\n",
      "Epoch: 30, Train_Loss: 0.22210314869880676, Test_Loss: 0.4752407371997833\n",
      "Epoch: 30, Train_Loss: 0.1977730691432953, Test_Loss: 0.20806103944778442 *\n",
      "Epoch: 30, Train_Loss: 0.1625126302242279, Test_Loss: 0.22995373606681824\n",
      "Epoch: 30, Train_Loss: 0.18930840492248535, Test_Loss: 0.14567169547080994 *\n",
      "Epoch: 30, Train_Loss: 0.16111920773983002, Test_Loss: 0.13718990981578827 *\n",
      "Epoch: 30, Train_Loss: 0.1438084989786148, Test_Loss: 0.14685535430908203\n",
      "Epoch: 30, Train_Loss: 0.13519927859306335, Test_Loss: 0.18300765752792358\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.13971588015556335, Test_Loss: 0.14319562911987305 *\n",
      "Epoch: 30, Train_Loss: 0.13726164400577545, Test_Loss: 0.1774315983057022\n",
      "Epoch: 30, Train_Loss: 0.14525936543941498, Test_Loss: 0.1872023046016693\n",
      "Epoch: 30, Train_Loss: 0.14767029881477356, Test_Loss: 0.3508092761039734\n",
      "Epoch: 30, Train_Loss: 0.14013800024986267, Test_Loss: 0.40150418877601624\n",
      "Epoch: 30, Train_Loss: 0.13989804685115814, Test_Loss: 0.39640581607818604 *\n",
      "Epoch: 30, Train_Loss: 0.35672491788864136, Test_Loss: 0.168413907289505 *\n",
      "Epoch: 30, Train_Loss: 0.30108553171157837, Test_Loss: 0.14253905415534973 *\n",
      "Epoch: 30, Train_Loss: 0.14926564693450928, Test_Loss: 0.14109167456626892 *\n",
      "Epoch: 30, Train_Loss: 0.17301307618618011, Test_Loss: 0.1406615525484085 *\n",
      "Epoch: 30, Train_Loss: 0.17498572170734406, Test_Loss: 0.2062583565711975\n",
      "Epoch: 30, Train_Loss: 0.17544111609458923, Test_Loss: 1.0190058946609497\n",
      "Epoch: 30, Train_Loss: 0.3884558081626892, Test_Loss: 4.7990007400512695\n",
      "Epoch: 30, Train_Loss: 0.15544435381889343, Test_Loss: 0.16557788848876953 *\n",
      "Epoch: 30, Train_Loss: 0.3574255704879761, Test_Loss: 0.14363133907318115 *\n",
      "Epoch: 30, Train_Loss: 0.1749066859483719, Test_Loss: 0.14914564788341522\n",
      "Epoch: 30, Train_Loss: 0.1900131106376648, Test_Loss: 0.1366482675075531 *\n",
      "Epoch: 30, Train_Loss: 0.16787052154541016, Test_Loss: 0.1455310732126236\n",
      "Epoch: 30, Train_Loss: 0.14508086442947388, Test_Loss: 0.15687872469425201\n",
      "Epoch: 30, Train_Loss: 0.16774234175682068, Test_Loss: 0.15414074063301086 *\n",
      "Epoch: 30, Train_Loss: 0.551217794418335, Test_Loss: 0.14291596412658691 *\n",
      "Epoch: 30, Train_Loss: 0.5135862827301025, Test_Loss: 0.14423906803131104\n",
      "Epoch: 30, Train_Loss: 0.14183390140533447, Test_Loss: 0.14303752779960632 *\n",
      "Epoch: 30, Train_Loss: 0.16328541934490204, Test_Loss: 0.21023178100585938\n",
      "Epoch: 30, Train_Loss: 0.13921913504600525, Test_Loss: 0.15006904304027557 *\n",
      "Epoch: 30, Train_Loss: 0.21123090386390686, Test_Loss: 0.14421667158603668 *\n",
      "Epoch: 30, Train_Loss: 0.45313960313796997, Test_Loss: 0.16465602815151215\n",
      "Epoch: 30, Train_Loss: 0.13793180882930756, Test_Loss: 0.14683742821216583 *\n",
      "Epoch: 30, Train_Loss: 0.2722158432006836, Test_Loss: 0.14566311240196228 *\n",
      "Epoch: 30, Train_Loss: 0.18388712406158447, Test_Loss: 0.1560095101594925\n",
      "Epoch: 30, Train_Loss: 0.1572144329547882, Test_Loss: 0.15165840089321136 *\n",
      "Epoch: 30, Train_Loss: 0.18457278609275818, Test_Loss: 0.14316870272159576 *\n",
      "Epoch: 30, Train_Loss: 0.24240410327911377, Test_Loss: 0.1712927520275116\n",
      "Epoch: 30, Train_Loss: 0.2530486285686493, Test_Loss: 0.13635818660259247 *\n",
      "Epoch: 30, Train_Loss: 0.1648842990398407, Test_Loss: 0.16204510629177094\n",
      "Epoch: 30, Train_Loss: 0.15169066190719604, Test_Loss: 0.1531333476305008 *\n",
      "Epoch: 30, Train_Loss: 0.2058829814195633, Test_Loss: 0.14243413507938385 *\n",
      "Epoch: 30, Train_Loss: 0.17336750030517578, Test_Loss: 0.1386071741580963 *\n",
      "Epoch: 30, Train_Loss: 0.16483525931835175, Test_Loss: 0.1604296714067459\n",
      "Epoch: 30, Train_Loss: 0.146698996424675, Test_Loss: 0.16129127144813538\n",
      "Epoch: 30, Train_Loss: 0.15638428926467896, Test_Loss: 0.14722666144371033 *\n",
      "Epoch: 30, Train_Loss: 0.17993761599063873, Test_Loss: 0.1599503606557846\n",
      "Epoch: 30, Train_Loss: 0.3037835955619812, Test_Loss: 0.26614660024642944\n",
      "Epoch: 30, Train_Loss: 0.4418572187423706, Test_Loss: 3.0727477073669434\n",
      "Epoch: 30, Train_Loss: 0.6178388595581055, Test_Loss: 2.2078351974487305 *\n",
      "Epoch: 30, Train_Loss: 0.3423178791999817, Test_Loss: 0.16347038745880127 *\n",
      "Epoch: 30, Train_Loss: 0.3677866458892822, Test_Loss: 0.1612500250339508 *\n",
      "Epoch: 30, Train_Loss: 0.18629436194896698, Test_Loss: 0.1665666699409485\n",
      "Epoch: 30, Train_Loss: 0.18086595833301544, Test_Loss: 0.183148592710495\n",
      "Epoch: 30, Train_Loss: 0.14879825711250305, Test_Loss: 0.1552213877439499 *\n",
      "Epoch: 30, Train_Loss: 0.14482824504375458, Test_Loss: 0.1867833137512207\n",
      "Epoch: 30, Train_Loss: 0.21728220582008362, Test_Loss: 0.18432846665382385 *\n",
      "Epoch: 30, Train_Loss: 0.4217853546142578, Test_Loss: 0.13682560622692108 *\n",
      "Epoch: 30, Train_Loss: 0.5042356252670288, Test_Loss: 0.1704699844121933\n",
      "Epoch: 30, Train_Loss: 0.8131922483444214, Test_Loss: 0.16785617172718048 *\n",
      "Epoch: 30, Train_Loss: 1.0097805261611938, Test_Loss: 0.18910640478134155\n",
      "Epoch: 30, Train_Loss: 0.29022884368896484, Test_Loss: 0.14299966394901276 *\n",
      "Epoch: 30, Train_Loss: 0.3701247572898865, Test_Loss: 0.2926059663295746\n",
      "Epoch: 30, Train_Loss: 0.13876378536224365, Test_Loss: 0.24608926475048065 *\n",
      "Epoch: 30, Train_Loss: 0.16086821258068085, Test_Loss: 0.21397355198860168 *\n",
      "Epoch: 30, Train_Loss: 0.2878885269165039, Test_Loss: 0.14774475991725922 *\n",
      "Epoch: 30, Train_Loss: 0.5636898875236511, Test_Loss: 0.19176417589187622\n",
      "Epoch: 30, Train_Loss: 0.19224877655506134, Test_Loss: 0.15769080817699432 *\n",
      "Epoch: 30, Train_Loss: 0.1731102466583252, Test_Loss: 0.31897270679473877\n",
      "Epoch: 30, Train_Loss: 0.1414179652929306, Test_Loss: 0.4415528178215027\n",
      "Epoch: 30, Train_Loss: 0.2646644115447998, Test_Loss: 0.49645286798477173\n",
      "Epoch: 30, Train_Loss: 0.4307602643966675, Test_Loss: 0.39419615268707275 *\n",
      "Epoch: 30, Train_Loss: 0.4133276045322418, Test_Loss: 0.38589122891426086 *\n",
      "Epoch: 30, Train_Loss: 0.2725507616996765, Test_Loss: 0.5243061780929565\n",
      "Epoch: 30, Train_Loss: 0.4009290933609009, Test_Loss: 0.35325419902801514 *\n",
      "Epoch: 30, Train_Loss: 0.13891050219535828, Test_Loss: 0.3466699719429016 *\n",
      "Epoch: 30, Train_Loss: 0.14588876068592072, Test_Loss: 0.18090787529945374 *\n",
      "Epoch: 30, Train_Loss: 0.14195790886878967, Test_Loss: 0.16831260919570923 *\n",
      "Epoch: 30, Train_Loss: 0.17138203978538513, Test_Loss: 0.13697141408920288 *\n",
      "Epoch: 30, Train_Loss: 0.16094255447387695, Test_Loss: 0.1543387621641159\n",
      "Epoch: 30, Train_Loss: 0.2035365104675293, Test_Loss: 0.19394439458847046\n",
      "Epoch: 30, Train_Loss: 14.3458251953125, Test_Loss: 0.20858356356620789\n",
      "Epoch: 30, Train_Loss: 0.4419383406639099, Test_Loss: 0.2595883011817932\n",
      "Epoch: 30, Train_Loss: 1.0269743204116821, Test_Loss: 0.1671077311038971 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.7412928342819214, Test_Loss: 0.17712759971618652\n",
      "Epoch: 30, Train_Loss: 0.1894904375076294, Test_Loss: 0.1661612093448639 *\n",
      "Epoch: 30, Train_Loss: 0.1777249425649643, Test_Loss: 0.1546175628900528 *\n",
      "Epoch: 30, Train_Loss: 1.6159169673919678, Test_Loss: 0.31295478343963623\n",
      "Epoch: 30, Train_Loss: 3.6384246349334717, Test_Loss: 0.17835848033428192 *\n",
      "Epoch: 30, Train_Loss: 0.3725742697715759, Test_Loss: 0.5029182434082031\n",
      "Epoch: 30, Train_Loss: 0.2890213429927826, Test_Loss: 0.20334003865718842 *\n",
      "Epoch: 30, Train_Loss: 3.7791366577148438, Test_Loss: 0.25558868050575256\n",
      "Epoch: 30, Train_Loss: 0.5692445635795593, Test_Loss: 0.22825253009796143 *\n",
      "Epoch: 30, Train_Loss: 0.22020840644836426, Test_Loss: 0.20863915979862213 *\n",
      "Epoch: 30, Train_Loss: 0.15444043278694153, Test_Loss: 0.362051784992218\n",
      "Epoch: 30, Train_Loss: 0.16530516743659973, Test_Loss: 0.16061681509017944 *\n",
      "Epoch: 30, Train_Loss: 0.22160938382148743, Test_Loss: 0.17739662528038025\n",
      "Epoch: 30, Train_Loss: 0.13773860037326813, Test_Loss: 0.15362296998500824 *\n",
      "Epoch: 30, Train_Loss: 0.15393109619617462, Test_Loss: 0.3662340044975281\n",
      "Epoch: 30, Train_Loss: 0.1347033977508545, Test_Loss: 1.7575949430465698\n",
      "Epoch: 30, Train_Loss: 0.13510501384735107, Test_Loss: 1.0329806804656982 *\n",
      "Epoch: 30, Train_Loss: 0.14116612076759338, Test_Loss: 0.29104501008987427 *\n",
      "Epoch: 30, Train_Loss: 0.1464523822069168, Test_Loss: 0.15400801599025726 *\n",
      "Epoch: 30, Train_Loss: 0.15518470108509064, Test_Loss: 0.1746923178434372\n",
      "Epoch: 30, Train_Loss: 0.16289588809013367, Test_Loss: 0.17745056748390198\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.18214958906173706, Test_Loss: 0.17546510696411133 *\n",
      "Epoch: 30, Train_Loss: 0.16731294989585876, Test_Loss: 0.15940438210964203 *\n",
      "Epoch: 30, Train_Loss: 0.14970053732395172, Test_Loss: 3.566131830215454\n",
      "Epoch: 30, Train_Loss: 0.13993781805038452, Test_Loss: 4.387438774108887\n",
      "Epoch: 30, Train_Loss: 0.1569335162639618, Test_Loss: 0.347886323928833 *\n",
      "Epoch: 30, Train_Loss: 0.13919180631637573, Test_Loss: 0.5068844556808472\n",
      "Epoch: 30, Train_Loss: 0.13355164229869843, Test_Loss: 0.6774709820747375\n",
      "Epoch: 30, Train_Loss: 0.13347367942333221, Test_Loss: 0.37584736943244934 *\n",
      "Epoch: 30, Train_Loss: 0.13438944518566132, Test_Loss: 0.5315412282943726\n",
      "Epoch: 30, Train_Loss: 0.13390539586544037, Test_Loss: 0.9646852016448975\n",
      "Epoch: 30, Train_Loss: 0.13362650573253632, Test_Loss: 0.7784184217453003 *\n",
      "Epoch: 30, Train_Loss: 0.13354280591011047, Test_Loss: 0.35417500138282776 *\n",
      "Epoch: 30, Train_Loss: 0.13680601119995117, Test_Loss: 0.6774374842643738\n",
      "Epoch: 30, Train_Loss: 0.15654997527599335, Test_Loss: 0.35380110144615173 *\n",
      "Epoch: 30, Train_Loss: 0.16597610712051392, Test_Loss: 1.436375379562378\n",
      "Epoch: 30, Train_Loss: 0.1832309514284134, Test_Loss: 0.7094415426254272 *\n",
      "Epoch: 30, Train_Loss: 0.15607991814613342, Test_Loss: 0.8346333503723145\n",
      "Epoch: 30, Train_Loss: 0.5044128894805908, Test_Loss: 0.3036719560623169 *\n",
      "Epoch: 30, Train_Loss: 3.621019124984741, Test_Loss: 0.1558149755001068 *\n",
      "Epoch: 30, Train_Loss: 0.2727256119251251, Test_Loss: 0.14093923568725586 *\n",
      "Epoch: 30, Train_Loss: 0.1598811149597168, Test_Loss: 0.20599733293056488\n",
      "Epoch: 30, Train_Loss: 0.1673690676689148, Test_Loss: 0.4309595823287964\n",
      "Epoch: 30, Train_Loss: 0.2474314421415329, Test_Loss: 0.15164929628372192 *\n",
      "Epoch: 30, Train_Loss: 0.1844060719013214, Test_Loss: 0.43456006050109863\n",
      "Epoch: 30, Train_Loss: 0.16553586721420288, Test_Loss: 0.14211207628250122 *\n",
      "Epoch: 30, Train_Loss: 0.16883288323879242, Test_Loss: 0.4611355662345886\n",
      "Epoch: 30, Train_Loss: 0.20844005048274994, Test_Loss: 0.5049270987510681\n",
      "Epoch: 30, Train_Loss: 0.21217215061187744, Test_Loss: 0.1519695520401001 *\n",
      "Epoch: 30, Train_Loss: 0.17146651446819305, Test_Loss: 0.14040236175060272 *\n",
      "Epoch: 30, Train_Loss: 0.13929861783981323, Test_Loss: 0.19607806205749512\n",
      "Epoch: 30, Train_Loss: 0.15345562994480133, Test_Loss: 0.17852213978767395 *\n",
      "Epoch: 30, Train_Loss: 0.13560006022453308, Test_Loss: 0.15281584858894348 *\n",
      "Epoch: 30, Train_Loss: 0.17046813666820526, Test_Loss: 0.25503456592559814\n",
      "Epoch: 30, Train_Loss: 0.13882189989089966, Test_Loss: 0.2633838653564453\n",
      "Epoch: 30, Train_Loss: 0.15397293865680695, Test_Loss: 4.391904830932617\n",
      "Epoch: 30, Train_Loss: 0.1500600278377533, Test_Loss: 1.5904656648635864 *\n",
      "Epoch: 30, Train_Loss: 0.15702195465564728, Test_Loss: 0.13975848257541656 *\n",
      "Epoch: 30, Train_Loss: 0.22358089685440063, Test_Loss: 0.2106335163116455\n",
      "Epoch: 30, Train_Loss: 0.26434481143951416, Test_Loss: 0.35336560010910034\n",
      "Epoch: 30, Train_Loss: 0.15402328968048096, Test_Loss: 0.21095578372478485 *\n",
      "Epoch: 30, Train_Loss: 0.13374556601047516, Test_Loss: 0.1448608785867691 *\n",
      "Epoch: 30, Train_Loss: 0.1345478892326355, Test_Loss: 0.20759519934654236\n",
      "Epoch: 30, Train_Loss: 0.6386116743087769, Test_Loss: 0.20658013224601746 *\n",
      "Epoch: 30, Train_Loss: 3.815591335296631, Test_Loss: 0.13640877604484558 *\n",
      "Epoch: 30, Train_Loss: 0.13767927885055542, Test_Loss: 0.16781143844127655\n",
      "Epoch: 30, Train_Loss: 0.13750480115413666, Test_Loss: 0.15151482820510864 *\n",
      "Epoch: 30, Train_Loss: 0.14008863270282745, Test_Loss: 0.15122054517269135 *\n",
      "Epoch: 30, Train_Loss: 0.13789363205432892, Test_Loss: 0.1873878538608551\n",
      "Epoch: 30, Train_Loss: 0.1365586519241333, Test_Loss: 0.2690066695213318\n",
      "Epoch: 30, Train_Loss: 0.13427212834358215, Test_Loss: 0.17813801765441895 *\n",
      "Epoch: 30, Train_Loss: 0.14214983582496643, Test_Loss: 0.2354094535112381\n",
      "Epoch: 30, Train_Loss: 0.15222403407096863, Test_Loss: 0.1917474865913391 *\n",
      "Epoch: 30, Train_Loss: 0.15784411132335663, Test_Loss: 0.15390978753566742 *\n",
      "Epoch: 30, Train_Loss: 0.13528969883918762, Test_Loss: 0.16656260192394257\n",
      "Epoch: 30, Train_Loss: 0.13431191444396973, Test_Loss: 0.19688716530799866\n",
      "Epoch: 30, Train_Loss: 0.13527357578277588, Test_Loss: 0.28404462337493896\n",
      "Epoch: 30, Train_Loss: 0.1513235867023468, Test_Loss: 0.242697611451149 *\n",
      "Epoch: 30, Train_Loss: 0.13582590222358704, Test_Loss: 0.16639862954616547 *\n",
      "Epoch: 30, Train_Loss: 0.1358843743801117, Test_Loss: 0.19486886262893677\n",
      "Epoch: 30, Train_Loss: 0.1557343304157257, Test_Loss: 0.27550140023231506\n",
      "Epoch: 30, Train_Loss: 0.15977250039577484, Test_Loss: 0.22729460895061493 *\n",
      "Epoch: 30, Train_Loss: 0.1439017355442047, Test_Loss: 0.14418992400169373 *\n",
      "Epoch: 30, Train_Loss: 0.13358721137046814, Test_Loss: 0.14318712055683136 *\n",
      "Epoch: 30, Train_Loss: 0.13966381549835205, Test_Loss: 0.14953941106796265\n",
      "Epoch: 30, Train_Loss: 0.19841614365577698, Test_Loss: 0.15090908110141754\n",
      "Epoch: 30, Train_Loss: 0.16015322506427765, Test_Loss: 0.1568649411201477\n",
      "Epoch: 30, Train_Loss: 0.16975057125091553, Test_Loss: 0.288192480802536\n",
      "Epoch: 30, Train_Loss: 0.17334100604057312, Test_Loss: 0.20774692296981812 *\n",
      "Epoch: 30, Train_Loss: 0.15797290205955505, Test_Loss: 0.2909463942050934\n",
      "Epoch: 30, Train_Loss: 0.16164559125900269, Test_Loss: 0.17538334429264069 *\n",
      "Epoch: 30, Train_Loss: 0.16992664337158203, Test_Loss: 0.21318098902702332\n",
      "Epoch: 30, Train_Loss: 0.14013893902301788, Test_Loss: 0.1516139954328537 *\n",
      "Epoch: 30, Train_Loss: 0.25972551107406616, Test_Loss: 0.20969943702220917\n",
      "Epoch: 30, Train_Loss: 0.15106289088726044, Test_Loss: 0.5546189546585083\n",
      "Epoch: 30, Train_Loss: 0.13572081923484802, Test_Loss: 0.7027829885482788\n",
      "Epoch: 30, Train_Loss: 0.1343587338924408, Test_Loss: 0.4976004362106323 *\n",
      "Epoch: 30, Train_Loss: 0.13378074765205383, Test_Loss: 0.21548253297805786 *\n",
      "Epoch: 30, Train_Loss: 0.13344018161296844, Test_Loss: 0.18195784091949463 *\n",
      "Epoch: 30, Train_Loss: 0.13350510597229004, Test_Loss: 0.1538693755865097 *\n",
      "Epoch: 30, Train_Loss: 0.6824153661727905, Test_Loss: 0.13697549700737 *\n",
      "Epoch: 30, Train_Loss: 3.619366407394409, Test_Loss: 0.15081799030303955\n",
      "Epoch: 30, Train_Loss: 0.14513970911502838, Test_Loss: 0.15630850195884705\n",
      "Epoch: 30, Train_Loss: 0.1380036175251007, Test_Loss: 0.16535638272762299\n",
      "Epoch: 30, Train_Loss: 0.14117835462093353, Test_Loss: 0.1376429796218872 *\n",
      "Epoch: 30, Train_Loss: 0.1341027468442917, Test_Loss: 0.22886478900909424\n",
      "Epoch: 30, Train_Loss: 0.13665100932121277, Test_Loss: 0.5116002559661865\n",
      "Epoch: 30, Train_Loss: 0.13916116952896118, Test_Loss: 0.2312057465314865 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.13546445965766907, Test_Loss: 0.34153133630752563\n",
      "Epoch: 30, Train_Loss: 0.13530884683132172, Test_Loss: 0.14804357290267944 *\n",
      "Epoch: 30, Train_Loss: 0.13654495775699615, Test_Loss: 0.14698323607444763 *\n",
      "Epoch: 30, Train_Loss: 0.14884498715400696, Test_Loss: 0.1465228945016861 *\n",
      "Epoch: 30, Train_Loss: 0.14358624815940857, Test_Loss: 0.1468518227338791\n",
      "Epoch: 30, Train_Loss: 0.16668695211410522, Test_Loss: 0.16138982772827148\n",
      "Epoch: 30, Train_Loss: 0.16483844816684723, Test_Loss: 5.137932777404785\n",
      "Epoch: 30, Train_Loss: 0.13458611071109772, Test_Loss: 2.0754635334014893 *\n",
      "Epoch: 30, Train_Loss: 0.25889062881469727, Test_Loss: 0.16441181302070618 *\n",
      "Epoch: 30, Train_Loss: 0.1625131219625473, Test_Loss: 0.17419660091400146\n",
      "Epoch: 30, Train_Loss: 0.15430963039398193, Test_Loss: 0.22123196721076965\n",
      "Epoch: 30, Train_Loss: 0.26868098974227905, Test_Loss: 0.13781851530075073 *\n",
      "Epoch: 30, Train_Loss: 0.13972565531730652, Test_Loss: 0.16820719838142395\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.13519717752933502, Test_Loss: 0.35897526144981384\n",
      "Epoch: 30, Train_Loss: 0.13909867405891418, Test_Loss: 0.21835176646709442 *\n",
      "Epoch: 30, Train_Loss: 0.1357649862766266, Test_Loss: 0.22371265292167664\n",
      "Epoch: 30, Train_Loss: 0.13652576506137848, Test_Loss: 0.2740992605686188\n",
      "Epoch: 30, Train_Loss: 0.14385490119457245, Test_Loss: 0.2012495994567871 *\n",
      "Epoch: 30, Train_Loss: 0.13848941028118134, Test_Loss: 0.44806286692619324\n",
      "Epoch: 30, Train_Loss: 0.13464003801345825, Test_Loss: 0.16312943398952484 *\n",
      "Epoch: 30, Train_Loss: 0.1463111788034439, Test_Loss: 0.1976276934146881\n",
      "Epoch: 30, Train_Loss: 0.1678866744041443, Test_Loss: 0.1580536663532257 *\n",
      "Epoch: 30, Train_Loss: 0.17723923921585083, Test_Loss: 0.13895569741725922 *\n",
      "Epoch: 30, Train_Loss: 0.14384725689888, Test_Loss: 0.144625723361969\n",
      "Epoch: 30, Train_Loss: 0.23506811261177063, Test_Loss: 0.16042640805244446\n",
      "Epoch: 30, Train_Loss: 0.16703380644321442, Test_Loss: 0.14080220460891724 *\n",
      "Epoch: 30, Train_Loss: 0.20873624086380005, Test_Loss: 0.1567964255809784\n",
      "Epoch: 30, Train_Loss: 0.19585071504116058, Test_Loss: 0.190873384475708\n",
      "Epoch: 30, Train_Loss: 0.19944007694721222, Test_Loss: 0.13868673145771027 *\n",
      "Epoch: 30, Train_Loss: 0.1627427488565445, Test_Loss: 0.14368243515491486\n",
      "Epoch: 30, Train_Loss: 0.27567899227142334, Test_Loss: 0.18208564817905426\n",
      "Epoch: 30, Train_Loss: 0.14891548454761505, Test_Loss: 0.13716715574264526 *\n",
      "Epoch: 30, Train_Loss: 0.16902121901512146, Test_Loss: 0.1354193240404129 *\n",
      "Epoch: 30, Train_Loss: 2.0918045043945312, Test_Loss: 0.15742342174053192\n",
      "Epoch: 30, Train_Loss: 0.41377705335617065, Test_Loss: 0.1481562703847885 *\n",
      "Epoch: 30, Train_Loss: 0.17878884077072144, Test_Loss: 0.13454410433769226 *\n",
      "Epoch: 30, Train_Loss: 0.1716257631778717, Test_Loss: 0.2495078295469284\n",
      "Epoch: 30, Train_Loss: 0.13823316991329193, Test_Loss: 0.22738665342330933 *\n",
      "Epoch: 30, Train_Loss: 0.16288720071315765, Test_Loss: 5.311825752258301\n",
      "Epoch: 30, Train_Loss: 0.15696385502815247, Test_Loss: 0.45460012555122375 *\n",
      "Epoch: 30, Train_Loss: 0.15578477084636688, Test_Loss: 0.14520005881786346 *\n",
      "Epoch: 30, Train_Loss: 0.18115869164466858, Test_Loss: 0.22519941627979279\n",
      "Epoch: 30, Train_Loss: 0.15812885761260986, Test_Loss: 0.2655879259109497\n",
      "Epoch: 30, Train_Loss: 0.15064777433872223, Test_Loss: 0.15901313722133636 *\n",
      "Epoch: 30, Train_Loss: 0.1411563903093338, Test_Loss: 0.14617480337619781 *\n",
      "Epoch: 30, Train_Loss: 0.1438179910182953, Test_Loss: 0.17627248167991638\n",
      "Epoch: 30, Train_Loss: 0.13555648922920227, Test_Loss: 0.17697902023792267\n",
      "Epoch: 30, Train_Loss: 0.14243538677692413, Test_Loss: 0.14073044061660767 *\n",
      "Epoch: 30, Train_Loss: 0.15698078274726868, Test_Loss: 0.15222296118736267\n",
      "Epoch: 30, Train_Loss: 0.14265602827072144, Test_Loss: 0.18706125020980835\n",
      "Epoch: 30, Train_Loss: 0.13351105153560638, Test_Loss: 0.16531822085380554 *\n",
      "Epoch: 30, Train_Loss: 0.14186888933181763, Test_Loss: 0.20055940747261047\n",
      "Epoch: 30, Train_Loss: 0.14074186980724335, Test_Loss: 0.15197686851024628 *\n",
      "Epoch: 30, Train_Loss: 0.13903576135635376, Test_Loss: 0.173709899187088\n",
      "Epoch: 30, Train_Loss: 0.13778585195541382, Test_Loss: 0.1567842662334442 *\n",
      "Epoch: 30, Train_Loss: 0.13423481583595276, Test_Loss: 0.19990050792694092\n",
      "Epoch: 30, Train_Loss: 0.13323576748371124, Test_Loss: 0.24131283164024353\n",
      "Epoch: 30, Train_Loss: 0.13457007706165314, Test_Loss: 0.20972932875156403 *\n",
      "Epoch: 30, Train_Loss: 0.14200115203857422, Test_Loss: 0.286823034286499\n",
      "Epoch: 30, Train_Loss: 0.13366064429283142, Test_Loss: 0.36578595638275146\n",
      "Epoch: 30, Train_Loss: 0.1342550367116928, Test_Loss: 0.3156273663043976 *\n",
      "Epoch: 30, Train_Loss: 0.13560926914215088, Test_Loss: 0.33827289938926697\n",
      "Epoch: 30, Train_Loss: 0.13361945748329163, Test_Loss: 0.3114204704761505 *\n",
      "Epoch: 30, Train_Loss: 0.13637760281562805, Test_Loss: 0.26771530508995056 *\n",
      "Epoch: 30, Train_Loss: 0.13442014157772064, Test_Loss: 0.23775336146354675 *\n",
      "Epoch: 30, Train_Loss: 0.13938583433628082, Test_Loss: 0.17573130130767822 *\n",
      "Epoch: 30, Train_Loss: 0.1387263834476471, Test_Loss: 0.14911668002605438 *\n",
      "Epoch: 30, Train_Loss: 0.13474346697330475, Test_Loss: 0.14384126663208008 *\n",
      "Epoch: 30, Train_Loss: 0.1659076064825058, Test_Loss: 0.15511640906333923\n",
      "Epoch: 30, Train_Loss: 0.16940753161907196, Test_Loss: 0.1944187432527542\n",
      "Epoch: 30, Train_Loss: 0.14352375268936157, Test_Loss: 0.35065698623657227\n",
      "Epoch: 30, Train_Loss: 0.14376243948936462, Test_Loss: 0.221262127161026 *\n",
      "Epoch: 30, Train_Loss: 0.14711599051952362, Test_Loss: 0.24137063324451447\n",
      "Epoch: 30, Train_Loss: 0.1601870059967041, Test_Loss: 0.18173420429229736 *\n",
      "Epoch: 30, Train_Loss: 0.13672393560409546, Test_Loss: 0.1623895764350891 *\n",
      "Epoch: 30, Train_Loss: 0.14258384704589844, Test_Loss: 0.14398910105228424 *\n",
      "Epoch: 30, Train_Loss: 0.13707345724105835, Test_Loss: 0.16036275029182434\n",
      "Epoch: 30, Train_Loss: 0.15016832947731018, Test_Loss: 0.3684301972389221\n",
      "Epoch: 30, Train_Loss: 0.20034855604171753, Test_Loss: 0.17386563122272491 *\n",
      "Epoch: 30, Train_Loss: 0.1518450379371643, Test_Loss: 0.3163321018218994\n",
      "Epoch: 30, Train_Loss: 0.14424628019332886, Test_Loss: 0.24183395504951477 *\n",
      "Epoch: 30, Train_Loss: 0.13466869294643402, Test_Loss: 0.14625418186187744 *\n",
      "Epoch: 30, Train_Loss: 0.1518860161304474, Test_Loss: 0.1411929875612259 *\n",
      "Epoch: 30, Train_Loss: 0.1548498421907425, Test_Loss: 0.13940462470054626 *\n",
      "Epoch: 30, Train_Loss: 0.1499163955450058, Test_Loss: 0.16294696927070618\n",
      "Epoch: 30, Train_Loss: 0.1557009220123291, Test_Loss: 0.14297237992286682 *\n",
      "Epoch: 30, Train_Loss: 0.15509186685085297, Test_Loss: 0.15556685626506805\n",
      "Epoch: 30, Train_Loss: 0.21183326840400696, Test_Loss: 0.14136283099651337 *\n",
      "Epoch: 30, Train_Loss: 0.16019544005393982, Test_Loss: 0.1864684820175171\n",
      "Epoch: 30, Train_Loss: 0.1558886468410492, Test_Loss: 0.48573213815689087\n",
      "Epoch: 30, Train_Loss: 0.1645451933145523, Test_Loss: 0.1652333289384842 *\n",
      "Epoch: 30, Train_Loss: 0.15867376327514648, Test_Loss: 0.28637510538101196\n",
      "Epoch: 30, Train_Loss: 0.14621253311634064, Test_Loss: 0.13322173058986664 *\n",
      "Epoch: 30, Train_Loss: 0.13709108531475067, Test_Loss: 0.13259871304035187 *\n",
      "Epoch: 30, Train_Loss: 0.15996575355529785, Test_Loss: 0.13284555077552795\n",
      "Epoch: 30, Train_Loss: 0.137553408741951, Test_Loss: 0.13463978469371796\n",
      "Epoch: 30, Train_Loss: 0.1417774260044098, Test_Loss: 0.15862123668193817\n",
      "Epoch: 30, Train_Loss: 0.21187421679496765, Test_Loss: 5.725686550140381\n",
      "Epoch: 30, Train_Loss: 0.14115940034389496, Test_Loss: 0.9607204794883728 *\n",
      "Epoch: 30, Train_Loss: 0.17912402749061584, Test_Loss: 0.1459445357322693 *\n",
      "Epoch: 30, Train_Loss: 0.15002688765525818, Test_Loss: 0.14407281577587128 *\n",
      "Epoch: 30, Train_Loss: 0.14622239768505096, Test_Loss: 0.14044952392578125 *\n",
      "Epoch: 30, Train_Loss: 0.2303832471370697, Test_Loss: 0.13698279857635498 *\n",
      "Epoch: 30, Train_Loss: 0.2686282992362976, Test_Loss: 0.15303440392017365\n",
      "Epoch: 30, Train_Loss: 0.13754476606845856, Test_Loss: 0.1655547171831131\n",
      "Epoch: 30, Train_Loss: 0.1649097204208374, Test_Loss: 0.15039819478988647 *\n",
      "Epoch: 30, Train_Loss: 0.1330755203962326, Test_Loss: 0.1485564112663269 *\n",
      "Epoch: 30, Train_Loss: 0.133262038230896, Test_Loss: 0.15981760621070862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train_Loss: 0.13698866963386536, Test_Loss: 0.17572586238384247\n",
      "Epoch: 30, Train_Loss: 0.13651397824287415, Test_Loss: 0.15786144137382507 *\n",
      "Epoch: 30, Train_Loss: 0.13394944369792938, Test_Loss: 0.15872716903686523\n",
      "Epoch: 30, Train_Loss: 0.1428717076778412, Test_Loss: 0.17577403783798218\n",
      "Epoch: 30, Train_Loss: 0.1415025144815445, Test_Loss: 0.13737230002880096 *\n",
      "Model saved at location C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt at epoch 30\n",
      "Epoch: 30, Train_Loss: 0.1392526477575302, Test_Loss: 0.13541564345359802 *\n",
      "Epoch: 30, Train_Loss: 0.14121492207050323, Test_Loss: 0.13355739414691925 *\n",
      "Epoch: 30, Train_Loss: 0.13701173663139343, Test_Loss: 0.140360027551651\n",
      "Epoch: 30, Train_Loss: 0.13435688614845276, Test_Loss: 0.13712455332279205 *\n",
      "Epoch: 30, Train_Loss: 0.13441725075244904, Test_Loss: 0.1453866809606552\n",
      "Epoch: 30, Train_Loss: 0.14407077431678772, Test_Loss: 0.14747254550457\n",
      "Epoch: 30, Train_Loss: 0.13687428832054138, Test_Loss: 0.13511788845062256 *\n",
      "Epoch: 30, Train_Loss: 0.15451668202877045, Test_Loss: 0.13687936961650848\n",
      "Epoch: 30, Train_Loss: 0.14057528972625732, Test_Loss: 0.13972076773643494\n",
      "Epoch: 30, Train_Loss: 0.1420207917690277, Test_Loss: 0.13472014665603638 *\n",
      "Epoch: 30, Train_Loss: 0.14342622458934784, Test_Loss: 0.1387278139591217\n",
      "Epoch: 30, Train_Loss: 0.1565181016921997, Test_Loss: 0.13748815655708313 *\n",
      "Epoch: 30, Train_Loss: 0.14202089607715607, Test_Loss: 0.1342688947916031 *\n",
      "Epoch: 30, Train_Loss: 0.15681827068328857, Test_Loss: 0.13368535041809082 *\n",
      "Epoch: 30, Train_Loss: 0.13422469794750214, Test_Loss: 0.21101966500282288\n",
      "Epoch: 30, Train_Loss: 0.14714458584785461, Test_Loss: 0.4371834099292755\n",
      "Epoch: 30, Train_Loss: 0.1525229513645172, Test_Loss: 5.572830677032471\n",
      "Epoch: 30, Train_Loss: 0.15805195271968842, Test_Loss: 0.17406637966632843 *\n",
      "Epoch: 30, Train_Loss: 1.9950932264328003, Test_Loss: 0.13659155368804932 *\n",
      "Epoch: 30, Train_Loss: 3.283327102661133, Test_Loss: 0.176778182387352\n",
      "Epoch: 30, Train_Loss: 0.14565278589725494, Test_Loss: 0.15043866634368896 *\n",
      "Epoch: 30, Train_Loss: 0.14430378377437592, Test_Loss: 0.14206692576408386 *\n",
      "Epoch: 30, Train_Loss: 0.15448012948036194, Test_Loss: 0.14360548555850983\n",
      "Epoch: 30, Train_Loss: 0.18621593713760376, Test_Loss: 0.20660075545310974\n",
      "Epoch: 30, Train_Loss: 0.1576591432094574, Test_Loss: 0.16661226749420166 *\n",
      "Epoch: 30, Train_Loss: 0.13992173969745636, Test_Loss: 0.13689735531806946 *\n",
      "Epoch: 30, Train_Loss: 0.1324796825647354, Test_Loss: 0.15214058756828308\n",
      "Epoch: 30, Train_Loss: 0.19186365604400635, Test_Loss: 0.15380385518074036\n",
      "Epoch: 30, Train_Loss: 0.15534797310829163, Test_Loss: 0.1407371312379837 *\n",
      "Epoch: 30, Train_Loss: 0.14670021831989288, Test_Loss: 0.177308589220047\n",
      "Epoch: 30, Train_Loss: 0.5106394290924072, Test_Loss: 0.15809056162834167 *\n",
      "Epoch: 30, Train_Loss: 0.28273260593414307, Test_Loss: 0.18574580550193787\n",
      "Epoch: 30, Train_Loss: 0.6524175405502319, Test_Loss: 0.14963380992412567 *\n",
      "Epoch: 30, Train_Loss: 0.18448218703269958, Test_Loss: 0.18382056057453156\n",
      "Epoch: 30, Train_Loss: 0.4448067545890808, Test_Loss: 0.1756380796432495 *\n",
      "Epoch: 30, Train_Loss: 0.468405157327652, Test_Loss: 0.19900420308113098\n",
      "Epoch: 30, Train_Loss: 0.7448318004608154, Test_Loss: 0.3774634599685669\n",
      "Epoch: 30, Train_Loss: 0.13406795263290405, Test_Loss: 0.3546239733695984 *\n",
      "Epoch: 30, Train_Loss: 0.15177461504936218, Test_Loss: 0.37447160482406616\n",
      "Epoch: 30, Train_Loss: 0.5149778723716736, Test_Loss: 0.40294671058654785\n",
      "Epoch: 30, Train_Loss: 0.2705535590648651, Test_Loss: 0.3104819655418396 *\n",
      "Epoch: 30, Train_Loss: 0.4990522265434265, Test_Loss: 0.2752966284751892 *\n",
      "Epoch: 30, Train_Loss: 0.13795198500156403, Test_Loss: 0.23028436303138733 *\n",
      "Epoch: 30, Train_Loss: 0.15309172868728638, Test_Loss: 0.16700774431228638 *\n",
      "Epoch: 30, Train_Loss: 0.36814284324645996, Test_Loss: 0.15361200273036957 *\n",
      "Epoch: 30, Train_Loss: 0.36257654428482056, Test_Loss: 0.13858750462532043 *\n",
      "Epoch: 30, Train_Loss: 0.201332688331604, Test_Loss: 0.1531451940536499\n",
      "Epoch: 30, Train_Loss: 0.17814144492149353, Test_Loss: 0.18406236171722412\n",
      "Epoch: 30, Train_Loss: 0.1691691130399704, Test_Loss: 0.3864355981349945\n",
      "Epoch: 30, Train_Loss: 0.18125571310520172, Test_Loss: 0.25650542974472046 *\n",
      "Epoch: 30, Train_Loss: 0.2645772397518158, Test_Loss: 0.2017822265625 *\n",
      "Epoch: 30, Train_Loss: 0.20699487626552582, Test_Loss: 0.18552404642105103 *\n",
      "Epoch: 30, Train_Loss: 0.16731686890125275, Test_Loss: 0.15946967899799347 *\n",
      "Epoch: 30, Train_Loss: 0.24672585725784302, Test_Loss: 0.14184994995594025 *\n",
      "Wall time: 14h 7min 5s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "SAVEDIR = \"../Saver/\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables() #it will return all the variables. Here, all the weights and biases are variables which\n",
    "#are trainable.\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "#since this is a regression problem so above loss is mean-squared-error loss\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 10**-4).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  #any big number can be given\n",
    "    \n",
    "    for i in range(int(len(x)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.8})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 1.0})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 1.0})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR,r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    \n",
    "#creating dataframe and record all the losses and accuracies at each epoch\n",
    "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "log_frame[\"Epoch\"] = epoch_number\n",
    "log_frame[\"Train Loss\"] = train_loss\n",
    "log_frame[\"Test Loss\"] = test_loss\n",
    "log_frame.to_csv(os.path.join(SAVEDIR, \"log.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "JC3FRgTaINOS",
    "outputId": "981310b4-995d-4ab8-c4e6-283102626e0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.823399</td>\n",
       "      <td>22.434342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14.091616</td>\n",
       "      <td>13.621994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.453352</td>\n",
       "      <td>8.787548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.878366</td>\n",
       "      <td>6.338967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.192993</td>\n",
       "      <td>4.920583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4.342285</td>\n",
       "      <td>4.022159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.889040</td>\n",
       "      <td>3.481969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.475485</td>\n",
       "      <td>3.113996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.932914</td>\n",
       "      <td>2.819652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.692303</td>\n",
       "      <td>2.610340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2.663958</td>\n",
       "      <td>2.487891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2.566728</td>\n",
       "      <td>2.375984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2.223095</td>\n",
       "      <td>2.107902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.134924</td>\n",
       "      <td>2.005807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2.206259</td>\n",
       "      <td>1.990308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.084121</td>\n",
       "      <td>2.071985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.849982</td>\n",
       "      <td>1.905845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.862292</td>\n",
       "      <td>1.817644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.933937</td>\n",
       "      <td>1.837570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.811197</td>\n",
       "      <td>2.048016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.660461</td>\n",
       "      <td>1.652541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.666206</td>\n",
       "      <td>1.613926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.739199</td>\n",
       "      <td>1.587677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.656748</td>\n",
       "      <td>1.671538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.511998</td>\n",
       "      <td>1.507372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.477048</td>\n",
       "      <td>1.566725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.579266</td>\n",
       "      <td>1.601163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.551501</td>\n",
       "      <td>1.656111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.413810</td>\n",
       "      <td>1.665223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.344026</td>\n",
       "      <td>1.605769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Train Loss  Test Loss\n",
       "0       0   22.823399  22.434342\n",
       "1       1   14.091616  13.621994\n",
       "2       2    9.453352   8.787548\n",
       "3       3    6.878366   6.338967\n",
       "4       4    5.192993   4.920583\n",
       "5       5    4.342285   4.022159\n",
       "6       6    3.889040   3.481969\n",
       "7       7    3.475485   3.113996\n",
       "8       8    2.932914   2.819652\n",
       "9       9    2.692303   2.610340\n",
       "10     10    2.663958   2.487891\n",
       "11     11    2.566728   2.375984\n",
       "12     12    2.223095   2.107902\n",
       "13     13    2.134924   2.005807\n",
       "14     14    2.206259   1.990308\n",
       "15     15    2.084121   2.071985\n",
       "16     16    1.849982   1.905845\n",
       "17     17    1.862292   1.817644\n",
       "18     18    1.933937   1.837570\n",
       "19     19    1.811197   2.048016\n",
       "20     20    1.660461   1.652541\n",
       "21     21    1.666206   1.613926\n",
       "22     22    1.739199   1.587677\n",
       "23     23    1.656748   1.671538\n",
       "24     24    1.511998   1.507372\n",
       "25     25    1.477048   1.566725\n",
       "26     26    1.579266   1.601163\n",
       "27     27    1.551501   1.656111\n",
       "28     28    1.413810   1.665223\n",
       "29     29    1.344026   1.605769"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.read_csv(os.path.join(SAVEDIR, \"log.csv\"))\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as both losses are almost similar for each value and gradually decreasing our output is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lezMfUI7dCB"
   },
   "outputs": [],
   "source": [
    "from os.path import join, expanduser\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wr4uJEoFLS1f",
    "outputId": "2fd95e74-3f06-4071-ea48-811a744ce03c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\save copy\\model.ckpt\")\n",
    "\n",
    "img = cv2.imread(r\"C:\\Users\\Administrator\\Downloads\\Autopilot-TensorFlow-master\\steering_wheel_image.jpg\",0)#here, second parameter '0' specifies that img.shape will return only height and\n",
    "#width of the image and not the number of channels. It is a colored image so number of channels = 3, which it will not return.\n",
    "rows, cols = img.shape\n",
    "\n",
    "i = 0\n",
    "\n",
    "while(cv2.waitKey(60) != ord(\"q\")):\n",
    "    full_image = cv2.imread(test_x[i])# cv2 is for computer vision and image processing.\n",
    "    cv2.imshow('Frame Window', full_image)# this shows us road frame window  onwhich vechile is driving\n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 1.0})[0][0] *180 / pi #here, we have converted the\n",
    "    #predicted degrees from radians to degrees.\n",
    "    #simple smoothing the steering angle.\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -degrees, 1) #this function rotate the image by a given degrees.\n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) #warpAffine function applies rotation to the image\n",
    "    cv2.imshow(\"Steering Wheel\", dst)# this shows steering window in which steering rotates this angle comes from model evaluation\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
